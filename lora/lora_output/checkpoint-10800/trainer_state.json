{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 10800,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002777777777777778,
      "grad_norm": 0.5415132641792297,
      "learning_rate": 0.0004995833333333333,
      "loss": 1.7802,
      "step": 10
    },
    {
      "epoch": 0.005555555555555556,
      "grad_norm": 1.1231003999710083,
      "learning_rate": 0.0004991203703703704,
      "loss": 1.4852,
      "step": 20
    },
    {
      "epoch": 0.008333333333333333,
      "grad_norm": 2.1442008018493652,
      "learning_rate": 0.0004986574074074074,
      "loss": 1.3398,
      "step": 30
    },
    {
      "epoch": 0.011111111111111112,
      "grad_norm": 1.3634874820709229,
      "learning_rate": 0.0004981944444444444,
      "loss": 1.3054,
      "step": 40
    },
    {
      "epoch": 0.013888888888888888,
      "grad_norm": 1.5484777688980103,
      "learning_rate": 0.0004977314814814815,
      "loss": 1.1542,
      "step": 50
    },
    {
      "epoch": 0.016666666666666666,
      "grad_norm": 1.2153621912002563,
      "learning_rate": 0.0004972685185185185,
      "loss": 1.0429,
      "step": 60
    },
    {
      "epoch": 0.019444444444444445,
      "grad_norm": 1.1214637756347656,
      "learning_rate": 0.0004968055555555556,
      "loss": 0.9281,
      "step": 70
    },
    {
      "epoch": 0.022222222222222223,
      "grad_norm": 2.2185983657836914,
      "learning_rate": 0.0004963425925925926,
      "loss": 1.1378,
      "step": 80
    },
    {
      "epoch": 0.025,
      "grad_norm": 2.1870877742767334,
      "learning_rate": 0.0004958796296296296,
      "loss": 1.0385,
      "step": 90
    },
    {
      "epoch": 0.027777777777777776,
      "grad_norm": 1.2924035787582397,
      "learning_rate": 0.0004954166666666667,
      "loss": 1.0272,
      "step": 100
    },
    {
      "epoch": 0.030555555555555555,
      "grad_norm": 1.4948315620422363,
      "learning_rate": 0.0004949537037037037,
      "loss": 1.1015,
      "step": 110
    },
    {
      "epoch": 0.03333333333333333,
      "grad_norm": 1.189732313156128,
      "learning_rate": 0.0004944907407407408,
      "loss": 0.9431,
      "step": 120
    },
    {
      "epoch": 0.03611111111111111,
      "grad_norm": 1.9049773216247559,
      "learning_rate": 0.0004940277777777778,
      "loss": 1.0342,
      "step": 130
    },
    {
      "epoch": 0.03888888888888889,
      "grad_norm": 1.1960526704788208,
      "learning_rate": 0.0004935648148148148,
      "loss": 1.0166,
      "step": 140
    },
    {
      "epoch": 0.041666666666666664,
      "grad_norm": 2.265439748764038,
      "learning_rate": 0.0004931018518518519,
      "loss": 1.0123,
      "step": 150
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 1.3109241724014282,
      "learning_rate": 0.0004926388888888889,
      "loss": 1.0761,
      "step": 160
    },
    {
      "epoch": 0.04722222222222222,
      "grad_norm": 1.834290623664856,
      "learning_rate": 0.000492175925925926,
      "loss": 1.1634,
      "step": 170
    },
    {
      "epoch": 0.05,
      "grad_norm": 1.5588880777359009,
      "learning_rate": 0.000491712962962963,
      "loss": 1.0462,
      "step": 180
    },
    {
      "epoch": 0.05277777777777778,
      "grad_norm": 1.7409865856170654,
      "learning_rate": 0.00049125,
      "loss": 1.112,
      "step": 190
    },
    {
      "epoch": 0.05555555555555555,
      "grad_norm": 1.2862030267715454,
      "learning_rate": 0.0004907870370370371,
      "loss": 1.1073,
      "step": 200
    },
    {
      "epoch": 0.058333333333333334,
      "grad_norm": 1.4972022771835327,
      "learning_rate": 0.0004903240740740741,
      "loss": 1.0208,
      "step": 210
    },
    {
      "epoch": 0.06111111111111111,
      "grad_norm": 1.3553742170333862,
      "learning_rate": 0.0004898611111111112,
      "loss": 0.8986,
      "step": 220
    },
    {
      "epoch": 0.06388888888888888,
      "grad_norm": 1.1785223484039307,
      "learning_rate": 0.0004893981481481482,
      "loss": 0.9641,
      "step": 230
    },
    {
      "epoch": 0.06666666666666667,
      "grad_norm": 1.1666438579559326,
      "learning_rate": 0.0004889351851851852,
      "loss": 0.9205,
      "step": 240
    },
    {
      "epoch": 0.06944444444444445,
      "grad_norm": 0.9372712969779968,
      "learning_rate": 0.0004884722222222222,
      "loss": 0.9783,
      "step": 250
    },
    {
      "epoch": 0.07222222222222222,
      "grad_norm": 1.2005733251571655,
      "learning_rate": 0.00048800925925925927,
      "loss": 0.9725,
      "step": 260
    },
    {
      "epoch": 0.075,
      "grad_norm": 1.0504088401794434,
      "learning_rate": 0.0004875462962962963,
      "loss": 0.9957,
      "step": 270
    },
    {
      "epoch": 0.07777777777777778,
      "grad_norm": 1.6283483505249023,
      "learning_rate": 0.00048708333333333335,
      "loss": 1.0524,
      "step": 280
    },
    {
      "epoch": 0.08055555555555556,
      "grad_norm": 1.715014934539795,
      "learning_rate": 0.0004866203703703704,
      "loss": 1.0506,
      "step": 290
    },
    {
      "epoch": 0.08333333333333333,
      "grad_norm": 1.5430935621261597,
      "learning_rate": 0.0004861574074074074,
      "loss": 1.095,
      "step": 300
    },
    {
      "epoch": 0.08611111111111111,
      "grad_norm": 1.0340830087661743,
      "learning_rate": 0.00048569444444444447,
      "loss": 0.9393,
      "step": 310
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 1.474832534790039,
      "learning_rate": 0.0004852314814814815,
      "loss": 0.9688,
      "step": 320
    },
    {
      "epoch": 0.09166666666666666,
      "grad_norm": 1.1158815622329712,
      "learning_rate": 0.00048476851851851855,
      "loss": 0.9245,
      "step": 330
    },
    {
      "epoch": 0.09444444444444444,
      "grad_norm": 1.8366488218307495,
      "learning_rate": 0.00048430555555555553,
      "loss": 0.9319,
      "step": 340
    },
    {
      "epoch": 0.09722222222222222,
      "grad_norm": 1.3385679721832275,
      "learning_rate": 0.0004838425925925926,
      "loss": 1.0538,
      "step": 350
    },
    {
      "epoch": 0.1,
      "grad_norm": 2.4615085124969482,
      "learning_rate": 0.0004833796296296296,
      "loss": 0.9497,
      "step": 360
    },
    {
      "epoch": 0.10277777777777777,
      "grad_norm": 1.4118977785110474,
      "learning_rate": 0.00048291666666666665,
      "loss": 1.0064,
      "step": 370
    },
    {
      "epoch": 0.10555555555555556,
      "grad_norm": 1.0973554849624634,
      "learning_rate": 0.00048245370370370374,
      "loss": 0.9127,
      "step": 380
    },
    {
      "epoch": 0.10833333333333334,
      "grad_norm": 1.0085755586624146,
      "learning_rate": 0.00048199074074074073,
      "loss": 0.8352,
      "step": 390
    },
    {
      "epoch": 0.1111111111111111,
      "grad_norm": 1.4634149074554443,
      "learning_rate": 0.00048152777777777777,
      "loss": 1.0194,
      "step": 400
    },
    {
      "epoch": 0.11388888888888889,
      "grad_norm": 1.2779648303985596,
      "learning_rate": 0.00048106481481481486,
      "loss": 0.9583,
      "step": 410
    },
    {
      "epoch": 0.11666666666666667,
      "grad_norm": 1.008146047592163,
      "learning_rate": 0.00048060185185185185,
      "loss": 0.9002,
      "step": 420
    },
    {
      "epoch": 0.11944444444444445,
      "grad_norm": 1.2991728782653809,
      "learning_rate": 0.0004801388888888889,
      "loss": 0.8931,
      "step": 430
    },
    {
      "epoch": 0.12222222222222222,
      "grad_norm": 1.6664727926254272,
      "learning_rate": 0.000479675925925926,
      "loss": 0.9893,
      "step": 440
    },
    {
      "epoch": 0.125,
      "grad_norm": 1.2003971338272095,
      "learning_rate": 0.00047921296296296297,
      "loss": 1.0408,
      "step": 450
    },
    {
      "epoch": 0.12777777777777777,
      "grad_norm": 1.7956372499465942,
      "learning_rate": 0.00047875,
      "loss": 1.0016,
      "step": 460
    },
    {
      "epoch": 0.13055555555555556,
      "grad_norm": 1.9430593252182007,
      "learning_rate": 0.000478287037037037,
      "loss": 1.0084,
      "step": 470
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 1.1222329139709473,
      "learning_rate": 0.0004778240740740741,
      "loss": 0.9909,
      "step": 480
    },
    {
      "epoch": 0.1361111111111111,
      "grad_norm": 1.496993899345398,
      "learning_rate": 0.00047736111111111113,
      "loss": 0.9446,
      "step": 490
    },
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 1.6385689973831177,
      "learning_rate": 0.0004768981481481481,
      "loss": 0.9419,
      "step": 500
    },
    {
      "epoch": 0.14166666666666666,
      "grad_norm": 1.4087355136871338,
      "learning_rate": 0.0004764351851851852,
      "loss": 1.0034,
      "step": 510
    },
    {
      "epoch": 0.14444444444444443,
      "grad_norm": 2.183936357498169,
      "learning_rate": 0.00047597222222222225,
      "loss": 1.037,
      "step": 520
    },
    {
      "epoch": 0.14722222222222223,
      "grad_norm": 1.149964451789856,
      "learning_rate": 0.00047550925925925923,
      "loss": 0.9865,
      "step": 530
    },
    {
      "epoch": 0.15,
      "grad_norm": 0.99824458360672,
      "learning_rate": 0.00047504629629629633,
      "loss": 0.8515,
      "step": 540
    },
    {
      "epoch": 0.1527777777777778,
      "grad_norm": 1.2980808019638062,
      "learning_rate": 0.00047458333333333337,
      "loss": 0.953,
      "step": 550
    },
    {
      "epoch": 0.15555555555555556,
      "grad_norm": 1.3153945207595825,
      "learning_rate": 0.00047412037037037035,
      "loss": 0.9552,
      "step": 560
    },
    {
      "epoch": 0.15833333333333333,
      "grad_norm": 1.3497637510299683,
      "learning_rate": 0.00047365740740740745,
      "loss": 0.9131,
      "step": 570
    },
    {
      "epoch": 0.16111111111111112,
      "grad_norm": 1.0649908781051636,
      "learning_rate": 0.00047319444444444443,
      "loss": 0.9279,
      "step": 580
    },
    {
      "epoch": 0.1638888888888889,
      "grad_norm": 1.4734729528427124,
      "learning_rate": 0.00047273148148148147,
      "loss": 0.9863,
      "step": 590
    },
    {
      "epoch": 0.16666666666666666,
      "grad_norm": 1.3924129009246826,
      "learning_rate": 0.00047226851851851857,
      "loss": 0.952,
      "step": 600
    },
    {
      "epoch": 0.16944444444444445,
      "grad_norm": 1.4364033937454224,
      "learning_rate": 0.00047180555555555555,
      "loss": 0.96,
      "step": 610
    },
    {
      "epoch": 0.17222222222222222,
      "grad_norm": 0.8516467809677124,
      "learning_rate": 0.0004713425925925926,
      "loss": 0.934,
      "step": 620
    },
    {
      "epoch": 0.175,
      "grad_norm": 1.8020738363265991,
      "learning_rate": 0.00047087962962962963,
      "loss": 0.938,
      "step": 630
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 1.4050402641296387,
      "learning_rate": 0.00047041666666666667,
      "loss": 0.9514,
      "step": 640
    },
    {
      "epoch": 0.18055555555555555,
      "grad_norm": 1.3274825811386108,
      "learning_rate": 0.0004699537037037037,
      "loss": 0.9555,
      "step": 650
    },
    {
      "epoch": 0.18333333333333332,
      "grad_norm": 1.5229893922805786,
      "learning_rate": 0.00046949074074074075,
      "loss": 0.9389,
      "step": 660
    },
    {
      "epoch": 0.18611111111111112,
      "grad_norm": 1.8043239116668701,
      "learning_rate": 0.0004690277777777778,
      "loss": 1.0376,
      "step": 670
    },
    {
      "epoch": 0.18888888888888888,
      "grad_norm": 0.966619074344635,
      "learning_rate": 0.00046856481481481483,
      "loss": 0.906,
      "step": 680
    },
    {
      "epoch": 0.19166666666666668,
      "grad_norm": 1.1976277828216553,
      "learning_rate": 0.0004681018518518518,
      "loss": 0.8581,
      "step": 690
    },
    {
      "epoch": 0.19444444444444445,
      "grad_norm": 1.467948079109192,
      "learning_rate": 0.0004676388888888889,
      "loss": 0.8906,
      "step": 700
    },
    {
      "epoch": 0.19722222222222222,
      "grad_norm": 0.9309801459312439,
      "learning_rate": 0.00046717592592592595,
      "loss": 0.8859,
      "step": 710
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.2122881412506104,
      "learning_rate": 0.00046671296296296294,
      "loss": 0.9942,
      "step": 720
    },
    {
      "epoch": 0.20277777777777778,
      "grad_norm": 1.1989690065383911,
      "learning_rate": 0.00046625000000000003,
      "loss": 0.8739,
      "step": 730
    },
    {
      "epoch": 0.20555555555555555,
      "grad_norm": 1.2552539110183716,
      "learning_rate": 0.00046578703703703707,
      "loss": 0.8919,
      "step": 740
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 1.225922703742981,
      "learning_rate": 0.00046532407407407406,
      "loss": 0.8806,
      "step": 750
    },
    {
      "epoch": 0.2111111111111111,
      "grad_norm": 1.465922236442566,
      "learning_rate": 0.00046486111111111115,
      "loss": 0.9346,
      "step": 760
    },
    {
      "epoch": 0.21388888888888888,
      "grad_norm": 1.068989634513855,
      "learning_rate": 0.0004643981481481482,
      "loss": 0.8951,
      "step": 770
    },
    {
      "epoch": 0.21666666666666667,
      "grad_norm": 1.9723312854766846,
      "learning_rate": 0.0004639351851851852,
      "loss": 0.9199,
      "step": 780
    },
    {
      "epoch": 0.21944444444444444,
      "grad_norm": 1.207298994064331,
      "learning_rate": 0.0004634722222222222,
      "loss": 0.8858,
      "step": 790
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 1.5852313041687012,
      "learning_rate": 0.00046300925925925925,
      "loss": 0.8824,
      "step": 800
    },
    {
      "epoch": 0.225,
      "grad_norm": 1.2684662342071533,
      "learning_rate": 0.0004625462962962963,
      "loss": 0.8275,
      "step": 810
    },
    {
      "epoch": 0.22777777777777777,
      "grad_norm": 1.2964484691619873,
      "learning_rate": 0.00046208333333333333,
      "loss": 0.9309,
      "step": 820
    },
    {
      "epoch": 0.23055555555555557,
      "grad_norm": 1.2718864679336548,
      "learning_rate": 0.0004616203703703704,
      "loss": 0.9483,
      "step": 830
    },
    {
      "epoch": 0.23333333333333334,
      "grad_norm": 1.1748474836349487,
      "learning_rate": 0.0004611574074074074,
      "loss": 0.8703,
      "step": 840
    },
    {
      "epoch": 0.2361111111111111,
      "grad_norm": 1.259770393371582,
      "learning_rate": 0.00046069444444444445,
      "loss": 0.9087,
      "step": 850
    },
    {
      "epoch": 0.2388888888888889,
      "grad_norm": 1.2291154861450195,
      "learning_rate": 0.0004602314814814815,
      "loss": 0.9081,
      "step": 860
    },
    {
      "epoch": 0.24166666666666667,
      "grad_norm": 1.1376546621322632,
      "learning_rate": 0.00045976851851851853,
      "loss": 0.9527,
      "step": 870
    },
    {
      "epoch": 0.24444444444444444,
      "grad_norm": 1.14698326587677,
      "learning_rate": 0.0004593055555555556,
      "loss": 0.9672,
      "step": 880
    },
    {
      "epoch": 0.24722222222222223,
      "grad_norm": 1.2002207040786743,
      "learning_rate": 0.0004588425925925926,
      "loss": 0.8371,
      "step": 890
    },
    {
      "epoch": 0.25,
      "grad_norm": 0.858273983001709,
      "learning_rate": 0.00045837962962962965,
      "loss": 0.8546,
      "step": 900
    },
    {
      "epoch": 0.25277777777777777,
      "grad_norm": 1.3510125875473022,
      "learning_rate": 0.0004579166666666667,
      "loss": 0.9175,
      "step": 910
    },
    {
      "epoch": 0.25555555555555554,
      "grad_norm": 1.5797569751739502,
      "learning_rate": 0.0004574537037037037,
      "loss": 0.9137,
      "step": 920
    },
    {
      "epoch": 0.25833333333333336,
      "grad_norm": 1.4038734436035156,
      "learning_rate": 0.00045699074074074077,
      "loss": 0.8868,
      "step": 930
    },
    {
      "epoch": 0.2611111111111111,
      "grad_norm": 1.2919621467590332,
      "learning_rate": 0.00045652777777777776,
      "loss": 0.9774,
      "step": 940
    },
    {
      "epoch": 0.2638888888888889,
      "grad_norm": 1.1606420278549194,
      "learning_rate": 0.0004560648148148148,
      "loss": 0.8645,
      "step": 950
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 1.1724287271499634,
      "learning_rate": 0.0004556018518518519,
      "loss": 0.8974,
      "step": 960
    },
    {
      "epoch": 0.26944444444444443,
      "grad_norm": 1.6783772706985474,
      "learning_rate": 0.0004551388888888889,
      "loss": 0.9813,
      "step": 970
    },
    {
      "epoch": 0.2722222222222222,
      "grad_norm": 1.3861593008041382,
      "learning_rate": 0.0004546759259259259,
      "loss": 0.846,
      "step": 980
    },
    {
      "epoch": 0.275,
      "grad_norm": 1.315226674079895,
      "learning_rate": 0.000454212962962963,
      "loss": 0.887,
      "step": 990
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 1.918197512626648,
      "learning_rate": 0.00045375,
      "loss": 0.951,
      "step": 1000
    },
    {
      "epoch": 0.28055555555555556,
      "grad_norm": 0.8402800559997559,
      "learning_rate": 0.00045328703703703704,
      "loss": 0.9109,
      "step": 1010
    },
    {
      "epoch": 0.2833333333333333,
      "grad_norm": 2.0053915977478027,
      "learning_rate": 0.00045282407407407413,
      "loss": 0.9913,
      "step": 1020
    },
    {
      "epoch": 0.2861111111111111,
      "grad_norm": 0.9567340016365051,
      "learning_rate": 0.0004523611111111111,
      "loss": 0.9093,
      "step": 1030
    },
    {
      "epoch": 0.28888888888888886,
      "grad_norm": 1.0819662809371948,
      "learning_rate": 0.00045189814814814816,
      "loss": 0.9059,
      "step": 1040
    },
    {
      "epoch": 0.2916666666666667,
      "grad_norm": 1.357256293296814,
      "learning_rate": 0.0004514351851851852,
      "loss": 1.0198,
      "step": 1050
    },
    {
      "epoch": 0.29444444444444445,
      "grad_norm": 1.4716579914093018,
      "learning_rate": 0.00045097222222222224,
      "loss": 0.9543,
      "step": 1060
    },
    {
      "epoch": 0.2972222222222222,
      "grad_norm": 1.3275113105773926,
      "learning_rate": 0.0004505092592592593,
      "loss": 0.9003,
      "step": 1070
    },
    {
      "epoch": 0.3,
      "grad_norm": 1.3003867864608765,
      "learning_rate": 0.00045004629629629626,
      "loss": 0.8283,
      "step": 1080
    },
    {
      "epoch": 0.30277777777777776,
      "grad_norm": 1.5770149230957031,
      "learning_rate": 0.00044958333333333336,
      "loss": 1.0468,
      "step": 1090
    },
    {
      "epoch": 0.3055555555555556,
      "grad_norm": 1.4716590642929077,
      "learning_rate": 0.0004491203703703704,
      "loss": 0.9421,
      "step": 1100
    },
    {
      "epoch": 0.30833333333333335,
      "grad_norm": 1.5536880493164062,
      "learning_rate": 0.0004486574074074074,
      "loss": 0.8578,
      "step": 1110
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 1.172020435333252,
      "learning_rate": 0.0004481944444444445,
      "loss": 0.954,
      "step": 1120
    },
    {
      "epoch": 0.3138888888888889,
      "grad_norm": 1.23866605758667,
      "learning_rate": 0.0004477314814814815,
      "loss": 0.8509,
      "step": 1130
    },
    {
      "epoch": 0.31666666666666665,
      "grad_norm": 1.4587384462356567,
      "learning_rate": 0.0004472685185185185,
      "loss": 0.9825,
      "step": 1140
    },
    {
      "epoch": 0.3194444444444444,
      "grad_norm": 2.466651678085327,
      "learning_rate": 0.0004468055555555556,
      "loss": 0.9507,
      "step": 1150
    },
    {
      "epoch": 0.32222222222222224,
      "grad_norm": 1.1492692232131958,
      "learning_rate": 0.0004463425925925926,
      "loss": 0.9119,
      "step": 1160
    },
    {
      "epoch": 0.325,
      "grad_norm": 1.4099230766296387,
      "learning_rate": 0.0004458796296296296,
      "loss": 0.9648,
      "step": 1170
    },
    {
      "epoch": 0.3277777777777778,
      "grad_norm": 1.4747235774993896,
      "learning_rate": 0.0004454166666666667,
      "loss": 0.9519,
      "step": 1180
    },
    {
      "epoch": 0.33055555555555555,
      "grad_norm": 1.4003239870071411,
      "learning_rate": 0.0004449537037037037,
      "loss": 0.9149,
      "step": 1190
    },
    {
      "epoch": 0.3333333333333333,
      "grad_norm": 1.3872712850570679,
      "learning_rate": 0.00044449074074074074,
      "loss": 0.9649,
      "step": 1200
    },
    {
      "epoch": 0.33611111111111114,
      "grad_norm": 1.180082082748413,
      "learning_rate": 0.00044402777777777783,
      "loss": 1.0352,
      "step": 1210
    },
    {
      "epoch": 0.3388888888888889,
      "grad_norm": 1.2466222047805786,
      "learning_rate": 0.0004435648148148148,
      "loss": 0.9217,
      "step": 1220
    },
    {
      "epoch": 0.3416666666666667,
      "grad_norm": 1.1681337356567383,
      "learning_rate": 0.00044310185185185186,
      "loss": 0.8667,
      "step": 1230
    },
    {
      "epoch": 0.34444444444444444,
      "grad_norm": 1.5946691036224365,
      "learning_rate": 0.0004426388888888889,
      "loss": 0.9529,
      "step": 1240
    },
    {
      "epoch": 0.3472222222222222,
      "grad_norm": 1.1419438123703003,
      "learning_rate": 0.00044217592592592594,
      "loss": 0.9047,
      "step": 1250
    },
    {
      "epoch": 0.35,
      "grad_norm": 1.0857490301132202,
      "learning_rate": 0.000441712962962963,
      "loss": 0.9399,
      "step": 1260
    },
    {
      "epoch": 0.3527777777777778,
      "grad_norm": 1.122656226158142,
      "learning_rate": 0.00044124999999999996,
      "loss": 0.8406,
      "step": 1270
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 1.0431363582611084,
      "learning_rate": 0.00044078703703703706,
      "loss": 0.8457,
      "step": 1280
    },
    {
      "epoch": 0.35833333333333334,
      "grad_norm": 0.9088615775108337,
      "learning_rate": 0.0004403240740740741,
      "loss": 0.7836,
      "step": 1290
    },
    {
      "epoch": 0.3611111111111111,
      "grad_norm": 1.3562899827957153,
      "learning_rate": 0.0004398611111111111,
      "loss": 0.9243,
      "step": 1300
    },
    {
      "epoch": 0.3638888888888889,
      "grad_norm": 0.8112387657165527,
      "learning_rate": 0.0004393981481481482,
      "loss": 0.8864,
      "step": 1310
    },
    {
      "epoch": 0.36666666666666664,
      "grad_norm": 1.19364333152771,
      "learning_rate": 0.0004389351851851852,
      "loss": 0.9143,
      "step": 1320
    },
    {
      "epoch": 0.36944444444444446,
      "grad_norm": 1.3702104091644287,
      "learning_rate": 0.0004384722222222222,
      "loss": 0.7953,
      "step": 1330
    },
    {
      "epoch": 0.37222222222222223,
      "grad_norm": 1.2003364562988281,
      "learning_rate": 0.0004380092592592593,
      "loss": 0.9418,
      "step": 1340
    },
    {
      "epoch": 0.375,
      "grad_norm": 1.037833571434021,
      "learning_rate": 0.00043754629629629634,
      "loss": 0.839,
      "step": 1350
    },
    {
      "epoch": 0.37777777777777777,
      "grad_norm": 1.1870743036270142,
      "learning_rate": 0.0004370833333333333,
      "loss": 0.9658,
      "step": 1360
    },
    {
      "epoch": 0.38055555555555554,
      "grad_norm": 1.052071213722229,
      "learning_rate": 0.00043662037037037036,
      "loss": 0.8554,
      "step": 1370
    },
    {
      "epoch": 0.38333333333333336,
      "grad_norm": 2.0413131713867188,
      "learning_rate": 0.0004361574074074074,
      "loss": 0.9976,
      "step": 1380
    },
    {
      "epoch": 0.3861111111111111,
      "grad_norm": 1.9844051599502563,
      "learning_rate": 0.00043569444444444444,
      "loss": 0.9475,
      "step": 1390
    },
    {
      "epoch": 0.3888888888888889,
      "grad_norm": 1.0980114936828613,
      "learning_rate": 0.0004352314814814815,
      "loss": 0.9266,
      "step": 1400
    },
    {
      "epoch": 0.39166666666666666,
      "grad_norm": 1.075271487236023,
      "learning_rate": 0.0004347685185185185,
      "loss": 0.8814,
      "step": 1410
    },
    {
      "epoch": 0.39444444444444443,
      "grad_norm": 1.4673258066177368,
      "learning_rate": 0.00043430555555555556,
      "loss": 0.9268,
      "step": 1420
    },
    {
      "epoch": 0.3972222222222222,
      "grad_norm": 1.2105917930603027,
      "learning_rate": 0.0004338425925925926,
      "loss": 0.9247,
      "step": 1430
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.4072977304458618,
      "learning_rate": 0.00043337962962962964,
      "loss": 0.9801,
      "step": 1440
    },
    {
      "epoch": 0.4027777777777778,
      "grad_norm": 1.7100025415420532,
      "learning_rate": 0.0004329166666666667,
      "loss": 0.9208,
      "step": 1450
    },
    {
      "epoch": 0.40555555555555556,
      "grad_norm": 1.3575847148895264,
      "learning_rate": 0.0004324537037037037,
      "loss": 0.9232,
      "step": 1460
    },
    {
      "epoch": 0.4083333333333333,
      "grad_norm": 1.2660318613052368,
      "learning_rate": 0.00043199074074074076,
      "loss": 0.8586,
      "step": 1470
    },
    {
      "epoch": 0.4111111111111111,
      "grad_norm": 1.1220009326934814,
      "learning_rate": 0.0004315277777777778,
      "loss": 0.9206,
      "step": 1480
    },
    {
      "epoch": 0.41388888888888886,
      "grad_norm": 2.738865613937378,
      "learning_rate": 0.0004310648148148148,
      "loss": 0.9567,
      "step": 1490
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 1.398033857345581,
      "learning_rate": 0.0004306018518518519,
      "loss": 0.8897,
      "step": 1500
    },
    {
      "epoch": 0.41944444444444445,
      "grad_norm": 1.1717995405197144,
      "learning_rate": 0.0004301388888888889,
      "loss": 0.8929,
      "step": 1510
    },
    {
      "epoch": 0.4222222222222222,
      "grad_norm": 1.981594443321228,
      "learning_rate": 0.0004296759259259259,
      "loss": 0.9124,
      "step": 1520
    },
    {
      "epoch": 0.425,
      "grad_norm": 1.1374571323394775,
      "learning_rate": 0.00042921296296296295,
      "loss": 0.9291,
      "step": 1530
    },
    {
      "epoch": 0.42777777777777776,
      "grad_norm": 1.585708498954773,
      "learning_rate": 0.00042875000000000004,
      "loss": 0.9003,
      "step": 1540
    },
    {
      "epoch": 0.4305555555555556,
      "grad_norm": 1.1342116594314575,
      "learning_rate": 0.000428287037037037,
      "loss": 0.8528,
      "step": 1550
    },
    {
      "epoch": 0.43333333333333335,
      "grad_norm": 1.7251238822937012,
      "learning_rate": 0.00042782407407407407,
      "loss": 0.9853,
      "step": 1560
    },
    {
      "epoch": 0.4361111111111111,
      "grad_norm": 1.765994668006897,
      "learning_rate": 0.00042736111111111116,
      "loss": 0.9369,
      "step": 1570
    },
    {
      "epoch": 0.4388888888888889,
      "grad_norm": 1.6346263885498047,
      "learning_rate": 0.00042689814814814815,
      "loss": 0.9215,
      "step": 1580
    },
    {
      "epoch": 0.44166666666666665,
      "grad_norm": 1.0330134630203247,
      "learning_rate": 0.0004264351851851852,
      "loss": 0.8719,
      "step": 1590
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 1.182052731513977,
      "learning_rate": 0.0004259722222222222,
      "loss": 0.9506,
      "step": 1600
    },
    {
      "epoch": 0.44722222222222224,
      "grad_norm": 1.1014529466629028,
      "learning_rate": 0.00042550925925925927,
      "loss": 0.9009,
      "step": 1610
    },
    {
      "epoch": 0.45,
      "grad_norm": 1.2583634853363037,
      "learning_rate": 0.0004250462962962963,
      "loss": 0.929,
      "step": 1620
    },
    {
      "epoch": 0.4527777777777778,
      "grad_norm": 1.533836007118225,
      "learning_rate": 0.00042458333333333334,
      "loss": 0.9231,
      "step": 1630
    },
    {
      "epoch": 0.45555555555555555,
      "grad_norm": 1.0509477853775024,
      "learning_rate": 0.0004241203703703704,
      "loss": 0.9591,
      "step": 1640
    },
    {
      "epoch": 0.4583333333333333,
      "grad_norm": 1.5148029327392578,
      "learning_rate": 0.0004236574074074074,
      "loss": 0.849,
      "step": 1650
    },
    {
      "epoch": 0.46111111111111114,
      "grad_norm": 1.5260058641433716,
      "learning_rate": 0.00042319444444444446,
      "loss": 1.0184,
      "step": 1660
    },
    {
      "epoch": 0.4638888888888889,
      "grad_norm": 1.6831938028335571,
      "learning_rate": 0.0004227314814814815,
      "loss": 0.8779,
      "step": 1670
    },
    {
      "epoch": 0.4666666666666667,
      "grad_norm": 0.9052222371101379,
      "learning_rate": 0.00042226851851851854,
      "loss": 0.9211,
      "step": 1680
    },
    {
      "epoch": 0.46944444444444444,
      "grad_norm": 1.55514395236969,
      "learning_rate": 0.00042180555555555553,
      "loss": 0.9504,
      "step": 1690
    },
    {
      "epoch": 0.4722222222222222,
      "grad_norm": 1.4412517547607422,
      "learning_rate": 0.0004213425925925926,
      "loss": 0.8822,
      "step": 1700
    },
    {
      "epoch": 0.475,
      "grad_norm": 0.9746518135070801,
      "learning_rate": 0.0004208796296296296,
      "loss": 0.8763,
      "step": 1710
    },
    {
      "epoch": 0.4777777777777778,
      "grad_norm": 1.3236833810806274,
      "learning_rate": 0.00042041666666666665,
      "loss": 0.7941,
      "step": 1720
    },
    {
      "epoch": 0.48055555555555557,
      "grad_norm": 1.1028834581375122,
      "learning_rate": 0.00041995370370370374,
      "loss": 0.9172,
      "step": 1730
    },
    {
      "epoch": 0.48333333333333334,
      "grad_norm": 0.9818318486213684,
      "learning_rate": 0.00041949074074074073,
      "loss": 0.9185,
      "step": 1740
    },
    {
      "epoch": 0.4861111111111111,
      "grad_norm": 1.0776636600494385,
      "learning_rate": 0.00041902777777777777,
      "loss": 0.992,
      "step": 1750
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 1.3305788040161133,
      "learning_rate": 0.00041856481481481486,
      "loss": 0.8959,
      "step": 1760
    },
    {
      "epoch": 0.49166666666666664,
      "grad_norm": 0.9491644501686096,
      "learning_rate": 0.00041810185185185185,
      "loss": 0.8997,
      "step": 1770
    },
    {
      "epoch": 0.49444444444444446,
      "grad_norm": 1.3107575178146362,
      "learning_rate": 0.0004176388888888889,
      "loss": 0.8761,
      "step": 1780
    },
    {
      "epoch": 0.49722222222222223,
      "grad_norm": 1.6950206756591797,
      "learning_rate": 0.000417175925925926,
      "loss": 0.9116,
      "step": 1790
    },
    {
      "epoch": 0.5,
      "grad_norm": 1.0794109106063843,
      "learning_rate": 0.00041671296296296297,
      "loss": 0.8568,
      "step": 1800
    },
    {
      "epoch": 0.5027777777777778,
      "grad_norm": 1.5723203420639038,
      "learning_rate": 0.00041625,
      "loss": 1.014,
      "step": 1810
    },
    {
      "epoch": 0.5055555555555555,
      "grad_norm": 1.1039490699768066,
      "learning_rate": 0.000415787037037037,
      "loss": 0.8943,
      "step": 1820
    },
    {
      "epoch": 0.5083333333333333,
      "grad_norm": 1.1153556108474731,
      "learning_rate": 0.0004153240740740741,
      "loss": 0.9078,
      "step": 1830
    },
    {
      "epoch": 0.5111111111111111,
      "grad_norm": 1.4117764234542847,
      "learning_rate": 0.0004148611111111111,
      "loss": 1.0061,
      "step": 1840
    },
    {
      "epoch": 0.5138888888888888,
      "grad_norm": 0.9179611206054688,
      "learning_rate": 0.0004143981481481481,
      "loss": 0.932,
      "step": 1850
    },
    {
      "epoch": 0.5166666666666667,
      "grad_norm": 1.2009834051132202,
      "learning_rate": 0.0004139351851851852,
      "loss": 0.8693,
      "step": 1860
    },
    {
      "epoch": 0.5194444444444445,
      "grad_norm": 1.5510133504867554,
      "learning_rate": 0.00041347222222222225,
      "loss": 0.8829,
      "step": 1870
    },
    {
      "epoch": 0.5222222222222223,
      "grad_norm": 0.9221265912055969,
      "learning_rate": 0.00041300925925925923,
      "loss": 0.9043,
      "step": 1880
    },
    {
      "epoch": 0.525,
      "grad_norm": 1.5053094625473022,
      "learning_rate": 0.0004125462962962963,
      "loss": 0.9398,
      "step": 1890
    },
    {
      "epoch": 0.5277777777777778,
      "grad_norm": 1.0479934215545654,
      "learning_rate": 0.00041208333333333337,
      "loss": 0.9029,
      "step": 1900
    },
    {
      "epoch": 0.5305555555555556,
      "grad_norm": 1.119367241859436,
      "learning_rate": 0.00041162037037037035,
      "loss": 0.8556,
      "step": 1910
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 1.0118552446365356,
      "learning_rate": 0.00041115740740740745,
      "loss": 0.8225,
      "step": 1920
    },
    {
      "epoch": 0.5361111111111111,
      "grad_norm": 1.1623077392578125,
      "learning_rate": 0.00041069444444444443,
      "loss": 0.9172,
      "step": 1930
    },
    {
      "epoch": 0.5388888888888889,
      "grad_norm": 1.8385717868804932,
      "learning_rate": 0.00041023148148148147,
      "loss": 0.9185,
      "step": 1940
    },
    {
      "epoch": 0.5416666666666666,
      "grad_norm": 1.214744210243225,
      "learning_rate": 0.00040976851851851857,
      "loss": 0.8442,
      "step": 1950
    },
    {
      "epoch": 0.5444444444444444,
      "grad_norm": 1.604249358177185,
      "learning_rate": 0.00040930555555555555,
      "loss": 0.94,
      "step": 1960
    },
    {
      "epoch": 0.5472222222222223,
      "grad_norm": 2.203831911087036,
      "learning_rate": 0.0004088425925925926,
      "loss": 0.9536,
      "step": 1970
    },
    {
      "epoch": 0.55,
      "grad_norm": 2.052213191986084,
      "learning_rate": 0.00040837962962962963,
      "loss": 0.9877,
      "step": 1980
    },
    {
      "epoch": 0.5527777777777778,
      "grad_norm": 1.5322521924972534,
      "learning_rate": 0.00040791666666666667,
      "loss": 0.9174,
      "step": 1990
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 1.84324049949646,
      "learning_rate": 0.0004074537037037037,
      "loss": 1.0167,
      "step": 2000
    },
    {
      "epoch": 0.5583333333333333,
      "grad_norm": 1.4926124811172485,
      "learning_rate": 0.00040699074074074075,
      "loss": 0.9807,
      "step": 2010
    },
    {
      "epoch": 0.5611111111111111,
      "grad_norm": 1.3837333917617798,
      "learning_rate": 0.0004065277777777778,
      "loss": 0.888,
      "step": 2020
    },
    {
      "epoch": 0.5638888888888889,
      "grad_norm": 1.2494014501571655,
      "learning_rate": 0.00040606481481481483,
      "loss": 0.9081,
      "step": 2030
    },
    {
      "epoch": 0.5666666666666667,
      "grad_norm": 1.6085480451583862,
      "learning_rate": 0.0004056018518518518,
      "loss": 0.99,
      "step": 2040
    },
    {
      "epoch": 0.5694444444444444,
      "grad_norm": 2.121648073196411,
      "learning_rate": 0.0004051388888888889,
      "loss": 1.0089,
      "step": 2050
    },
    {
      "epoch": 0.5722222222222222,
      "grad_norm": 1.391332983970642,
      "learning_rate": 0.00040467592592592595,
      "loss": 0.9788,
      "step": 2060
    },
    {
      "epoch": 0.575,
      "grad_norm": 2.2653305530548096,
      "learning_rate": 0.00040421296296296293,
      "loss": 0.939,
      "step": 2070
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 1.6871365308761597,
      "learning_rate": 0.00040375000000000003,
      "loss": 0.9252,
      "step": 2080
    },
    {
      "epoch": 0.5805555555555556,
      "grad_norm": 1.334465503692627,
      "learning_rate": 0.00040328703703703707,
      "loss": 0.9146,
      "step": 2090
    },
    {
      "epoch": 0.5833333333333334,
      "grad_norm": 1.2952301502227783,
      "learning_rate": 0.00040282407407407405,
      "loss": 0.9112,
      "step": 2100
    },
    {
      "epoch": 0.5861111111111111,
      "grad_norm": 1.541038155555725,
      "learning_rate": 0.00040236111111111115,
      "loss": 0.9133,
      "step": 2110
    },
    {
      "epoch": 0.5888888888888889,
      "grad_norm": 1.5393123626708984,
      "learning_rate": 0.0004018981481481482,
      "loss": 0.9707,
      "step": 2120
    },
    {
      "epoch": 0.5916666666666667,
      "grad_norm": 1.2088134288787842,
      "learning_rate": 0.0004014351851851852,
      "loss": 0.9081,
      "step": 2130
    },
    {
      "epoch": 0.5944444444444444,
      "grad_norm": 1.0310606956481934,
      "learning_rate": 0.0004009722222222222,
      "loss": 0.8649,
      "step": 2140
    },
    {
      "epoch": 0.5972222222222222,
      "grad_norm": 1.148405909538269,
      "learning_rate": 0.00040050925925925925,
      "loss": 0.921,
      "step": 2150
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.128427267074585,
      "learning_rate": 0.0004000462962962963,
      "loss": 0.8445,
      "step": 2160
    },
    {
      "epoch": 0.6027777777777777,
      "grad_norm": 1.487304449081421,
      "learning_rate": 0.00039958333333333333,
      "loss": 0.9098,
      "step": 2170
    },
    {
      "epoch": 0.6055555555555555,
      "grad_norm": 1.186935544013977,
      "learning_rate": 0.0003991203703703704,
      "loss": 0.8735,
      "step": 2180
    },
    {
      "epoch": 0.6083333333333333,
      "grad_norm": 1.0467755794525146,
      "learning_rate": 0.0003986574074074074,
      "loss": 1.0664,
      "step": 2190
    },
    {
      "epoch": 0.6111111111111112,
      "grad_norm": 1.4079935550689697,
      "learning_rate": 0.00039819444444444445,
      "loss": 0.865,
      "step": 2200
    },
    {
      "epoch": 0.6138888888888889,
      "grad_norm": 1.092393159866333,
      "learning_rate": 0.0003977314814814815,
      "loss": 0.8135,
      "step": 2210
    },
    {
      "epoch": 0.6166666666666667,
      "grad_norm": 1.5156306028366089,
      "learning_rate": 0.00039726851851851853,
      "loss": 0.9818,
      "step": 2220
    },
    {
      "epoch": 0.6194444444444445,
      "grad_norm": 4.7936272621154785,
      "learning_rate": 0.00039680555555555557,
      "loss": 0.9273,
      "step": 2230
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 1.2152390480041504,
      "learning_rate": 0.0003963425925925926,
      "loss": 0.9172,
      "step": 2240
    },
    {
      "epoch": 0.625,
      "grad_norm": 1.347139835357666,
      "learning_rate": 0.00039587962962962965,
      "loss": 1.0444,
      "step": 2250
    },
    {
      "epoch": 0.6277777777777778,
      "grad_norm": 0.9573273062705994,
      "learning_rate": 0.0003954166666666667,
      "loss": 1.0026,
      "step": 2260
    },
    {
      "epoch": 0.6305555555555555,
      "grad_norm": 1.2037537097930908,
      "learning_rate": 0.0003949537037037037,
      "loss": 1.002,
      "step": 2270
    },
    {
      "epoch": 0.6333333333333333,
      "grad_norm": 1.7512478828430176,
      "learning_rate": 0.00039449074074074077,
      "loss": 0.9904,
      "step": 2280
    },
    {
      "epoch": 0.6361111111111111,
      "grad_norm": 1.827575445175171,
      "learning_rate": 0.00039402777777777776,
      "loss": 1.0852,
      "step": 2290
    },
    {
      "epoch": 0.6388888888888888,
      "grad_norm": 1.87510085105896,
      "learning_rate": 0.0003935648148148148,
      "loss": 1.0834,
      "step": 2300
    },
    {
      "epoch": 0.6416666666666667,
      "grad_norm": 1.5947355031967163,
      "learning_rate": 0.0003931018518518519,
      "loss": 0.8876,
      "step": 2310
    },
    {
      "epoch": 0.6444444444444445,
      "grad_norm": 1.7945830821990967,
      "learning_rate": 0.0003926388888888889,
      "loss": 0.9531,
      "step": 2320
    },
    {
      "epoch": 0.6472222222222223,
      "grad_norm": 1.3963285684585571,
      "learning_rate": 0.0003921759259259259,
      "loss": 0.9092,
      "step": 2330
    },
    {
      "epoch": 0.65,
      "grad_norm": 1.2108958959579468,
      "learning_rate": 0.000391712962962963,
      "loss": 0.9906,
      "step": 2340
    },
    {
      "epoch": 0.6527777777777778,
      "grad_norm": 1.1617411375045776,
      "learning_rate": 0.00039125,
      "loss": 0.8379,
      "step": 2350
    },
    {
      "epoch": 0.6555555555555556,
      "grad_norm": 1.5961079597473145,
      "learning_rate": 0.00039078703703703704,
      "loss": 0.9646,
      "step": 2360
    },
    {
      "epoch": 0.6583333333333333,
      "grad_norm": 1.7923732995986938,
      "learning_rate": 0.00039032407407407413,
      "loss": 0.994,
      "step": 2370
    },
    {
      "epoch": 0.6611111111111111,
      "grad_norm": 1.6662495136260986,
      "learning_rate": 0.0003898611111111111,
      "loss": 0.9437,
      "step": 2380
    },
    {
      "epoch": 0.6638888888888889,
      "grad_norm": 1.2894171476364136,
      "learning_rate": 0.00038939814814814816,
      "loss": 0.9159,
      "step": 2390
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 1.1375550031661987,
      "learning_rate": 0.0003889351851851852,
      "loss": 0.9693,
      "step": 2400
    },
    {
      "epoch": 0.6694444444444444,
      "grad_norm": 2.0191490650177,
      "learning_rate": 0.00038847222222222224,
      "loss": 0.9245,
      "step": 2410
    },
    {
      "epoch": 0.6722222222222223,
      "grad_norm": 1.0962167978286743,
      "learning_rate": 0.0003880092592592593,
      "loss": 0.8536,
      "step": 2420
    },
    {
      "epoch": 0.675,
      "grad_norm": 1.9081008434295654,
      "learning_rate": 0.00038754629629629626,
      "loss": 1.0516,
      "step": 2430
    },
    {
      "epoch": 0.6777777777777778,
      "grad_norm": 1.431825876235962,
      "learning_rate": 0.00038708333333333335,
      "loss": 0.9378,
      "step": 2440
    },
    {
      "epoch": 0.6805555555555556,
      "grad_norm": 1.6917767524719238,
      "learning_rate": 0.0003866203703703704,
      "loss": 0.9408,
      "step": 2450
    },
    {
      "epoch": 0.6833333333333333,
      "grad_norm": 1.3762619495391846,
      "learning_rate": 0.0003861574074074074,
      "loss": 0.9229,
      "step": 2460
    },
    {
      "epoch": 0.6861111111111111,
      "grad_norm": 2.109375476837158,
      "learning_rate": 0.0003856944444444445,
      "loss": 1.0366,
      "step": 2470
    },
    {
      "epoch": 0.6888888888888889,
      "grad_norm": 1.5715314149856567,
      "learning_rate": 0.0003852314814814815,
      "loss": 0.9848,
      "step": 2480
    },
    {
      "epoch": 0.6916666666666667,
      "grad_norm": 1.378267526626587,
      "learning_rate": 0.0003847685185185185,
      "loss": 0.9756,
      "step": 2490
    },
    {
      "epoch": 0.6944444444444444,
      "grad_norm": 2.0039494037628174,
      "learning_rate": 0.0003843055555555556,
      "loss": 0.958,
      "step": 2500
    },
    {
      "epoch": 0.6972222222222222,
      "grad_norm": 1.161647081375122,
      "learning_rate": 0.0003838425925925926,
      "loss": 0.861,
      "step": 2510
    },
    {
      "epoch": 0.7,
      "grad_norm": 1.948643445968628,
      "learning_rate": 0.0003833796296296296,
      "loss": 0.9047,
      "step": 2520
    },
    {
      "epoch": 0.7027777777777777,
      "grad_norm": 1.4551177024841309,
      "learning_rate": 0.0003829166666666667,
      "loss": 0.97,
      "step": 2530
    },
    {
      "epoch": 0.7055555555555556,
      "grad_norm": 1.0535025596618652,
      "learning_rate": 0.0003824537037037037,
      "loss": 0.8931,
      "step": 2540
    },
    {
      "epoch": 0.7083333333333334,
      "grad_norm": 1.4316776990890503,
      "learning_rate": 0.00038199074074074074,
      "loss": 0.9203,
      "step": 2550
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 1.0402216911315918,
      "learning_rate": 0.00038152777777777783,
      "loss": 0.9374,
      "step": 2560
    },
    {
      "epoch": 0.7138888888888889,
      "grad_norm": 1.3863556385040283,
      "learning_rate": 0.0003810648148148148,
      "loss": 1.0137,
      "step": 2570
    },
    {
      "epoch": 0.7166666666666667,
      "grad_norm": 2.582249641418457,
      "learning_rate": 0.00038060185185185186,
      "loss": 1.0426,
      "step": 2580
    },
    {
      "epoch": 0.7194444444444444,
      "grad_norm": 1.215476393699646,
      "learning_rate": 0.0003801388888888889,
      "loss": 0.9492,
      "step": 2590
    },
    {
      "epoch": 0.7222222222222222,
      "grad_norm": 1.4967432022094727,
      "learning_rate": 0.00037967592592592594,
      "loss": 0.9651,
      "step": 2600
    },
    {
      "epoch": 0.725,
      "grad_norm": 1.558264970779419,
      "learning_rate": 0.000379212962962963,
      "loss": 0.9032,
      "step": 2610
    },
    {
      "epoch": 0.7277777777777777,
      "grad_norm": 1.5519726276397705,
      "learning_rate": 0.00037874999999999996,
      "loss": 0.9606,
      "step": 2620
    },
    {
      "epoch": 0.7305555555555555,
      "grad_norm": 1.4090765714645386,
      "learning_rate": 0.00037828703703703706,
      "loss": 0.908,
      "step": 2630
    },
    {
      "epoch": 0.7333333333333333,
      "grad_norm": 1.893864393234253,
      "learning_rate": 0.0003778240740740741,
      "loss": 0.9654,
      "step": 2640
    },
    {
      "epoch": 0.7361111111111112,
      "grad_norm": 1.4483981132507324,
      "learning_rate": 0.0003773611111111111,
      "loss": 0.9337,
      "step": 2650
    },
    {
      "epoch": 0.7388888888888889,
      "grad_norm": 2.2089171409606934,
      "learning_rate": 0.0003768981481481482,
      "loss": 1.1205,
      "step": 2660
    },
    {
      "epoch": 0.7416666666666667,
      "grad_norm": 1.0863878726959229,
      "learning_rate": 0.0003764351851851852,
      "loss": 0.9288,
      "step": 2670
    },
    {
      "epoch": 0.7444444444444445,
      "grad_norm": 0.9988771677017212,
      "learning_rate": 0.0003759722222222222,
      "loss": 0.9179,
      "step": 2680
    },
    {
      "epoch": 0.7472222222222222,
      "grad_norm": 2.0022807121276855,
      "learning_rate": 0.0003755092592592593,
      "loss": 0.9797,
      "step": 2690
    },
    {
      "epoch": 0.75,
      "grad_norm": 1.5264451503753662,
      "learning_rate": 0.00037504629629629634,
      "loss": 0.9499,
      "step": 2700
    },
    {
      "epoch": 0.7527777777777778,
      "grad_norm": 0.7755150198936462,
      "learning_rate": 0.0003745833333333333,
      "loss": 0.9097,
      "step": 2710
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 1.409143328666687,
      "learning_rate": 0.00037412037037037036,
      "loss": 0.974,
      "step": 2720
    },
    {
      "epoch": 0.7583333333333333,
      "grad_norm": 1.227077841758728,
      "learning_rate": 0.0003736574074074074,
      "loss": 1.0138,
      "step": 2730
    },
    {
      "epoch": 0.7611111111111111,
      "grad_norm": 1.159605860710144,
      "learning_rate": 0.00037319444444444444,
      "loss": 0.918,
      "step": 2740
    },
    {
      "epoch": 0.7638888888888888,
      "grad_norm": 1.1113250255584717,
      "learning_rate": 0.0003727314814814815,
      "loss": 0.8999,
      "step": 2750
    },
    {
      "epoch": 0.7666666666666667,
      "grad_norm": 1.3032790422439575,
      "learning_rate": 0.0003722685185185185,
      "loss": 0.9617,
      "step": 2760
    },
    {
      "epoch": 0.7694444444444445,
      "grad_norm": 1.2794986963272095,
      "learning_rate": 0.00037180555555555556,
      "loss": 0.8821,
      "step": 2770
    },
    {
      "epoch": 0.7722222222222223,
      "grad_norm": 1.2137209177017212,
      "learning_rate": 0.0003713425925925926,
      "loss": 0.912,
      "step": 2780
    },
    {
      "epoch": 0.775,
      "grad_norm": 2.415536642074585,
      "learning_rate": 0.00037087962962962964,
      "loss": 0.9887,
      "step": 2790
    },
    {
      "epoch": 0.7777777777777778,
      "grad_norm": 1.4501575231552124,
      "learning_rate": 0.0003704166666666667,
      "loss": 0.8814,
      "step": 2800
    },
    {
      "epoch": 0.7805555555555556,
      "grad_norm": 3.92681622505188,
      "learning_rate": 0.0003699537037037037,
      "loss": 0.9417,
      "step": 2810
    },
    {
      "epoch": 0.7833333333333333,
      "grad_norm": 1.438948154449463,
      "learning_rate": 0.00036949074074074076,
      "loss": 0.8784,
      "step": 2820
    },
    {
      "epoch": 0.7861111111111111,
      "grad_norm": 1.635411024093628,
      "learning_rate": 0.0003690277777777778,
      "loss": 0.908,
      "step": 2830
    },
    {
      "epoch": 0.7888888888888889,
      "grad_norm": 1.166778564453125,
      "learning_rate": 0.0003685648148148148,
      "loss": 0.9578,
      "step": 2840
    },
    {
      "epoch": 0.7916666666666666,
      "grad_norm": 2.0508711338043213,
      "learning_rate": 0.0003681018518518519,
      "loss": 1.1284,
      "step": 2850
    },
    {
      "epoch": 0.7944444444444444,
      "grad_norm": 1.1354678869247437,
      "learning_rate": 0.0003676388888888889,
      "loss": 0.958,
      "step": 2860
    },
    {
      "epoch": 0.7972222222222223,
      "grad_norm": 1.2902679443359375,
      "learning_rate": 0.0003671759259259259,
      "loss": 0.9078,
      "step": 2870
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.713193416595459,
      "learning_rate": 0.00036671296296296295,
      "loss": 0.966,
      "step": 2880
    },
    {
      "epoch": 0.8027777777777778,
      "grad_norm": 1.1317124366760254,
      "learning_rate": 0.00036625000000000004,
      "loss": 0.854,
      "step": 2890
    },
    {
      "epoch": 0.8055555555555556,
      "grad_norm": 1.8374457359313965,
      "learning_rate": 0.000365787037037037,
      "loss": 1.0801,
      "step": 2900
    },
    {
      "epoch": 0.8083333333333333,
      "grad_norm": 1.7839478254318237,
      "learning_rate": 0.00036532407407407406,
      "loss": 1.0703,
      "step": 2910
    },
    {
      "epoch": 0.8111111111111111,
      "grad_norm": 1.685563564300537,
      "learning_rate": 0.00036486111111111116,
      "loss": 1.0248,
      "step": 2920
    },
    {
      "epoch": 0.8138888888888889,
      "grad_norm": 1.5103260278701782,
      "learning_rate": 0.00036439814814814814,
      "loss": 1.0768,
      "step": 2930
    },
    {
      "epoch": 0.8166666666666667,
      "grad_norm": 2.039963483810425,
      "learning_rate": 0.0003639351851851852,
      "loss": 0.9991,
      "step": 2940
    },
    {
      "epoch": 0.8194444444444444,
      "grad_norm": 1.4806311130523682,
      "learning_rate": 0.0003634722222222222,
      "loss": 0.9192,
      "step": 2950
    },
    {
      "epoch": 0.8222222222222222,
      "grad_norm": 1.6211509704589844,
      "learning_rate": 0.00036300925925925926,
      "loss": 0.9844,
      "step": 2960
    },
    {
      "epoch": 0.825,
      "grad_norm": 1.2805957794189453,
      "learning_rate": 0.0003625462962962963,
      "loss": 0.9417,
      "step": 2970
    },
    {
      "epoch": 0.8277777777777777,
      "grad_norm": 1.5661587715148926,
      "learning_rate": 0.00036208333333333334,
      "loss": 0.9883,
      "step": 2980
    },
    {
      "epoch": 0.8305555555555556,
      "grad_norm": 1.4443833827972412,
      "learning_rate": 0.0003616203703703704,
      "loss": 0.9894,
      "step": 2990
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 1.1980096101760864,
      "learning_rate": 0.0003611574074074074,
      "loss": 1.0386,
      "step": 3000
    },
    {
      "epoch": 0.8361111111111111,
      "grad_norm": 1.608750581741333,
      "learning_rate": 0.00036069444444444446,
      "loss": 0.9379,
      "step": 3010
    },
    {
      "epoch": 0.8388888888888889,
      "grad_norm": 2.5759146213531494,
      "learning_rate": 0.0003602314814814815,
      "loss": 1.0007,
      "step": 3020
    },
    {
      "epoch": 0.8416666666666667,
      "grad_norm": 1.1228512525558472,
      "learning_rate": 0.00035976851851851854,
      "loss": 0.9852,
      "step": 3030
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 1.3492729663848877,
      "learning_rate": 0.00035930555555555553,
      "loss": 0.9888,
      "step": 3040
    },
    {
      "epoch": 0.8472222222222222,
      "grad_norm": 0.7981839179992676,
      "learning_rate": 0.0003588425925925926,
      "loss": 0.8907,
      "step": 3050
    },
    {
      "epoch": 0.85,
      "grad_norm": 0.8759351372718811,
      "learning_rate": 0.0003583796296296296,
      "loss": 0.9024,
      "step": 3060
    },
    {
      "epoch": 0.8527777777777777,
      "grad_norm": 1.2792789936065674,
      "learning_rate": 0.00035791666666666665,
      "loss": 0.9531,
      "step": 3070
    },
    {
      "epoch": 0.8555555555555555,
      "grad_norm": 1.5829200744628906,
      "learning_rate": 0.00035745370370370374,
      "loss": 0.9569,
      "step": 3080
    },
    {
      "epoch": 0.8583333333333333,
      "grad_norm": 1.600546956062317,
      "learning_rate": 0.00035699074074074073,
      "loss": 0.8971,
      "step": 3090
    },
    {
      "epoch": 0.8611111111111112,
      "grad_norm": 1.8910315036773682,
      "learning_rate": 0.00035652777777777777,
      "loss": 0.8821,
      "step": 3100
    },
    {
      "epoch": 0.8638888888888889,
      "grad_norm": 1.3309696912765503,
      "learning_rate": 0.00035606481481481486,
      "loss": 0.9656,
      "step": 3110
    },
    {
      "epoch": 0.8666666666666667,
      "grad_norm": 2.462552309036255,
      "learning_rate": 0.00035560185185185185,
      "loss": 0.9563,
      "step": 3120
    },
    {
      "epoch": 0.8694444444444445,
      "grad_norm": 1.7356185913085938,
      "learning_rate": 0.0003551388888888889,
      "loss": 0.9766,
      "step": 3130
    },
    {
      "epoch": 0.8722222222222222,
      "grad_norm": 1.3161052465438843,
      "learning_rate": 0.000354675925925926,
      "loss": 0.9797,
      "step": 3140
    },
    {
      "epoch": 0.875,
      "grad_norm": 1.5171304941177368,
      "learning_rate": 0.00035421296296296297,
      "loss": 1.0086,
      "step": 3150
    },
    {
      "epoch": 0.8777777777777778,
      "grad_norm": 1.65755033493042,
      "learning_rate": 0.00035375,
      "loss": 0.9261,
      "step": 3160
    },
    {
      "epoch": 0.8805555555555555,
      "grad_norm": 1.1877745389938354,
      "learning_rate": 0.000353287037037037,
      "loss": 0.961,
      "step": 3170
    },
    {
      "epoch": 0.8833333333333333,
      "grad_norm": 1.7818045616149902,
      "learning_rate": 0.0003528240740740741,
      "loss": 0.9273,
      "step": 3180
    },
    {
      "epoch": 0.8861111111111111,
      "grad_norm": 2.013075828552246,
      "learning_rate": 0.0003523611111111111,
      "loss": 0.887,
      "step": 3190
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 1.4532673358917236,
      "learning_rate": 0.0003518981481481481,
      "loss": 0.9522,
      "step": 3200
    },
    {
      "epoch": 0.8916666666666667,
      "grad_norm": 1.7587302923202515,
      "learning_rate": 0.0003514351851851852,
      "loss": 0.9805,
      "step": 3210
    },
    {
      "epoch": 0.8944444444444445,
      "grad_norm": 1.672318696975708,
      "learning_rate": 0.00035097222222222225,
      "loss": 0.9321,
      "step": 3220
    },
    {
      "epoch": 0.8972222222222223,
      "grad_norm": 0.8746587038040161,
      "learning_rate": 0.00035050925925925923,
      "loss": 0.9893,
      "step": 3230
    },
    {
      "epoch": 0.9,
      "grad_norm": 2.5944600105285645,
      "learning_rate": 0.0003500462962962963,
      "loss": 1.0143,
      "step": 3240
    },
    {
      "epoch": 0.9027777777777778,
      "grad_norm": 1.30802321434021,
      "learning_rate": 0.00034958333333333336,
      "loss": 1.0016,
      "step": 3250
    },
    {
      "epoch": 0.9055555555555556,
      "grad_norm": 1.1526882648468018,
      "learning_rate": 0.00034912037037037035,
      "loss": 0.9202,
      "step": 3260
    },
    {
      "epoch": 0.9083333333333333,
      "grad_norm": 1.0446209907531738,
      "learning_rate": 0.00034865740740740744,
      "loss": 0.9759,
      "step": 3270
    },
    {
      "epoch": 0.9111111111111111,
      "grad_norm": 1.1541095972061157,
      "learning_rate": 0.00034819444444444443,
      "loss": 0.9094,
      "step": 3280
    },
    {
      "epoch": 0.9138888888888889,
      "grad_norm": 1.241509199142456,
      "learning_rate": 0.00034773148148148147,
      "loss": 0.9478,
      "step": 3290
    },
    {
      "epoch": 0.9166666666666666,
      "grad_norm": 1.053043246269226,
      "learning_rate": 0.00034726851851851856,
      "loss": 1.0235,
      "step": 3300
    },
    {
      "epoch": 0.9194444444444444,
      "grad_norm": 1.8131718635559082,
      "learning_rate": 0.00034680555555555555,
      "loss": 1.1284,
      "step": 3310
    },
    {
      "epoch": 0.9222222222222223,
      "grad_norm": 1.8309850692749023,
      "learning_rate": 0.0003463425925925926,
      "loss": 1.0374,
      "step": 3320
    },
    {
      "epoch": 0.925,
      "grad_norm": 1.0991146564483643,
      "learning_rate": 0.00034587962962962963,
      "loss": 0.9788,
      "step": 3330
    },
    {
      "epoch": 0.9277777777777778,
      "grad_norm": 1.0787739753723145,
      "learning_rate": 0.00034541666666666667,
      "loss": 0.9755,
      "step": 3340
    },
    {
      "epoch": 0.9305555555555556,
      "grad_norm": 1.640211582183838,
      "learning_rate": 0.0003449537037037037,
      "loss": 1.0444,
      "step": 3350
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 1.314778447151184,
      "learning_rate": 0.00034449074074074075,
      "loss": 0.9035,
      "step": 3360
    },
    {
      "epoch": 0.9361111111111111,
      "grad_norm": 1.1159491539001465,
      "learning_rate": 0.0003440277777777778,
      "loss": 0.9125,
      "step": 3370
    },
    {
      "epoch": 0.9388888888888889,
      "grad_norm": 1.4778224229812622,
      "learning_rate": 0.00034356481481481483,
      "loss": 1.0187,
      "step": 3380
    },
    {
      "epoch": 0.9416666666666667,
      "grad_norm": 0.8435239195823669,
      "learning_rate": 0.0003431018518518518,
      "loss": 0.9792,
      "step": 3390
    },
    {
      "epoch": 0.9444444444444444,
      "grad_norm": 1.639775276184082,
      "learning_rate": 0.0003426388888888889,
      "loss": 0.9351,
      "step": 3400
    },
    {
      "epoch": 0.9472222222222222,
      "grad_norm": 1.7732213735580444,
      "learning_rate": 0.00034217592592592595,
      "loss": 0.9932,
      "step": 3410
    },
    {
      "epoch": 0.95,
      "grad_norm": 0.8838902711868286,
      "learning_rate": 0.00034171296296296293,
      "loss": 0.9641,
      "step": 3420
    },
    {
      "epoch": 0.9527777777777777,
      "grad_norm": 1.0509073734283447,
      "learning_rate": 0.00034125000000000003,
      "loss": 0.9274,
      "step": 3430
    },
    {
      "epoch": 0.9555555555555556,
      "grad_norm": 1.3919340372085571,
      "learning_rate": 0.00034078703703703707,
      "loss": 0.9543,
      "step": 3440
    },
    {
      "epoch": 0.9583333333333334,
      "grad_norm": 1.4692615270614624,
      "learning_rate": 0.00034032407407407405,
      "loss": 1.0148,
      "step": 3450
    },
    {
      "epoch": 0.9611111111111111,
      "grad_norm": 1.4093644618988037,
      "learning_rate": 0.00033986111111111115,
      "loss": 1.0227,
      "step": 3460
    },
    {
      "epoch": 0.9638888888888889,
      "grad_norm": 2.03787899017334,
      "learning_rate": 0.0003393981481481482,
      "loss": 0.9702,
      "step": 3470
    },
    {
      "epoch": 0.9666666666666667,
      "grad_norm": 1.43126380443573,
      "learning_rate": 0.00033893518518518517,
      "loss": 0.9863,
      "step": 3480
    },
    {
      "epoch": 0.9694444444444444,
      "grad_norm": 0.9792706966400146,
      "learning_rate": 0.0003384722222222222,
      "loss": 0.8335,
      "step": 3490
    },
    {
      "epoch": 0.9722222222222222,
      "grad_norm": 1.3284711837768555,
      "learning_rate": 0.00033800925925925925,
      "loss": 0.9265,
      "step": 3500
    },
    {
      "epoch": 0.975,
      "grad_norm": 1.610673189163208,
      "learning_rate": 0.0003375462962962963,
      "loss": 0.9327,
      "step": 3510
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 1.2268388271331787,
      "learning_rate": 0.00033708333333333333,
      "loss": 0.8915,
      "step": 3520
    },
    {
      "epoch": 0.9805555555555555,
      "grad_norm": 1.2672263383865356,
      "learning_rate": 0.00033662037037037037,
      "loss": 0.9781,
      "step": 3530
    },
    {
      "epoch": 0.9833333333333333,
      "grad_norm": 1.8639417886734009,
      "learning_rate": 0.0003361574074074074,
      "loss": 1.0435,
      "step": 3540
    },
    {
      "epoch": 0.9861111111111112,
      "grad_norm": 0.9299412369728088,
      "learning_rate": 0.00033569444444444445,
      "loss": 0.9588,
      "step": 3550
    },
    {
      "epoch": 0.9888888888888889,
      "grad_norm": 1.3046435117721558,
      "learning_rate": 0.0003352314814814815,
      "loss": 0.9316,
      "step": 3560
    },
    {
      "epoch": 0.9916666666666667,
      "grad_norm": 1.037905216217041,
      "learning_rate": 0.00033476851851851853,
      "loss": 0.9622,
      "step": 3570
    },
    {
      "epoch": 0.9944444444444445,
      "grad_norm": 1.6637823581695557,
      "learning_rate": 0.00033430555555555557,
      "loss": 0.9361,
      "step": 3580
    },
    {
      "epoch": 0.9972222222222222,
      "grad_norm": 1.205613613128662,
      "learning_rate": 0.0003338425925925926,
      "loss": 0.9311,
      "step": 3590
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.3629469871520996,
      "learning_rate": 0.00033337962962962965,
      "loss": 1.0333,
      "step": 3600
    },
    {
      "epoch": 1.0027777777777778,
      "grad_norm": 1.203555941581726,
      "learning_rate": 0.0003329166666666667,
      "loss": 0.9195,
      "step": 3610
    },
    {
      "epoch": 1.0055555555555555,
      "grad_norm": 1.600598931312561,
      "learning_rate": 0.0003324537037037037,
      "loss": 0.9168,
      "step": 3620
    },
    {
      "epoch": 1.0083333333333333,
      "grad_norm": 2.366262912750244,
      "learning_rate": 0.00033199074074074077,
      "loss": 0.8499,
      "step": 3630
    },
    {
      "epoch": 1.011111111111111,
      "grad_norm": 1.610153079032898,
      "learning_rate": 0.00033152777777777776,
      "loss": 0.8725,
      "step": 3640
    },
    {
      "epoch": 1.0138888888888888,
      "grad_norm": 1.4878076314926147,
      "learning_rate": 0.0003310648148148148,
      "loss": 1.0009,
      "step": 3650
    },
    {
      "epoch": 1.0166666666666666,
      "grad_norm": 1.1845436096191406,
      "learning_rate": 0.0003306018518518519,
      "loss": 1.0066,
      "step": 3660
    },
    {
      "epoch": 1.0194444444444444,
      "grad_norm": 1.8733636140823364,
      "learning_rate": 0.0003301388888888889,
      "loss": 0.8712,
      "step": 3670
    },
    {
      "epoch": 1.0222222222222221,
      "grad_norm": 2.378018856048584,
      "learning_rate": 0.0003296759259259259,
      "loss": 1.0143,
      "step": 3680
    },
    {
      "epoch": 1.025,
      "grad_norm": 1.7198752164840698,
      "learning_rate": 0.000329212962962963,
      "loss": 1.0239,
      "step": 3690
    },
    {
      "epoch": 1.0277777777777777,
      "grad_norm": 1.562486171722412,
      "learning_rate": 0.00032875,
      "loss": 0.8879,
      "step": 3700
    },
    {
      "epoch": 1.0305555555555554,
      "grad_norm": 1.61729896068573,
      "learning_rate": 0.00032828703703703703,
      "loss": 0.9924,
      "step": 3710
    },
    {
      "epoch": 1.0333333333333334,
      "grad_norm": 1.360790491104126,
      "learning_rate": 0.00032782407407407413,
      "loss": 0.8676,
      "step": 3720
    },
    {
      "epoch": 1.0361111111111112,
      "grad_norm": 1.2908345460891724,
      "learning_rate": 0.0003273611111111111,
      "loss": 0.9837,
      "step": 3730
    },
    {
      "epoch": 1.038888888888889,
      "grad_norm": 1.4541208744049072,
      "learning_rate": 0.00032689814814814815,
      "loss": 0.8743,
      "step": 3740
    },
    {
      "epoch": 1.0416666666666667,
      "grad_norm": 1.3550617694854736,
      "learning_rate": 0.0003264351851851852,
      "loss": 0.8946,
      "step": 3750
    },
    {
      "epoch": 1.0444444444444445,
      "grad_norm": 1.544461727142334,
      "learning_rate": 0.00032597222222222223,
      "loss": 0.8605,
      "step": 3760
    },
    {
      "epoch": 1.0472222222222223,
      "grad_norm": 2.3040268421173096,
      "learning_rate": 0.0003255092592592593,
      "loss": 0.9161,
      "step": 3770
    },
    {
      "epoch": 1.05,
      "grad_norm": 1.587257742881775,
      "learning_rate": 0.00032504629629629626,
      "loss": 0.9596,
      "step": 3780
    },
    {
      "epoch": 1.0527777777777778,
      "grad_norm": 1.5914071798324585,
      "learning_rate": 0.00032458333333333335,
      "loss": 0.8993,
      "step": 3790
    },
    {
      "epoch": 1.0555555555555556,
      "grad_norm": 1.3046116828918457,
      "learning_rate": 0.0003241203703703704,
      "loss": 0.923,
      "step": 3800
    },
    {
      "epoch": 1.0583333333333333,
      "grad_norm": 1.452170968055725,
      "learning_rate": 0.0003236574074074074,
      "loss": 0.9701,
      "step": 3810
    },
    {
      "epoch": 1.0611111111111111,
      "grad_norm": 1.3989070653915405,
      "learning_rate": 0.0003231944444444445,
      "loss": 0.9617,
      "step": 3820
    },
    {
      "epoch": 1.0638888888888889,
      "grad_norm": 1.3793416023254395,
      "learning_rate": 0.0003227314814814815,
      "loss": 0.9236,
      "step": 3830
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 1.1202119588851929,
      "learning_rate": 0.0003222685185185185,
      "loss": 1.0023,
      "step": 3840
    },
    {
      "epoch": 1.0694444444444444,
      "grad_norm": 1.6849702596664429,
      "learning_rate": 0.0003218055555555556,
      "loss": 1.0176,
      "step": 3850
    },
    {
      "epoch": 1.0722222222222222,
      "grad_norm": 1.0204155445098877,
      "learning_rate": 0.0003213425925925926,
      "loss": 0.9515,
      "step": 3860
    },
    {
      "epoch": 1.075,
      "grad_norm": 1.576777458190918,
      "learning_rate": 0.0003208796296296296,
      "loss": 1.0377,
      "step": 3870
    },
    {
      "epoch": 1.0777777777777777,
      "grad_norm": 1.6420130729675293,
      "learning_rate": 0.0003204166666666667,
      "loss": 0.9482,
      "step": 3880
    },
    {
      "epoch": 1.0805555555555555,
      "grad_norm": 1.3162420988082886,
      "learning_rate": 0.0003199537037037037,
      "loss": 0.9915,
      "step": 3890
    },
    {
      "epoch": 1.0833333333333333,
      "grad_norm": 1.0137113332748413,
      "learning_rate": 0.00031949074074074074,
      "loss": 0.9373,
      "step": 3900
    },
    {
      "epoch": 1.086111111111111,
      "grad_norm": 1.5111714601516724,
      "learning_rate": 0.00031902777777777783,
      "loss": 0.9901,
      "step": 3910
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 0.8561367392539978,
      "learning_rate": 0.0003185648148148148,
      "loss": 0.9943,
      "step": 3920
    },
    {
      "epoch": 1.0916666666666666,
      "grad_norm": 1.5471587181091309,
      "learning_rate": 0.00031810185185185186,
      "loss": 1.013,
      "step": 3930
    },
    {
      "epoch": 1.0944444444444446,
      "grad_norm": 1.6353713274002075,
      "learning_rate": 0.0003176388888888889,
      "loss": 0.9563,
      "step": 3940
    },
    {
      "epoch": 1.0972222222222223,
      "grad_norm": 1.1821298599243164,
      "learning_rate": 0.00031717592592592594,
      "loss": 0.9917,
      "step": 3950
    },
    {
      "epoch": 1.1,
      "grad_norm": 2.2056241035461426,
      "learning_rate": 0.000316712962962963,
      "loss": 0.9138,
      "step": 3960
    },
    {
      "epoch": 1.1027777777777779,
      "grad_norm": 1.0878046751022339,
      "learning_rate": 0.00031624999999999996,
      "loss": 0.9422,
      "step": 3970
    },
    {
      "epoch": 1.1055555555555556,
      "grad_norm": 1.879827618598938,
      "learning_rate": 0.00031578703703703706,
      "loss": 0.9384,
      "step": 3980
    },
    {
      "epoch": 1.1083333333333334,
      "grad_norm": 2.069315195083618,
      "learning_rate": 0.0003153240740740741,
      "loss": 0.9436,
      "step": 3990
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.982427716255188,
      "learning_rate": 0.0003148611111111111,
      "loss": 0.8634,
      "step": 4000
    },
    {
      "epoch": 1.113888888888889,
      "grad_norm": 1.2910419702529907,
      "learning_rate": 0.0003143981481481482,
      "loss": 0.8673,
      "step": 4010
    },
    {
      "epoch": 1.1166666666666667,
      "grad_norm": 1.1548924446105957,
      "learning_rate": 0.0003139351851851852,
      "loss": 0.8726,
      "step": 4020
    },
    {
      "epoch": 1.1194444444444445,
      "grad_norm": 1.3201498985290527,
      "learning_rate": 0.0003134722222222222,
      "loss": 0.9086,
      "step": 4030
    },
    {
      "epoch": 1.1222222222222222,
      "grad_norm": 1.2492791414260864,
      "learning_rate": 0.0003130092592592593,
      "loss": 1.0068,
      "step": 4040
    },
    {
      "epoch": 1.125,
      "grad_norm": 1.2298113107681274,
      "learning_rate": 0.00031254629629629634,
      "loss": 0.884,
      "step": 4050
    },
    {
      "epoch": 1.1277777777777778,
      "grad_norm": 1.2581504583358765,
      "learning_rate": 0.0003120833333333333,
      "loss": 0.906,
      "step": 4060
    },
    {
      "epoch": 1.1305555555555555,
      "grad_norm": 1.2786895036697388,
      "learning_rate": 0.00031162037037037036,
      "loss": 1.0098,
      "step": 4070
    },
    {
      "epoch": 1.1333333333333333,
      "grad_norm": 6.417592525482178,
      "learning_rate": 0.0003111574074074074,
      "loss": 0.8989,
      "step": 4080
    },
    {
      "epoch": 1.136111111111111,
      "grad_norm": 1.3112692832946777,
      "learning_rate": 0.00031069444444444444,
      "loss": 0.9343,
      "step": 4090
    },
    {
      "epoch": 1.1388888888888888,
      "grad_norm": 1.1338082551956177,
      "learning_rate": 0.0003102314814814815,
      "loss": 0.9373,
      "step": 4100
    },
    {
      "epoch": 1.1416666666666666,
      "grad_norm": 1.3855159282684326,
      "learning_rate": 0.0003097685185185185,
      "loss": 0.9604,
      "step": 4110
    },
    {
      "epoch": 1.1444444444444444,
      "grad_norm": 1.4904332160949707,
      "learning_rate": 0.00030930555555555556,
      "loss": 0.956,
      "step": 4120
    },
    {
      "epoch": 1.1472222222222221,
      "grad_norm": 1.8204056024551392,
      "learning_rate": 0.0003088425925925926,
      "loss": 0.9854,
      "step": 4130
    },
    {
      "epoch": 1.15,
      "grad_norm": 1.3657851219177246,
      "learning_rate": 0.00030837962962962964,
      "loss": 0.9408,
      "step": 4140
    },
    {
      "epoch": 1.1527777777777777,
      "grad_norm": 1.7391228675842285,
      "learning_rate": 0.0003079166666666667,
      "loss": 0.9222,
      "step": 4150
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 1.4376755952835083,
      "learning_rate": 0.0003074537037037037,
      "loss": 0.9439,
      "step": 4160
    },
    {
      "epoch": 1.1583333333333332,
      "grad_norm": 1.3748334646224976,
      "learning_rate": 0.00030699074074074076,
      "loss": 0.9987,
      "step": 4170
    },
    {
      "epoch": 1.1611111111111112,
      "grad_norm": 3.106443405151367,
      "learning_rate": 0.0003065277777777778,
      "loss": 0.9848,
      "step": 4180
    },
    {
      "epoch": 1.163888888888889,
      "grad_norm": 1.448082447052002,
      "learning_rate": 0.0003060648148148148,
      "loss": 0.9904,
      "step": 4190
    },
    {
      "epoch": 1.1666666666666667,
      "grad_norm": 1.4089444875717163,
      "learning_rate": 0.0003056018518518519,
      "loss": 0.8729,
      "step": 4200
    },
    {
      "epoch": 1.1694444444444445,
      "grad_norm": 1.0753211975097656,
      "learning_rate": 0.0003051388888888889,
      "loss": 0.9661,
      "step": 4210
    },
    {
      "epoch": 1.1722222222222223,
      "grad_norm": 1.6062110662460327,
      "learning_rate": 0.0003046759259259259,
      "loss": 0.908,
      "step": 4220
    },
    {
      "epoch": 1.175,
      "grad_norm": 1.5382766723632812,
      "learning_rate": 0.00030421296296296294,
      "loss": 0.8846,
      "step": 4230
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 1.0302313566207886,
      "learning_rate": 0.00030375000000000004,
      "loss": 0.8385,
      "step": 4240
    },
    {
      "epoch": 1.1805555555555556,
      "grad_norm": 1.2527151107788086,
      "learning_rate": 0.000303287037037037,
      "loss": 0.9356,
      "step": 4250
    },
    {
      "epoch": 1.1833333333333333,
      "grad_norm": 1.105635643005371,
      "learning_rate": 0.00030282407407407406,
      "loss": 0.9056,
      "step": 4260
    },
    {
      "epoch": 1.1861111111111111,
      "grad_norm": 1.6630901098251343,
      "learning_rate": 0.00030236111111111116,
      "loss": 0.8961,
      "step": 4270
    },
    {
      "epoch": 1.1888888888888889,
      "grad_norm": 2.30450439453125,
      "learning_rate": 0.00030189814814814814,
      "loss": 0.9129,
      "step": 4280
    },
    {
      "epoch": 1.1916666666666667,
      "grad_norm": 1.0337002277374268,
      "learning_rate": 0.0003014351851851852,
      "loss": 0.8421,
      "step": 4290
    },
    {
      "epoch": 1.1944444444444444,
      "grad_norm": 1.985643744468689,
      "learning_rate": 0.0003009722222222222,
      "loss": 0.9613,
      "step": 4300
    },
    {
      "epoch": 1.1972222222222222,
      "grad_norm": 1.8907504081726074,
      "learning_rate": 0.00030050925925925926,
      "loss": 1.0465,
      "step": 4310
    },
    {
      "epoch": 1.2,
      "grad_norm": 1.4472100734710693,
      "learning_rate": 0.0003000462962962963,
      "loss": 1.0031,
      "step": 4320
    },
    {
      "epoch": 1.2027777777777777,
      "grad_norm": 1.074934720993042,
      "learning_rate": 0.00029958333333333334,
      "loss": 0.9187,
      "step": 4330
    },
    {
      "epoch": 1.2055555555555555,
      "grad_norm": 1.1459765434265137,
      "learning_rate": 0.0002991203703703704,
      "loss": 0.9552,
      "step": 4340
    },
    {
      "epoch": 1.2083333333333333,
      "grad_norm": 2.1164402961730957,
      "learning_rate": 0.0002986574074074074,
      "loss": 0.9987,
      "step": 4350
    },
    {
      "epoch": 1.211111111111111,
      "grad_norm": 1.0240399837493896,
      "learning_rate": 0.00029819444444444446,
      "loss": 0.9225,
      "step": 4360
    },
    {
      "epoch": 1.2138888888888888,
      "grad_norm": 1.465808629989624,
      "learning_rate": 0.0002977314814814815,
      "loss": 0.9044,
      "step": 4370
    },
    {
      "epoch": 1.2166666666666668,
      "grad_norm": 1.5746396780014038,
      "learning_rate": 0.00029726851851851854,
      "loss": 0.9106,
      "step": 4380
    },
    {
      "epoch": 1.2194444444444446,
      "grad_norm": 1.385507345199585,
      "learning_rate": 0.0002968055555555555,
      "loss": 0.9986,
      "step": 4390
    },
    {
      "epoch": 1.2222222222222223,
      "grad_norm": 1.0875709056854248,
      "learning_rate": 0.0002963425925925926,
      "loss": 0.9046,
      "step": 4400
    },
    {
      "epoch": 1.225,
      "grad_norm": 1.321047067642212,
      "learning_rate": 0.0002958796296296296,
      "loss": 1.0178,
      "step": 4410
    },
    {
      "epoch": 1.2277777777777779,
      "grad_norm": 0.9476124048233032,
      "learning_rate": 0.00029541666666666665,
      "loss": 0.9388,
      "step": 4420
    },
    {
      "epoch": 1.2305555555555556,
      "grad_norm": 1.5306322574615479,
      "learning_rate": 0.00029495370370370374,
      "loss": 0.9636,
      "step": 4430
    },
    {
      "epoch": 1.2333333333333334,
      "grad_norm": 1.8007301092147827,
      "learning_rate": 0.0002944907407407407,
      "loss": 0.9766,
      "step": 4440
    },
    {
      "epoch": 1.2361111111111112,
      "grad_norm": 1.3694913387298584,
      "learning_rate": 0.00029402777777777777,
      "loss": 0.8939,
      "step": 4450
    },
    {
      "epoch": 1.238888888888889,
      "grad_norm": 1.1915334463119507,
      "learning_rate": 0.00029356481481481486,
      "loss": 0.9316,
      "step": 4460
    },
    {
      "epoch": 1.2416666666666667,
      "grad_norm": 1.5680445432662964,
      "learning_rate": 0.00029310185185185185,
      "loss": 0.8339,
      "step": 4470
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 1.268444299697876,
      "learning_rate": 0.0002926388888888889,
      "loss": 1.0339,
      "step": 4480
    },
    {
      "epoch": 1.2472222222222222,
      "grad_norm": 1.7337634563446045,
      "learning_rate": 0.000292175925925926,
      "loss": 0.9071,
      "step": 4490
    },
    {
      "epoch": 1.25,
      "grad_norm": 1.0935019254684448,
      "learning_rate": 0.00029171296296296297,
      "loss": 0.9772,
      "step": 4500
    },
    {
      "epoch": 1.2527777777777778,
      "grad_norm": 1.5936228036880493,
      "learning_rate": 0.00029125,
      "loss": 0.8885,
      "step": 4510
    },
    {
      "epoch": 1.2555555555555555,
      "grad_norm": 1.4821398258209229,
      "learning_rate": 0.000290787037037037,
      "loss": 1.0001,
      "step": 4520
    },
    {
      "epoch": 1.2583333333333333,
      "grad_norm": 1.0332157611846924,
      "learning_rate": 0.0002903240740740741,
      "loss": 0.92,
      "step": 4530
    },
    {
      "epoch": 1.261111111111111,
      "grad_norm": 1.5716139078140259,
      "learning_rate": 0.0002898611111111111,
      "loss": 0.9426,
      "step": 4540
    },
    {
      "epoch": 1.2638888888888888,
      "grad_norm": 1.4635236263275146,
      "learning_rate": 0.0002893981481481481,
      "loss": 0.938,
      "step": 4550
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 4.007880687713623,
      "learning_rate": 0.0002889351851851852,
      "loss": 0.9132,
      "step": 4560
    },
    {
      "epoch": 1.2694444444444444,
      "grad_norm": 1.2220442295074463,
      "learning_rate": 0.00028847222222222224,
      "loss": 0.8981,
      "step": 4570
    },
    {
      "epoch": 1.2722222222222221,
      "grad_norm": 0.8145832419395447,
      "learning_rate": 0.00028800925925925923,
      "loss": 0.8753,
      "step": 4580
    },
    {
      "epoch": 1.275,
      "grad_norm": 2.4319934844970703,
      "learning_rate": 0.0002875462962962963,
      "loss": 0.8754,
      "step": 4590
    },
    {
      "epoch": 1.2777777777777777,
      "grad_norm": 1.3695976734161377,
      "learning_rate": 0.00028708333333333336,
      "loss": 0.875,
      "step": 4600
    },
    {
      "epoch": 1.2805555555555554,
      "grad_norm": 1.2351405620574951,
      "learning_rate": 0.00028662037037037035,
      "loss": 0.9354,
      "step": 4610
    },
    {
      "epoch": 1.2833333333333332,
      "grad_norm": 1.4522408246994019,
      "learning_rate": 0.00028615740740740744,
      "loss": 0.8699,
      "step": 4620
    },
    {
      "epoch": 1.286111111111111,
      "grad_norm": 1.4558013677597046,
      "learning_rate": 0.00028569444444444443,
      "loss": 0.9237,
      "step": 4630
    },
    {
      "epoch": 1.2888888888888888,
      "grad_norm": 1.2106947898864746,
      "learning_rate": 0.00028523148148148147,
      "loss": 0.9455,
      "step": 4640
    },
    {
      "epoch": 1.2916666666666667,
      "grad_norm": 1.8185670375823975,
      "learning_rate": 0.00028476851851851856,
      "loss": 0.9662,
      "step": 4650
    },
    {
      "epoch": 1.2944444444444445,
      "grad_norm": 1.1460118293762207,
      "learning_rate": 0.00028430555555555555,
      "loss": 0.9285,
      "step": 4660
    },
    {
      "epoch": 1.2972222222222223,
      "grad_norm": 1.194729208946228,
      "learning_rate": 0.0002838425925925926,
      "loss": 0.9851,
      "step": 4670
    },
    {
      "epoch": 1.3,
      "grad_norm": 2.1087167263031006,
      "learning_rate": 0.00028337962962962963,
      "loss": 0.9427,
      "step": 4680
    },
    {
      "epoch": 1.3027777777777778,
      "grad_norm": 1.442001223564148,
      "learning_rate": 0.00028291666666666667,
      "loss": 0.98,
      "step": 4690
    },
    {
      "epoch": 1.3055555555555556,
      "grad_norm": 1.0532684326171875,
      "learning_rate": 0.0002824537037037037,
      "loss": 0.8987,
      "step": 4700
    },
    {
      "epoch": 1.3083333333333333,
      "grad_norm": 0.8291841149330139,
      "learning_rate": 0.00028199074074074075,
      "loss": 0.8609,
      "step": 4710
    },
    {
      "epoch": 1.3111111111111111,
      "grad_norm": 2.020505666732788,
      "learning_rate": 0.0002815277777777778,
      "loss": 0.9139,
      "step": 4720
    },
    {
      "epoch": 1.3138888888888889,
      "grad_norm": 1.7107067108154297,
      "learning_rate": 0.00028106481481481483,
      "loss": 0.9618,
      "step": 4730
    },
    {
      "epoch": 1.3166666666666667,
      "grad_norm": 1.169777512550354,
      "learning_rate": 0.0002806018518518518,
      "loss": 0.9097,
      "step": 4740
    },
    {
      "epoch": 1.3194444444444444,
      "grad_norm": 3.4602811336517334,
      "learning_rate": 0.0002801388888888889,
      "loss": 1.0022,
      "step": 4750
    },
    {
      "epoch": 1.3222222222222222,
      "grad_norm": 2.096109390258789,
      "learning_rate": 0.00027967592592592595,
      "loss": 0.9709,
      "step": 4760
    },
    {
      "epoch": 1.325,
      "grad_norm": 1.2382616996765137,
      "learning_rate": 0.00027921296296296293,
      "loss": 0.9544,
      "step": 4770
    },
    {
      "epoch": 1.3277777777777777,
      "grad_norm": 1.0960159301757812,
      "learning_rate": 0.00027875,
      "loss": 0.9339,
      "step": 4780
    },
    {
      "epoch": 1.3305555555555555,
      "grad_norm": 1.0024197101593018,
      "learning_rate": 0.00027828703703703707,
      "loss": 0.9306,
      "step": 4790
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 1.5622247457504272,
      "learning_rate": 0.00027782407407407405,
      "loss": 0.9537,
      "step": 4800
    },
    {
      "epoch": 1.3361111111111112,
      "grad_norm": 1.319867730140686,
      "learning_rate": 0.00027736111111111115,
      "loss": 0.9943,
      "step": 4810
    },
    {
      "epoch": 1.338888888888889,
      "grad_norm": 1.2890313863754272,
      "learning_rate": 0.0002768981481481482,
      "loss": 0.9681,
      "step": 4820
    },
    {
      "epoch": 1.3416666666666668,
      "grad_norm": 1.2886756658554077,
      "learning_rate": 0.00027643518518518517,
      "loss": 0.9488,
      "step": 4830
    },
    {
      "epoch": 1.3444444444444446,
      "grad_norm": 1.4494050741195679,
      "learning_rate": 0.0002759722222222222,
      "loss": 0.971,
      "step": 4840
    },
    {
      "epoch": 1.3472222222222223,
      "grad_norm": 1.5540721416473389,
      "learning_rate": 0.00027550925925925925,
      "loss": 0.9912,
      "step": 4850
    },
    {
      "epoch": 1.35,
      "grad_norm": 1.2468500137329102,
      "learning_rate": 0.0002750462962962963,
      "loss": 0.9173,
      "step": 4860
    },
    {
      "epoch": 1.3527777777777779,
      "grad_norm": 1.3072926998138428,
      "learning_rate": 0.00027458333333333333,
      "loss": 0.9817,
      "step": 4870
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 1.5308045148849487,
      "learning_rate": 0.00027412037037037037,
      "loss": 1.0049,
      "step": 4880
    },
    {
      "epoch": 1.3583333333333334,
      "grad_norm": 0.9423808455467224,
      "learning_rate": 0.0002736574074074074,
      "loss": 0.8174,
      "step": 4890
    },
    {
      "epoch": 1.3611111111111112,
      "grad_norm": 2.0694520473480225,
      "learning_rate": 0.00027319444444444445,
      "loss": 0.986,
      "step": 4900
    },
    {
      "epoch": 1.363888888888889,
      "grad_norm": 0.7984654307365417,
      "learning_rate": 0.0002727314814814815,
      "loss": 0.8888,
      "step": 4910
    },
    {
      "epoch": 1.3666666666666667,
      "grad_norm": 1.2726609706878662,
      "learning_rate": 0.00027226851851851853,
      "loss": 0.826,
      "step": 4920
    },
    {
      "epoch": 1.3694444444444445,
      "grad_norm": 1.252550721168518,
      "learning_rate": 0.00027180555555555557,
      "loss": 0.9168,
      "step": 4930
    },
    {
      "epoch": 1.3722222222222222,
      "grad_norm": 3.140131950378418,
      "learning_rate": 0.0002713425925925926,
      "loss": 0.8917,
      "step": 4940
    },
    {
      "epoch": 1.375,
      "grad_norm": 1.0168975591659546,
      "learning_rate": 0.00027087962962962965,
      "loss": 0.893,
      "step": 4950
    },
    {
      "epoch": 1.3777777777777778,
      "grad_norm": 1.3481721878051758,
      "learning_rate": 0.0002704166666666667,
      "loss": 1.0334,
      "step": 4960
    },
    {
      "epoch": 1.3805555555555555,
      "grad_norm": 1.3979642391204834,
      "learning_rate": 0.0002699537037037037,
      "loss": 1.026,
      "step": 4970
    },
    {
      "epoch": 1.3833333333333333,
      "grad_norm": 1.434470534324646,
      "learning_rate": 0.00026949074074074077,
      "loss": 0.9608,
      "step": 4980
    },
    {
      "epoch": 1.386111111111111,
      "grad_norm": 1.095261573791504,
      "learning_rate": 0.00026902777777777775,
      "loss": 0.8566,
      "step": 4990
    },
    {
      "epoch": 1.3888888888888888,
      "grad_norm": 1.3042017221450806,
      "learning_rate": 0.0002685648148148148,
      "loss": 0.8943,
      "step": 5000
    },
    {
      "epoch": 1.3916666666666666,
      "grad_norm": 1.3453149795532227,
      "learning_rate": 0.0002681018518518519,
      "loss": 0.9448,
      "step": 5010
    },
    {
      "epoch": 1.3944444444444444,
      "grad_norm": 1.377098560333252,
      "learning_rate": 0.0002676388888888889,
      "loss": 0.9286,
      "step": 5020
    },
    {
      "epoch": 1.3972222222222221,
      "grad_norm": 1.4078385829925537,
      "learning_rate": 0.0002671759259259259,
      "loss": 0.9017,
      "step": 5030
    },
    {
      "epoch": 1.4,
      "grad_norm": 0.8615010380744934,
      "learning_rate": 0.000266712962962963,
      "loss": 0.9435,
      "step": 5040
    },
    {
      "epoch": 1.4027777777777777,
      "grad_norm": 2.444314956665039,
      "learning_rate": 0.00026625,
      "loss": 0.9549,
      "step": 5050
    },
    {
      "epoch": 1.4055555555555554,
      "grad_norm": 1.102774977684021,
      "learning_rate": 0.00026578703703703703,
      "loss": 0.8974,
      "step": 5060
    },
    {
      "epoch": 1.4083333333333332,
      "grad_norm": 2.0028040409088135,
      "learning_rate": 0.00026532407407407413,
      "loss": 1.0353,
      "step": 5070
    },
    {
      "epoch": 1.411111111111111,
      "grad_norm": 1.1312028169631958,
      "learning_rate": 0.0002648611111111111,
      "loss": 0.8952,
      "step": 5080
    },
    {
      "epoch": 1.4138888888888888,
      "grad_norm": 1.512522578239441,
      "learning_rate": 0.00026439814814814815,
      "loss": 0.965,
      "step": 5090
    },
    {
      "epoch": 1.4166666666666667,
      "grad_norm": 1.0391710996627808,
      "learning_rate": 0.0002639351851851852,
      "loss": 0.8728,
      "step": 5100
    },
    {
      "epoch": 1.4194444444444445,
      "grad_norm": 1.3059970140457153,
      "learning_rate": 0.00026347222222222223,
      "loss": 0.9345,
      "step": 5110
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 1.4478193521499634,
      "learning_rate": 0.00026300925925925927,
      "loss": 1.1058,
      "step": 5120
    },
    {
      "epoch": 1.425,
      "grad_norm": 1.5037364959716797,
      "learning_rate": 0.00026254629629629626,
      "loss": 0.8959,
      "step": 5130
    },
    {
      "epoch": 1.4277777777777778,
      "grad_norm": 1.53831946849823,
      "learning_rate": 0.00026208333333333335,
      "loss": 0.9224,
      "step": 5140
    },
    {
      "epoch": 1.4305555555555556,
      "grad_norm": 1.728713870048523,
      "learning_rate": 0.0002616203703703704,
      "loss": 0.9751,
      "step": 5150
    },
    {
      "epoch": 1.4333333333333333,
      "grad_norm": 1.6001988649368286,
      "learning_rate": 0.0002611574074074074,
      "loss": 0.968,
      "step": 5160
    },
    {
      "epoch": 1.4361111111111111,
      "grad_norm": 1.1050094366073608,
      "learning_rate": 0.00026069444444444447,
      "loss": 0.9134,
      "step": 5170
    },
    {
      "epoch": 1.4388888888888889,
      "grad_norm": 1.3261932134628296,
      "learning_rate": 0.0002602314814814815,
      "loss": 0.9546,
      "step": 5180
    },
    {
      "epoch": 1.4416666666666667,
      "grad_norm": 1.377912998199463,
      "learning_rate": 0.0002597685185185185,
      "loss": 1.0094,
      "step": 5190
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 1.7688930034637451,
      "learning_rate": 0.0002593055555555556,
      "loss": 1.04,
      "step": 5200
    },
    {
      "epoch": 1.4472222222222222,
      "grad_norm": 1.3167616128921509,
      "learning_rate": 0.0002588425925925926,
      "loss": 0.874,
      "step": 5210
    },
    {
      "epoch": 1.45,
      "grad_norm": 1.2512322664260864,
      "learning_rate": 0.0002583796296296296,
      "loss": 0.8242,
      "step": 5220
    },
    {
      "epoch": 1.4527777777777777,
      "grad_norm": 2.597134828567505,
      "learning_rate": 0.0002579166666666667,
      "loss": 0.8918,
      "step": 5230
    },
    {
      "epoch": 1.4555555555555555,
      "grad_norm": 1.2630707025527954,
      "learning_rate": 0.0002574537037037037,
      "loss": 0.8988,
      "step": 5240
    },
    {
      "epoch": 1.4583333333333333,
      "grad_norm": 0.7064247727394104,
      "learning_rate": 0.00025699074074074074,
      "loss": 1.0044,
      "step": 5250
    },
    {
      "epoch": 1.4611111111111112,
      "grad_norm": 1.9688260555267334,
      "learning_rate": 0.00025652777777777783,
      "loss": 1.039,
      "step": 5260
    },
    {
      "epoch": 1.463888888888889,
      "grad_norm": 1.697064757347107,
      "learning_rate": 0.0002560648148148148,
      "loss": 0.988,
      "step": 5270
    },
    {
      "epoch": 1.4666666666666668,
      "grad_norm": 1.2345881462097168,
      "learning_rate": 0.00025560185185185186,
      "loss": 0.9267,
      "step": 5280
    },
    {
      "epoch": 1.4694444444444446,
      "grad_norm": 1.5193477869033813,
      "learning_rate": 0.0002551388888888889,
      "loss": 0.9229,
      "step": 5290
    },
    {
      "epoch": 1.4722222222222223,
      "grad_norm": 1.348248839378357,
      "learning_rate": 0.00025467592592592594,
      "loss": 0.8883,
      "step": 5300
    },
    {
      "epoch": 1.475,
      "grad_norm": 1.7373894453048706,
      "learning_rate": 0.000254212962962963,
      "loss": 0.9086,
      "step": 5310
    },
    {
      "epoch": 1.4777777777777779,
      "grad_norm": 1.397039771080017,
      "learning_rate": 0.00025374999999999996,
      "loss": 0.9482,
      "step": 5320
    },
    {
      "epoch": 1.4805555555555556,
      "grad_norm": 1.7578262090682983,
      "learning_rate": 0.00025328703703703705,
      "loss": 0.8861,
      "step": 5330
    },
    {
      "epoch": 1.4833333333333334,
      "grad_norm": 1.246684193611145,
      "learning_rate": 0.0002528240740740741,
      "loss": 0.9832,
      "step": 5340
    },
    {
      "epoch": 1.4861111111111112,
      "grad_norm": 1.6009103059768677,
      "learning_rate": 0.0002523611111111111,
      "loss": 0.96,
      "step": 5350
    },
    {
      "epoch": 1.488888888888889,
      "grad_norm": 1.2957017421722412,
      "learning_rate": 0.0002518981481481482,
      "loss": 0.9498,
      "step": 5360
    },
    {
      "epoch": 1.4916666666666667,
      "grad_norm": 1.4440072774887085,
      "learning_rate": 0.0002514351851851852,
      "loss": 0.8437,
      "step": 5370
    },
    {
      "epoch": 1.4944444444444445,
      "grad_norm": 2.807109832763672,
      "learning_rate": 0.0002509722222222222,
      "loss": 0.9273,
      "step": 5380
    },
    {
      "epoch": 1.4972222222222222,
      "grad_norm": 1.4105031490325928,
      "learning_rate": 0.0002505092592592593,
      "loss": 0.9788,
      "step": 5390
    },
    {
      "epoch": 1.5,
      "grad_norm": 1.2305724620819092,
      "learning_rate": 0.00025004629629629633,
      "loss": 0.8949,
      "step": 5400
    },
    {
      "epoch": 1.5027777777777778,
      "grad_norm": 1.5798976421356201,
      "learning_rate": 0.0002495833333333333,
      "loss": 1.041,
      "step": 5410
    },
    {
      "epoch": 1.5055555555555555,
      "grad_norm": 1.0350074768066406,
      "learning_rate": 0.00024912037037037036,
      "loss": 0.8737,
      "step": 5420
    },
    {
      "epoch": 1.5083333333333333,
      "grad_norm": 1.0048253536224365,
      "learning_rate": 0.0002486574074074074,
      "loss": 0.9778,
      "step": 5430
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 1.723285436630249,
      "learning_rate": 0.00024819444444444444,
      "loss": 0.9006,
      "step": 5440
    },
    {
      "epoch": 1.5138888888888888,
      "grad_norm": 1.4907618761062622,
      "learning_rate": 0.0002477314814814815,
      "loss": 0.9194,
      "step": 5450
    },
    {
      "epoch": 1.5166666666666666,
      "grad_norm": 1.3178961277008057,
      "learning_rate": 0.0002472685185185185,
      "loss": 0.983,
      "step": 5460
    },
    {
      "epoch": 1.5194444444444444,
      "grad_norm": 1.4969336986541748,
      "learning_rate": 0.00024680555555555556,
      "loss": 0.9245,
      "step": 5470
    },
    {
      "epoch": 1.5222222222222221,
      "grad_norm": 1.9864797592163086,
      "learning_rate": 0.0002463425925925926,
      "loss": 1.0485,
      "step": 5480
    },
    {
      "epoch": 1.525,
      "grad_norm": 1.2876378297805786,
      "learning_rate": 0.00024587962962962964,
      "loss": 0.9857,
      "step": 5490
    },
    {
      "epoch": 1.5277777777777777,
      "grad_norm": 0.978484570980072,
      "learning_rate": 0.0002454166666666667,
      "loss": 0.9935,
      "step": 5500
    },
    {
      "epoch": 1.5305555555555554,
      "grad_norm": 1.263780117034912,
      "learning_rate": 0.0002449537037037037,
      "loss": 1.0048,
      "step": 5510
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 1.3879808187484741,
      "learning_rate": 0.00024449074074074076,
      "loss": 0.9134,
      "step": 5520
    },
    {
      "epoch": 1.536111111111111,
      "grad_norm": 1.515508770942688,
      "learning_rate": 0.0002440277777777778,
      "loss": 0.8826,
      "step": 5530
    },
    {
      "epoch": 1.5388888888888888,
      "grad_norm": 1.2846643924713135,
      "learning_rate": 0.0002435648148148148,
      "loss": 0.8413,
      "step": 5540
    },
    {
      "epoch": 1.5416666666666665,
      "grad_norm": 1.1312264204025269,
      "learning_rate": 0.00024310185185185185,
      "loss": 0.9512,
      "step": 5550
    },
    {
      "epoch": 1.5444444444444443,
      "grad_norm": 1.3606398105621338,
      "learning_rate": 0.0002426388888888889,
      "loss": 0.8891,
      "step": 5560
    },
    {
      "epoch": 1.5472222222222223,
      "grad_norm": 1.4077246189117432,
      "learning_rate": 0.00024217592592592593,
      "loss": 0.9808,
      "step": 5570
    },
    {
      "epoch": 1.55,
      "grad_norm": 1.622786283493042,
      "learning_rate": 0.00024171296296296297,
      "loss": 0.9812,
      "step": 5580
    },
    {
      "epoch": 1.5527777777777778,
      "grad_norm": 1.544255018234253,
      "learning_rate": 0.00024125,
      "loss": 0.962,
      "step": 5590
    },
    {
      "epoch": 1.5555555555555556,
      "grad_norm": 1.256593108177185,
      "learning_rate": 0.00024078703703703705,
      "loss": 0.8818,
      "step": 5600
    },
    {
      "epoch": 1.5583333333333333,
      "grad_norm": 1.4219839572906494,
      "learning_rate": 0.0002403240740740741,
      "loss": 0.8964,
      "step": 5610
    },
    {
      "epoch": 1.5611111111111111,
      "grad_norm": 1.487182855606079,
      "learning_rate": 0.0002398611111111111,
      "loss": 0.965,
      "step": 5620
    },
    {
      "epoch": 1.5638888888888889,
      "grad_norm": 4.4092583656311035,
      "learning_rate": 0.00023939814814814814,
      "loss": 0.9334,
      "step": 5630
    },
    {
      "epoch": 1.5666666666666667,
      "grad_norm": 1.147076964378357,
      "learning_rate": 0.0002389351851851852,
      "loss": 0.8853,
      "step": 5640
    },
    {
      "epoch": 1.5694444444444444,
      "grad_norm": 1.2266490459442139,
      "learning_rate": 0.00023847222222222222,
      "loss": 0.9143,
      "step": 5650
    },
    {
      "epoch": 1.5722222222222222,
      "grad_norm": 1.1288560628890991,
      "learning_rate": 0.00023800925925925926,
      "loss": 0.9697,
      "step": 5660
    },
    {
      "epoch": 1.575,
      "grad_norm": 1.3949719667434692,
      "learning_rate": 0.0002375462962962963,
      "loss": 0.8945,
      "step": 5670
    },
    {
      "epoch": 1.5777777777777777,
      "grad_norm": 1.5597429275512695,
      "learning_rate": 0.00023708333333333334,
      "loss": 0.9787,
      "step": 5680
    },
    {
      "epoch": 1.5805555555555557,
      "grad_norm": 1.6086645126342773,
      "learning_rate": 0.00023662037037037038,
      "loss": 0.9073,
      "step": 5690
    },
    {
      "epoch": 1.5833333333333335,
      "grad_norm": 1.4088759422302246,
      "learning_rate": 0.0002361574074074074,
      "loss": 0.886,
      "step": 5700
    },
    {
      "epoch": 1.5861111111111112,
      "grad_norm": 1.9636541604995728,
      "learning_rate": 0.00023569444444444446,
      "loss": 0.9514,
      "step": 5710
    },
    {
      "epoch": 1.588888888888889,
      "grad_norm": 2.536822557449341,
      "learning_rate": 0.0002352314814814815,
      "loss": 0.8902,
      "step": 5720
    },
    {
      "epoch": 1.5916666666666668,
      "grad_norm": 2.131769895553589,
      "learning_rate": 0.0002347685185185185,
      "loss": 1.0306,
      "step": 5730
    },
    {
      "epoch": 1.5944444444444446,
      "grad_norm": 1.5068188905715942,
      "learning_rate": 0.00023430555555555555,
      "loss": 0.9188,
      "step": 5740
    },
    {
      "epoch": 1.5972222222222223,
      "grad_norm": 1.3405847549438477,
      "learning_rate": 0.00023384259259259262,
      "loss": 0.9117,
      "step": 5750
    },
    {
      "epoch": 1.6,
      "grad_norm": 1.059118390083313,
      "learning_rate": 0.00023337962962962963,
      "loss": 0.9088,
      "step": 5760
    },
    {
      "epoch": 1.6027777777777779,
      "grad_norm": 1.4384355545043945,
      "learning_rate": 0.00023291666666666667,
      "loss": 0.8922,
      "step": 5770
    },
    {
      "epoch": 1.6055555555555556,
      "grad_norm": 1.2254143953323364,
      "learning_rate": 0.00023245370370370368,
      "loss": 0.9163,
      "step": 5780
    },
    {
      "epoch": 1.6083333333333334,
      "grad_norm": 0.9638590216636658,
      "learning_rate": 0.00023199074074074075,
      "loss": 0.8428,
      "step": 5790
    },
    {
      "epoch": 1.6111111111111112,
      "grad_norm": 1.3418869972229004,
      "learning_rate": 0.0002315277777777778,
      "loss": 0.9444,
      "step": 5800
    },
    {
      "epoch": 1.613888888888889,
      "grad_norm": 1.7190661430358887,
      "learning_rate": 0.0002310648148148148,
      "loss": 0.9297,
      "step": 5810
    },
    {
      "epoch": 1.6166666666666667,
      "grad_norm": 1.7611218690872192,
      "learning_rate": 0.00023060185185185187,
      "loss": 0.989,
      "step": 5820
    },
    {
      "epoch": 1.6194444444444445,
      "grad_norm": 1.4493263959884644,
      "learning_rate": 0.00023013888888888888,
      "loss": 0.9704,
      "step": 5830
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 1.4728554487228394,
      "learning_rate": 0.00022967592592592592,
      "loss": 0.9188,
      "step": 5840
    },
    {
      "epoch": 1.625,
      "grad_norm": 1.3376121520996094,
      "learning_rate": 0.00022921296296296296,
      "loss": 0.7817,
      "step": 5850
    },
    {
      "epoch": 1.6277777777777778,
      "grad_norm": 1.1123783588409424,
      "learning_rate": 0.00022875,
      "loss": 1.0526,
      "step": 5860
    },
    {
      "epoch": 1.6305555555555555,
      "grad_norm": 1.3542922735214233,
      "learning_rate": 0.00022828703703703704,
      "loss": 0.8839,
      "step": 5870
    },
    {
      "epoch": 1.6333333333333333,
      "grad_norm": 1.534995675086975,
      "learning_rate": 0.00022782407407407408,
      "loss": 0.9672,
      "step": 5880
    },
    {
      "epoch": 1.636111111111111,
      "grad_norm": 1.1104559898376465,
      "learning_rate": 0.00022736111111111112,
      "loss": 0.9581,
      "step": 5890
    },
    {
      "epoch": 1.6388888888888888,
      "grad_norm": 2.0436251163482666,
      "learning_rate": 0.00022689814814814816,
      "loss": 0.9743,
      "step": 5900
    },
    {
      "epoch": 1.6416666666666666,
      "grad_norm": 1.4256170988082886,
      "learning_rate": 0.00022643518518518518,
      "loss": 0.9457,
      "step": 5910
    },
    {
      "epoch": 1.6444444444444444,
      "grad_norm": 1.079181432723999,
      "learning_rate": 0.00022597222222222222,
      "loss": 0.8773,
      "step": 5920
    },
    {
      "epoch": 1.6472222222222221,
      "grad_norm": 1.2517427206039429,
      "learning_rate": 0.00022550925925925928,
      "loss": 0.9065,
      "step": 5930
    },
    {
      "epoch": 1.65,
      "grad_norm": 1.8916573524475098,
      "learning_rate": 0.0002250462962962963,
      "loss": 0.9671,
      "step": 5940
    },
    {
      "epoch": 1.6527777777777777,
      "grad_norm": 0.891333818435669,
      "learning_rate": 0.00022458333333333334,
      "loss": 0.954,
      "step": 5950
    },
    {
      "epoch": 1.6555555555555554,
      "grad_norm": 1.3169227838516235,
      "learning_rate": 0.00022412037037037037,
      "loss": 0.9706,
      "step": 5960
    },
    {
      "epoch": 1.6583333333333332,
      "grad_norm": 1.0794997215270996,
      "learning_rate": 0.00022365740740740741,
      "loss": 0.8645,
      "step": 5970
    },
    {
      "epoch": 1.661111111111111,
      "grad_norm": 1.4701178073883057,
      "learning_rate": 0.00022319444444444445,
      "loss": 0.9051,
      "step": 5980
    },
    {
      "epoch": 1.6638888888888888,
      "grad_norm": 1.9778972864151,
      "learning_rate": 0.00022273148148148147,
      "loss": 0.9628,
      "step": 5990
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 1.2548335790634155,
      "learning_rate": 0.00022226851851851853,
      "loss": 0.9336,
      "step": 6000
    },
    {
      "epoch": 1.6694444444444443,
      "grad_norm": 1.8300693035125732,
      "learning_rate": 0.00022180555555555557,
      "loss": 0.8965,
      "step": 6010
    },
    {
      "epoch": 1.6722222222222223,
      "grad_norm": 1.6763546466827393,
      "learning_rate": 0.0002213425925925926,
      "loss": 0.9398,
      "step": 6020
    },
    {
      "epoch": 1.675,
      "grad_norm": 1.2353123426437378,
      "learning_rate": 0.00022087962962962963,
      "loss": 0.9658,
      "step": 6030
    },
    {
      "epoch": 1.6777777777777778,
      "grad_norm": 1.9756799936294556,
      "learning_rate": 0.0002204166666666667,
      "loss": 0.9988,
      "step": 6040
    },
    {
      "epoch": 1.6805555555555556,
      "grad_norm": 1.181517481803894,
      "learning_rate": 0.0002199537037037037,
      "loss": 0.9215,
      "step": 6050
    },
    {
      "epoch": 1.6833333333333333,
      "grad_norm": 0.9082151055335999,
      "learning_rate": 0.00021949074074074075,
      "loss": 0.9611,
      "step": 6060
    },
    {
      "epoch": 1.6861111111111111,
      "grad_norm": 1.6792997121810913,
      "learning_rate": 0.00021902777777777776,
      "loss": 0.8543,
      "step": 6070
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 1.1373577117919922,
      "learning_rate": 0.00021856481481481483,
      "loss": 0.9081,
      "step": 6080
    },
    {
      "epoch": 1.6916666666666667,
      "grad_norm": 1.8097095489501953,
      "learning_rate": 0.00021810185185185187,
      "loss": 0.9031,
      "step": 6090
    },
    {
      "epoch": 1.6944444444444444,
      "grad_norm": 1.371770977973938,
      "learning_rate": 0.00021763888888888888,
      "loss": 0.9206,
      "step": 6100
    },
    {
      "epoch": 1.6972222222222222,
      "grad_norm": 1.1581532955169678,
      "learning_rate": 0.00021717592592592595,
      "loss": 1.046,
      "step": 6110
    },
    {
      "epoch": 1.7,
      "grad_norm": 1.0441398620605469,
      "learning_rate": 0.00021671296296296299,
      "loss": 0.8178,
      "step": 6120
    },
    {
      "epoch": 1.7027777777777777,
      "grad_norm": 1.4812886714935303,
      "learning_rate": 0.00021625,
      "loss": 0.9197,
      "step": 6130
    },
    {
      "epoch": 1.7055555555555557,
      "grad_norm": 1.2000300884246826,
      "learning_rate": 0.00021578703703703704,
      "loss": 0.9746,
      "step": 6140
    },
    {
      "epoch": 1.7083333333333335,
      "grad_norm": 3.6854536533355713,
      "learning_rate": 0.00021532407407407408,
      "loss": 0.9666,
      "step": 6150
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 1.8215992450714111,
      "learning_rate": 0.00021486111111111112,
      "loss": 0.9349,
      "step": 6160
    },
    {
      "epoch": 1.713888888888889,
      "grad_norm": 1.8687211275100708,
      "learning_rate": 0.00021439814814814816,
      "loss": 0.8935,
      "step": 6170
    },
    {
      "epoch": 1.7166666666666668,
      "grad_norm": 0.8745041489601135,
      "learning_rate": 0.00021393518518518517,
      "loss": 0.8927,
      "step": 6180
    },
    {
      "epoch": 1.7194444444444446,
      "grad_norm": 1.1560276746749878,
      "learning_rate": 0.00021347222222222224,
      "loss": 0.9941,
      "step": 6190
    },
    {
      "epoch": 1.7222222222222223,
      "grad_norm": 1.0057629346847534,
      "learning_rate": 0.00021300925925925928,
      "loss": 0.9166,
      "step": 6200
    },
    {
      "epoch": 1.725,
      "grad_norm": 1.6559020280838013,
      "learning_rate": 0.0002125462962962963,
      "loss": 0.8886,
      "step": 6210
    },
    {
      "epoch": 1.7277777777777779,
      "grad_norm": 2.2627317905426025,
      "learning_rate": 0.00021208333333333336,
      "loss": 0.9778,
      "step": 6220
    },
    {
      "epoch": 1.7305555555555556,
      "grad_norm": 1.5722359418869019,
      "learning_rate": 0.00021162037037037037,
      "loss": 0.9934,
      "step": 6230
    },
    {
      "epoch": 1.7333333333333334,
      "grad_norm": 0.8160022497177124,
      "learning_rate": 0.0002111574074074074,
      "loss": 0.8756,
      "step": 6240
    },
    {
      "epoch": 1.7361111111111112,
      "grad_norm": 1.2374953031539917,
      "learning_rate": 0.00021069444444444445,
      "loss": 0.9761,
      "step": 6250
    },
    {
      "epoch": 1.738888888888889,
      "grad_norm": 1.279855728149414,
      "learning_rate": 0.0002102314814814815,
      "loss": 0.9734,
      "step": 6260
    },
    {
      "epoch": 1.7416666666666667,
      "grad_norm": 1.2282047271728516,
      "learning_rate": 0.00020976851851851853,
      "loss": 0.8517,
      "step": 6270
    },
    {
      "epoch": 1.7444444444444445,
      "grad_norm": 1.574471116065979,
      "learning_rate": 0.00020930555555555554,
      "loss": 0.9808,
      "step": 6280
    },
    {
      "epoch": 1.7472222222222222,
      "grad_norm": 2.3490941524505615,
      "learning_rate": 0.00020884259259259258,
      "loss": 0.8591,
      "step": 6290
    },
    {
      "epoch": 1.75,
      "grad_norm": 2.040919303894043,
      "learning_rate": 0.00020837962962962965,
      "loss": 0.9933,
      "step": 6300
    },
    {
      "epoch": 1.7527777777777778,
      "grad_norm": 1.034289836883545,
      "learning_rate": 0.00020791666666666666,
      "loss": 0.9409,
      "step": 6310
    },
    {
      "epoch": 1.7555555555555555,
      "grad_norm": 1.7589174509048462,
      "learning_rate": 0.0002074537037037037,
      "loss": 0.8941,
      "step": 6320
    },
    {
      "epoch": 1.7583333333333333,
      "grad_norm": 1.8139337301254272,
      "learning_rate": 0.00020699074074074077,
      "loss": 0.9459,
      "step": 6330
    },
    {
      "epoch": 1.761111111111111,
      "grad_norm": 1.629348635673523,
      "learning_rate": 0.00020652777777777778,
      "loss": 0.9086,
      "step": 6340
    },
    {
      "epoch": 1.7638888888888888,
      "grad_norm": 1.8587937355041504,
      "learning_rate": 0.00020606481481481482,
      "loss": 0.8452,
      "step": 6350
    },
    {
      "epoch": 1.7666666666666666,
      "grad_norm": 1.4398118257522583,
      "learning_rate": 0.00020560185185185183,
      "loss": 0.9632,
      "step": 6360
    },
    {
      "epoch": 1.7694444444444444,
      "grad_norm": 1.316418170928955,
      "learning_rate": 0.0002051388888888889,
      "loss": 0.9997,
      "step": 6370
    },
    {
      "epoch": 1.7722222222222221,
      "grad_norm": 1.3677928447723389,
      "learning_rate": 0.00020467592592592594,
      "loss": 0.9577,
      "step": 6380
    },
    {
      "epoch": 1.775,
      "grad_norm": 1.575919508934021,
      "learning_rate": 0.00020421296296296295,
      "loss": 0.9983,
      "step": 6390
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 1.7637840509414673,
      "learning_rate": 0.00020375,
      "loss": 0.9059,
      "step": 6400
    },
    {
      "epoch": 1.7805555555555554,
      "grad_norm": 1.6261390447616577,
      "learning_rate": 0.00020328703703703706,
      "loss": 0.9719,
      "step": 6410
    },
    {
      "epoch": 1.7833333333333332,
      "grad_norm": 1.6572819948196411,
      "learning_rate": 0.00020282407407407407,
      "loss": 0.8932,
      "step": 6420
    },
    {
      "epoch": 1.786111111111111,
      "grad_norm": 1.6813769340515137,
      "learning_rate": 0.0002023611111111111,
      "loss": 0.9113,
      "step": 6430
    },
    {
      "epoch": 1.7888888888888888,
      "grad_norm": 1.4245966672897339,
      "learning_rate": 0.00020189814814814815,
      "loss": 0.9289,
      "step": 6440
    },
    {
      "epoch": 1.7916666666666665,
      "grad_norm": 1.15824556350708,
      "learning_rate": 0.0002014351851851852,
      "loss": 0.9346,
      "step": 6450
    },
    {
      "epoch": 1.7944444444444443,
      "grad_norm": 2.236802577972412,
      "learning_rate": 0.00020097222222222223,
      "loss": 0.9589,
      "step": 6460
    },
    {
      "epoch": 1.7972222222222223,
      "grad_norm": 1.3638439178466797,
      "learning_rate": 0.00020050925925925924,
      "loss": 0.898,
      "step": 6470
    },
    {
      "epoch": 1.8,
      "grad_norm": 1.4347141981124878,
      "learning_rate": 0.0002000462962962963,
      "loss": 0.9089,
      "step": 6480
    },
    {
      "epoch": 1.8027777777777778,
      "grad_norm": 1.3294563293457031,
      "learning_rate": 0.00019958333333333335,
      "loss": 1.0779,
      "step": 6490
    },
    {
      "epoch": 1.8055555555555556,
      "grad_norm": 5.057348728179932,
      "learning_rate": 0.00019912037037037036,
      "loss": 0.9788,
      "step": 6500
    },
    {
      "epoch": 1.8083333333333333,
      "grad_norm": 1.1680591106414795,
      "learning_rate": 0.0001986574074074074,
      "loss": 0.8415,
      "step": 6510
    },
    {
      "epoch": 1.8111111111111111,
      "grad_norm": 1.358420491218567,
      "learning_rate": 0.00019819444444444444,
      "loss": 0.9442,
      "step": 6520
    },
    {
      "epoch": 1.8138888888888889,
      "grad_norm": 0.99285489320755,
      "learning_rate": 0.00019773148148148148,
      "loss": 0.9485,
      "step": 6530
    },
    {
      "epoch": 1.8166666666666667,
      "grad_norm": 1.1726890802383423,
      "learning_rate": 0.00019726851851851852,
      "loss": 0.9401,
      "step": 6540
    },
    {
      "epoch": 1.8194444444444444,
      "grad_norm": 1.9068355560302734,
      "learning_rate": 0.00019680555555555556,
      "loss": 0.9494,
      "step": 6550
    },
    {
      "epoch": 1.8222222222222222,
      "grad_norm": 1.820510745048523,
      "learning_rate": 0.0001963425925925926,
      "loss": 0.9114,
      "step": 6560
    },
    {
      "epoch": 1.825,
      "grad_norm": 2.3119640350341797,
      "learning_rate": 0.00019587962962962964,
      "loss": 0.9844,
      "step": 6570
    },
    {
      "epoch": 1.8277777777777777,
      "grad_norm": 0.9709125757217407,
      "learning_rate": 0.00019541666666666666,
      "loss": 0.9601,
      "step": 6580
    },
    {
      "epoch": 1.8305555555555557,
      "grad_norm": 1.1395502090454102,
      "learning_rate": 0.00019495370370370372,
      "loss": 0.9115,
      "step": 6590
    },
    {
      "epoch": 1.8333333333333335,
      "grad_norm": 1.1984353065490723,
      "learning_rate": 0.00019449074074074073,
      "loss": 0.9936,
      "step": 6600
    },
    {
      "epoch": 1.8361111111111112,
      "grad_norm": 1.7422553300857544,
      "learning_rate": 0.00019402777777777777,
      "loss": 0.9205,
      "step": 6610
    },
    {
      "epoch": 1.838888888888889,
      "grad_norm": 1.2797788381576538,
      "learning_rate": 0.00019356481481481484,
      "loss": 0.8706,
      "step": 6620
    },
    {
      "epoch": 1.8416666666666668,
      "grad_norm": 1.6122286319732666,
      "learning_rate": 0.00019310185185185185,
      "loss": 0.9597,
      "step": 6630
    },
    {
      "epoch": 1.8444444444444446,
      "grad_norm": 1.3270782232284546,
      "learning_rate": 0.0001926388888888889,
      "loss": 0.9429,
      "step": 6640
    },
    {
      "epoch": 1.8472222222222223,
      "grad_norm": 1.6065047979354858,
      "learning_rate": 0.00019217592592592593,
      "loss": 0.9085,
      "step": 6650
    },
    {
      "epoch": 1.85,
      "grad_norm": 1.2782334089279175,
      "learning_rate": 0.00019171296296296297,
      "loss": 0.9517,
      "step": 6660
    },
    {
      "epoch": 1.8527777777777779,
      "grad_norm": 1.3576416969299316,
      "learning_rate": 0.00019125000000000001,
      "loss": 0.9758,
      "step": 6670
    },
    {
      "epoch": 1.8555555555555556,
      "grad_norm": 0.8864341974258423,
      "learning_rate": 0.00019078703703703703,
      "loss": 0.8168,
      "step": 6680
    },
    {
      "epoch": 1.8583333333333334,
      "grad_norm": 1.8151633739471436,
      "learning_rate": 0.00019032407407407407,
      "loss": 1.0056,
      "step": 6690
    },
    {
      "epoch": 1.8611111111111112,
      "grad_norm": 1.4235275983810425,
      "learning_rate": 0.00018986111111111113,
      "loss": 0.9075,
      "step": 6700
    },
    {
      "epoch": 1.863888888888889,
      "grad_norm": 1.7337113618850708,
      "learning_rate": 0.00018939814814814815,
      "loss": 0.9467,
      "step": 6710
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 1.403464913368225,
      "learning_rate": 0.00018893518518518519,
      "loss": 0.9286,
      "step": 6720
    },
    {
      "epoch": 1.8694444444444445,
      "grad_norm": 1.2560675144195557,
      "learning_rate": 0.00018847222222222225,
      "loss": 0.9526,
      "step": 6730
    },
    {
      "epoch": 1.8722222222222222,
      "grad_norm": 2.4781453609466553,
      "learning_rate": 0.00018800925925925927,
      "loss": 0.915,
      "step": 6740
    },
    {
      "epoch": 1.875,
      "grad_norm": 1.2554826736450195,
      "learning_rate": 0.0001875462962962963,
      "loss": 0.9328,
      "step": 6750
    },
    {
      "epoch": 1.8777777777777778,
      "grad_norm": 1.586047649383545,
      "learning_rate": 0.00018708333333333332,
      "loss": 0.934,
      "step": 6760
    },
    {
      "epoch": 1.8805555555555555,
      "grad_norm": 1.9280660152435303,
      "learning_rate": 0.00018662037037037039,
      "loss": 1.1161,
      "step": 6770
    },
    {
      "epoch": 1.8833333333333333,
      "grad_norm": 1.093762993812561,
      "learning_rate": 0.00018615740740740742,
      "loss": 0.9283,
      "step": 6780
    },
    {
      "epoch": 1.886111111111111,
      "grad_norm": 1.4158403873443604,
      "learning_rate": 0.00018569444444444444,
      "loss": 1.0187,
      "step": 6790
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 1.715380072593689,
      "learning_rate": 0.00018523148148148148,
      "loss": 0.9735,
      "step": 6800
    },
    {
      "epoch": 1.8916666666666666,
      "grad_norm": 1.4719117879867554,
      "learning_rate": 0.00018476851851851852,
      "loss": 0.9495,
      "step": 6810
    },
    {
      "epoch": 1.8944444444444444,
      "grad_norm": 1.2146214246749878,
      "learning_rate": 0.00018430555555555556,
      "loss": 0.8945,
      "step": 6820
    },
    {
      "epoch": 1.8972222222222221,
      "grad_norm": 1.516519546508789,
      "learning_rate": 0.0001838425925925926,
      "loss": 0.9594,
      "step": 6830
    },
    {
      "epoch": 1.9,
      "grad_norm": 1.3733536005020142,
      "learning_rate": 0.00018337962962962964,
      "loss": 0.8344,
      "step": 6840
    },
    {
      "epoch": 1.9027777777777777,
      "grad_norm": 1.5006849765777588,
      "learning_rate": 0.00018291666666666668,
      "loss": 0.9652,
      "step": 6850
    },
    {
      "epoch": 1.9055555555555554,
      "grad_norm": 1.5700833797454834,
      "learning_rate": 0.00018245370370370372,
      "loss": 0.9256,
      "step": 6860
    },
    {
      "epoch": 1.9083333333333332,
      "grad_norm": 2.2767159938812256,
      "learning_rate": 0.00018199074074074073,
      "loss": 0.9822,
      "step": 6870
    },
    {
      "epoch": 1.911111111111111,
      "grad_norm": 1.8161687850952148,
      "learning_rate": 0.0001815277777777778,
      "loss": 0.9215,
      "step": 6880
    },
    {
      "epoch": 1.9138888888888888,
      "grad_norm": 1.3605189323425293,
      "learning_rate": 0.0001810648148148148,
      "loss": 0.9624,
      "step": 6890
    },
    {
      "epoch": 1.9166666666666665,
      "grad_norm": 1.7117815017700195,
      "learning_rate": 0.00018060185185185185,
      "loss": 1.0133,
      "step": 6900
    },
    {
      "epoch": 1.9194444444444443,
      "grad_norm": 1.7238575220108032,
      "learning_rate": 0.0001801388888888889,
      "loss": 0.9781,
      "step": 6910
    },
    {
      "epoch": 1.9222222222222223,
      "grad_norm": 1.2633343935012817,
      "learning_rate": 0.00017967592592592593,
      "loss": 0.95,
      "step": 6920
    },
    {
      "epoch": 1.925,
      "grad_norm": 1.2317317724227905,
      "learning_rate": 0.00017921296296296297,
      "loss": 0.9522,
      "step": 6930
    },
    {
      "epoch": 1.9277777777777778,
      "grad_norm": 1.1122183799743652,
      "learning_rate": 0.00017875,
      "loss": 0.9588,
      "step": 6940
    },
    {
      "epoch": 1.9305555555555556,
      "grad_norm": 1.3761215209960938,
      "learning_rate": 0.00017828703703703705,
      "loss": 0.962,
      "step": 6950
    },
    {
      "epoch": 1.9333333333333333,
      "grad_norm": 1.2222563028335571,
      "learning_rate": 0.0001778240740740741,
      "loss": 0.8972,
      "step": 6960
    },
    {
      "epoch": 1.9361111111111111,
      "grad_norm": 1.3194222450256348,
      "learning_rate": 0.0001773611111111111,
      "loss": 0.9837,
      "step": 6970
    },
    {
      "epoch": 1.9388888888888889,
      "grad_norm": 1.1624988317489624,
      "learning_rate": 0.00017689814814814814,
      "loss": 0.9461,
      "step": 6980
    },
    {
      "epoch": 1.9416666666666667,
      "grad_norm": 1.853593111038208,
      "learning_rate": 0.0001764351851851852,
      "loss": 0.951,
      "step": 6990
    },
    {
      "epoch": 1.9444444444444444,
      "grad_norm": 2.4312195777893066,
      "learning_rate": 0.00017597222222222222,
      "loss": 0.9394,
      "step": 7000
    },
    {
      "epoch": 1.9472222222222222,
      "grad_norm": 1.9934722185134888,
      "learning_rate": 0.00017550925925925926,
      "loss": 1.0109,
      "step": 7010
    },
    {
      "epoch": 1.95,
      "grad_norm": 1.1242152452468872,
      "learning_rate": 0.0001750462962962963,
      "loss": 0.937,
      "step": 7020
    },
    {
      "epoch": 1.9527777777777777,
      "grad_norm": 1.685095191001892,
      "learning_rate": 0.00017458333333333334,
      "loss": 0.9635,
      "step": 7030
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 1.5207562446594238,
      "learning_rate": 0.00017412037037037038,
      "loss": 0.9589,
      "step": 7040
    },
    {
      "epoch": 1.9583333333333335,
      "grad_norm": 1.1054476499557495,
      "learning_rate": 0.0001736574074074074,
      "loss": 0.906,
      "step": 7050
    },
    {
      "epoch": 1.9611111111111112,
      "grad_norm": 1.108997106552124,
      "learning_rate": 0.00017319444444444446,
      "loss": 0.9312,
      "step": 7060
    },
    {
      "epoch": 1.963888888888889,
      "grad_norm": 1.1059558391571045,
      "learning_rate": 0.0001727314814814815,
      "loss": 0.8684,
      "step": 7070
    },
    {
      "epoch": 1.9666666666666668,
      "grad_norm": 1.187538743019104,
      "learning_rate": 0.0001722685185185185,
      "loss": 0.9168,
      "step": 7080
    },
    {
      "epoch": 1.9694444444444446,
      "grad_norm": 1.3543130159378052,
      "learning_rate": 0.00017180555555555555,
      "loss": 0.9922,
      "step": 7090
    },
    {
      "epoch": 1.9722222222222223,
      "grad_norm": 1.7785357236862183,
      "learning_rate": 0.00017134259259259262,
      "loss": 0.9836,
      "step": 7100
    },
    {
      "epoch": 1.975,
      "grad_norm": 1.218937635421753,
      "learning_rate": 0.00017087962962962963,
      "loss": 0.906,
      "step": 7110
    },
    {
      "epoch": 1.9777777777777779,
      "grad_norm": 2.2780752182006836,
      "learning_rate": 0.00017041666666666667,
      "loss": 1.0226,
      "step": 7120
    },
    {
      "epoch": 1.9805555555555556,
      "grad_norm": 1.708168864250183,
      "learning_rate": 0.00016995370370370368,
      "loss": 1.0026,
      "step": 7130
    },
    {
      "epoch": 1.9833333333333334,
      "grad_norm": 1.2138097286224365,
      "learning_rate": 0.00016949074074074075,
      "loss": 0.8632,
      "step": 7140
    },
    {
      "epoch": 1.9861111111111112,
      "grad_norm": 1.6897112131118774,
      "learning_rate": 0.0001690277777777778,
      "loss": 1.0769,
      "step": 7150
    },
    {
      "epoch": 1.988888888888889,
      "grad_norm": 1.169217824935913,
      "learning_rate": 0.0001685648148148148,
      "loss": 0.8864,
      "step": 7160
    },
    {
      "epoch": 1.9916666666666667,
      "grad_norm": 1.4421831369400024,
      "learning_rate": 0.00016810185185185187,
      "loss": 0.8997,
      "step": 7170
    },
    {
      "epoch": 1.9944444444444445,
      "grad_norm": 2.6442372798919678,
      "learning_rate": 0.00016763888888888888,
      "loss": 1.011,
      "step": 7180
    },
    {
      "epoch": 1.9972222222222222,
      "grad_norm": 1.7957165241241455,
      "learning_rate": 0.00016717592592592592,
      "loss": 0.9657,
      "step": 7190
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.324308156967163,
      "learning_rate": 0.00016671296296296296,
      "loss": 0.9598,
      "step": 7200
    },
    {
      "epoch": 2.0027777777777778,
      "grad_norm": 1.1909788846969604,
      "learning_rate": 0.00016625,
      "loss": 0.9291,
      "step": 7210
    },
    {
      "epoch": 2.0055555555555555,
      "grad_norm": 0.949760913848877,
      "learning_rate": 0.00016578703703703704,
      "loss": 0.9812,
      "step": 7220
    },
    {
      "epoch": 2.0083333333333333,
      "grad_norm": 1.1197692155838013,
      "learning_rate": 0.00016532407407407408,
      "loss": 0.8388,
      "step": 7230
    },
    {
      "epoch": 2.011111111111111,
      "grad_norm": 1.5945738554000854,
      "learning_rate": 0.00016486111111111112,
      "loss": 0.9515,
      "step": 7240
    },
    {
      "epoch": 2.013888888888889,
      "grad_norm": 2.1710352897644043,
      "learning_rate": 0.00016439814814814816,
      "loss": 0.9024,
      "step": 7250
    },
    {
      "epoch": 2.0166666666666666,
      "grad_norm": 1.7795319557189941,
      "learning_rate": 0.00016393518518518517,
      "loss": 0.9872,
      "step": 7260
    },
    {
      "epoch": 2.0194444444444444,
      "grad_norm": 1.2567710876464844,
      "learning_rate": 0.00016347222222222221,
      "loss": 0.9666,
      "step": 7270
    },
    {
      "epoch": 2.022222222222222,
      "grad_norm": 2.1864850521087646,
      "learning_rate": 0.00016300925925925928,
      "loss": 0.8713,
      "step": 7280
    },
    {
      "epoch": 2.025,
      "grad_norm": 1.7348273992538452,
      "learning_rate": 0.0001625462962962963,
      "loss": 0.9755,
      "step": 7290
    },
    {
      "epoch": 2.0277777777777777,
      "grad_norm": 1.3216451406478882,
      "learning_rate": 0.00016208333333333333,
      "loss": 0.8414,
      "step": 7300
    },
    {
      "epoch": 2.0305555555555554,
      "grad_norm": 1.3069146871566772,
      "learning_rate": 0.00016162037037037037,
      "loss": 1.0215,
      "step": 7310
    },
    {
      "epoch": 2.033333333333333,
      "grad_norm": 1.9729983806610107,
      "learning_rate": 0.0001611574074074074,
      "loss": 0.8322,
      "step": 7320
    },
    {
      "epoch": 2.036111111111111,
      "grad_norm": 1.385109782218933,
      "learning_rate": 0.00016069444444444445,
      "loss": 0.9146,
      "step": 7330
    },
    {
      "epoch": 2.0388888888888888,
      "grad_norm": 1.1941933631896973,
      "learning_rate": 0.00016023148148148147,
      "loss": 0.9103,
      "step": 7340
    },
    {
      "epoch": 2.0416666666666665,
      "grad_norm": 1.8060933351516724,
      "learning_rate": 0.00015976851851851853,
      "loss": 0.8709,
      "step": 7350
    },
    {
      "epoch": 2.0444444444444443,
      "grad_norm": 1.2453042268753052,
      "learning_rate": 0.00015930555555555557,
      "loss": 0.9323,
      "step": 7360
    },
    {
      "epoch": 2.047222222222222,
      "grad_norm": 1.2247713804244995,
      "learning_rate": 0.00015884259259259259,
      "loss": 0.9165,
      "step": 7370
    },
    {
      "epoch": 2.05,
      "grad_norm": 1.3692983388900757,
      "learning_rate": 0.00015837962962962963,
      "loss": 0.9943,
      "step": 7380
    },
    {
      "epoch": 2.0527777777777776,
      "grad_norm": 1.91129732131958,
      "learning_rate": 0.0001579166666666667,
      "loss": 0.943,
      "step": 7390
    },
    {
      "epoch": 2.0555555555555554,
      "grad_norm": 1.1260806322097778,
      "learning_rate": 0.0001574537037037037,
      "loss": 0.9218,
      "step": 7400
    },
    {
      "epoch": 2.058333333333333,
      "grad_norm": 1.2632062435150146,
      "learning_rate": 0.00015699074074074074,
      "loss": 0.9646,
      "step": 7410
    },
    {
      "epoch": 2.061111111111111,
      "grad_norm": 1.2101714611053467,
      "learning_rate": 0.00015652777777777776,
      "loss": 0.8207,
      "step": 7420
    },
    {
      "epoch": 2.063888888888889,
      "grad_norm": 2.164452075958252,
      "learning_rate": 0.00015606481481481482,
      "loss": 0.8759,
      "step": 7430
    },
    {
      "epoch": 2.066666666666667,
      "grad_norm": 1.27642822265625,
      "learning_rate": 0.00015560185185185186,
      "loss": 0.9887,
      "step": 7440
    },
    {
      "epoch": 2.0694444444444446,
      "grad_norm": 1.807470679283142,
      "learning_rate": 0.00015513888888888888,
      "loss": 0.9959,
      "step": 7450
    },
    {
      "epoch": 2.0722222222222224,
      "grad_norm": 1.5691121816635132,
      "learning_rate": 0.00015467592592592594,
      "loss": 0.9327,
      "step": 7460
    },
    {
      "epoch": 2.075,
      "grad_norm": 1.0859582424163818,
      "learning_rate": 0.00015421296296296298,
      "loss": 0.9911,
      "step": 7470
    },
    {
      "epoch": 2.077777777777778,
      "grad_norm": 1.6754738092422485,
      "learning_rate": 0.00015375,
      "loss": 0.9572,
      "step": 7480
    },
    {
      "epoch": 2.0805555555555557,
      "grad_norm": 1.8494902849197388,
      "learning_rate": 0.00015328703703703704,
      "loss": 0.9734,
      "step": 7490
    },
    {
      "epoch": 2.0833333333333335,
      "grad_norm": 1.5237809419631958,
      "learning_rate": 0.00015282407407407408,
      "loss": 0.9978,
      "step": 7500
    },
    {
      "epoch": 2.0861111111111112,
      "grad_norm": 1.7465823888778687,
      "learning_rate": 0.00015236111111111112,
      "loss": 0.957,
      "step": 7510
    },
    {
      "epoch": 2.088888888888889,
      "grad_norm": 1.6900144815444946,
      "learning_rate": 0.00015189814814814816,
      "loss": 0.9837,
      "step": 7520
    },
    {
      "epoch": 2.091666666666667,
      "grad_norm": 1.662278413772583,
      "learning_rate": 0.00015143518518518517,
      "loss": 0.9676,
      "step": 7530
    },
    {
      "epoch": 2.0944444444444446,
      "grad_norm": 0.9819433093070984,
      "learning_rate": 0.00015097222222222224,
      "loss": 0.8317,
      "step": 7540
    },
    {
      "epoch": 2.0972222222222223,
      "grad_norm": 0.9628384709358215,
      "learning_rate": 0.00015050925925925928,
      "loss": 0.941,
      "step": 7550
    },
    {
      "epoch": 2.1,
      "grad_norm": 1.2128236293792725,
      "learning_rate": 0.0001500462962962963,
      "loss": 0.9534,
      "step": 7560
    },
    {
      "epoch": 2.102777777777778,
      "grad_norm": 1.5679210424423218,
      "learning_rate": 0.00014958333333333336,
      "loss": 1.0221,
      "step": 7570
    },
    {
      "epoch": 2.1055555555555556,
      "grad_norm": 1.1530274152755737,
      "learning_rate": 0.00014912037037037037,
      "loss": 0.9419,
      "step": 7580
    },
    {
      "epoch": 2.1083333333333334,
      "grad_norm": 2.052863836288452,
      "learning_rate": 0.0001486574074074074,
      "loss": 1.0184,
      "step": 7590
    },
    {
      "epoch": 2.111111111111111,
      "grad_norm": 1.8879469633102417,
      "learning_rate": 0.00014819444444444445,
      "loss": 0.8999,
      "step": 7600
    },
    {
      "epoch": 2.113888888888889,
      "grad_norm": 1.6962218284606934,
      "learning_rate": 0.0001477314814814815,
      "loss": 0.9085,
      "step": 7610
    },
    {
      "epoch": 2.1166666666666667,
      "grad_norm": 1.4075024127960205,
      "learning_rate": 0.00014726851851851853,
      "loss": 0.9691,
      "step": 7620
    },
    {
      "epoch": 2.1194444444444445,
      "grad_norm": 2.0006370544433594,
      "learning_rate": 0.00014680555555555554,
      "loss": 0.9418,
      "step": 7630
    },
    {
      "epoch": 2.1222222222222222,
      "grad_norm": 1.4902058839797974,
      "learning_rate": 0.00014634259259259258,
      "loss": 0.9612,
      "step": 7640
    },
    {
      "epoch": 2.125,
      "grad_norm": 2.104703187942505,
      "learning_rate": 0.00014587962962962965,
      "loss": 0.9814,
      "step": 7650
    },
    {
      "epoch": 2.1277777777777778,
      "grad_norm": 1.697593331336975,
      "learning_rate": 0.00014541666666666666,
      "loss": 0.9716,
      "step": 7660
    },
    {
      "epoch": 2.1305555555555555,
      "grad_norm": 1.2376673221588135,
      "learning_rate": 0.0001449537037037037,
      "loss": 0.8289,
      "step": 7670
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 1.371238350868225,
      "learning_rate": 0.00014449074074074077,
      "loss": 0.9818,
      "step": 7680
    },
    {
      "epoch": 2.136111111111111,
      "grad_norm": 0.9535771608352661,
      "learning_rate": 0.00014402777777777778,
      "loss": 0.9799,
      "step": 7690
    },
    {
      "epoch": 2.138888888888889,
      "grad_norm": 1.3216558694839478,
      "learning_rate": 0.00014356481481481482,
      "loss": 0.9976,
      "step": 7700
    },
    {
      "epoch": 2.1416666666666666,
      "grad_norm": 1.449402093887329,
      "learning_rate": 0.00014310185185185183,
      "loss": 0.8464,
      "step": 7710
    },
    {
      "epoch": 2.1444444444444444,
      "grad_norm": 2.4958994388580322,
      "learning_rate": 0.0001426388888888889,
      "loss": 1.0321,
      "step": 7720
    },
    {
      "epoch": 2.147222222222222,
      "grad_norm": 1.4955910444259644,
      "learning_rate": 0.00014217592592592594,
      "loss": 0.8625,
      "step": 7730
    },
    {
      "epoch": 2.15,
      "grad_norm": 1.2172560691833496,
      "learning_rate": 0.00014171296296296295,
      "loss": 0.9466,
      "step": 7740
    },
    {
      "epoch": 2.1527777777777777,
      "grad_norm": 1.0985116958618164,
      "learning_rate": 0.00014125,
      "loss": 0.9006,
      "step": 7750
    },
    {
      "epoch": 2.1555555555555554,
      "grad_norm": 1.2127355337142944,
      "learning_rate": 0.00014078703703703706,
      "loss": 0.8278,
      "step": 7760
    },
    {
      "epoch": 2.158333333333333,
      "grad_norm": 1.1063882112503052,
      "learning_rate": 0.00014032407407407407,
      "loss": 0.9541,
      "step": 7770
    },
    {
      "epoch": 2.161111111111111,
      "grad_norm": 1.03187894821167,
      "learning_rate": 0.0001398611111111111,
      "loss": 0.8838,
      "step": 7780
    },
    {
      "epoch": 2.1638888888888888,
      "grad_norm": 4.015143871307373,
      "learning_rate": 0.00013939814814814815,
      "loss": 0.9723,
      "step": 7790
    },
    {
      "epoch": 2.1666666666666665,
      "grad_norm": 1.202360987663269,
      "learning_rate": 0.0001389351851851852,
      "loss": 0.9964,
      "step": 7800
    },
    {
      "epoch": 2.1694444444444443,
      "grad_norm": 1.2368515729904175,
      "learning_rate": 0.00013847222222222223,
      "loss": 0.8993,
      "step": 7810
    },
    {
      "epoch": 2.172222222222222,
      "grad_norm": 1.1974371671676636,
      "learning_rate": 0.00013800925925925924,
      "loss": 0.7931,
      "step": 7820
    },
    {
      "epoch": 2.175,
      "grad_norm": 1.291899561882019,
      "learning_rate": 0.0001375462962962963,
      "loss": 0.886,
      "step": 7830
    },
    {
      "epoch": 2.1777777777777776,
      "grad_norm": 1.3248554468154907,
      "learning_rate": 0.00013708333333333335,
      "loss": 0.8527,
      "step": 7840
    },
    {
      "epoch": 2.1805555555555554,
      "grad_norm": 1.6508212089538574,
      "learning_rate": 0.00013662037037037036,
      "loss": 0.9885,
      "step": 7850
    },
    {
      "epoch": 2.183333333333333,
      "grad_norm": 2.2929489612579346,
      "learning_rate": 0.0001361574074074074,
      "loss": 0.9325,
      "step": 7860
    },
    {
      "epoch": 2.186111111111111,
      "grad_norm": 1.1897974014282227,
      "learning_rate": 0.00013569444444444444,
      "loss": 0.9076,
      "step": 7870
    },
    {
      "epoch": 2.188888888888889,
      "grad_norm": 2.1259067058563232,
      "learning_rate": 0.00013523148148148148,
      "loss": 0.9458,
      "step": 7880
    },
    {
      "epoch": 2.191666666666667,
      "grad_norm": 1.6147406101226807,
      "learning_rate": 0.00013476851851851852,
      "loss": 0.849,
      "step": 7890
    },
    {
      "epoch": 2.1944444444444446,
      "grad_norm": 1.7357791662216187,
      "learning_rate": 0.00013430555555555556,
      "loss": 0.9943,
      "step": 7900
    },
    {
      "epoch": 2.1972222222222224,
      "grad_norm": 1.1775684356689453,
      "learning_rate": 0.0001338425925925926,
      "loss": 0.9054,
      "step": 7910
    },
    {
      "epoch": 2.2,
      "grad_norm": 1.6360434293746948,
      "learning_rate": 0.00013337962962962964,
      "loss": 0.9429,
      "step": 7920
    },
    {
      "epoch": 2.202777777777778,
      "grad_norm": 1.1337906122207642,
      "learning_rate": 0.00013291666666666665,
      "loss": 0.8854,
      "step": 7930
    },
    {
      "epoch": 2.2055555555555557,
      "grad_norm": 1.8513528108596802,
      "learning_rate": 0.00013245370370370372,
      "loss": 0.9201,
      "step": 7940
    },
    {
      "epoch": 2.2083333333333335,
      "grad_norm": 1.3622767925262451,
      "learning_rate": 0.00013199074074074073,
      "loss": 0.7773,
      "step": 7950
    },
    {
      "epoch": 2.2111111111111112,
      "grad_norm": 2.090229034423828,
      "learning_rate": 0.00013152777777777777,
      "loss": 0.8821,
      "step": 7960
    },
    {
      "epoch": 2.213888888888889,
      "grad_norm": 1.80543053150177,
      "learning_rate": 0.00013106481481481484,
      "loss": 0.9519,
      "step": 7970
    },
    {
      "epoch": 2.216666666666667,
      "grad_norm": 1.3595492839813232,
      "learning_rate": 0.00013060185185185185,
      "loss": 1.0148,
      "step": 7980
    },
    {
      "epoch": 2.2194444444444446,
      "grad_norm": 1.5269256830215454,
      "learning_rate": 0.0001301388888888889,
      "loss": 0.8432,
      "step": 7990
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 1.7821258306503296,
      "learning_rate": 0.00012967592592592593,
      "loss": 0.9013,
      "step": 8000
    },
    {
      "epoch": 2.225,
      "grad_norm": 1.343567132949829,
      "learning_rate": 0.00012921296296296297,
      "loss": 0.9956,
      "step": 8010
    },
    {
      "epoch": 2.227777777777778,
      "grad_norm": 1.0746407508850098,
      "learning_rate": 0.00012875,
      "loss": 0.9361,
      "step": 8020
    },
    {
      "epoch": 2.2305555555555556,
      "grad_norm": 1.2110973596572876,
      "learning_rate": 0.00012828703703703703,
      "loss": 0.9963,
      "step": 8030
    },
    {
      "epoch": 2.2333333333333334,
      "grad_norm": 0.9915282726287842,
      "learning_rate": 0.00012782407407407407,
      "loss": 0.9134,
      "step": 8040
    },
    {
      "epoch": 2.236111111111111,
      "grad_norm": 1.4776405096054077,
      "learning_rate": 0.00012736111111111113,
      "loss": 0.9926,
      "step": 8050
    },
    {
      "epoch": 2.238888888888889,
      "grad_norm": 1.124361515045166,
      "learning_rate": 0.00012689814814814814,
      "loss": 0.9874,
      "step": 8060
    },
    {
      "epoch": 2.2416666666666667,
      "grad_norm": 1.1763914823532104,
      "learning_rate": 0.00012643518518518518,
      "loss": 0.9182,
      "step": 8070
    },
    {
      "epoch": 2.2444444444444445,
      "grad_norm": 1.6771796941757202,
      "learning_rate": 0.00012597222222222225,
      "loss": 0.8833,
      "step": 8080
    },
    {
      "epoch": 2.2472222222222222,
      "grad_norm": 1.083120584487915,
      "learning_rate": 0.00012550925925925926,
      "loss": 0.8658,
      "step": 8090
    },
    {
      "epoch": 2.25,
      "grad_norm": 0.9693548083305359,
      "learning_rate": 0.0001250462962962963,
      "loss": 0.9525,
      "step": 8100
    },
    {
      "epoch": 2.2527777777777778,
      "grad_norm": 1.4301974773406982,
      "learning_rate": 0.00012458333333333334,
      "loss": 0.9665,
      "step": 8110
    },
    {
      "epoch": 2.2555555555555555,
      "grad_norm": 1.433249592781067,
      "learning_rate": 0.00012412037037037036,
      "loss": 0.8387,
      "step": 8120
    },
    {
      "epoch": 2.2583333333333333,
      "grad_norm": 1.1536623239517212,
      "learning_rate": 0.00012365740740740742,
      "loss": 0.8995,
      "step": 8130
    },
    {
      "epoch": 2.261111111111111,
      "grad_norm": 1.2351047992706299,
      "learning_rate": 0.00012319444444444444,
      "loss": 0.8765,
      "step": 8140
    },
    {
      "epoch": 2.263888888888889,
      "grad_norm": 0.9400542974472046,
      "learning_rate": 0.00012273148148148148,
      "loss": 0.9048,
      "step": 8150
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 1.4397588968276978,
      "learning_rate": 0.00012226851851851852,
      "loss": 0.8678,
      "step": 8160
    },
    {
      "epoch": 2.2694444444444444,
      "grad_norm": 1.71975576877594,
      "learning_rate": 0.00012180555555555556,
      "loss": 0.9386,
      "step": 8170
    },
    {
      "epoch": 2.272222222222222,
      "grad_norm": 1.2160966396331787,
      "learning_rate": 0.0001213425925925926,
      "loss": 0.9331,
      "step": 8180
    },
    {
      "epoch": 2.275,
      "grad_norm": 2.350905656814575,
      "learning_rate": 0.00012087962962962964,
      "loss": 0.9144,
      "step": 8190
    },
    {
      "epoch": 2.2777777777777777,
      "grad_norm": 1.092485785484314,
      "learning_rate": 0.00012041666666666668,
      "loss": 0.933,
      "step": 8200
    },
    {
      "epoch": 2.2805555555555554,
      "grad_norm": 1.3231604099273682,
      "learning_rate": 0.0001199537037037037,
      "loss": 0.8879,
      "step": 8210
    },
    {
      "epoch": 2.283333333333333,
      "grad_norm": 1.0068001747131348,
      "learning_rate": 0.00011949074074074074,
      "loss": 0.9605,
      "step": 8220
    },
    {
      "epoch": 2.286111111111111,
      "grad_norm": 1.4145854711532593,
      "learning_rate": 0.00011902777777777778,
      "loss": 0.8615,
      "step": 8230
    },
    {
      "epoch": 2.2888888888888888,
      "grad_norm": 2.576486349105835,
      "learning_rate": 0.00011856481481481482,
      "loss": 0.9296,
      "step": 8240
    },
    {
      "epoch": 2.2916666666666665,
      "grad_norm": 1.4322850704193115,
      "learning_rate": 0.00011810185185185185,
      "loss": 0.9641,
      "step": 8250
    },
    {
      "epoch": 2.2944444444444443,
      "grad_norm": 1.9321680068969727,
      "learning_rate": 0.00011763888888888889,
      "loss": 0.9797,
      "step": 8260
    },
    {
      "epoch": 2.297222222222222,
      "grad_norm": 3.8264544010162354,
      "learning_rate": 0.00011717592592592593,
      "loss": 0.9568,
      "step": 8270
    },
    {
      "epoch": 2.3,
      "grad_norm": 1.4211586713790894,
      "learning_rate": 0.00011671296296296297,
      "loss": 0.9595,
      "step": 8280
    },
    {
      "epoch": 2.3027777777777776,
      "grad_norm": 1.0299413204193115,
      "learning_rate": 0.00011625,
      "loss": 0.9389,
      "step": 8290
    },
    {
      "epoch": 2.3055555555555554,
      "grad_norm": 2.2317800521850586,
      "learning_rate": 0.00011578703703703703,
      "loss": 0.9589,
      "step": 8300
    },
    {
      "epoch": 2.3083333333333336,
      "grad_norm": 1.694348931312561,
      "learning_rate": 0.00011532407407407409,
      "loss": 0.9612,
      "step": 8310
    },
    {
      "epoch": 2.311111111111111,
      "grad_norm": 0.6873835921287537,
      "learning_rate": 0.00011486111111111111,
      "loss": 0.9244,
      "step": 8320
    },
    {
      "epoch": 2.313888888888889,
      "grad_norm": 1.4031318426132202,
      "learning_rate": 0.00011439814814814815,
      "loss": 0.9706,
      "step": 8330
    },
    {
      "epoch": 2.3166666666666664,
      "grad_norm": 1.980040192604065,
      "learning_rate": 0.00011393518518518518,
      "loss": 0.8903,
      "step": 8340
    },
    {
      "epoch": 2.3194444444444446,
      "grad_norm": 2.4636435508728027,
      "learning_rate": 0.00011347222222222223,
      "loss": 0.9566,
      "step": 8350
    },
    {
      "epoch": 2.3222222222222224,
      "grad_norm": 1.236715316772461,
      "learning_rate": 0.00011300925925925926,
      "loss": 0.9766,
      "step": 8360
    },
    {
      "epoch": 2.325,
      "grad_norm": 1.0002329349517822,
      "learning_rate": 0.0001125462962962963,
      "loss": 0.9762,
      "step": 8370
    },
    {
      "epoch": 2.327777777777778,
      "grad_norm": 1.2417616844177246,
      "learning_rate": 0.00011208333333333332,
      "loss": 0.9004,
      "step": 8380
    },
    {
      "epoch": 2.3305555555555557,
      "grad_norm": 1.1861152648925781,
      "learning_rate": 0.00011162037037037038,
      "loss": 0.9697,
      "step": 8390
    },
    {
      "epoch": 2.3333333333333335,
      "grad_norm": 1.1607614755630493,
      "learning_rate": 0.00011115740740740742,
      "loss": 0.9713,
      "step": 8400
    },
    {
      "epoch": 2.3361111111111112,
      "grad_norm": 1.115474820137024,
      "learning_rate": 0.00011069444444444444,
      "loss": 0.9058,
      "step": 8410
    },
    {
      "epoch": 2.338888888888889,
      "grad_norm": 1.5065513849258423,
      "learning_rate": 0.00011023148148148148,
      "loss": 0.9603,
      "step": 8420
    },
    {
      "epoch": 2.341666666666667,
      "grad_norm": 0.9988471865653992,
      "learning_rate": 0.00010976851851851852,
      "loss": 0.9262,
      "step": 8430
    },
    {
      "epoch": 2.3444444444444446,
      "grad_norm": 1.3542544841766357,
      "learning_rate": 0.00010930555555555556,
      "loss": 0.821,
      "step": 8440
    },
    {
      "epoch": 2.3472222222222223,
      "grad_norm": 1.1355957984924316,
      "learning_rate": 0.00010884259259259259,
      "loss": 0.9068,
      "step": 8450
    },
    {
      "epoch": 2.35,
      "grad_norm": 2.6716933250427246,
      "learning_rate": 0.00010837962962962963,
      "loss": 0.8966,
      "step": 8460
    },
    {
      "epoch": 2.352777777777778,
      "grad_norm": 1.3689218759536743,
      "learning_rate": 0.00010791666666666667,
      "loss": 0.8428,
      "step": 8470
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 1.3319694995880127,
      "learning_rate": 0.00010745370370370371,
      "loss": 1.0427,
      "step": 8480
    },
    {
      "epoch": 2.3583333333333334,
      "grad_norm": 1.4146091938018799,
      "learning_rate": 0.00010699074074074075,
      "loss": 0.9123,
      "step": 8490
    },
    {
      "epoch": 2.361111111111111,
      "grad_norm": 0.5315817594528198,
      "learning_rate": 0.00010652777777777778,
      "loss": 0.8057,
      "step": 8500
    },
    {
      "epoch": 2.363888888888889,
      "grad_norm": 1.6087932586669922,
      "learning_rate": 0.00010606481481481483,
      "loss": 0.9446,
      "step": 8510
    },
    {
      "epoch": 2.3666666666666667,
      "grad_norm": 1.9821605682373047,
      "learning_rate": 0.00010560185185185186,
      "loss": 0.9256,
      "step": 8520
    },
    {
      "epoch": 2.3694444444444445,
      "grad_norm": 1.6039605140686035,
      "learning_rate": 0.0001051388888888889,
      "loss": 0.9546,
      "step": 8530
    },
    {
      "epoch": 2.3722222222222222,
      "grad_norm": 2.477123975753784,
      "learning_rate": 0.00010467592592592592,
      "loss": 1.007,
      "step": 8540
    },
    {
      "epoch": 2.375,
      "grad_norm": 1.719655990600586,
      "learning_rate": 0.00010421296296296296,
      "loss": 0.9274,
      "step": 8550
    },
    {
      "epoch": 2.3777777777777778,
      "grad_norm": 1.576402187347412,
      "learning_rate": 0.00010375,
      "loss": 0.9392,
      "step": 8560
    },
    {
      "epoch": 2.3805555555555555,
      "grad_norm": 1.5468165874481201,
      "learning_rate": 0.00010328703703703704,
      "loss": 0.9118,
      "step": 8570
    },
    {
      "epoch": 2.3833333333333333,
      "grad_norm": 1.876474142074585,
      "learning_rate": 0.00010282407407407407,
      "loss": 1.0078,
      "step": 8580
    },
    {
      "epoch": 2.386111111111111,
      "grad_norm": 1.2736098766326904,
      "learning_rate": 0.00010236111111111111,
      "loss": 0.9701,
      "step": 8590
    },
    {
      "epoch": 2.388888888888889,
      "grad_norm": 1.6892189979553223,
      "learning_rate": 0.00010189814814814816,
      "loss": 0.9115,
      "step": 8600
    },
    {
      "epoch": 2.3916666666666666,
      "grad_norm": 1.94313645362854,
      "learning_rate": 0.00010143518518518519,
      "loss": 0.9861,
      "step": 8610
    },
    {
      "epoch": 2.3944444444444444,
      "grad_norm": 1.5208313465118408,
      "learning_rate": 0.00010097222222222223,
      "loss": 0.8925,
      "step": 8620
    },
    {
      "epoch": 2.397222222222222,
      "grad_norm": 1.5022324323654175,
      "learning_rate": 0.00010050925925925925,
      "loss": 0.9656,
      "step": 8630
    },
    {
      "epoch": 2.4,
      "grad_norm": 1.868601679801941,
      "learning_rate": 0.0001000462962962963,
      "loss": 0.962,
      "step": 8640
    },
    {
      "epoch": 2.4027777777777777,
      "grad_norm": 3.4524006843566895,
      "learning_rate": 9.958333333333333e-05,
      "loss": 1.0001,
      "step": 8650
    },
    {
      "epoch": 2.4055555555555554,
      "grad_norm": 1.695278286933899,
      "learning_rate": 9.912037037037037e-05,
      "loss": 1.0168,
      "step": 8660
    },
    {
      "epoch": 2.408333333333333,
      "grad_norm": 0.8523101210594177,
      "learning_rate": 9.86574074074074e-05,
      "loss": 0.8633,
      "step": 8670
    },
    {
      "epoch": 2.411111111111111,
      "grad_norm": 1.172692894935608,
      "learning_rate": 9.819444444444445e-05,
      "loss": 0.7954,
      "step": 8680
    },
    {
      "epoch": 2.4138888888888888,
      "grad_norm": 1.793499231338501,
      "learning_rate": 9.773148148148148e-05,
      "loss": 0.9665,
      "step": 8690
    },
    {
      "epoch": 2.4166666666666665,
      "grad_norm": 1.2358719110488892,
      "learning_rate": 9.726851851851852e-05,
      "loss": 0.9498,
      "step": 8700
    },
    {
      "epoch": 2.4194444444444443,
      "grad_norm": 6.37314510345459,
      "learning_rate": 9.680555555555556e-05,
      "loss": 0.9091,
      "step": 8710
    },
    {
      "epoch": 2.422222222222222,
      "grad_norm": 3.3742423057556152,
      "learning_rate": 9.63425925925926e-05,
      "loss": 0.8649,
      "step": 8720
    },
    {
      "epoch": 2.425,
      "grad_norm": 1.1545740365982056,
      "learning_rate": 9.587962962962964e-05,
      "loss": 0.9172,
      "step": 8730
    },
    {
      "epoch": 2.4277777777777776,
      "grad_norm": 1.8239346742630005,
      "learning_rate": 9.541666666666666e-05,
      "loss": 1.0269,
      "step": 8740
    },
    {
      "epoch": 2.4305555555555554,
      "grad_norm": 2.1701784133911133,
      "learning_rate": 9.49537037037037e-05,
      "loss": 0.9677,
      "step": 8750
    },
    {
      "epoch": 2.4333333333333336,
      "grad_norm": 1.0846487283706665,
      "learning_rate": 9.449074074074074e-05,
      "loss": 0.8959,
      "step": 8760
    },
    {
      "epoch": 2.436111111111111,
      "grad_norm": 1.108816146850586,
      "learning_rate": 9.402777777777778e-05,
      "loss": 0.8962,
      "step": 8770
    },
    {
      "epoch": 2.438888888888889,
      "grad_norm": 1.2379045486450195,
      "learning_rate": 9.356481481481481e-05,
      "loss": 0.8653,
      "step": 8780
    },
    {
      "epoch": 2.4416666666666664,
      "grad_norm": 1.5447112321853638,
      "learning_rate": 9.310185185185185e-05,
      "loss": 0.8786,
      "step": 8790
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 1.0519344806671143,
      "learning_rate": 9.26388888888889e-05,
      "loss": 0.9205,
      "step": 8800
    },
    {
      "epoch": 2.4472222222222224,
      "grad_norm": 1.6212291717529297,
      "learning_rate": 9.217592592592593e-05,
      "loss": 0.9641,
      "step": 8810
    },
    {
      "epoch": 2.45,
      "grad_norm": 1.3472092151641846,
      "learning_rate": 9.171296296296297e-05,
      "loss": 0.8877,
      "step": 8820
    },
    {
      "epoch": 2.452777777777778,
      "grad_norm": 1.132436752319336,
      "learning_rate": 9.125e-05,
      "loss": 0.8744,
      "step": 8830
    },
    {
      "epoch": 2.4555555555555557,
      "grad_norm": 1.200108528137207,
      "learning_rate": 9.078703703703705e-05,
      "loss": 0.9785,
      "step": 8840
    },
    {
      "epoch": 2.4583333333333335,
      "grad_norm": 1.3507699966430664,
      "learning_rate": 9.032407407407408e-05,
      "loss": 0.9296,
      "step": 8850
    },
    {
      "epoch": 2.4611111111111112,
      "grad_norm": 1.6305339336395264,
      "learning_rate": 8.986111111111111e-05,
      "loss": 0.9083,
      "step": 8860
    },
    {
      "epoch": 2.463888888888889,
      "grad_norm": 2.570150136947632,
      "learning_rate": 8.939814814814814e-05,
      "loss": 0.9191,
      "step": 8870
    },
    {
      "epoch": 2.466666666666667,
      "grad_norm": 1.3290683031082153,
      "learning_rate": 8.89351851851852e-05,
      "loss": 0.9086,
      "step": 8880
    },
    {
      "epoch": 2.4694444444444446,
      "grad_norm": 1.4693118333816528,
      "learning_rate": 8.847222222222222e-05,
      "loss": 0.888,
      "step": 8890
    },
    {
      "epoch": 2.4722222222222223,
      "grad_norm": 1.7005140781402588,
      "learning_rate": 8.800925925925926e-05,
      "loss": 0.9523,
      "step": 8900
    },
    {
      "epoch": 2.475,
      "grad_norm": 1.6548199653625488,
      "learning_rate": 8.75462962962963e-05,
      "loss": 0.8979,
      "step": 8910
    },
    {
      "epoch": 2.477777777777778,
      "grad_norm": 1.8880146741867065,
      "learning_rate": 8.708333333333334e-05,
      "loss": 0.9332,
      "step": 8920
    },
    {
      "epoch": 2.4805555555555556,
      "grad_norm": 1.1833726167678833,
      "learning_rate": 8.662037037037038e-05,
      "loss": 0.8666,
      "step": 8930
    },
    {
      "epoch": 2.4833333333333334,
      "grad_norm": 1.5799005031585693,
      "learning_rate": 8.61574074074074e-05,
      "loss": 0.9966,
      "step": 8940
    },
    {
      "epoch": 2.486111111111111,
      "grad_norm": 0.8956646919250488,
      "learning_rate": 8.569444444444445e-05,
      "loss": 0.8654,
      "step": 8950
    },
    {
      "epoch": 2.488888888888889,
      "grad_norm": 1.2223286628723145,
      "learning_rate": 8.523148148148147e-05,
      "loss": 0.8409,
      "step": 8960
    },
    {
      "epoch": 2.4916666666666667,
      "grad_norm": 1.1469769477844238,
      "learning_rate": 8.476851851851853e-05,
      "loss": 0.9774,
      "step": 8970
    },
    {
      "epoch": 2.4944444444444445,
      "grad_norm": 2.6894845962524414,
      "learning_rate": 8.430555555555555e-05,
      "loss": 0.9899,
      "step": 8980
    },
    {
      "epoch": 2.4972222222222222,
      "grad_norm": 1.77978515625,
      "learning_rate": 8.384259259259259e-05,
      "loss": 0.849,
      "step": 8990
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.3135380744934082,
      "learning_rate": 8.337962962962962e-05,
      "loss": 0.924,
      "step": 9000
    },
    {
      "epoch": 2.5027777777777778,
      "grad_norm": 2.1646697521209717,
      "learning_rate": 8.291666666666667e-05,
      "loss": 0.876,
      "step": 9010
    },
    {
      "epoch": 2.5055555555555555,
      "grad_norm": 1.2744059562683105,
      "learning_rate": 8.245370370370371e-05,
      "loss": 0.8108,
      "step": 9020
    },
    {
      "epoch": 2.5083333333333333,
      "grad_norm": 1.3040077686309814,
      "learning_rate": 8.199074074074074e-05,
      "loss": 0.9814,
      "step": 9030
    },
    {
      "epoch": 2.511111111111111,
      "grad_norm": 0.773323655128479,
      "learning_rate": 8.152777777777778e-05,
      "loss": 0.8414,
      "step": 9040
    },
    {
      "epoch": 2.513888888888889,
      "grad_norm": 1.762400507926941,
      "learning_rate": 8.106481481481482e-05,
      "loss": 0.944,
      "step": 9050
    },
    {
      "epoch": 2.5166666666666666,
      "grad_norm": 1.6388741731643677,
      "learning_rate": 8.060185185185186e-05,
      "loss": 0.8812,
      "step": 9060
    },
    {
      "epoch": 2.5194444444444444,
      "grad_norm": 1.4051928520202637,
      "learning_rate": 8.013888888888888e-05,
      "loss": 0.9448,
      "step": 9070
    },
    {
      "epoch": 2.522222222222222,
      "grad_norm": 1.4451625347137451,
      "learning_rate": 7.967592592592592e-05,
      "loss": 0.9794,
      "step": 9080
    },
    {
      "epoch": 2.525,
      "grad_norm": 1.5048186779022217,
      "learning_rate": 7.921296296296296e-05,
      "loss": 0.9316,
      "step": 9090
    },
    {
      "epoch": 2.5277777777777777,
      "grad_norm": 1.1206656694412231,
      "learning_rate": 7.875e-05,
      "loss": 0.9369,
      "step": 9100
    },
    {
      "epoch": 2.5305555555555554,
      "grad_norm": 1.4379676580429077,
      "learning_rate": 7.828703703703704e-05,
      "loss": 0.841,
      "step": 9110
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 1.3144900798797607,
      "learning_rate": 7.782407407407407e-05,
      "loss": 1.0029,
      "step": 9120
    },
    {
      "epoch": 2.536111111111111,
      "grad_norm": 1.425931692123413,
      "learning_rate": 7.736111111111112e-05,
      "loss": 1.0098,
      "step": 9130
    },
    {
      "epoch": 2.5388888888888888,
      "grad_norm": 1.2532291412353516,
      "learning_rate": 7.689814814814815e-05,
      "loss": 0.9617,
      "step": 9140
    },
    {
      "epoch": 2.5416666666666665,
      "grad_norm": 1.3137729167938232,
      "learning_rate": 7.643518518518519e-05,
      "loss": 0.8678,
      "step": 9150
    },
    {
      "epoch": 2.5444444444444443,
      "grad_norm": 1.4326832294464111,
      "learning_rate": 7.597222222222222e-05,
      "loss": 0.9193,
      "step": 9160
    },
    {
      "epoch": 2.5472222222222225,
      "grad_norm": 1.7968088388442993,
      "learning_rate": 7.550925925925927e-05,
      "loss": 0.9508,
      "step": 9170
    },
    {
      "epoch": 2.55,
      "grad_norm": 1.167512059211731,
      "learning_rate": 7.50462962962963e-05,
      "loss": 0.8666,
      "step": 9180
    },
    {
      "epoch": 2.552777777777778,
      "grad_norm": 1.0492351055145264,
      "learning_rate": 7.458333333333333e-05,
      "loss": 0.9245,
      "step": 9190
    },
    {
      "epoch": 2.5555555555555554,
      "grad_norm": 1.128529667854309,
      "learning_rate": 7.412037037037036e-05,
      "loss": 0.9013,
      "step": 9200
    },
    {
      "epoch": 2.5583333333333336,
      "grad_norm": 1.3641802072525024,
      "learning_rate": 7.365740740740741e-05,
      "loss": 0.8637,
      "step": 9210
    },
    {
      "epoch": 2.561111111111111,
      "grad_norm": 1.3881977796554565,
      "learning_rate": 7.319444444444445e-05,
      "loss": 0.9265,
      "step": 9220
    },
    {
      "epoch": 2.563888888888889,
      "grad_norm": 1.0820947885513306,
      "learning_rate": 7.273148148148148e-05,
      "loss": 0.9268,
      "step": 9230
    },
    {
      "epoch": 2.5666666666666664,
      "grad_norm": 0.8964859247207642,
      "learning_rate": 7.226851851851852e-05,
      "loss": 0.9195,
      "step": 9240
    },
    {
      "epoch": 2.5694444444444446,
      "grad_norm": 1.1477283239364624,
      "learning_rate": 7.180555555555556e-05,
      "loss": 0.9466,
      "step": 9250
    },
    {
      "epoch": 2.572222222222222,
      "grad_norm": 1.4162830114364624,
      "learning_rate": 7.13425925925926e-05,
      "loss": 1.0415,
      "step": 9260
    },
    {
      "epoch": 2.575,
      "grad_norm": 0.9307308197021484,
      "learning_rate": 7.087962962962963e-05,
      "loss": 0.9797,
      "step": 9270
    },
    {
      "epoch": 2.5777777777777775,
      "grad_norm": 1.5157009363174438,
      "learning_rate": 7.041666666666667e-05,
      "loss": 0.8791,
      "step": 9280
    },
    {
      "epoch": 2.5805555555555557,
      "grad_norm": 1.712752103805542,
      "learning_rate": 6.99537037037037e-05,
      "loss": 1.0437,
      "step": 9290
    },
    {
      "epoch": 2.5833333333333335,
      "grad_norm": 2.059433937072754,
      "learning_rate": 6.949074074074075e-05,
      "loss": 0.9683,
      "step": 9300
    },
    {
      "epoch": 2.5861111111111112,
      "grad_norm": 1.064527988433838,
      "learning_rate": 6.902777777777777e-05,
      "loss": 0.9711,
      "step": 9310
    },
    {
      "epoch": 2.588888888888889,
      "grad_norm": 1.3357658386230469,
      "learning_rate": 6.856481481481481e-05,
      "loss": 0.8679,
      "step": 9320
    },
    {
      "epoch": 2.591666666666667,
      "grad_norm": 1.393730878829956,
      "learning_rate": 6.810185185185187e-05,
      "loss": 0.8674,
      "step": 9330
    },
    {
      "epoch": 2.5944444444444446,
      "grad_norm": 1.4904530048370361,
      "learning_rate": 6.763888888888889e-05,
      "loss": 0.8847,
      "step": 9340
    },
    {
      "epoch": 2.5972222222222223,
      "grad_norm": 1.2622120380401611,
      "learning_rate": 6.717592592592593e-05,
      "loss": 0.8681,
      "step": 9350
    },
    {
      "epoch": 2.6,
      "grad_norm": 2.1265242099761963,
      "learning_rate": 6.671296296296296e-05,
      "loss": 0.9068,
      "step": 9360
    },
    {
      "epoch": 2.602777777777778,
      "grad_norm": 0.924288809299469,
      "learning_rate": 6.625000000000001e-05,
      "loss": 0.961,
      "step": 9370
    },
    {
      "epoch": 2.6055555555555556,
      "grad_norm": 1.462645411491394,
      "learning_rate": 6.578703703703704e-05,
      "loss": 1.0464,
      "step": 9380
    },
    {
      "epoch": 2.6083333333333334,
      "grad_norm": 1.427559733390808,
      "learning_rate": 6.532407407407408e-05,
      "loss": 1.0124,
      "step": 9390
    },
    {
      "epoch": 2.611111111111111,
      "grad_norm": 1.1651359796524048,
      "learning_rate": 6.48611111111111e-05,
      "loss": 0.9203,
      "step": 9400
    },
    {
      "epoch": 2.613888888888889,
      "grad_norm": 1.2208259105682373,
      "learning_rate": 6.439814814814814e-05,
      "loss": 0.9401,
      "step": 9410
    },
    {
      "epoch": 2.6166666666666667,
      "grad_norm": 1.5156407356262207,
      "learning_rate": 6.39351851851852e-05,
      "loss": 0.8884,
      "step": 9420
    },
    {
      "epoch": 2.6194444444444445,
      "grad_norm": 1.199906826019287,
      "learning_rate": 6.347222222222222e-05,
      "loss": 0.8481,
      "step": 9430
    },
    {
      "epoch": 2.6222222222222222,
      "grad_norm": 1.6140440702438354,
      "learning_rate": 6.300925925925926e-05,
      "loss": 0.9648,
      "step": 9440
    },
    {
      "epoch": 2.625,
      "grad_norm": 1.4736636877059937,
      "learning_rate": 6.254629629629629e-05,
      "loss": 0.8961,
      "step": 9450
    },
    {
      "epoch": 2.6277777777777778,
      "grad_norm": 2.110703229904175,
      "learning_rate": 6.208333333333333e-05,
      "loss": 1.075,
      "step": 9460
    },
    {
      "epoch": 2.6305555555555555,
      "grad_norm": 1.5735676288604736,
      "learning_rate": 6.162037037037037e-05,
      "loss": 0.9054,
      "step": 9470
    },
    {
      "epoch": 2.6333333333333333,
      "grad_norm": 1.4139735698699951,
      "learning_rate": 6.115740740740741e-05,
      "loss": 0.9368,
      "step": 9480
    },
    {
      "epoch": 2.636111111111111,
      "grad_norm": 0.9155236482620239,
      "learning_rate": 6.069444444444445e-05,
      "loss": 0.9843,
      "step": 9490
    },
    {
      "epoch": 2.638888888888889,
      "grad_norm": 1.1651378870010376,
      "learning_rate": 6.023148148148148e-05,
      "loss": 0.9331,
      "step": 9500
    },
    {
      "epoch": 2.6416666666666666,
      "grad_norm": 1.780482292175293,
      "learning_rate": 5.976851851851852e-05,
      "loss": 0.9997,
      "step": 9510
    },
    {
      "epoch": 2.6444444444444444,
      "grad_norm": 1.4046086072921753,
      "learning_rate": 5.9305555555555555e-05,
      "loss": 0.8921,
      "step": 9520
    },
    {
      "epoch": 2.647222222222222,
      "grad_norm": 1.2434080839157104,
      "learning_rate": 5.8842592592592594e-05,
      "loss": 0.8434,
      "step": 9530
    },
    {
      "epoch": 2.65,
      "grad_norm": 1.5326201915740967,
      "learning_rate": 5.837962962962963e-05,
      "loss": 0.9692,
      "step": 9540
    },
    {
      "epoch": 2.6527777777777777,
      "grad_norm": 1.6961365938186646,
      "learning_rate": 5.791666666666667e-05,
      "loss": 0.8917,
      "step": 9550
    },
    {
      "epoch": 2.6555555555555554,
      "grad_norm": 1.2361161708831787,
      "learning_rate": 5.74537037037037e-05,
      "loss": 0.8674,
      "step": 9560
    },
    {
      "epoch": 2.658333333333333,
      "grad_norm": 2.1753406524658203,
      "learning_rate": 5.699074074074074e-05,
      "loss": 0.867,
      "step": 9570
    },
    {
      "epoch": 2.661111111111111,
      "grad_norm": 1.6577746868133545,
      "learning_rate": 5.652777777777778e-05,
      "loss": 0.8952,
      "step": 9580
    },
    {
      "epoch": 2.6638888888888888,
      "grad_norm": 1.744515299797058,
      "learning_rate": 5.606481481481482e-05,
      "loss": 0.8722,
      "step": 9590
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 2.502521514892578,
      "learning_rate": 5.560185185185185e-05,
      "loss": 1.0409,
      "step": 9600
    },
    {
      "epoch": 2.6694444444444443,
      "grad_norm": 0.9235899448394775,
      "learning_rate": 5.513888888888889e-05,
      "loss": 0.9107,
      "step": 9610
    },
    {
      "epoch": 2.6722222222222225,
      "grad_norm": 2.5527331829071045,
      "learning_rate": 5.4675925925925926e-05,
      "loss": 0.8721,
      "step": 9620
    },
    {
      "epoch": 2.675,
      "grad_norm": 1.2669224739074707,
      "learning_rate": 5.4212962962962966e-05,
      "loss": 0.9575,
      "step": 9630
    },
    {
      "epoch": 2.677777777777778,
      "grad_norm": 1.45762038230896,
      "learning_rate": 5.375e-05,
      "loss": 0.8922,
      "step": 9640
    },
    {
      "epoch": 2.6805555555555554,
      "grad_norm": 1.3173668384552002,
      "learning_rate": 5.328703703703704e-05,
      "loss": 0.9602,
      "step": 9650
    },
    {
      "epoch": 2.6833333333333336,
      "grad_norm": 1.6308428049087524,
      "learning_rate": 5.282407407407407e-05,
      "loss": 0.9644,
      "step": 9660
    },
    {
      "epoch": 2.686111111111111,
      "grad_norm": 1.6060969829559326,
      "learning_rate": 5.236111111111111e-05,
      "loss": 0.8802,
      "step": 9670
    },
    {
      "epoch": 2.688888888888889,
      "grad_norm": 1.4465879201889038,
      "learning_rate": 5.189814814814815e-05,
      "loss": 0.938,
      "step": 9680
    },
    {
      "epoch": 2.6916666666666664,
      "grad_norm": 1.5761078596115112,
      "learning_rate": 5.143518518518519e-05,
      "loss": 0.9151,
      "step": 9690
    },
    {
      "epoch": 2.6944444444444446,
      "grad_norm": 1.4391120672225952,
      "learning_rate": 5.0972222222222224e-05,
      "loss": 0.9695,
      "step": 9700
    },
    {
      "epoch": 2.697222222222222,
      "grad_norm": 1.831111192703247,
      "learning_rate": 5.0509259259259264e-05,
      "loss": 1.0536,
      "step": 9710
    },
    {
      "epoch": 2.7,
      "grad_norm": 2.37362003326416,
      "learning_rate": 5.00462962962963e-05,
      "loss": 0.9226,
      "step": 9720
    },
    {
      "epoch": 2.7027777777777775,
      "grad_norm": 1.425923466682434,
      "learning_rate": 4.958333333333334e-05,
      "loss": 0.8744,
      "step": 9730
    },
    {
      "epoch": 2.7055555555555557,
      "grad_norm": 1.0070279836654663,
      "learning_rate": 4.912037037037037e-05,
      "loss": 0.9443,
      "step": 9740
    },
    {
      "epoch": 2.7083333333333335,
      "grad_norm": 1.6937263011932373,
      "learning_rate": 4.865740740740741e-05,
      "loss": 0.8749,
      "step": 9750
    },
    {
      "epoch": 2.7111111111111112,
      "grad_norm": 1.2494316101074219,
      "learning_rate": 4.819444444444444e-05,
      "loss": 0.9275,
      "step": 9760
    },
    {
      "epoch": 2.713888888888889,
      "grad_norm": 1.218604326248169,
      "learning_rate": 4.773148148148148e-05,
      "loss": 0.9379,
      "step": 9770
    },
    {
      "epoch": 2.716666666666667,
      "grad_norm": 1.5147172212600708,
      "learning_rate": 4.7268518518518516e-05,
      "loss": 0.8856,
      "step": 9780
    },
    {
      "epoch": 2.7194444444444446,
      "grad_norm": 1.7230719327926636,
      "learning_rate": 4.680555555555556e-05,
      "loss": 0.9781,
      "step": 9790
    },
    {
      "epoch": 2.7222222222222223,
      "grad_norm": 1.1820608377456665,
      "learning_rate": 4.6342592592592595e-05,
      "loss": 0.9326,
      "step": 9800
    },
    {
      "epoch": 2.725,
      "grad_norm": 1.6785866022109985,
      "learning_rate": 4.5879629629629635e-05,
      "loss": 0.876,
      "step": 9810
    },
    {
      "epoch": 2.727777777777778,
      "grad_norm": 0.9392068982124329,
      "learning_rate": 4.541666666666667e-05,
      "loss": 0.9246,
      "step": 9820
    },
    {
      "epoch": 2.7305555555555556,
      "grad_norm": 1.5715913772583008,
      "learning_rate": 4.495370370370371e-05,
      "loss": 0.9733,
      "step": 9830
    },
    {
      "epoch": 2.7333333333333334,
      "grad_norm": 1.3336490392684937,
      "learning_rate": 4.449074074074074e-05,
      "loss": 0.9035,
      "step": 9840
    },
    {
      "epoch": 2.736111111111111,
      "grad_norm": 1.7152907848358154,
      "learning_rate": 4.402777777777778e-05,
      "loss": 0.9969,
      "step": 9850
    },
    {
      "epoch": 2.738888888888889,
      "grad_norm": 1.481990098953247,
      "learning_rate": 4.3564814814814814e-05,
      "loss": 0.9504,
      "step": 9860
    },
    {
      "epoch": 2.7416666666666667,
      "grad_norm": 1.4220285415649414,
      "learning_rate": 4.310185185185185e-05,
      "loss": 0.9505,
      "step": 9870
    },
    {
      "epoch": 2.7444444444444445,
      "grad_norm": 1.241881012916565,
      "learning_rate": 4.263888888888889e-05,
      "loss": 0.9242,
      "step": 9880
    },
    {
      "epoch": 2.7472222222222222,
      "grad_norm": 1.3535228967666626,
      "learning_rate": 4.217592592592593e-05,
      "loss": 0.8656,
      "step": 9890
    },
    {
      "epoch": 2.75,
      "grad_norm": 1.895330786705017,
      "learning_rate": 4.171296296296297e-05,
      "loss": 0.8948,
      "step": 9900
    },
    {
      "epoch": 2.7527777777777778,
      "grad_norm": 0.933299720287323,
      "learning_rate": 4.125e-05,
      "loss": 0.8571,
      "step": 9910
    },
    {
      "epoch": 2.7555555555555555,
      "grad_norm": 1.2204878330230713,
      "learning_rate": 4.078703703703704e-05,
      "loss": 0.9585,
      "step": 9920
    },
    {
      "epoch": 2.7583333333333333,
      "grad_norm": 1.0865353345870972,
      "learning_rate": 4.032407407407407e-05,
      "loss": 0.8974,
      "step": 9930
    },
    {
      "epoch": 2.761111111111111,
      "grad_norm": 1.5620735883712769,
      "learning_rate": 3.986111111111111e-05,
      "loss": 0.9688,
      "step": 9940
    },
    {
      "epoch": 2.763888888888889,
      "grad_norm": 1.0338244438171387,
      "learning_rate": 3.9398148148148146e-05,
      "loss": 0.9036,
      "step": 9950
    },
    {
      "epoch": 2.7666666666666666,
      "grad_norm": 1.796406865119934,
      "learning_rate": 3.8935185185185185e-05,
      "loss": 0.957,
      "step": 9960
    },
    {
      "epoch": 2.7694444444444444,
      "grad_norm": 1.1785647869110107,
      "learning_rate": 3.847222222222222e-05,
      "loss": 0.8625,
      "step": 9970
    },
    {
      "epoch": 2.772222222222222,
      "grad_norm": 1.2269233465194702,
      "learning_rate": 3.800925925925926e-05,
      "loss": 0.9619,
      "step": 9980
    },
    {
      "epoch": 2.775,
      "grad_norm": 1.634590983390808,
      "learning_rate": 3.754629629629629e-05,
      "loss": 0.9197,
      "step": 9990
    },
    {
      "epoch": 2.7777777777777777,
      "grad_norm": 1.9209414720535278,
      "learning_rate": 3.708333333333334e-05,
      "loss": 0.938,
      "step": 10000
    },
    {
      "epoch": 2.7805555555555554,
      "grad_norm": 2.103663682937622,
      "learning_rate": 3.662037037037037e-05,
      "loss": 0.9851,
      "step": 10010
    },
    {
      "epoch": 2.783333333333333,
      "grad_norm": 1.591740369796753,
      "learning_rate": 3.615740740740741e-05,
      "loss": 0.8151,
      "step": 10020
    },
    {
      "epoch": 2.786111111111111,
      "grad_norm": 2.287766695022583,
      "learning_rate": 3.5694444444444444e-05,
      "loss": 0.9147,
      "step": 10030
    },
    {
      "epoch": 2.7888888888888888,
      "grad_norm": 2.24454927444458,
      "learning_rate": 3.5231481481481484e-05,
      "loss": 0.8353,
      "step": 10040
    },
    {
      "epoch": 2.7916666666666665,
      "grad_norm": 1.1862518787384033,
      "learning_rate": 3.476851851851852e-05,
      "loss": 0.8674,
      "step": 10050
    },
    {
      "epoch": 2.7944444444444443,
      "grad_norm": 1.8384618759155273,
      "learning_rate": 3.430555555555556e-05,
      "loss": 0.9734,
      "step": 10060
    },
    {
      "epoch": 2.7972222222222225,
      "grad_norm": 1.4895330667495728,
      "learning_rate": 3.384259259259259e-05,
      "loss": 0.7976,
      "step": 10070
    },
    {
      "epoch": 2.8,
      "grad_norm": 1.0825754404067993,
      "learning_rate": 3.337962962962963e-05,
      "loss": 0.8742,
      "step": 10080
    },
    {
      "epoch": 2.802777777777778,
      "grad_norm": 1.326170563697815,
      "learning_rate": 3.291666666666666e-05,
      "loss": 1.0332,
      "step": 10090
    },
    {
      "epoch": 2.8055555555555554,
      "grad_norm": 1.2170531749725342,
      "learning_rate": 3.245370370370371e-05,
      "loss": 0.968,
      "step": 10100
    },
    {
      "epoch": 2.8083333333333336,
      "grad_norm": 1.7256801128387451,
      "learning_rate": 3.199074074074074e-05,
      "loss": 0.9217,
      "step": 10110
    },
    {
      "epoch": 2.811111111111111,
      "grad_norm": 1.3644222021102905,
      "learning_rate": 3.152777777777778e-05,
      "loss": 0.9739,
      "step": 10120
    },
    {
      "epoch": 2.813888888888889,
      "grad_norm": 1.2912875413894653,
      "learning_rate": 3.1064814814814815e-05,
      "loss": 0.9441,
      "step": 10130
    },
    {
      "epoch": 2.8166666666666664,
      "grad_norm": 1.3817622661590576,
      "learning_rate": 3.0601851851851855e-05,
      "loss": 0.9905,
      "step": 10140
    },
    {
      "epoch": 2.8194444444444446,
      "grad_norm": 1.635204553604126,
      "learning_rate": 3.0138888888888888e-05,
      "loss": 0.9283,
      "step": 10150
    },
    {
      "epoch": 2.822222222222222,
      "grad_norm": 1.7081421613693237,
      "learning_rate": 2.9675925925925925e-05,
      "loss": 0.9063,
      "step": 10160
    },
    {
      "epoch": 2.825,
      "grad_norm": 1.7527166604995728,
      "learning_rate": 2.921296296296296e-05,
      "loss": 0.924,
      "step": 10170
    },
    {
      "epoch": 2.8277777777777775,
      "grad_norm": 2.102631092071533,
      "learning_rate": 2.875e-05,
      "loss": 0.9049,
      "step": 10180
    },
    {
      "epoch": 2.8305555555555557,
      "grad_norm": 1.3899118900299072,
      "learning_rate": 2.8287037037037037e-05,
      "loss": 0.9627,
      "step": 10190
    },
    {
      "epoch": 2.8333333333333335,
      "grad_norm": 0.47658005356788635,
      "learning_rate": 2.7824074074074074e-05,
      "loss": 0.7869,
      "step": 10200
    },
    {
      "epoch": 2.8361111111111112,
      "grad_norm": 2.1858370304107666,
      "learning_rate": 2.736111111111111e-05,
      "loss": 0.9721,
      "step": 10210
    },
    {
      "epoch": 2.838888888888889,
      "grad_norm": 1.4311647415161133,
      "learning_rate": 2.6898148148148147e-05,
      "loss": 0.9401,
      "step": 10220
    },
    {
      "epoch": 2.841666666666667,
      "grad_norm": 3.0960140228271484,
      "learning_rate": 2.6435185185185187e-05,
      "loss": 1.0237,
      "step": 10230
    },
    {
      "epoch": 2.8444444444444446,
      "grad_norm": 1.2855896949768066,
      "learning_rate": 2.5972222222222223e-05,
      "loss": 1.0499,
      "step": 10240
    },
    {
      "epoch": 2.8472222222222223,
      "grad_norm": 1.9431201219558716,
      "learning_rate": 2.550925925925926e-05,
      "loss": 0.8725,
      "step": 10250
    },
    {
      "epoch": 2.85,
      "grad_norm": 1.5386831760406494,
      "learning_rate": 2.5046296296296296e-05,
      "loss": 0.9716,
      "step": 10260
    },
    {
      "epoch": 2.852777777777778,
      "grad_norm": 1.3648687601089478,
      "learning_rate": 2.4583333333333332e-05,
      "loss": 0.8676,
      "step": 10270
    },
    {
      "epoch": 2.8555555555555556,
      "grad_norm": 2.153379201889038,
      "learning_rate": 2.4120370370370372e-05,
      "loss": 0.9719,
      "step": 10280
    },
    {
      "epoch": 2.8583333333333334,
      "grad_norm": 0.8905815482139587,
      "learning_rate": 2.365740740740741e-05,
      "loss": 0.9173,
      "step": 10290
    },
    {
      "epoch": 2.861111111111111,
      "grad_norm": 2.3505513668060303,
      "learning_rate": 2.3194444444444445e-05,
      "loss": 0.929,
      "step": 10300
    },
    {
      "epoch": 2.863888888888889,
      "grad_norm": 1.3883740901947021,
      "learning_rate": 2.273148148148148e-05,
      "loss": 0.9205,
      "step": 10310
    },
    {
      "epoch": 2.8666666666666667,
      "grad_norm": 1.5958020687103271,
      "learning_rate": 2.2268518518518518e-05,
      "loss": 1.0154,
      "step": 10320
    },
    {
      "epoch": 2.8694444444444445,
      "grad_norm": 0.8463643193244934,
      "learning_rate": 2.1805555555555558e-05,
      "loss": 0.962,
      "step": 10330
    },
    {
      "epoch": 2.8722222222222222,
      "grad_norm": 1.192266583442688,
      "learning_rate": 2.1342592592592594e-05,
      "loss": 0.9798,
      "step": 10340
    },
    {
      "epoch": 2.875,
      "grad_norm": 2.3997368812561035,
      "learning_rate": 2.087962962962963e-05,
      "loss": 0.8843,
      "step": 10350
    },
    {
      "epoch": 2.8777777777777778,
      "grad_norm": 2.5328915119171143,
      "learning_rate": 2.0416666666666667e-05,
      "loss": 0.9202,
      "step": 10360
    },
    {
      "epoch": 2.8805555555555555,
      "grad_norm": 1.292099118232727,
      "learning_rate": 1.9953703703703704e-05,
      "loss": 0.8984,
      "step": 10370
    },
    {
      "epoch": 2.8833333333333333,
      "grad_norm": 1.163648247718811,
      "learning_rate": 1.949074074074074e-05,
      "loss": 0.9755,
      "step": 10380
    },
    {
      "epoch": 2.886111111111111,
      "grad_norm": 2.4284472465515137,
      "learning_rate": 1.902777777777778e-05,
      "loss": 0.9174,
      "step": 10390
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 1.515344262123108,
      "learning_rate": 1.8564814814814816e-05,
      "loss": 1.0127,
      "step": 10400
    },
    {
      "epoch": 2.8916666666666666,
      "grad_norm": 1.2529383897781372,
      "learning_rate": 1.8101851851851853e-05,
      "loss": 0.8583,
      "step": 10410
    },
    {
      "epoch": 2.8944444444444444,
      "grad_norm": 2.047262191772461,
      "learning_rate": 1.763888888888889e-05,
      "loss": 0.9064,
      "step": 10420
    },
    {
      "epoch": 2.897222222222222,
      "grad_norm": 1.4073237180709839,
      "learning_rate": 1.7175925925925926e-05,
      "loss": 0.9133,
      "step": 10430
    },
    {
      "epoch": 2.9,
      "grad_norm": 1.6386717557907104,
      "learning_rate": 1.6712962962962966e-05,
      "loss": 0.8721,
      "step": 10440
    },
    {
      "epoch": 2.9027777777777777,
      "grad_norm": 1.8051129579544067,
      "learning_rate": 1.6250000000000002e-05,
      "loss": 0.9749,
      "step": 10450
    },
    {
      "epoch": 2.9055555555555554,
      "grad_norm": 1.304119348526001,
      "learning_rate": 1.578703703703704e-05,
      "loss": 0.9544,
      "step": 10460
    },
    {
      "epoch": 2.908333333333333,
      "grad_norm": 1.3802109956741333,
      "learning_rate": 1.5324074074074075e-05,
      "loss": 0.9516,
      "step": 10470
    },
    {
      "epoch": 2.911111111111111,
      "grad_norm": 1.5916486978530884,
      "learning_rate": 1.4861111111111111e-05,
      "loss": 0.9449,
      "step": 10480
    },
    {
      "epoch": 2.9138888888888888,
      "grad_norm": 1.7147706747055054,
      "learning_rate": 1.4398148148148148e-05,
      "loss": 0.9605,
      "step": 10490
    },
    {
      "epoch": 2.9166666666666665,
      "grad_norm": 1.2397050857543945,
      "learning_rate": 1.3935185185185186e-05,
      "loss": 0.879,
      "step": 10500
    },
    {
      "epoch": 2.9194444444444443,
      "grad_norm": 1.0678895711898804,
      "learning_rate": 1.3472222222222222e-05,
      "loss": 0.9562,
      "step": 10510
    },
    {
      "epoch": 2.9222222222222225,
      "grad_norm": 1.6341384649276733,
      "learning_rate": 1.3009259259259259e-05,
      "loss": 0.9075,
      "step": 10520
    },
    {
      "epoch": 2.925,
      "grad_norm": 1.8417150974273682,
      "learning_rate": 1.2546296296296297e-05,
      "loss": 0.9486,
      "step": 10530
    },
    {
      "epoch": 2.927777777777778,
      "grad_norm": 1.7981948852539062,
      "learning_rate": 1.2083333333333333e-05,
      "loss": 0.9588,
      "step": 10540
    },
    {
      "epoch": 2.9305555555555554,
      "grad_norm": 1.5501132011413574,
      "learning_rate": 1.1620370370370372e-05,
      "loss": 0.8819,
      "step": 10550
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 1.6241061687469482,
      "learning_rate": 1.1157407407407408e-05,
      "loss": 0.9337,
      "step": 10560
    },
    {
      "epoch": 2.936111111111111,
      "grad_norm": 1.408655047416687,
      "learning_rate": 1.0694444444444444e-05,
      "loss": 0.9463,
      "step": 10570
    },
    {
      "epoch": 2.938888888888889,
      "grad_norm": 2.1507489681243896,
      "learning_rate": 1.0231481481481483e-05,
      "loss": 0.9134,
      "step": 10580
    },
    {
      "epoch": 2.9416666666666664,
      "grad_norm": 1.3263636827468872,
      "learning_rate": 9.768518518518519e-06,
      "loss": 0.891,
      "step": 10590
    },
    {
      "epoch": 2.9444444444444446,
      "grad_norm": 1.1127095222473145,
      "learning_rate": 9.305555555555555e-06,
      "loss": 0.8523,
      "step": 10600
    },
    {
      "epoch": 2.947222222222222,
      "grad_norm": 1.9061768054962158,
      "learning_rate": 8.842592592592594e-06,
      "loss": 0.958,
      "step": 10610
    },
    {
      "epoch": 2.95,
      "grad_norm": 1.1071135997772217,
      "learning_rate": 8.37962962962963e-06,
      "loss": 0.839,
      "step": 10620
    },
    {
      "epoch": 2.9527777777777775,
      "grad_norm": 1.3503620624542236,
      "learning_rate": 7.916666666666668e-06,
      "loss": 0.8993,
      "step": 10630
    },
    {
      "epoch": 2.9555555555555557,
      "grad_norm": 0.9905372858047485,
      "learning_rate": 7.453703703703704e-06,
      "loss": 0.8312,
      "step": 10640
    },
    {
      "epoch": 2.9583333333333335,
      "grad_norm": 1.0987417697906494,
      "learning_rate": 6.990740740740741e-06,
      "loss": 0.8785,
      "step": 10650
    },
    {
      "epoch": 2.9611111111111112,
      "grad_norm": 1.4275587797164917,
      "learning_rate": 6.5277777777777784e-06,
      "loss": 0.8939,
      "step": 10660
    },
    {
      "epoch": 2.963888888888889,
      "grad_norm": 1.7033580541610718,
      "learning_rate": 6.064814814814815e-06,
      "loss": 0.954,
      "step": 10670
    },
    {
      "epoch": 2.966666666666667,
      "grad_norm": 1.6836810111999512,
      "learning_rate": 5.601851851851852e-06,
      "loss": 0.9539,
      "step": 10680
    },
    {
      "epoch": 2.9694444444444446,
      "grad_norm": 1.3696774244308472,
      "learning_rate": 5.1388888888888895e-06,
      "loss": 0.8329,
      "step": 10690
    },
    {
      "epoch": 2.9722222222222223,
      "grad_norm": 1.4494928121566772,
      "learning_rate": 4.675925925925927e-06,
      "loss": 0.8971,
      "step": 10700
    },
    {
      "epoch": 2.975,
      "grad_norm": 1.8841906785964966,
      "learning_rate": 4.212962962962962e-06,
      "loss": 0.8609,
      "step": 10710
    },
    {
      "epoch": 2.977777777777778,
      "grad_norm": 2.049356460571289,
      "learning_rate": 3.75e-06,
      "loss": 0.9544,
      "step": 10720
    },
    {
      "epoch": 2.9805555555555556,
      "grad_norm": 1.1334933042526245,
      "learning_rate": 3.287037037037037e-06,
      "loss": 0.9407,
      "step": 10730
    },
    {
      "epoch": 2.9833333333333334,
      "grad_norm": 1.1025208234786987,
      "learning_rate": 2.824074074074074e-06,
      "loss": 0.8931,
      "step": 10740
    },
    {
      "epoch": 2.986111111111111,
      "grad_norm": 1.2961689233779907,
      "learning_rate": 2.361111111111111e-06,
      "loss": 0.8993,
      "step": 10750
    },
    {
      "epoch": 2.988888888888889,
      "grad_norm": 1.3298301696777344,
      "learning_rate": 1.8981481481481482e-06,
      "loss": 1.0194,
      "step": 10760
    },
    {
      "epoch": 2.9916666666666667,
      "grad_norm": 1.3971725702285767,
      "learning_rate": 1.4351851851851853e-06,
      "loss": 0.8624,
      "step": 10770
    },
    {
      "epoch": 2.9944444444444445,
      "grad_norm": 1.4916104078292847,
      "learning_rate": 9.722222222222222e-07,
      "loss": 0.8626,
      "step": 10780
    },
    {
      "epoch": 2.9972222222222222,
      "grad_norm": 1.3108340501785278,
      "learning_rate": 5.092592592592593e-07,
      "loss": 0.9524,
      "step": 10790
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.2352701425552368,
      "learning_rate": 4.6296296296296295e-08,
      "loss": 0.9157,
      "step": 10800
    }
  ],
  "logging_steps": 10,
  "max_steps": 10800,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.304528320935895e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
