Step 1: {'loss': 0.0054, 'grad_norm': 0.0751953125, 'learning_rate': 0.0, 'epoch': 0.0071111111111111115}
Step 2: {'loss': 0.0057, 'grad_norm': 0.06396484375, 'learning_rate': 4.651162790697674e-06, 'epoch': 0.014222222222222223}
Step 3: {'loss': 0.0123, 'grad_norm': 0.173828125, 'learning_rate': 9.302325581395349e-06, 'epoch': 0.021333333333333333}
Step 4: {'loss': 0.013, 'grad_norm': 0.2236328125, 'learning_rate': 1.3953488372093024e-05, 'epoch': 0.028444444444444446}
Step 5: {'loss': 0.0117, 'grad_norm': 0.15625, 'learning_rate': 1.8604651162790697e-05, 'epoch': 0.035555555555555556}
Step 6: {'loss': 0.008, 'grad_norm': 0.0869140625, 'learning_rate': 2.3255813953488374e-05, 'epoch': 0.042666666666666665}
Step 7: {'loss': 0.0123, 'grad_norm': 0.1123046875, 'learning_rate': 2.7906976744186048e-05, 'epoch': 0.049777777777777775}
Step 8: {'loss': 0.014, 'grad_norm': 0.1103515625, 'learning_rate': 3.2558139534883724e-05, 'epoch': 0.05688888888888889}
Step 9: {'loss': 0.0201, 'grad_norm': 0.173828125, 'learning_rate': 3.7209302325581394e-05, 'epoch': 0.064}
Step 10: {'loss': 0.012, 'grad_norm': 0.0908203125, 'learning_rate': 4.186046511627907e-05, 'epoch': 0.07111111111111111}
Step 11: {'loss': 0.0203, 'grad_norm': 0.1728515625, 'learning_rate': 4.651162790697675e-05, 'epoch': 0.07822222222222222}
Step 12: {'loss': 0.0103, 'grad_norm': 0.08837890625, 'learning_rate': 5.1162790697674425e-05, 'epoch': 0.08533333333333333}
Step 13: {'loss': 0.0106, 'grad_norm': 0.08349609375, 'learning_rate': 5.5813953488372095e-05, 'epoch': 0.09244444444444444}
Step 14: {'loss': 0.0108, 'grad_norm': 0.087890625, 'learning_rate': 6.0465116279069765e-05, 'epoch': 0.09955555555555555}
Step 15: {'loss': 0.0091, 'grad_norm': 0.0751953125, 'learning_rate': 6.511627906976745e-05, 'epoch': 0.10666666666666667}
Step 16: {'loss': 0.0086, 'grad_norm': 0.09228515625, 'learning_rate': 6.976744186046513e-05, 'epoch': 0.11377777777777778}
Step 17: {'loss': 0.0095, 'grad_norm': 0.1162109375, 'learning_rate': 7.441860465116279e-05, 'epoch': 0.12088888888888889}
Step 18: {'loss': 0.0109, 'grad_norm': 0.1162109375, 'learning_rate': 7.906976744186047e-05, 'epoch': 0.128}
Step 19: {'loss': 0.0087, 'grad_norm': 0.10791015625, 'learning_rate': 8.372093023255814e-05, 'epoch': 0.1351111111111111}
Step 20: {'loss': 0.0122, 'grad_norm': 0.1337890625, 'learning_rate': 8.837209302325582e-05, 'epoch': 0.14222222222222222}
Step 21: {'loss': 0.006, 'grad_norm': 0.049560546875, 'learning_rate': 9.30232558139535e-05, 'epoch': 0.14933333333333335}
Step 22: {'loss': 0.0062, 'grad_norm': 0.041015625, 'learning_rate': 9.767441860465116e-05, 'epoch': 0.15644444444444444}
Step 23: {'loss': 0.0061, 'grad_norm': 0.034912109375, 'learning_rate': 0.00010232558139534885, 'epoch': 0.16355555555555557}
Step 24: {'loss': 0.0025, 'grad_norm': 0.017578125, 'learning_rate': 0.00010697674418604651, 'epoch': 0.17066666666666666}
Step 25: {'loss': 0.006, 'grad_norm': 0.03369140625, 'learning_rate': 0.00011162790697674419, 'epoch': 0.17777777777777778}
Step 26: {'loss': 0.0056, 'grad_norm': 0.0291748046875, 'learning_rate': 0.00011627906976744187, 'epoch': 0.18488888888888888}
Step 27: {'loss': 0.0054, 'grad_norm': 0.0244140625, 'learning_rate': 0.00012093023255813953, 'epoch': 0.192}
Step 28: {'loss': 0.0042, 'grad_norm': 0.0169677734375, 'learning_rate': 0.0001255813953488372, 'epoch': 0.1991111111111111}
Step 29: {'loss': 0.0055, 'grad_norm': 0.0185546875, 'learning_rate': 0.0001302325581395349, 'epoch': 0.20622222222222222}
Step 30: {'loss': 0.0041, 'grad_norm': 0.01263427734375, 'learning_rate': 0.00013488372093023256, 'epoch': 0.21333333333333335}
Step 31: {'loss': 0.0037, 'grad_norm': 0.0113525390625, 'learning_rate': 0.00013953488372093025, 'epoch': 0.22044444444444444}
Step 32: {'loss': 0.005, 'grad_norm': 0.01123046875, 'learning_rate': 0.00014418604651162791, 'epoch': 0.22755555555555557}
Step 33: {'loss': 0.0086, 'grad_norm': 0.01434326171875, 'learning_rate': 0.00014883720930232558, 'epoch': 0.23466666666666666}
Step 34: {'loss': 0.0053, 'grad_norm': 0.0096435546875, 'learning_rate': 0.00015348837209302327, 'epoch': 0.24177777777777779}
Step 35: {'loss': 0.0075, 'grad_norm': 0.0106201171875, 'learning_rate': 0.00015813953488372093, 'epoch': 0.24888888888888888}
Step 36: {'loss': 0.0059, 'grad_norm': 0.01043701171875, 'learning_rate': 0.00016279069767441862, 'epoch': 0.256}
Step 37: {'loss': 0.0041, 'grad_norm': 0.01080322265625, 'learning_rate': 0.00016744186046511629, 'epoch': 0.26311111111111113}
Step 38: {'loss': 0.0054, 'grad_norm': 0.0113525390625, 'learning_rate': 0.00017209302325581395, 'epoch': 0.2702222222222222}
Step 39: {'loss': 0.0051, 'grad_norm': 0.01385498046875, 'learning_rate': 0.00017674418604651164, 'epoch': 0.2773333333333333}
Step 40: {'loss': 0.0039, 'grad_norm': 0.01373291015625, 'learning_rate': 0.0001813953488372093, 'epoch': 0.28444444444444444}
Step 41: {'loss': 0.0043, 'grad_norm': 0.01422119140625, 'learning_rate': 0.000186046511627907, 'epoch': 0.29155555555555557}
Step 42: {'loss': 0.0026, 'grad_norm': 0.01507568359375, 'learning_rate': 0.00019069767441860466, 'epoch': 0.2986666666666667}
Step 43: {'loss': 0.0028, 'grad_norm': 0.01483154296875, 'learning_rate': 0.00019534883720930232, 'epoch': 0.30577777777777776}
Step 44: {'loss': 0.0037, 'grad_norm': 0.0145263671875, 'learning_rate': 0.0002, 'epoch': 0.3128888888888889}
Step 45: {'loss': 0.0069, 'grad_norm': 0.015625, 'learning_rate': 0.00019999658256641747, 'epoch': 0.32}
Step 46: {'loss': 0.004, 'grad_norm': 0.01220703125, 'learning_rate': 0.0001999863304992469, 'epoch': 0.32711111111111113}
Step 47: {'loss': 0.0047, 'grad_norm': 0.0111083984375, 'learning_rate': 0.0001999692444992035, 'epoch': 0.3342222222222222}
Step 48: {'loss': 0.0065, 'grad_norm': 0.0118408203125, 'learning_rate': 0.00019994532573409262, 'epoch': 0.3413333333333333}
Step 49: {'loss': 0.0058, 'grad_norm': 0.01080322265625, 'learning_rate': 0.0001999145758387301, 'epoch': 0.34844444444444445}
Step 50: {'loss': 0.0044, 'grad_norm': 0.010986328125, 'learning_rate': 0.00019987699691483048, 'epoch': 0.35555555555555557}
Step 51: {'loss': 0.005, 'grad_norm': 0.01080322265625, 'learning_rate': 0.00019983259153086327, 'epoch': 0.3626666666666667}
Step 52: {'loss': 0.0067, 'grad_norm': 0.0091552734375, 'learning_rate': 0.00019978136272187747, 'epoch': 0.36977777777777776}
Step 53: {'loss': 0.0088, 'grad_norm': 0.00885009765625, 'learning_rate': 0.0001997233139892941, 'epoch': 0.3768888888888889}
Step 54: {'loss': 0.0059, 'grad_norm': 0.007354736328125, 'learning_rate': 0.000199658449300667, 'epoch': 0.384}
Step 55: {'loss': 0.0041, 'grad_norm': 0.006378173828125, 'learning_rate': 0.00019958677308941139, 'epoch': 0.39111111111111113}
Step 56: {'loss': 0.0078, 'grad_norm': 0.0068359375, 'learning_rate': 0.00019950829025450114, 'epoch': 0.3982222222222222}
Step 57: {'loss': 0.0075, 'grad_norm': 0.00787353515625, 'learning_rate': 0.0001994230061601338, 'epoch': 0.4053333333333333}
Step 58: {'loss': 0.0044, 'grad_norm': 0.00543212890625, 'learning_rate': 0.00019933092663536382, 'epoch': 0.41244444444444445}
Step 59: {'loss': 0.0036, 'grad_norm': 0.004241943359375, 'learning_rate': 0.0001992320579737045, 'epoch': 0.41955555555555557}
Step 60: {'loss': 0.0084, 'grad_norm': 0.00640869140625, 'learning_rate': 0.00019912640693269752, 'epoch': 0.4266666666666667}
Step 61: {'loss': 0.0065, 'grad_norm': 0.0064697265625, 'learning_rate': 0.00019901398073345118, 'epoch': 0.43377777777777776}
Step 62: {'loss': 0.0034, 'grad_norm': 0.004180908203125, 'learning_rate': 0.00019889478706014687, 'epoch': 0.4408888888888889}
Step 63: {'loss': 0.0033, 'grad_norm': 0.00494384765625, 'learning_rate': 0.00019876883405951377, 'epoch': 0.448}
Step 64: {'loss': 0.0049, 'grad_norm': 0.004791259765625, 'learning_rate': 0.00019863613034027224, 'epoch': 0.45511111111111113}
Step 65: {'loss': 0.0059, 'grad_norm': 0.00506591796875, 'learning_rate': 0.0001984966849725452, 'epoch': 0.4622222222222222}
Step 66: {'loss': 0.0056, 'grad_norm': 0.005126953125, 'learning_rate': 0.00019835050748723824, 'epoch': 0.4693333333333333}
Step 67: {'loss': 0.0047, 'grad_norm': 0.0036773681640625, 'learning_rate': 0.0001981976078753884, 'epoch': 0.47644444444444445}
Step 68: {'loss': 0.0069, 'grad_norm': 0.004791259765625, 'learning_rate': 0.00019803799658748094, 'epoch': 0.48355555555555557}
Step 69: {'loss': 0.0047, 'grad_norm': 0.0042724609375, 'learning_rate': 0.00019787168453273544, 'epoch': 0.49066666666666664}
Step 70: {'loss': 0.0036, 'grad_norm': 0.00299072265625, 'learning_rate': 0.00019769868307835994, 'epoch': 0.49777777777777776}
Step 71: {'loss': 0.0049, 'grad_norm': 0.00421142578125, 'learning_rate': 0.000197519004048774, 'epoch': 0.5048888888888889}
Step 72: {'loss': 0.0047, 'grad_norm': 0.0030670166015625, 'learning_rate': 0.0001973326597248006, 'epoch': 0.512}
Step 73: {'loss': 0.0028, 'grad_norm': 0.0033416748046875, 'learning_rate': 0.00019713966284282678, 'epoch': 0.5191111111111111}
Step 74: {'loss': 0.0035, 'grad_norm': 0.004302978515625, 'learning_rate': 0.00019694002659393305, 'epoch': 0.5262222222222223}
Step 75: {'loss': 0.0026, 'grad_norm': 0.00335693359375, 'learning_rate': 0.00019673376462299184, 'epoch': 0.5333333333333333}
Step 76: {'loss': 0.0037, 'grad_norm': 0.0029449462890625, 'learning_rate': 0.00019652089102773488, 'epoch': 0.5404444444444444}
Step 77: {'loss': 0.0062, 'grad_norm': 0.003753662109375, 'learning_rate': 0.00019630142035778964, 'epoch': 0.5475555555555556}
Step 78: {'loss': 0.003, 'grad_norm': 0.0028076171875, 'learning_rate': 0.00019607536761368484, 'epoch': 0.5546666666666666}
Step 79: {'loss': 0.0034, 'grad_norm': 0.002777099609375, 'learning_rate': 0.0001958427482458253, 'epoch': 0.5617777777777778}
Step 80: {'loss': 0.0066, 'grad_norm': 0.003936767578125, 'learning_rate': 0.00019560357815343577, 'epoch': 0.5688888888888889}
Step 81: {'loss': 0.0019, 'grad_norm': 0.003143310546875, 'learning_rate': 0.00019535787368347442, 'epoch': 0.576}
Step 82: {'loss': 0.0044, 'grad_norm': 0.0032806396484375, 'learning_rate': 0.00019510565162951537, 'epoch': 0.5831111111111111}
Step 83: {'loss': 0.0026, 'grad_norm': 0.00286865234375, 'learning_rate': 0.00019484692923060095, 'epoch': 0.5902222222222222}
Step 84: {'loss': 0.0061, 'grad_norm': 0.003936767578125, 'learning_rate': 0.00019458172417006347, 'epoch': 0.5973333333333334}
Step 85: {'loss': 0.0052, 'grad_norm': 0.0036468505859375, 'learning_rate': 0.00019431005457431653, 'epoch': 0.6044444444444445}
Step 86: {'loss': 0.0053, 'grad_norm': 0.00433349609375, 'learning_rate': 0.00019403193901161613, 'epoch': 0.6115555555555555}
Step 87: {'loss': 0.004, 'grad_norm': 0.0038604736328125, 'learning_rate': 0.00019374739649079153, 'epoch': 0.6186666666666667}
Step 88: {'loss': 0.0075, 'grad_norm': 0.005096435546875, 'learning_rate': 0.0001934564464599461, 'epoch': 0.6257777777777778}
Step 89: {'loss': 0.0038, 'grad_norm': 0.0036468505859375, 'learning_rate': 0.0001931591088051279, 'epoch': 0.6328888888888888}
Step 90: {'loss': 0.0048, 'grad_norm': 0.0045166015625, 'learning_rate': 0.00019285540384897073, 'epoch': 0.64}
Step 91: {'loss': 0.0043, 'grad_norm': 0.003692626953125, 'learning_rate': 0.00019254535234930486, 'epoch': 0.6471111111111111}
Step 92: {'loss': 0.0073, 'grad_norm': 0.00579833984375, 'learning_rate': 0.00019222897549773848, 'epoch': 0.6542222222222223}
Step 93: {'loss': 0.006, 'grad_norm': 0.004241943359375, 'learning_rate': 0.00019190629491820912, 'epoch': 0.6613333333333333}
Step 94: {'loss': 0.0059, 'grad_norm': 0.00439453125, 'learning_rate': 0.00019157733266550575, 'epoch': 0.6684444444444444}
Step 95: {'loss': 0.0039, 'grad_norm': 0.003631591796875, 'learning_rate': 0.00019124211122376137, 'epoch': 0.6755555555555556}
Step 96: {'loss': 0.0036, 'grad_norm': 0.0034942626953125, 'learning_rate': 0.00019090065350491626, 'epoch': 0.6826666666666666}
Step 97: {'loss': 0.0042, 'grad_norm': 0.003692626953125, 'learning_rate': 0.00019055298284715192, 'epoch': 0.6897777777777778}
Step 98: {'loss': 0.0077, 'grad_norm': 0.004669189453125, 'learning_rate': 0.00019019912301329592, 'epoch': 0.6968888888888889}
Step 99: {'loss': 0.0051, 'grad_norm': 0.0054931640625, 'learning_rate': 0.0001898390981891979, 'epoch': 0.704}
Step 100: {'loss': 0.0027, 'grad_norm': 0.00372314453125, 'learning_rate': 0.00018947293298207635, 'epoch': 0.7111111111111111}
Step 101: {'loss': 0.0071, 'grad_norm': 0.005767822265625, 'learning_rate': 0.0001891006524188368, 'epoch': 0.7182222222222222}
Step 102: {'loss': 0.0023, 'grad_norm': 0.0040283203125, 'learning_rate': 0.0001887222819443612, 'epoch': 0.7253333333333334}
Step 103: {'loss': 0.0039, 'grad_norm': 0.004302978515625, 'learning_rate': 0.0001883378474197689, 'epoch': 0.7324444444444445}
Step 104: {'loss': 0.0068, 'grad_norm': 0.0036773681640625, 'learning_rate': 0.0001879473751206489, 'epoch': 0.7395555555555555}
Step 105: {'loss': 0.0044, 'grad_norm': 0.003265380859375, 'learning_rate': 0.0001875508917352643, 'epoch': 0.7466666666666667}
Step 106: {'loss': 0.0052, 'grad_norm': 0.0032806396484375, 'learning_rate': 0.00018714842436272773, 'epoch': 0.7537777777777778}
Step 107: {'loss': 0.0029, 'grad_norm': 0.0028533935546875, 'learning_rate': 0.00018674000051114952, 'epoch': 0.7608888888888888}
Step 108: {'loss': 0.0037, 'grad_norm': 0.0034637451171875, 'learning_rate': 0.00018632564809575742, 'epoch': 0.768}
Step 109: {'loss': 0.005, 'grad_norm': 0.003204345703125, 'learning_rate': 0.00018590539543698854, 'epoch': 0.7751111111111111}
Step 110: {'loss': 0.0047, 'grad_norm': 0.0037841796875, 'learning_rate': 0.0001854792712585539, 'epoch': 0.7822222222222223}
Step 111: {'loss': 0.0026, 'grad_norm': 0.0028533935546875, 'learning_rate': 0.0001850473046854751, 'epoch': 0.7893333333333333}
Step 112: {'loss': 0.0012, 'grad_norm': 0.003326416015625, 'learning_rate': 0.00018460952524209355, 'epoch': 0.7964444444444444}
Step 113: {'loss': 0.0039, 'grad_norm': 0.0034942626953125, 'learning_rate': 0.00018416596285005272, 'epoch': 0.8035555555555556}
Step 114: {'loss': 0.0048, 'grad_norm': 0.0032501220703125, 'learning_rate': 0.00018371664782625287, 'epoch': 0.8106666666666666}
Step 115: {'loss': 0.0031, 'grad_norm': 0.002960205078125, 'learning_rate': 0.00018326161088077903, 'epoch': 0.8177777777777778}
Step 116: {'loss': 0.0053, 'grad_norm': 0.0034027099609375, 'learning_rate': 0.00018280088311480201, 'epoch': 0.8248888888888889}
Step 117: {'loss': 0.0031, 'grad_norm': 0.0029296875, 'learning_rate': 0.00018233449601845258, 'epoch': 0.832}
Step 118: {'loss': 0.0034, 'grad_norm': 0.003448486328125, 'learning_rate': 0.00018186248146866927, 'epoch': 0.8391111111111111}
Step 119: {'loss': 0.0049, 'grad_norm': 0.0035552978515625, 'learning_rate': 0.0001813848717270195, 'epoch': 0.8462222222222222}
Step 120: {'loss': 0.0052, 'grad_norm': 0.00372314453125, 'learning_rate': 0.00018090169943749476, 'epoch': 0.8533333333333334}
Step 121: {'loss': 0.0059, 'grad_norm': 0.003265380859375, 'learning_rate': 0.00018041299762427916, 'epoch': 0.8604444444444445}
Step 122: {'loss': 0.0072, 'grad_norm': 0.0038909912109375, 'learning_rate': 0.0001799187996894925, 'epoch': 0.8675555555555555}
Step 123: {'loss': 0.01, 'grad_norm': 0.004913330078125, 'learning_rate': 0.0001794191394109071, 'epoch': 0.8746666666666667}
Step 124: {'loss': 0.0039, 'grad_norm': 0.0033111572265625, 'learning_rate': 0.00017891405093963938, 'epoch': 0.8817777777777778}
Step 125: {'loss': 0.0043, 'grad_norm': 0.0037841796875, 'learning_rate': 0.0001784035687978153, 'epoch': 0.8888888888888888}
Step 126: {'loss': 0.0034, 'grad_norm': 0.005126953125, 'learning_rate': 0.00017788772787621126, 'epoch': 0.896}
Step 127: {'loss': 0.0053, 'grad_norm': 0.003997802734375, 'learning_rate': 0.00017736656343186896, 'epoch': 0.9031111111111111}
Step 128: {'loss': 0.0045, 'grad_norm': 0.0034332275390625, 'learning_rate': 0.00017684011108568592, 'epoch': 0.9102222222222223}
Step 129: {'loss': 0.003, 'grad_norm': 0.003082275390625, 'learning_rate': 0.00017630840681998066, 'epoch': 0.9173333333333333}
Step 130: {'loss': 0.0031, 'grad_norm': 0.0031280517578125, 'learning_rate': 0.0001757714869760335, 'epoch': 0.9244444444444444}
Step 131: {'loss': 0.0031, 'grad_norm': 0.0032501220703125, 'learning_rate': 0.0001752293882516025, 'epoch': 0.9315555555555556}
Step 132: {'loss': 0.0056, 'grad_norm': 0.0036468505859375, 'learning_rate': 0.0001746821476984154, 'epoch': 0.9386666666666666}
Step 133: {'loss': 0.0047, 'grad_norm': 0.003173828125, 'learning_rate': 0.0001741298027196371, 'epoch': 0.9457777777777778}
Step 134: {'loss': 0.0048, 'grad_norm': 0.003265380859375, 'learning_rate': 0.00017357239106731317, 'epoch': 0.9528888888888889}
Step 135: {'loss': 0.0058, 'grad_norm': 0.0035247802734375, 'learning_rate': 0.00017300995083978965, 'epoch': 0.96}
Step 136: {'loss': 0.0057, 'grad_norm': 0.00341796875, 'learning_rate': 0.00017244252047910892, 'epoch': 0.9671111111111111}
Step 137: {'loss': 0.0035, 'grad_norm': 0.0030670166015625, 'learning_rate': 0.0001718701387683824, 'epoch': 0.9742222222222222}
Step 138: {'loss': 0.0044, 'grad_norm': 0.003387451171875, 'learning_rate': 0.00017129284482913972, 'epoch': 0.9813333333333333}
Step 139: {'loss': 0.0043, 'grad_norm': 0.00311279296875, 'learning_rate': 0.00017071067811865476, 'epoch': 0.9884444444444445}
Step 140: {'loss': 0.0055, 'grad_norm': 0.0028839111328125, 'learning_rate': 0.00017012367842724887, 'epoch': 0.9955555555555555}
Step 141: {'loss': 0.0009, 'grad_norm': 0.00262451171875, 'learning_rate': 0.00016953188587557122, 'epoch': 1.0}
Step 142: {'loss': 0.0039, 'grad_norm': 0.0029144287109375, 'learning_rate': 0.0001689353409118566, 'epoch': 1.007111111111111}
Step 143: {'loss': 0.0028, 'grad_norm': 0.003082275390625, 'learning_rate': 0.00016833408430916085, 'epoch': 1.0142222222222221}
Step 144: {'loss': 0.0054, 'grad_norm': 0.0032958984375, 'learning_rate': 0.00016772815716257412, 'epoch': 1.0213333333333334}
Step 145: {'loss': 0.0033, 'grad_norm': 0.0031280517578125, 'learning_rate': 0.00016711760088641196, 'epoch': 1.0284444444444445}
Step 146: {'loss': 0.0046, 'grad_norm': 0.0028839111328125, 'learning_rate': 0.0001665024572113848, 'epoch': 1.0355555555555556}
Step 147: {'loss': 0.0061, 'grad_norm': 0.0032501220703125, 'learning_rate': 0.0001658827681817458, 'epoch': 1.0426666666666666}
Step 148: {'loss': 0.0038, 'grad_norm': 0.00335693359375, 'learning_rate': 0.00016525857615241687, 'epoch': 1.0497777777777777}
Step 149: {'loss': 0.0054, 'grad_norm': 0.00311279296875, 'learning_rate': 0.00016462992378609407, 'epoch': 1.056888888888889}
Step 150: {'loss': 0.0042, 'grad_norm': 0.00335693359375, 'learning_rate': 0.00016399685405033167, 'epoch': 1.064}
Step 151: {'loss': 0.0047, 'grad_norm': 0.003143310546875, 'learning_rate': 0.00016335941021460506, 'epoch': 1.0711111111111111}
Step 152: {'loss': 0.005, 'grad_norm': 0.0030517578125, 'learning_rate': 0.0001627176358473537, 'epoch': 1.0782222222222222}
Step 153: {'loss': 0.0036, 'grad_norm': 0.0030517578125, 'learning_rate': 0.00016207157481300312, 'epoch': 1.0853333333333333}
Step 154: {'loss': 0.0043, 'grad_norm': 0.00323486328125, 'learning_rate': 0.0001614212712689668, 'epoch': 1.0924444444444443}
Step 155: {'loss': 0.0034, 'grad_norm': 0.003021240234375, 'learning_rate': 0.00016076676966262813, 'epoch': 1.0995555555555556}
Step 156: {'loss': 0.0065, 'grad_norm': 0.0037841796875, 'learning_rate': 0.00016010811472830252, 'epoch': 1.1066666666666667}
Step 157: {'loss': 0.0043, 'grad_norm': 0.004119873046875, 'learning_rate': 0.00015944535148417982, 'epoch': 1.1137777777777778}
Step 158: {'loss': 0.0036, 'grad_norm': 0.003753662109375, 'learning_rate': 0.00015877852522924732, 'epoch': 1.1208888888888888}
Step 159: {'loss': 0.0037, 'grad_norm': 0.00323486328125, 'learning_rate': 0.00015810768154019385, 'epoch': 1.1280000000000001}
Step 160: {'loss': 0.0039, 'grad_norm': 0.0030517578125, 'learning_rate': 0.00015743286626829437, 'epoch': 1.1351111111111112}
Step 161: {'loss': 0.0028, 'grad_norm': 0.0028228759765625, 'learning_rate': 0.00015675412553627639, 'epoch': 1.1422222222222222}
Step 162: {'loss': 0.0048, 'grad_norm': 0.00347900390625, 'learning_rate': 0.0001560715057351673, 'epoch': 1.1493333333333333}
Step 163: {'loss': 0.0049, 'grad_norm': 0.003570556640625, 'learning_rate': 0.00015538505352112375, 'epoch': 1.1564444444444444}
Step 164: {'loss': 0.0024, 'grad_norm': 0.0029754638671875, 'learning_rate': 0.00015469481581224272, 'epoch': 1.1635555555555555}
Step 165: {'loss': 0.0059, 'grad_norm': 0.0034942626953125, 'learning_rate': 0.00015400083978535473, 'epoch': 1.1706666666666667}
Step 166: {'loss': 0.0053, 'grad_norm': 0.003631591796875, 'learning_rate': 0.0001533031728727994, 'epoch': 1.1777777777777778}
Step 167: {'loss': 0.003, 'grad_norm': 0.002899169921875, 'learning_rate': 0.00015260186275918342, 'epoch': 1.1848888888888889}
Step 168: {'loss': 0.006, 'grad_norm': 0.0037078857421875, 'learning_rate': 0.00015189695737812152, 'epoch': 1.192}
Step 169: {'loss': 0.0062, 'grad_norm': 0.0030364990234375, 'learning_rate': 0.00015118850490896012, 'epoch': 1.199111111111111}
Step 170: {'loss': 0.0044, 'grad_norm': 0.002960205078125, 'learning_rate': 0.0001504765537734844, 'epoch': 1.2062222222222223}
Step 171: {'loss': 0.0064, 'grad_norm': 0.0037994384765625, 'learning_rate': 0.00014976115263260875, 'epoch': 1.2133333333333334}
Step 172: {'loss': 0.0034, 'grad_norm': 0.0028839111328125, 'learning_rate': 0.00014904235038305083, 'epoch': 1.2204444444444444}
Step 173: {'loss': 0.0047, 'grad_norm': 0.0032958984375, 'learning_rate': 0.00014832019615398963, 'epoch': 1.2275555555555555}
Step 174: {'loss': 0.0056, 'grad_norm': 0.0037994384765625, 'learning_rate': 0.00014759473930370736, 'epoch': 1.2346666666666666}
Step 175: {'loss': 0.0027, 'grad_norm': 0.0027618408203125, 'learning_rate': 0.00014686602941621615, 'epoch': 1.2417777777777779}
Step 176: {'loss': 0.0063, 'grad_norm': 0.0035247802734375, 'learning_rate': 0.0001461341162978688, 'epoch': 1.248888888888889}
Step 177: {'loss': 0.0029, 'grad_norm': 0.0030364990234375, 'learning_rate': 0.00014539904997395468, 'epoch': 1.256}
Step 178: {'loss': 0.0078, 'grad_norm': 0.004364013671875, 'learning_rate': 0.00014466088068528068, 'epoch': 1.263111111111111}
Step 179: {'loss': 0.005, 'grad_norm': 0.003143310546875, 'learning_rate': 0.00014391965888473703, 'epoch': 1.2702222222222221}
Step 180: {'loss': 0.0052, 'grad_norm': 0.00372314453125, 'learning_rate': 0.00014317543523384928, 'epoch': 1.2773333333333334}
Step 181: {'loss': 0.0063, 'grad_norm': 0.003204345703125, 'learning_rate': 0.00014242826059931537, 'epoch': 1.2844444444444445}
Step 182: {'loss': 0.0053, 'grad_norm': 0.004302978515625, 'learning_rate': 0.00014167818604952906, 'epoch': 1.2915555555555556}
Step 183: {'loss': 0.0054, 'grad_norm': 0.0038299560546875, 'learning_rate': 0.0001409252628510894, 'epoch': 1.2986666666666666}
Step 184: {'loss': 0.004, 'grad_norm': 0.0034637451171875, 'learning_rate': 0.00014016954246529696, 'epoch': 1.3057777777777777}
Step 185: {'loss': 0.0029, 'grad_norm': 0.0036773681640625, 'learning_rate': 0.0001394110765446362, 'epoch': 1.3128888888888888}
Step 186: {'loss': 0.0041, 'grad_norm': 0.0029144287109375, 'learning_rate': 0.00013864991692924523, 'epoch': 1.32}
Step 187: {'loss': 0.003, 'grad_norm': 0.0029296875, 'learning_rate': 0.00013788611564337277, 'epoch': 1.3271111111111111}
Step 188: {'loss': 0.0057, 'grad_norm': 0.002838134765625, 'learning_rate': 0.00013711972489182208, 'epoch': 1.3342222222222222}
Step 189: {'loss': 0.0047, 'grad_norm': 0.0034027099609375, 'learning_rate': 0.00013635079705638298, 'epoch': 1.3413333333333333}
Step 190: {'loss': 0.0034, 'grad_norm': 0.0032806396484375, 'learning_rate': 0.00013557938469225167, 'epoch': 1.3484444444444446}
Step 191: {'loss': 0.0048, 'grad_norm': 0.00421142578125, 'learning_rate': 0.00013480554052443846, 'epoch': 1.3555555555555556}
Step 192: {'loss': 0.0059, 'grad_norm': 0.0033111572265625, 'learning_rate': 0.00013402931744416433, 'epoch': 1.3626666666666667}
Step 193: {'loss': 0.0038, 'grad_norm': 0.00274658203125, 'learning_rate': 0.0001332507685052457, 'epoch': 1.3697777777777778}
Step 194: {'loss': 0.0047, 'grad_norm': 0.0037689208984375, 'learning_rate': 0.00013246994692046836, 'epoch': 1.3768888888888888}
Step 195: {'loss': 0.0044, 'grad_norm': 0.0028533935546875, 'learning_rate': 0.00013168690605795045, 'epoch': 1.384}
Step 196: {'loss': 0.0073, 'grad_norm': 0.0033721923828125, 'learning_rate': 0.00013090169943749476, 'epoch': 1.3911111111111112}
Step 197: {'loss': 0.0047, 'grad_norm': 0.0031585693359375, 'learning_rate': 0.00013011438072693077, 'epoch': 1.3982222222222223}
Step 198: {'loss': 0.0044, 'grad_norm': 0.0028839111328125, 'learning_rate': 0.0001293250037384465, 'epoch': 1.4053333333333333}
Step 199: {'loss': 0.0075, 'grad_norm': 0.0030517578125, 'learning_rate': 0.00012853362242491053, 'epoch': 1.4124444444444444}
Step 200: {'loss': 0.0036, 'grad_norm': 0.0029449462890625, 'learning_rate': 0.00012774029087618446, 'epoch': 1.4195555555555557}
Step 201: {'loss': 0.0035, 'grad_norm': 0.003021240234375, 'learning_rate': 0.0001269450633154258, 'epoch': 1.4266666666666667}
Step 202: {'loss': 0.0032, 'grad_norm': 0.00274658203125, 'learning_rate': 0.00012614799409538198, 'epoch': 1.4337777777777778}
Step 203: {'loss': 0.0037, 'grad_norm': 0.003753662109375, 'learning_rate': 0.0001253491376946754, 'epoch': 1.4408888888888889}
Step 204: {'loss': 0.0034, 'grad_norm': 0.0029144287109375, 'learning_rate': 0.00012454854871407994, 'epoch': 1.448}
Step 205: {'loss': 0.0028, 'grad_norm': 0.003143310546875, 'learning_rate': 0.00012374628187278888, 'epoch': 1.455111111111111}
Step 206: {'loss': 0.0064, 'grad_norm': 0.0031585693359375, 'learning_rate': 0.00012294239200467516, 'epoch': 1.462222222222222}
Step 207: {'loss': 0.0051, 'grad_norm': 0.0031585693359375, 'learning_rate': 0.00012213693405454344, 'epoch': 1.4693333333333334}
Step 208: {'loss': 0.0048, 'grad_norm': 0.0027618408203125, 'learning_rate': 0.0001213299630743747, 'epoch': 1.4764444444444444}
Step 209: {'loss': 0.0023, 'grad_norm': 0.003143310546875, 'learning_rate': 0.00012052153421956342, 'epoch': 1.4835555555555555}
Step 210: {'loss': 0.0032, 'grad_norm': 0.0025482177734375, 'learning_rate': 0.00011971170274514802, 'epoch': 1.4906666666666666}
Step 211: {'loss': 0.0036, 'grad_norm': 0.002716064453125, 'learning_rate': 0.00011890052400203404, 'epoch': 1.4977777777777779}
Step 212: {'loss': 0.0062, 'grad_norm': 0.003509521484375, 'learning_rate': 0.000118088053433211, 'epoch': 1.504888888888889}
Step 213: {'loss': 0.0025, 'grad_norm': 0.0026702880859375, 'learning_rate': 0.00011727434656996305, 'epoch': 1.512}
Step 214: {'loss': 0.0057, 'grad_norm': 0.0030059814453125, 'learning_rate': 0.00011645945902807341, 'epoch': 1.519111111111111}
Step 215: {'loss': 0.003, 'grad_norm': 0.0029144287109375, 'learning_rate': 0.0001156434465040231, 'epoch': 1.5262222222222221}
Step 216: {'loss': 0.0094, 'grad_norm': 0.00390625, 'learning_rate': 0.0001148263647711842, 'epoch': 1.5333333333333332}
Step 217: {'loss': 0.0028, 'grad_norm': 0.0028228759765625, 'learning_rate': 0.0001140082696760078, 'epoch': 1.5404444444444443}
Step 218: {'loss': 0.0044, 'grad_norm': 0.0031280517578125, 'learning_rate': 0.00011318921713420691, 'epoch': 1.5475555555555556}
Step 219: {'loss': 0.0049, 'grad_norm': 0.0029144287109375, 'learning_rate': 0.00011236926312693479, 'epoch': 1.5546666666666666}
Step 220: {'loss': 0.0036, 'grad_norm': 0.0030059814453125, 'learning_rate': 0.00011154846369695863, 'epoch': 1.561777777777778}
Step 221: {'loss': 0.005, 'grad_norm': 0.0031890869140625, 'learning_rate': 0.00011072687494482919, 'epoch': 1.568888888888889}
Step 222: {'loss': 0.007, 'grad_norm': 0.0036163330078125, 'learning_rate': 0.0001099045530250463, 'epoch': 1.576}
Step 223: {'loss': 0.003, 'grad_norm': 0.0029754638671875, 'learning_rate': 0.00010908155414222083, 'epoch': 1.5831111111111111}
Step 224: {'loss': 0.0028, 'grad_norm': 0.0031280517578125, 'learning_rate': 0.00010825793454723325, 'epoch': 1.5902222222222222}
Step 225: {'loss': 0.009, 'grad_norm': 0.002960205078125, 'learning_rate': 0.00010743375053338877, 'epoch': 1.5973333333333333}
Step 226: {'loss': 0.0053, 'grad_norm': 0.00360107421875, 'learning_rate': 0.00010660905843256994, 'epoch': 1.6044444444444443}
Step 227: {'loss': 0.0045, 'grad_norm': 0.0028839111328125, 'learning_rate': 0.00010578391461138641, 'epoch': 1.6115555555555554}
Step 228: {'loss': 0.0048, 'grad_norm': 0.0038604736328125, 'learning_rate': 0.00010495837546732224, 'epoch': 1.6186666666666667}
Step 229: {'loss': 0.0034, 'grad_norm': 0.002685546875, 'learning_rate': 0.00010413249742488131, 'epoch': 1.6257777777777778}
Step 230: {'loss': 0.0065, 'grad_norm': 0.003021240234375, 'learning_rate': 0.00010330633693173082, 'epoch': 1.6328888888888888}
Step 231: {'loss': 0.0033, 'grad_norm': 0.002838134765625, 'learning_rate': 0.00010247995045484302, 'epoch': 1.6400000000000001}
Step 232: {'loss': 0.0041, 'grad_norm': 0.0035247802734375, 'learning_rate': 0.00010165339447663587, 'epoch': 1.6471111111111112}
Step 233: {'loss': 0.0063, 'grad_norm': 0.0029296875, 'learning_rate': 0.0001008267254911125, 'epoch': 1.6542222222222223}
Step 234: {'loss': 0.0028, 'grad_norm': 0.0027618408203125, 'learning_rate': 0.0001, 'epoch': 1.6613333333333333}
Step 235: {'loss': 0.0035, 'grad_norm': 0.00347900390625, 'learning_rate': 9.917327450888751e-05, 'epoch': 1.6684444444444444}
Step 236: {'loss': 0.0049, 'grad_norm': 0.0036163330078125, 'learning_rate': 9.834660552336415e-05, 'epoch': 1.6755555555555555}
Step 237: {'loss': 0.0032, 'grad_norm': 0.0033416748046875, 'learning_rate': 9.7520049545157e-05, 'epoch': 1.6826666666666665}
Step 238: {'loss': 0.0029, 'grad_norm': 0.0030364990234375, 'learning_rate': 9.669366306826919e-05, 'epoch': 1.6897777777777778}
Step 239: {'loss': 0.0044, 'grad_norm': 0.0028839111328125, 'learning_rate': 9.586750257511867e-05, 'epoch': 1.696888888888889}
Step 240: {'loss': 0.0041, 'grad_norm': 0.00299072265625, 'learning_rate': 9.504162453267777e-05, 'epoch': 1.704}
Step 241: {'loss': 0.0065, 'grad_norm': 0.0036773681640625, 'learning_rate': 9.421608538861361e-05, 'epoch': 1.7111111111111112}
Step 242: {'loss': 0.0035, 'grad_norm': 0.0034332275390625, 'learning_rate': 9.339094156743007e-05, 'epoch': 1.7182222222222223}
Step 243: {'loss': 0.0035, 'grad_norm': 0.003021240234375, 'learning_rate': 9.256624946661125e-05, 'epoch': 1.7253333333333334}
Step 244: {'loss': 0.0031, 'grad_norm': 0.003997802734375, 'learning_rate': 9.174206545276677e-05, 'epoch': 1.7324444444444445}
Step 245: {'loss': 0.0065, 'grad_norm': 0.0035858154296875, 'learning_rate': 9.091844585777918e-05, 'epoch': 1.7395555555555555}
Step 246: {'loss': 0.0064, 'grad_norm': 0.0034027099609375, 'learning_rate': 9.009544697495374e-05, 'epoch': 1.7466666666666666}
Step 247: {'loss': 0.0044, 'grad_norm': 0.0031280517578125, 'learning_rate': 8.927312505517085e-05, 'epoch': 1.7537777777777777}
Step 248: {'loss': 0.0033, 'grad_norm': 0.0029296875, 'learning_rate': 8.845153630304139e-05, 'epoch': 1.7608888888888887}
Step 249: {'loss': 0.0069, 'grad_norm': 0.0036468505859375, 'learning_rate': 8.763073687306524e-05, 'epoch': 1.768}
Step 250: {'loss': 0.0044, 'grad_norm': 0.0029449462890625, 'learning_rate': 8.681078286579311e-05, 'epoch': 1.775111111111111}
Step 251: {'loss': 0.0042, 'grad_norm': 0.00347900390625, 'learning_rate': 8.599173032399221e-05, 'epoch': 1.7822222222222224}
Step 252: {'loss': 0.0023, 'grad_norm': 0.003387451171875, 'learning_rate': 8.517363522881579e-05, 'epoch': 1.7893333333333334}
Step 253: {'loss': 0.0034, 'grad_norm': 0.0029144287109375, 'learning_rate': 8.435655349597689e-05, 'epoch': 1.7964444444444445}
Step 254: {'loss': 0.0047, 'grad_norm': 0.003143310546875, 'learning_rate': 8.35405409719266e-05, 'epoch': 1.8035555555555556}
Step 255: {'loss': 0.0036, 'grad_norm': 0.0032196044921875, 'learning_rate': 8.2725653430037e-05, 'epoch': 1.8106666666666666}
Step 256: {'loss': 0.0043, 'grad_norm': 0.0034637451171875, 'learning_rate': 8.191194656678904e-05, 'epoch': 1.8177777777777777}
Step 257: {'loss': 0.0042, 'grad_norm': 0.0030364990234375, 'learning_rate': 8.1099475997966e-05, 'epoch': 1.8248888888888888}
Step 258: {'loss': 0.0035, 'grad_norm': 0.0030364990234375, 'learning_rate': 8.028829725485199e-05, 'epoch': 1.8319999999999999}
Step 259: {'loss': 0.0066, 'grad_norm': 0.0036163330078125, 'learning_rate': 7.947846578043659e-05, 'epoch': 1.8391111111111111}
Step 260: {'loss': 0.0052, 'grad_norm': 0.0030670166015625, 'learning_rate': 7.867003692562534e-05, 'epoch': 1.8462222222222222}
Step 261: {'loss': 0.005, 'grad_norm': 0.00299072265625, 'learning_rate': 7.786306594545657e-05, 'epoch': 1.8533333333333335}
Step 262: {'loss': 0.0055, 'grad_norm': 0.003265380859375, 'learning_rate': 7.705760799532485e-05, 'epoch': 1.8604444444444446}
Step 263: {'loss': 0.0061, 'grad_norm': 0.003936767578125, 'learning_rate': 7.625371812721114e-05, 'epoch': 1.8675555555555556}
Step 264: {'loss': 0.0048, 'grad_norm': 0.003173828125, 'learning_rate': 7.54514512859201e-05, 'epoch': 1.8746666666666667}
Step 265: {'loss': 0.005, 'grad_norm': 0.0030517578125, 'learning_rate': 7.46508623053246e-05, 'epoch': 1.8817777777777778}
Step 266: {'loss': 0.0024, 'grad_norm': 0.0030670166015625, 'learning_rate': 7.385200590461803e-05, 'epoch': 1.8888888888888888}
Step 267: {'loss': 0.0063, 'grad_norm': 0.003662109375, 'learning_rate': 7.30549366845742e-05, 'epoch': 1.896}
Step 268: {'loss': 0.0034, 'grad_norm': 0.0038909912109375, 'learning_rate': 7.225970912381556e-05, 'epoch': 1.903111111111111}
Step 269: {'loss': 0.0036, 'grad_norm': 0.00274658203125, 'learning_rate': 7.146637757508949e-05, 'epoch': 1.9102222222222223}
Step 270: {'loss': 0.0039, 'grad_norm': 0.003021240234375, 'learning_rate': 7.067499626155354e-05, 'epoch': 1.9173333333333333}
Step 271: {'loss': 0.0037, 'grad_norm': 0.0035858154296875, 'learning_rate': 6.988561927306927e-05, 'epoch': 1.9244444444444444}
Step 272: {'loss': 0.0029, 'grad_norm': 0.00299072265625, 'learning_rate': 6.909830056250527e-05, 'epoch': 1.9315555555555557}
Step 273: {'loss': 0.0059, 'grad_norm': 0.0037994384765625, 'learning_rate': 6.831309394204957e-05, 'epoch': 1.9386666666666668}
Step 274: {'loss': 0.0061, 'grad_norm': 0.0037384033203125, 'learning_rate': 6.753005307953167e-05, 'epoch': 1.9457777777777778}
Step 275: {'loss': 0.0075, 'grad_norm': 0.0033111572265625, 'learning_rate': 6.674923149475432e-05, 'epoch': 1.952888888888889}
Step 276: {'loss': 0.0041, 'grad_norm': 0.003448486328125, 'learning_rate': 6.59706825558357e-05, 'epoch': 1.96}
Step 277: {'loss': 0.0036, 'grad_norm': 0.003021240234375, 'learning_rate': 6.519445947556155e-05, 'epoch': 1.967111111111111}
Step 278: {'loss': 0.0065, 'grad_norm': 0.003387451171875, 'learning_rate': 6.442061530774834e-05, 'epoch': 1.974222222222222}
Step 279: {'loss': 0.006, 'grad_norm': 0.0038299560546875, 'learning_rate': 6.3649202943617e-05, 'epoch': 1.9813333333333332}
Step 280: {'loss': 0.0049, 'grad_norm': 0.003326416015625, 'learning_rate': 6.28802751081779e-05, 'epoch': 1.9884444444444445}
Step 281: {'loss': 0.0058, 'grad_norm': 0.003265380859375, 'learning_rate': 6.211388435662721e-05, 'epoch': 1.9955555555555555}
Step 282: {'loss': 0.0022, 'grad_norm': 0.00238037109375, 'learning_rate': 6.135008307075481e-05, 'epoch': 2.0}
Step 283: {'loss': 0.0031, 'grad_norm': 0.003936767578125, 'learning_rate': 6.0588923455363864e-05, 'epoch': 2.007111111111111}
Step 284: {'loss': 0.0034, 'grad_norm': 0.003814697265625, 'learning_rate': 5.983045753470308e-05, 'epoch': 2.014222222222222}
Step 285: {'loss': 0.0035, 'grad_norm': 0.0042724609375, 'learning_rate': 5.907473714891061e-05, 'epoch': 2.021333333333333}
Step 286: {'loss': 0.0035, 'grad_norm': 0.0033111572265625, 'learning_rate': 5.832181395047098e-05, 'epoch': 2.0284444444444443}
Step 287: {'loss': 0.0052, 'grad_norm': 0.0034027099609375, 'learning_rate': 5.757173940068464e-05, 'epoch': 2.0355555555555553}
Step 288: {'loss': 0.0055, 'grad_norm': 0.0038909912109375, 'learning_rate': 5.6824564766150726e-05, 'epoch': 2.042666666666667}
Step 289: {'loss': 0.0065, 'grad_norm': 0.00445556640625, 'learning_rate': 5.608034111526298e-05, 'epoch': 2.049777777777778}
Step 290: {'loss': 0.0053, 'grad_norm': 0.0034637451171875, 'learning_rate': 5.533911931471936e-05, 'epoch': 2.056888888888889}
Step 291: {'loss': 0.004, 'grad_norm': 0.0033416748046875, 'learning_rate': 5.4600950026045326e-05, 'epoch': 2.064}
Step 292: {'loss': 0.0027, 'grad_norm': 0.003204345703125, 'learning_rate': 5.386588370213124e-05, 'epoch': 2.071111111111111}
Step 293: {'loss': 0.0027, 'grad_norm': 0.0030364990234375, 'learning_rate': 5.313397058378386e-05, 'epoch': 2.078222222222222}
Step 294: {'loss': 0.0039, 'grad_norm': 0.00335693359375, 'learning_rate': 5.240526069629265e-05, 'epoch': 2.0853333333333333}
Step 295: {'loss': 0.004, 'grad_norm': 0.0030517578125, 'learning_rate': 5.167980384601041e-05, 'epoch': 2.0924444444444443}
Step 296: {'loss': 0.0074, 'grad_norm': 0.003662109375, 'learning_rate': 5.095764961694922e-05, 'epoch': 2.0995555555555554}
Step 297: {'loss': 0.0052, 'grad_norm': 0.003326416015625, 'learning_rate': 5.023884736739132e-05, 'epoch': 2.1066666666666665}
Step 298: {'loss': 0.0042, 'grad_norm': 0.0032196044921875, 'learning_rate': 4.952344622651566e-05, 'epoch': 2.113777777777778}
Step 299: {'loss': 0.0035, 'grad_norm': 0.0035247802734375, 'learning_rate': 4.8811495091039926e-05, 'epoch': 2.120888888888889}
Step 300: {'loss': 0.0037, 'grad_norm': 0.0030364990234375, 'learning_rate': 4.810304262187852e-05, 'epoch': 2.128}
Step 301: {'loss': 0.0057, 'grad_norm': 0.004302978515625, 'learning_rate': 4.739813724081661e-05, 'epoch': 2.135111111111111}
Step 302: {'loss': 0.0065, 'grad_norm': 0.00311279296875, 'learning_rate': 4.669682712720065e-05, 'epoch': 2.1422222222222222}
Step 303: {'loss': 0.0033, 'grad_norm': 0.003326416015625, 'learning_rate': 4.599916021464531e-05, 'epoch': 2.1493333333333333}
Step 304: {'loss': 0.0023, 'grad_norm': 0.0029754638671875, 'learning_rate': 4.530518418775733e-05, 'epoch': 2.1564444444444444}
Step 305: {'loss': 0.005, 'grad_norm': 0.0030517578125, 'learning_rate': 4.461494647887631e-05, 'epoch': 2.1635555555555555}
Step 306: {'loss': 0.0052, 'grad_norm': 0.004119873046875, 'learning_rate': 4.392849426483274e-05, 'epoch': 2.1706666666666665}
Step 307: {'loss': 0.005, 'grad_norm': 0.0028228759765625, 'learning_rate': 4.3245874463723645e-05, 'epoch': 2.1777777777777776}
Step 308: {'loss': 0.0046, 'grad_norm': 0.003631591796875, 'learning_rate': 4.256713373170564e-05, 'epoch': 2.1848888888888887}
Step 309: {'loss': 0.0022, 'grad_norm': 0.0029144287109375, 'learning_rate': 4.189231845980618e-05, 'epoch': 2.192}
Step 310: {'loss': 0.0025, 'grad_norm': 0.003082275390625, 'learning_rate': 4.12214747707527e-05, 'epoch': 2.1991111111111112}
Step 311: {'loss': 0.0028, 'grad_norm': 0.0028076171875, 'learning_rate': 4.055464851582021e-05, 'epoch': 2.2062222222222223}
Step 312: {'loss': 0.0041, 'grad_norm': 0.003997802734375, 'learning_rate': 3.9891885271697496e-05, 'epoch': 2.2133333333333334}
Step 313: {'loss': 0.0057, 'grad_norm': 0.00360107421875, 'learning_rate': 3.9233230337371886e-05, 'epoch': 2.2204444444444444}
Step 314: {'loss': 0.0046, 'grad_norm': 0.0032196044921875, 'learning_rate': 3.857872873103322e-05, 'epoch': 2.2275555555555555}
Step 315: {'loss': 0.0048, 'grad_norm': 0.0033721923828125, 'learning_rate': 3.7928425186996885e-05, 'epoch': 2.2346666666666666}
Step 316: {'loss': 0.006, 'grad_norm': 0.0031890869140625, 'learning_rate': 3.7282364152646297e-05, 'epoch': 2.2417777777777776}
Step 317: {'loss': 0.0048, 'grad_norm': 0.0035858154296875, 'learning_rate': 3.6640589785394955e-05, 'epoch': 2.2488888888888887}
Step 318: {'loss': 0.0044, 'grad_norm': 0.0035247802734375, 'learning_rate': 3.600314594966834e-05, 'epoch': 2.2560000000000002}
Step 319: {'loss': 0.0047, 'grad_norm': 0.0032806396484375, 'learning_rate': 3.53700762139059e-05, 'epoch': 2.2631111111111113}
Step 320: {'loss': 0.0075, 'grad_norm': 0.00390625, 'learning_rate': 3.4741423847583134e-05, 'epoch': 2.2702222222222224}
Step 321: {'loss': 0.0043, 'grad_norm': 0.0032958984375, 'learning_rate': 3.41172318182542e-05, 'epoch': 2.2773333333333334}
Step 322: {'loss': 0.006, 'grad_norm': 0.003997802734375, 'learning_rate': 3.349754278861517e-05, 'epoch': 2.2844444444444445}
Step 323: {'loss': 0.0061, 'grad_norm': 0.004180908203125, 'learning_rate': 3.2882399113588066e-05, 'epoch': 2.2915555555555556}
Step 324: {'loss': 0.0022, 'grad_norm': 0.0030059814453125, 'learning_rate': 3.227184283742591e-05, 'epoch': 2.2986666666666666}
Step 325: {'loss': 0.0053, 'grad_norm': 0.0036468505859375, 'learning_rate': 3.166591569083916e-05, 'epoch': 2.3057777777777777}
Step 326: {'loss': 0.0022, 'grad_norm': 0.002960205078125, 'learning_rate': 3.106465908814342e-05, 'epoch': 2.3128888888888888}
Step 327: {'loss': 0.0075, 'grad_norm': 0.003387451171875, 'learning_rate': 3.0468114124428803e-05, 'epoch': 2.32}
Step 328: {'loss': 0.0052, 'grad_norm': 0.002899169921875, 'learning_rate': 2.9876321572751144e-05, 'epoch': 2.327111111111111}
Step 329: {'loss': 0.0062, 'grad_norm': 0.002960205078125, 'learning_rate': 2.9289321881345254e-05, 'epoch': 2.3342222222222224}
Step 330: {'loss': 0.0052, 'grad_norm': 0.0035552978515625, 'learning_rate': 2.87071551708603e-05, 'epoch': 2.3413333333333335}
Step 331: {'loss': 0.0022, 'grad_norm': 0.0037384033203125, 'learning_rate': 2.8129861231617615e-05, 'epoch': 2.3484444444444446}
Step 332: {'loss': 0.0043, 'grad_norm': 0.0034942626953125, 'learning_rate': 2.7557479520891104e-05, 'epoch': 2.3555555555555556}
Step 333: {'loss': 0.0073, 'grad_norm': 0.0035552978515625, 'learning_rate': 2.6990049160210385e-05, 'epoch': 2.3626666666666667}
Step 334: {'loss': 0.0027, 'grad_norm': 0.00286865234375, 'learning_rate': 2.6427608932686843e-05, 'epoch': 2.3697777777777778}
Step 335: {'loss': 0.0022, 'grad_norm': 0.0033416748046875, 'learning_rate': 2.587019728036292e-05, 'epoch': 2.376888888888889}
Step 336: {'loss': 0.0056, 'grad_norm': 0.0032806396484375, 'learning_rate': 2.5317852301584643e-05, 'epoch': 2.384}
Step 337: {'loss': 0.0065, 'grad_norm': 0.00390625, 'learning_rate': 2.4770611748397553e-05, 'epoch': 2.391111111111111}
Step 338: {'loss': 0.0049, 'grad_norm': 0.003082275390625, 'learning_rate': 2.422851302396655e-05, 'epoch': 2.398222222222222}
Step 339: {'loss': 0.0046, 'grad_norm': 0.0035247802734375, 'learning_rate': 2.3691593180019366e-05, 'epoch': 2.405333333333333}
Step 340: {'loss': 0.0038, 'grad_norm': 0.003997802734375, 'learning_rate': 2.315988891431412e-05, 'epoch': 2.4124444444444446}
Step 341: {'loss': 0.0053, 'grad_norm': 0.002716064453125, 'learning_rate': 2.2633436568131074e-05, 'epoch': 2.4195555555555557}
Step 342: {'loss': 0.0026, 'grad_norm': 0.00408935546875, 'learning_rate': 2.2112272123788768e-05, 'epoch': 2.4266666666666667}
Step 343: {'loss': 0.0039, 'grad_norm': 0.00341796875, 'learning_rate': 2.1596431202184708e-05, 'epoch': 2.433777777777778}
Step 344: {'loss': 0.0016, 'grad_norm': 0.0027923583984375, 'learning_rate': 2.1085949060360654e-05, 'epoch': 2.440888888888889}
Step 345: {'loss': 0.006, 'grad_norm': 0.003570556640625, 'learning_rate': 2.0580860589092897e-05, 'epoch': 2.448}
Step 346: {'loss': 0.0037, 'grad_norm': 0.002899169921875, 'learning_rate': 2.008120031050753e-05, 'epoch': 2.455111111111111}
Step 347: {'loss': 0.004, 'grad_norm': 0.002899169921875, 'learning_rate': 1.9587002375720864e-05, 'epoch': 2.462222222222222}
Step 348: {'loss': 0.0035, 'grad_norm': 0.0029449462890625, 'learning_rate': 1.9098300562505266e-05, 'epoch': 2.469333333333333}
Step 349: {'loss': 0.0028, 'grad_norm': 0.0033721923828125, 'learning_rate': 1.861512827298051e-05, 'epoch': 2.4764444444444447}
Step 350: {'loss': 0.0036, 'grad_norm': 0.00299072265625, 'learning_rate': 1.8137518531330767e-05, 'epoch': 2.4835555555555557}
Step 351: {'loss': 0.0038, 'grad_norm': 0.00341796875, 'learning_rate': 1.7665503981547428e-05, 'epoch': 2.490666666666667}
Step 352: {'loss': 0.0061, 'grad_norm': 0.003570556640625, 'learning_rate': 1.7199116885197995e-05, 'epoch': 2.497777777777778}
Step 353: {'loss': 0.0016, 'grad_norm': 0.003326416015625, 'learning_rate': 1.6738389119220964e-05, 'epoch': 2.504888888888889}
Step 354: {'loss': 0.0067, 'grad_norm': 0.0034637451171875, 'learning_rate': 1.6283352173747145e-05, 'epoch': 2.512}
Step 355: {'loss': 0.0046, 'grad_norm': 0.0032501220703125, 'learning_rate': 1.583403714994729e-05, 'epoch': 2.519111111111111}
Step 356: {'loss': 0.0048, 'grad_norm': 0.0036468505859375, 'learning_rate': 1.5390474757906446e-05, 'epoch': 2.526222222222222}
Step 357: {'loss': 0.0033, 'grad_norm': 0.00323486328125, 'learning_rate': 1.4952695314524912e-05, 'epoch': 2.533333333333333}
Step 358: {'loss': 0.005, 'grad_norm': 0.004974365234375, 'learning_rate': 1.4520728741446089e-05, 'epoch': 2.5404444444444443}
Step 359: {'loss': 0.0065, 'grad_norm': 0.0030670166015625, 'learning_rate': 1.4094604563011472e-05, 'epoch': 2.5475555555555554}
Step 360: {'loss': 0.0033, 'grad_norm': 0.0035552978515625, 'learning_rate': 1.3674351904242611e-05, 'epoch': 2.554666666666667}
Step 361: {'loss': 0.0047, 'grad_norm': 0.00408935546875, 'learning_rate': 1.3259999488850472e-05, 'epoch': 2.561777777777778}
Step 362: {'loss': 0.007, 'grad_norm': 0.0042724609375, 'learning_rate': 1.2851575637272262e-05, 'epoch': 2.568888888888889}
Step 363: {'loss': 0.0061, 'grad_norm': 0.0028839111328125, 'learning_rate': 1.244910826473572e-05, 'epoch': 2.576}
Step 364: {'loss': 0.0072, 'grad_norm': 0.0035247802734375, 'learning_rate': 1.2052624879351104e-05, 'epoch': 2.583111111111111}
Step 365: {'loss': 0.0043, 'grad_norm': 0.0034637451171875, 'learning_rate': 1.1662152580231145e-05, 'epoch': 2.590222222222222}
Step 366: {'loss': 0.0015, 'grad_norm': 0.0030364990234375, 'learning_rate': 1.1277718055638819e-05, 'epoch': 2.5973333333333333}
Step 367: {'loss': 0.0056, 'grad_norm': 0.0031280517578125, 'learning_rate': 1.0899347581163221e-05, 'epoch': 2.6044444444444443}
Step 368: {'loss': 0.004, 'grad_norm': 0.003143310546875, 'learning_rate': 1.0527067017923654e-05, 'epoch': 2.6115555555555554}
Step 369: {'loss': 0.0045, 'grad_norm': 0.0029754638671875, 'learning_rate': 1.0160901810802115e-05, 'epoch': 2.618666666666667}
Step 370: {'loss': 0.003, 'grad_norm': 0.0027618408203125, 'learning_rate': 9.80087698670411e-06, 'epoch': 2.6257777777777775}
Step 371: {'loss': 0.0033, 'grad_norm': 0.0031890869140625, 'learning_rate': 9.447017152848125e-06, 'epoch': 2.632888888888889}
Step 372: {'loss': 0.0038, 'grad_norm': 0.00311279296875, 'learning_rate': 9.09934649508375e-06, 'epoch': 2.64}
Step 373: {'loss': 0.0027, 'grad_norm': 0.002960205078125, 'learning_rate': 8.757888776238621e-06, 'epoch': 2.647111111111111}
Step 374: {'loss': 0.0066, 'grad_norm': 0.00390625, 'learning_rate': 8.422667334494249e-06, 'epoch': 2.6542222222222223}
Step 375: {'loss': 0.0038, 'grad_norm': 0.003387451171875, 'learning_rate': 8.093705081790893e-06, 'epoch': 2.6613333333333333}
Step 376: {'loss': 0.0052, 'grad_norm': 0.0034942626953125, 'learning_rate': 7.771024502261526e-06, 'epoch': 2.6684444444444444}
Step 377: {'loss': 0.0066, 'grad_norm': 0.003509521484375, 'learning_rate': 7.454647650695157e-06, 'epoch': 2.6755555555555555}
Step 378: {'loss': 0.0048, 'grad_norm': 0.0032958984375, 'learning_rate': 7.144596151029303e-06, 'epoch': 2.6826666666666665}
Step 379: {'loss': 0.0053, 'grad_norm': 0.004913330078125, 'learning_rate': 6.840891194872112e-06, 'epoch': 2.6897777777777776}
Step 380: {'loss': 0.0046, 'grad_norm': 0.0030364990234375, 'learning_rate': 6.543553540053926e-06, 'epoch': 2.696888888888889}
Step 381: {'loss': 0.005, 'grad_norm': 0.003326416015625, 'learning_rate': 6.252603509208466e-06, 'epoch': 2.7039999999999997}
Step 382: {'loss': 0.0069, 'grad_norm': 0.0029754638671875, 'learning_rate': 5.968060988383883e-06, 'epoch': 2.7111111111111112}
Step 383: {'loss': 0.002, 'grad_norm': 0.0026702880859375, 'learning_rate': 5.689945425683474e-06, 'epoch': 2.7182222222222223}
Step 384: {'loss': 0.0056, 'grad_norm': 0.0033111572265625, 'learning_rate': 5.418275829936537e-06, 'epoch': 2.7253333333333334}
Step 385: {'loss': 0.0044, 'grad_norm': 0.0033721923828125, 'learning_rate': 5.15307076939906e-06, 'epoch': 2.7324444444444445}
Step 386: {'loss': 0.003, 'grad_norm': 0.0025634765625, 'learning_rate': 4.8943483704846475e-06, 'epoch': 2.7395555555555555}
Step 387: {'loss': 0.0036, 'grad_norm': 0.0032806396484375, 'learning_rate': 4.642126316525586e-06, 'epoch': 2.7466666666666666}
Step 388: {'loss': 0.0047, 'grad_norm': 0.003387451171875, 'learning_rate': 4.3964218465642355e-06, 'epoch': 2.7537777777777777}
Step 389: {'loss': 0.0055, 'grad_norm': 0.002899169921875, 'learning_rate': 4.1572517541747294e-06, 'epoch': 2.7608888888888887}
Step 390: {'loss': 0.0041, 'grad_norm': 0.003204345703125, 'learning_rate': 3.924632386315186e-06, 'epoch': 2.768}
Step 391: {'loss': 0.0065, 'grad_norm': 0.0035247802734375, 'learning_rate': 3.698579642210398e-06, 'epoch': 2.7751111111111113}
Step 392: {'loss': 0.0051, 'grad_norm': 0.0028533935546875, 'learning_rate': 3.4791089722651436e-06, 'epoch': 2.7822222222222224}
Step 393: {'loss': 0.0045, 'grad_norm': 0.0037689208984375, 'learning_rate': 3.2662353770081756e-06, 'epoch': 2.7893333333333334}
Step 394: {'loss': 0.0062, 'grad_norm': 0.0030517578125, 'learning_rate': 3.059973406066963e-06, 'epoch': 2.7964444444444445}
Step 395: {'loss': 0.0039, 'grad_norm': 0.003082275390625, 'learning_rate': 2.8603371571732428e-06, 'epoch': 2.8035555555555556}
Step 396: {'loss': 0.0047, 'grad_norm': 0.0034637451171875, 'learning_rate': 2.667340275199426e-06, 'epoch': 2.8106666666666666}
Step 397: {'loss': 0.0042, 'grad_norm': 0.004150390625, 'learning_rate': 2.4809959512260285e-06, 'epoch': 2.8177777777777777}
Step 398: {'loss': 0.008, 'grad_norm': 0.0035858154296875, 'learning_rate': 2.3013169216400733e-06, 'epoch': 2.824888888888889}
Step 399: {'loss': 0.0054, 'grad_norm': 0.0037384033203125, 'learning_rate': 2.128315467264552e-06, 'epoch': 2.832}
Step 400: {'loss': 0.0063, 'grad_norm': 0.0030517578125, 'learning_rate': 1.9620034125190644e-06, 'epoch': 2.8391111111111114}
Step 401: {'loss': 0.0045, 'grad_norm': 0.003173828125, 'learning_rate': 1.8023921246116405e-06, 'epoch': 2.846222222222222}
Step 402: {'loss': 0.0057, 'grad_norm': 0.0032958984375, 'learning_rate': 1.6494925127617634e-06, 'epoch': 2.8533333333333335}
Step 403: {'loss': 0.0034, 'grad_norm': 0.003173828125, 'learning_rate': 1.5033150274548324e-06, 'epoch': 2.8604444444444446}
Step 404: {'loss': 0.0038, 'grad_norm': 0.0030975341796875, 'learning_rate': 1.3638696597277679e-06, 'epoch': 2.8675555555555556}
Step 405: {'loss': 0.0047, 'grad_norm': 0.0030517578125, 'learning_rate': 1.231165940486234e-06, 'epoch': 2.8746666666666667}
Step 406: {'loss': 0.0052, 'grad_norm': 0.0032196044921875, 'learning_rate': 1.1052129398531507e-06, 'epoch': 2.8817777777777778}
Step 407: {'loss': 0.0031, 'grad_norm': 0.0033416748046875, 'learning_rate': 9.86019266548821e-07, 'epoch': 2.888888888888889}
Step 408: {'loss': 0.0052, 'grad_norm': 0.0031890869140625, 'learning_rate': 8.735930673024806e-07, 'epoch': 2.896}
Step 409: {'loss': 0.0043, 'grad_norm': 0.002899169921875, 'learning_rate': 7.679420262954984e-07, 'epoch': 2.903111111111111}
Step 410: {'loss': 0.0056, 'grad_norm': 0.00390625, 'learning_rate': 6.690733646361857e-07, 'epoch': 2.910222222222222}
Step 411: {'loss': 0.0042, 'grad_norm': 0.003143310546875, 'learning_rate': 5.769938398662355e-07, 'epoch': 2.9173333333333336}
Step 412: {'loss': 0.0065, 'grad_norm': 0.0035400390625, 'learning_rate': 4.917097454988584e-07, 'epoch': 2.924444444444444}
Step 413: {'loss': 0.0036, 'grad_norm': 0.0030670166015625, 'learning_rate': 4.132269105886155e-07, 'epoch': 2.9315555555555557}
Step 414: {'loss': 0.0049, 'grad_norm': 0.0031280517578125, 'learning_rate': 3.415506993330153e-07, 'epoch': 2.9386666666666668}
Step 415: {'loss': 0.0047, 'grad_norm': 0.003204345703125, 'learning_rate': 2.766860107058844e-07, 'epoch': 2.945777777777778}
Step 416: {'loss': 0.0054, 'grad_norm': 0.003387451171875, 'learning_rate': 2.1863727812254653e-07, 'epoch': 2.952888888888889}
Step 417: {'loss': 0.0037, 'grad_norm': 0.00286865234375, 'learning_rate': 1.674084691367428e-07, 'epoch': 2.96}
Step 418: {'loss': 0.003, 'grad_norm': 0.0027923583984375, 'learning_rate': 1.230030851695263e-07, 'epoch': 2.967111111111111}
Step 419: {'loss': 0.0049, 'grad_norm': 0.006439208984375, 'learning_rate': 8.542416126989805e-08, 'epoch': 2.974222222222222}
Step 420: {'loss': 0.0045, 'grad_norm': 0.0033416748046875, 'learning_rate': 5.467426590739511e-08, 'epoch': 2.981333333333333}
Step 421: {'loss': 0.0044, 'grad_norm': 0.0030364990234375, 'learning_rate': 3.0755500796531004e-08, 'epoch': 2.9884444444444442}
Step 422: {'loss': 0.0041, 'grad_norm': 0.0038299560546875, 'learning_rate': 1.3669500753099585e-08, 'epoch': 2.9955555555555557}
Step 423: {'loss': 0.003, 'grad_norm': 0.0024871826171875, 'learning_rate': 3.4174335825420957e-09, 'epoch': 3.0}
Step 423: {'train_runtime': 2659.7394, 'train_samples_per_second': 10.151, 'train_steps_per_second': 0.159, 'total_flos': 0.0, 'train_loss': 0.004958099899725465, 'epoch': 3.0}
