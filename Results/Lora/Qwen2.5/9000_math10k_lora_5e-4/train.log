Step 10: {'loss': 1.4144, 'grad_norm': 1.2117514610290527, 'learning_rate': 0.000499375, 'epoch': 0.002777777777777778}
Step 20: {'loss': 0.9877, 'grad_norm': 2.6735596656799316, 'learning_rate': 0.0004986805555555556, 'epoch': 0.005555555555555556}
Step 30: {'loss': 0.7642, 'grad_norm': 5.272462368011475, 'learning_rate': 0.0004979861111111112, 'epoch': 0.008333333333333333}
Step 40: {'loss': 0.7412, 'grad_norm': 1.644792914390564, 'learning_rate': 0.0004972916666666667, 'epoch': 0.011111111111111112}
Step 50: {'loss': 0.609, 'grad_norm': 0.951207160949707, 'learning_rate': 0.0004965972222222223, 'epoch': 0.013888888888888888}
Step 60: {'loss': 0.5602, 'grad_norm': 0.8759020566940308, 'learning_rate': 0.0004959027777777778, 'epoch': 0.016666666666666666}
Step 70: {'loss': 0.5835, 'grad_norm': 1.4131115674972534, 'learning_rate': 0.0004952083333333333, 'epoch': 0.019444444444444445}
Step 80: {'loss': 0.4979, 'grad_norm': 1.9867441654205322, 'learning_rate': 0.0004945138888888888, 'epoch': 0.022222222222222223}
Step 90: {'loss': 0.5174, 'grad_norm': 1.2726683616638184, 'learning_rate': 0.0004938194444444444, 'epoch': 0.025}
Step 100: {'loss': 0.4754, 'grad_norm': 0.936336100101471, 'learning_rate': 0.000493125, 'epoch': 0.027777777777777776}
Step 110: {'loss': 0.5226, 'grad_norm': 1.4194415807724, 'learning_rate': 0.0004924305555555555, 'epoch': 0.030555555555555555}
Step 120: {'loss': 0.4154, 'grad_norm': 1.1655038595199585, 'learning_rate': 0.0004917361111111111, 'epoch': 0.03333333333333333}
Step 130: {'loss': 0.4666, 'grad_norm': 1.2874966859817505, 'learning_rate': 0.0004910416666666666, 'epoch': 0.03611111111111111}
Step 140: {'loss': 0.4828, 'grad_norm': 1.6870298385620117, 'learning_rate': 0.0004903472222222222, 'epoch': 0.03888888888888889}
Step 150: {'loss': 0.4581, 'grad_norm': 1.5914554595947266, 'learning_rate': 0.0004896527777777778, 'epoch': 0.041666666666666664}
Step 160: {'loss': 0.4762, 'grad_norm': 1.5433790683746338, 'learning_rate': 0.0004889583333333333, 'epoch': 0.044444444444444446}
Step 170: {'loss': 0.387, 'grad_norm': 1.2967110872268677, 'learning_rate': 0.0004882638888888889, 'epoch': 0.04722222222222222}
Step 180: {'loss': 0.4217, 'grad_norm': 2.0406529903411865, 'learning_rate': 0.00048756944444444444, 'epoch': 0.05}
Step 190: {'loss': 0.4568, 'grad_norm': 1.7789828777313232, 'learning_rate': 0.000486875, 'epoch': 0.05277777777777778}
Step 200: {'loss': 0.4656, 'grad_norm': 1.672402262687683, 'learning_rate': 0.00048618055555555556, 'epoch': 0.05555555555555555}
Step 210: {'loss': 0.4232, 'grad_norm': 1.83814537525177, 'learning_rate': 0.0004854861111111111, 'epoch': 0.058333333333333334}
Step 220: {'loss': 0.4137, 'grad_norm': 1.474382996559143, 'learning_rate': 0.0004847916666666667, 'epoch': 0.06111111111111111}
Step 230: {'loss': 0.4438, 'grad_norm': 1.645384669303894, 'learning_rate': 0.00048409722222222224, 'epoch': 0.06388888888888888}
Step 240: {'loss': 0.4093, 'grad_norm': 1.6820429563522339, 'learning_rate': 0.0004834027777777778, 'epoch': 0.06666666666666667}
Step 250: {'loss': 0.4589, 'grad_norm': 1.307945728302002, 'learning_rate': 0.00048270833333333336, 'epoch': 0.06944444444444445}
Step 260: {'loss': 0.4311, 'grad_norm': 1.1675184965133667, 'learning_rate': 0.0004820138888888889, 'epoch': 0.07222222222222222}
Step 270: {'loss': 0.4046, 'grad_norm': 1.2860664129257202, 'learning_rate': 0.0004813194444444445, 'epoch': 0.075}
Step 280: {'loss': 0.4306, 'grad_norm': 1.743473768234253, 'learning_rate': 0.00048062500000000004, 'epoch': 0.07777777777777778}
Step 290: {'loss': 0.4914, 'grad_norm': 2.1729726791381836, 'learning_rate': 0.0004799305555555556, 'epoch': 0.08055555555555556}
Step 300: {'loss': 0.3894, 'grad_norm': 1.1377986669540405, 'learning_rate': 0.0004792361111111111, 'epoch': 0.08333333333333333}
Step 310: {'loss': 0.4151, 'grad_norm': 0.5515738129615784, 'learning_rate': 0.00047854166666666667, 'epoch': 0.08611111111111111}
Step 320: {'loss': 0.3298, 'grad_norm': 1.5133329629898071, 'learning_rate': 0.0004778472222222222, 'epoch': 0.08888888888888889}
Step 330: {'loss': 0.4337, 'grad_norm': 1.5151002407073975, 'learning_rate': 0.0004771527777777778, 'epoch': 0.09166666666666666}
Step 340: {'loss': 0.3884, 'grad_norm': 1.7611463069915771, 'learning_rate': 0.00047645833333333335, 'epoch': 0.09444444444444444}
Step 350: {'loss': 0.3759, 'grad_norm': 1.6741143465042114, 'learning_rate': 0.0004757638888888889, 'epoch': 0.09722222222222222}
Step 360: {'loss': 0.4487, 'grad_norm': 1.0333696603775024, 'learning_rate': 0.00047506944444444446, 'epoch': 0.1}
Step 370: {'loss': 0.3964, 'grad_norm': 0.9143617749214172, 'learning_rate': 0.000474375, 'epoch': 0.10277777777777777}
Step 380: {'loss': 0.3652, 'grad_norm': 1.2979084253311157, 'learning_rate': 0.0004736805555555556, 'epoch': 0.10555555555555556}
Step 390: {'loss': 0.3571, 'grad_norm': 1.2160614728927612, 'learning_rate': 0.00047298611111111114, 'epoch': 0.10833333333333334}
Step 400: {'loss': 0.3796, 'grad_norm': 1.0828287601470947, 'learning_rate': 0.00047229166666666665, 'epoch': 0.1111111111111111}
Step 410: {'loss': 0.3838, 'grad_norm': 1.1290175914764404, 'learning_rate': 0.0004715972222222222, 'epoch': 0.11388888888888889}
Step 420: {'loss': 0.367, 'grad_norm': 1.0195176601409912, 'learning_rate': 0.00047090277777777777, 'epoch': 0.11666666666666667}
Step 430: {'loss': 0.4353, 'grad_norm': 0.7091899514198303, 'learning_rate': 0.00047020833333333333, 'epoch': 0.11944444444444445}
Step 440: {'loss': 0.4254, 'grad_norm': 0.9604369401931763, 'learning_rate': 0.0004695138888888889, 'epoch': 0.12222222222222222}
Step 450: {'loss': 0.417, 'grad_norm': 1.8075804710388184, 'learning_rate': 0.00046881944444444445, 'epoch': 0.125}
Step 460: {'loss': 0.3649, 'grad_norm': 0.8364666700363159, 'learning_rate': 0.000468125, 'epoch': 0.12777777777777777}
Step 470: {'loss': 0.395, 'grad_norm': 1.487540364265442, 'learning_rate': 0.00046743055555555557, 'epoch': 0.13055555555555556}
Step 480: {'loss': 0.4081, 'grad_norm': 1.1498932838439941, 'learning_rate': 0.00046673611111111113, 'epoch': 0.13333333333333333}
Step 490: {'loss': 0.3787, 'grad_norm': 2.00784969329834, 'learning_rate': 0.0004660416666666667, 'epoch': 0.1361111111111111}
Step 500: {'loss': 0.4064, 'grad_norm': 1.0999406576156616, 'learning_rate': 0.00046534722222222225, 'epoch': 0.1388888888888889}
Step 510: {'loss': 0.3938, 'grad_norm': 1.1617380380630493, 'learning_rate': 0.0004646527777777778, 'epoch': 0.14166666666666666}
Step 520: {'loss': 0.3774, 'grad_norm': 1.7142025232315063, 'learning_rate': 0.0004639583333333333, 'epoch': 0.14444444444444443}
Step 530: {'loss': 0.4146, 'grad_norm': 1.41725492477417, 'learning_rate': 0.00046326388888888887, 'epoch': 0.14722222222222223}
Step 540: {'loss': 0.3627, 'grad_norm': 1.1923317909240723, 'learning_rate': 0.00046256944444444443, 'epoch': 0.15}
Step 550: {'loss': 0.3527, 'grad_norm': 1.0615111589431763, 'learning_rate': 0.000461875, 'epoch': 0.1527777777777778}
Step 560: {'loss': 0.4208, 'grad_norm': 0.9688472151756287, 'learning_rate': 0.00046118055555555555, 'epoch': 0.15555555555555556}
Step 570: {'loss': 0.3572, 'grad_norm': 1.2051117420196533, 'learning_rate': 0.0004604861111111111, 'epoch': 0.15833333333333333}
Step 580: {'loss': 0.3885, 'grad_norm': 1.0704251527786255, 'learning_rate': 0.00045979166666666667, 'epoch': 0.16111111111111112}
Step 590: {'loss': 0.345, 'grad_norm': 0.9877228736877441, 'learning_rate': 0.00045909722222222223, 'epoch': 0.1638888888888889}
Step 600: {'loss': 0.4818, 'grad_norm': 1.3635077476501465, 'learning_rate': 0.0004584027777777778, 'epoch': 0.16666666666666666}
Step 610: {'loss': 0.3708, 'grad_norm': 1.1024998426437378, 'learning_rate': 0.00045770833333333335, 'epoch': 0.16944444444444445}
Step 620: {'loss': 0.4324, 'grad_norm': 0.926609218120575, 'learning_rate': 0.0004570138888888889, 'epoch': 0.17222222222222222}
Step 630: {'loss': 0.3742, 'grad_norm': 1.2536574602127075, 'learning_rate': 0.00045631944444444447, 'epoch': 0.175}
Step 640: {'loss': 0.3838, 'grad_norm': 1.2062889337539673, 'learning_rate': 0.00045562500000000003, 'epoch': 0.17777777777777778}
Step 650: {'loss': 0.4181, 'grad_norm': 1.0565458536148071, 'learning_rate': 0.0004549305555555556, 'epoch': 0.18055555555555555}
Step 660: {'loss': 0.3909, 'grad_norm': 1.3733909130096436, 'learning_rate': 0.00045423611111111115, 'epoch': 0.18333333333333332}
Step 670: {'loss': 0.3125, 'grad_norm': 0.9865121841430664, 'learning_rate': 0.0004535416666666667, 'epoch': 0.18611111111111112}
Step 680: {'loss': 0.441, 'grad_norm': 1.8127970695495605, 'learning_rate': 0.00045284722222222227, 'epoch': 0.18888888888888888}
Step 690: {'loss': 0.4078, 'grad_norm': 1.4230210781097412, 'learning_rate': 0.00045215277777777783, 'epoch': 0.19166666666666668}
Step 700: {'loss': 0.3533, 'grad_norm': 0.8664080500602722, 'learning_rate': 0.00045145833333333333, 'epoch': 0.19444444444444445}
Step 710: {'loss': 0.382, 'grad_norm': 0.9445675611495972, 'learning_rate': 0.0004507638888888889, 'epoch': 0.19722222222222222}
Step 720: {'loss': 0.3946, 'grad_norm': 1.0373033285140991, 'learning_rate': 0.00045006944444444445, 'epoch': 0.2}
Step 730: {'loss': 0.3809, 'grad_norm': 0.7982164025306702, 'learning_rate': 0.000449375, 'epoch': 0.20277777777777778}
Step 740: {'loss': 0.3473, 'grad_norm': 0.8561151623725891, 'learning_rate': 0.0004486805555555556, 'epoch': 0.20555555555555555}
Step 750: {'loss': 0.4063, 'grad_norm': 1.2812269926071167, 'learning_rate': 0.0004479861111111111, 'epoch': 0.20833333333333334}
Step 760: {'loss': 0.3906, 'grad_norm': 1.3691105842590332, 'learning_rate': 0.00044729166666666664, 'epoch': 0.2111111111111111}
Step 770: {'loss': 0.4026, 'grad_norm': 1.0791330337524414, 'learning_rate': 0.0004465972222222222, 'epoch': 0.21388888888888888}
Step 780: {'loss': 0.3824, 'grad_norm': 1.0223212242126465, 'learning_rate': 0.00044590277777777776, 'epoch': 0.21666666666666667}
Step 790: {'loss': 0.4044, 'grad_norm': 1.3347527980804443, 'learning_rate': 0.0004452083333333333, 'epoch': 0.21944444444444444}
Step 800: {'loss': 0.344, 'grad_norm': 0.9883424639701843, 'learning_rate': 0.0004445138888888889, 'epoch': 0.2222222222222222}
Step 810: {'loss': 0.3733, 'grad_norm': 1.3518117666244507, 'learning_rate': 0.00044381944444444444, 'epoch': 0.225}
Step 820: {'loss': 0.346, 'grad_norm': 2.3118834495544434, 'learning_rate': 0.000443125, 'epoch': 0.22777777777777777}
Step 830: {'loss': 0.3788, 'grad_norm': 0.9864935278892517, 'learning_rate': 0.00044243055555555556, 'epoch': 0.23055555555555557}
Step 840: {'loss': 0.3896, 'grad_norm': 1.0245393514633179, 'learning_rate': 0.0004417361111111111, 'epoch': 0.23333333333333334}
Step 850: {'loss': 0.3855, 'grad_norm': 1.3029835224151611, 'learning_rate': 0.0004410416666666667, 'epoch': 0.2361111111111111}
Step 860: {'loss': 0.4043, 'grad_norm': 0.8417986631393433, 'learning_rate': 0.00044034722222222224, 'epoch': 0.2388888888888889}
Step 870: {'loss': 0.3628, 'grad_norm': 1.0007206201553345, 'learning_rate': 0.0004396527777777778, 'epoch': 0.24166666666666667}
Step 880: {'loss': 0.4284, 'grad_norm': 0.9429223537445068, 'learning_rate': 0.00043895833333333336, 'epoch': 0.24444444444444444}
Step 890: {'loss': 0.3824, 'grad_norm': 1.0964457988739014, 'learning_rate': 0.0004382638888888889, 'epoch': 0.24722222222222223}
Step 900: {'loss': 0.3886, 'grad_norm': 0.7342764735221863, 'learning_rate': 0.0004375694444444445, 'epoch': 0.25}
Step 910: {'loss': 0.3431, 'grad_norm': 1.1895424127578735, 'learning_rate': 0.00043687500000000003, 'epoch': 0.25277777777777777}
Step 920: {'loss': 0.3773, 'grad_norm': 0.6557338237762451, 'learning_rate': 0.0004361805555555556, 'epoch': 0.25555555555555554}
Step 930: {'loss': 0.3481, 'grad_norm': 1.1027802228927612, 'learning_rate': 0.00043548611111111115, 'epoch': 0.25833333333333336}
Step 940: {'loss': 0.3329, 'grad_norm': 0.9691938161849976, 'learning_rate': 0.0004347916666666667, 'epoch': 0.2611111111111111}
Step 950: {'loss': 0.3529, 'grad_norm': 0.8076277375221252, 'learning_rate': 0.0004340972222222223, 'epoch': 0.2638888888888889}
Step 960: {'loss': 0.3831, 'grad_norm': 1.2350468635559082, 'learning_rate': 0.00043340277777777783, 'epoch': 0.26666666666666666}
Step 970: {'loss': 0.3684, 'grad_norm': 1.2398302555084229, 'learning_rate': 0.00043270833333333334, 'epoch': 0.26944444444444443}
Step 980: {'loss': 0.4394, 'grad_norm': 1.136033296585083, 'learning_rate': 0.0004320138888888889, 'epoch': 0.2722222222222222}
Step 990: {'loss': 0.4026, 'grad_norm': 1.1330368518829346, 'learning_rate': 0.0004313194444444444, 'epoch': 0.275}
Step 1000: {'loss': 0.3998, 'grad_norm': 1.2213304042816162, 'learning_rate': 0.00043062499999999996, 'epoch': 0.2777777777777778}
Step 1010: {'loss': 0.3579, 'grad_norm': 0.8120335340499878, 'learning_rate': 0.0004299305555555555, 'epoch': 0.28055555555555556}
Step 1020: {'loss': 0.3659, 'grad_norm': 1.1880967617034912, 'learning_rate': 0.0004292361111111111, 'epoch': 0.2833333333333333}
Step 1030: {'loss': 0.3698, 'grad_norm': 0.8826575875282288, 'learning_rate': 0.00042854166666666664, 'epoch': 0.2861111111111111}
Step 1040: {'loss': 0.3923, 'grad_norm': 0.9452767968177795, 'learning_rate': 0.0004278472222222222, 'epoch': 0.28888888888888886}
Step 1050: {'loss': 0.3635, 'grad_norm': 0.997456431388855, 'learning_rate': 0.00042715277777777776, 'epoch': 0.2916666666666667}
Step 1060: {'loss': 0.4123, 'grad_norm': 0.877448320388794, 'learning_rate': 0.0004264583333333333, 'epoch': 0.29444444444444445}
Step 1070: {'loss': 0.3467, 'grad_norm': 0.8055737018585205, 'learning_rate': 0.0004257638888888889, 'epoch': 0.2972222222222222}
Step 1080: {'loss': 0.3342, 'grad_norm': 0.6592509746551514, 'learning_rate': 0.00042506944444444444, 'epoch': 0.3}
Step 1090: {'loss': 0.3779, 'grad_norm': 1.0343248844146729, 'learning_rate': 0.000424375, 'epoch': 0.30277777777777776}
Step 1100: {'loss': 0.3697, 'grad_norm': 1.4372053146362305, 'learning_rate': 0.00042368055555555556, 'epoch': 0.3055555555555556}
Step 1110: {'loss': 0.3466, 'grad_norm': 1.0178087949752808, 'learning_rate': 0.0004229861111111111, 'epoch': 0.30833333333333335}
Step 1120: {'loss': 0.3779, 'grad_norm': 1.323952555656433, 'learning_rate': 0.0004222916666666667, 'epoch': 0.3111111111111111}
Step 1130: {'loss': 0.4571, 'grad_norm': 1.249851107597351, 'learning_rate': 0.00042159722222222224, 'epoch': 0.3138888888888889}
Step 1140: {'loss': 0.4056, 'grad_norm': 1.238394021987915, 'learning_rate': 0.0004209027777777778, 'epoch': 0.31666666666666665}
Step 1150: {'loss': 0.3398, 'grad_norm': 1.11251962184906, 'learning_rate': 0.00042020833333333336, 'epoch': 0.3194444444444444}
Step 1160: {'loss': 0.3738, 'grad_norm': 1.460518717765808, 'learning_rate': 0.0004195138888888889, 'epoch': 0.32222222222222224}
Step 1170: {'loss': 0.4627, 'grad_norm': 0.9398897886276245, 'learning_rate': 0.0004188194444444445, 'epoch': 0.325}
Step 1180: {'loss': 0.3214, 'grad_norm': 0.6634659767150879, 'learning_rate': 0.00041812500000000004, 'epoch': 0.3277777777777778}
Step 1190: {'loss': 0.3372, 'grad_norm': 1.1268713474273682, 'learning_rate': 0.0004174305555555556, 'epoch': 0.33055555555555555}
Step 1200: {'loss': 0.3261, 'grad_norm': 0.9326437711715698, 'learning_rate': 0.0004167361111111111, 'epoch': 0.3333333333333333}
Step 1210: {'loss': 0.3474, 'grad_norm': 1.1548906564712524, 'learning_rate': 0.00041604166666666666, 'epoch': 0.33611111111111114}
Step 1220: {'loss': 0.3688, 'grad_norm': 1.085240125656128, 'learning_rate': 0.0004153472222222222, 'epoch': 0.3388888888888889}
Step 1230: {'loss': 0.3825, 'grad_norm': 0.8244478106498718, 'learning_rate': 0.0004146527777777778, 'epoch': 0.3416666666666667}
Step 1240: {'loss': 0.381, 'grad_norm': 1.1119285821914673, 'learning_rate': 0.00041395833333333334, 'epoch': 0.34444444444444444}
Step 1250: {'loss': 0.3509, 'grad_norm': 1.802090048789978, 'learning_rate': 0.0004132638888888889, 'epoch': 0.3472222222222222}
Step 1260: {'loss': 0.3495, 'grad_norm': 0.9575648307800293, 'learning_rate': 0.00041256944444444446, 'epoch': 0.35}
Step 1270: {'loss': 0.3733, 'grad_norm': 1.187514066696167, 'learning_rate': 0.000411875, 'epoch': 0.3527777777777778}
Step 1280: {'loss': 0.3637, 'grad_norm': 1.3188436031341553, 'learning_rate': 0.0004111805555555556, 'epoch': 0.35555555555555557}
Step 1290: {'loss': 0.3516, 'grad_norm': 1.1381880044937134, 'learning_rate': 0.00041048611111111114, 'epoch': 0.35833333333333334}
Step 1300: {'loss': 0.3775, 'grad_norm': 1.0339407920837402, 'learning_rate': 0.00040979166666666665, 'epoch': 0.3611111111111111}
Step 1310: {'loss': 0.4266, 'grad_norm': 0.765146017074585, 'learning_rate': 0.0004090972222222222, 'epoch': 0.3638888888888889}
Step 1320: {'loss': 0.3687, 'grad_norm': 0.7541882395744324, 'learning_rate': 0.00040840277777777777, 'epoch': 0.36666666666666664}
Step 1330: {'loss': 0.4467, 'grad_norm': 0.976137638092041, 'learning_rate': 0.00040770833333333333, 'epoch': 0.36944444444444446}
Step 1340: {'loss': 0.3797, 'grad_norm': 0.945640504360199, 'learning_rate': 0.0004070138888888889, 'epoch': 0.37222222222222223}
Step 1350: {'loss': 0.3281, 'grad_norm': 0.765379011631012, 'learning_rate': 0.00040631944444444445, 'epoch': 0.375}
Step 1360: {'loss': 0.3329, 'grad_norm': 0.5744723081588745, 'learning_rate': 0.000405625, 'epoch': 0.37777777777777777}
Step 1370: {'loss': 0.4046, 'grad_norm': 1.3236207962036133, 'learning_rate': 0.00040493055555555557, 'epoch': 0.38055555555555554}
Step 1380: {'loss': 0.3936, 'grad_norm': 1.081357717514038, 'learning_rate': 0.0004042361111111111, 'epoch': 0.38333333333333336}
Step 1390: {'loss': 0.386, 'grad_norm': 0.7887274622917175, 'learning_rate': 0.0004035416666666667, 'epoch': 0.3861111111111111}
Step 1400: {'loss': 0.3846, 'grad_norm': 1.3197640180587769, 'learning_rate': 0.00040284722222222225, 'epoch': 0.3888888888888889}
Step 1410: {'loss': 0.3764, 'grad_norm': 0.9225779175758362, 'learning_rate': 0.0004021527777777778, 'epoch': 0.39166666666666666}
Step 1420: {'loss': 0.2978, 'grad_norm': 0.9491463899612427, 'learning_rate': 0.0004014583333333333, 'epoch': 0.39444444444444443}
Step 1430: {'loss': 0.2943, 'grad_norm': 0.5678438544273376, 'learning_rate': 0.00040076388888888887, 'epoch': 0.3972222222222222}
Step 1440: {'loss': 0.3489, 'grad_norm': 1.119893193244934, 'learning_rate': 0.00040006944444444443, 'epoch': 0.4}
Step 1450: {'loss': 0.4226, 'grad_norm': 1.0691478252410889, 'learning_rate': 0.000399375, 'epoch': 0.4027777777777778}
Step 1460: {'loss': 0.3769, 'grad_norm': 1.7757734060287476, 'learning_rate': 0.00039868055555555555, 'epoch': 0.40555555555555556}
Step 1470: {'loss': 0.3635, 'grad_norm': 0.7044659852981567, 'learning_rate': 0.0003979861111111111, 'epoch': 0.4083333333333333}
Step 1480: {'loss': 0.3682, 'grad_norm': 1.2844496965408325, 'learning_rate': 0.00039729166666666667, 'epoch': 0.4111111111111111}
Step 1490: {'loss': 0.3784, 'grad_norm': 0.7367399334907532, 'learning_rate': 0.00039659722222222223, 'epoch': 0.41388888888888886}
Step 1500: {'loss': 0.3577, 'grad_norm': 1.0370914936065674, 'learning_rate': 0.0003959027777777778, 'epoch': 0.4166666666666667}
Step 1510: {'loss': 0.3917, 'grad_norm': 1.2628036737442017, 'learning_rate': 0.00039520833333333335, 'epoch': 0.41944444444444445}
Step 1520: {'loss': 0.3724, 'grad_norm': 1.7763923406600952, 'learning_rate': 0.0003945138888888889, 'epoch': 0.4222222222222222}
Step 1530: {'loss': 0.2789, 'grad_norm': 0.916321873664856, 'learning_rate': 0.00039381944444444447, 'epoch': 0.425}
Step 1540: {'loss': 0.3998, 'grad_norm': 1.3626627922058105, 'learning_rate': 0.00039312500000000003, 'epoch': 0.42777777777777776}
Step 1550: {'loss': 0.3749, 'grad_norm': 1.0784698724746704, 'learning_rate': 0.0003924305555555556, 'epoch': 0.4305555555555556}
Step 1560: {'loss': 0.3922, 'grad_norm': 1.201602578163147, 'learning_rate': 0.00039173611111111115, 'epoch': 0.43333333333333335}
Step 1570: {'loss': 0.3746, 'grad_norm': 1.0362999439239502, 'learning_rate': 0.0003910416666666667, 'epoch': 0.4361111111111111}
Step 1580: {'loss': 0.385, 'grad_norm': 1.0163408517837524, 'learning_rate': 0.00039034722222222227, 'epoch': 0.4388888888888889}
Step 1590: {'loss': 0.3778, 'grad_norm': 1.131038784980774, 'learning_rate': 0.0003896527777777778, 'epoch': 0.44166666666666665}
Step 1600: {'loss': 0.4039, 'grad_norm': 0.9888573288917542, 'learning_rate': 0.00038895833333333333, 'epoch': 0.4444444444444444}
Step 1610: {'loss': 0.3904, 'grad_norm': 0.9366194605827332, 'learning_rate': 0.0003882638888888889, 'epoch': 0.44722222222222224}
Step 1620: {'loss': 0.4124, 'grad_norm': 0.8315017819404602, 'learning_rate': 0.00038756944444444445, 'epoch': 0.45}
Step 1630: {'loss': 0.36, 'grad_norm': 0.6670006513595581, 'learning_rate': 0.000386875, 'epoch': 0.4527777777777778}
Step 1640: {'loss': 0.4433, 'grad_norm': 2.0014736652374268, 'learning_rate': 0.00038618055555555557, 'epoch': 0.45555555555555555}
Step 1650: {'loss': 0.4246, 'grad_norm': 1.6584527492523193, 'learning_rate': 0.0003854861111111111, 'epoch': 0.4583333333333333}
Step 1660: {'loss': 0.4132, 'grad_norm': 0.8339616060256958, 'learning_rate': 0.00038479166666666664, 'epoch': 0.46111111111111114}
Step 1670: {'loss': 0.3997, 'grad_norm': 1.362134575843811, 'learning_rate': 0.0003840972222222222, 'epoch': 0.4638888888888889}
Step 1680: {'loss': 0.3746, 'grad_norm': 0.7264243364334106, 'learning_rate': 0.00038340277777777776, 'epoch': 0.4666666666666667}
Step 1690: {'loss': 0.3668, 'grad_norm': 2.6151418685913086, 'learning_rate': 0.0003827083333333333, 'epoch': 0.46944444444444444}
Step 1700: {'loss': 0.353, 'grad_norm': 0.745149552822113, 'learning_rate': 0.0003820138888888889, 'epoch': 0.4722222222222222}
Step 1710: {'loss': 0.3619, 'grad_norm': 1.2898398637771606, 'learning_rate': 0.00038131944444444444, 'epoch': 0.475}
Step 1720: {'loss': 0.3453, 'grad_norm': 0.9238334894180298, 'learning_rate': 0.000380625, 'epoch': 0.4777777777777778}
Step 1730: {'loss': 0.3898, 'grad_norm': 1.093860149383545, 'learning_rate': 0.00037993055555555556, 'epoch': 0.48055555555555557}
Step 1740: {'loss': 0.388, 'grad_norm': 0.8209968209266663, 'learning_rate': 0.0003792361111111111, 'epoch': 0.48333333333333334}
Step 1750: {'loss': 0.3943, 'grad_norm': 1.3725690841674805, 'learning_rate': 0.0003785416666666667, 'epoch': 0.4861111111111111}
Step 1760: {'loss': 0.4126, 'grad_norm': 1.353716492652893, 'learning_rate': 0.00037784722222222223, 'epoch': 0.4888888888888889}
Step 1770: {'loss': 0.4268, 'grad_norm': 0.9157738089561462, 'learning_rate': 0.0003771527777777778, 'epoch': 0.49166666666666664}
Step 1780: {'loss': 0.3689, 'grad_norm': 1.3342297077178955, 'learning_rate': 0.00037645833333333335, 'epoch': 0.49444444444444446}
Step 1790: {'loss': 0.4053, 'grad_norm': 0.81595379114151, 'learning_rate': 0.0003757638888888889, 'epoch': 0.49722222222222223}
Step 1800: {'loss': 0.3671, 'grad_norm': 1.037966012954712, 'learning_rate': 0.0003750694444444445, 'epoch': 0.5}
Step 1810: {'loss': 0.3497, 'grad_norm': 0.6298289895057678, 'learning_rate': 0.00037437500000000003, 'epoch': 0.5027777777777778}
Step 1820: {'loss': 0.3629, 'grad_norm': 0.9972666501998901, 'learning_rate': 0.0003736805555555556, 'epoch': 0.5055555555555555}
Step 1830: {'loss': 0.413, 'grad_norm': 1.0407174825668335, 'learning_rate': 0.00037298611111111115, 'epoch': 0.5083333333333333}
Step 1840: {'loss': 0.3168, 'grad_norm': 0.9036585092544556, 'learning_rate': 0.0003722916666666667, 'epoch': 0.5111111111111111}
Step 1850: {'loss': 0.3477, 'grad_norm': 1.3692560195922852, 'learning_rate': 0.00037159722222222227, 'epoch': 0.5138888888888888}
Step 1860: {'loss': 0.4048, 'grad_norm': 0.7898165583610535, 'learning_rate': 0.00037090277777777783, 'epoch': 0.5166666666666667}
Step 1870: {'loss': 0.3641, 'grad_norm': 1.0349050760269165, 'learning_rate': 0.00037020833333333334, 'epoch': 0.5194444444444445}
Step 1880: {'loss': 0.3383, 'grad_norm': 1.0910531282424927, 'learning_rate': 0.0003695138888888889, 'epoch': 0.5222222222222223}
Step 1890: {'loss': 0.3919, 'grad_norm': 0.8351585268974304, 'learning_rate': 0.0003688194444444444, 'epoch': 0.525}
Step 1900: {'loss': 0.3775, 'grad_norm': 0.8598793148994446, 'learning_rate': 0.00036812499999999996, 'epoch': 0.5277777777777778}
Step 1910: {'loss': 0.3412, 'grad_norm': 0.764677882194519, 'learning_rate': 0.0003674305555555555, 'epoch': 0.5305555555555556}
Step 1920: {'loss': 0.3409, 'grad_norm': 0.8768752217292786, 'learning_rate': 0.0003667361111111111, 'epoch': 0.5333333333333333}
Step 1930: {'loss': 0.3735, 'grad_norm': 1.025903582572937, 'learning_rate': 0.00036604166666666664, 'epoch': 0.5361111111111111}
Step 1940: {'loss': 0.3753, 'grad_norm': 1.053056240081787, 'learning_rate': 0.0003653472222222222, 'epoch': 0.5388888888888889}
Step 1950: {'loss': 0.3264, 'grad_norm': 0.5793028473854065, 'learning_rate': 0.00036465277777777776, 'epoch': 0.5416666666666666}
Step 1960: {'loss': 0.3393, 'grad_norm': 0.9234711527824402, 'learning_rate': 0.0003639583333333333, 'epoch': 0.5444444444444444}
Step 1970: {'loss': 0.3985, 'grad_norm': 0.8914355039596558, 'learning_rate': 0.0003632638888888889, 'epoch': 0.5472222222222223}
Step 1980: {'loss': 0.416, 'grad_norm': 0.7274377942085266, 'learning_rate': 0.00036256944444444444, 'epoch': 0.55}
Step 1990: {'loss': 0.4145, 'grad_norm': 0.8065058588981628, 'learning_rate': 0.000361875, 'epoch': 0.5527777777777778}
Step 2000: {'loss': 0.4356, 'grad_norm': 1.1391493082046509, 'learning_rate': 0.00036118055555555556, 'epoch': 0.5555555555555556}
Step 2010: {'loss': 0.4275, 'grad_norm': 1.2012019157409668, 'learning_rate': 0.0003604861111111111, 'epoch': 0.5583333333333333}
Step 2020: {'loss': 0.3954, 'grad_norm': 1.1937111616134644, 'learning_rate': 0.0003597916666666667, 'epoch': 0.5611111111111111}
Step 2030: {'loss': 0.3521, 'grad_norm': 0.5966895222663879, 'learning_rate': 0.00035909722222222224, 'epoch': 0.5638888888888889}
Step 2040: {'loss': 0.406, 'grad_norm': 1.0251004695892334, 'learning_rate': 0.0003584027777777778, 'epoch': 0.5666666666666667}
Step 2050: {'loss': 0.3776, 'grad_norm': 0.8426511287689209, 'learning_rate': 0.00035770833333333336, 'epoch': 0.5694444444444444}
Step 2060: {'loss': 0.3245, 'grad_norm': 1.1209518909454346, 'learning_rate': 0.0003570138888888889, 'epoch': 0.5722222222222222}
Step 2070: {'loss': 0.3131, 'grad_norm': 1.1421551704406738, 'learning_rate': 0.0003563194444444445, 'epoch': 0.575}
Step 2080: {'loss': 0.3613, 'grad_norm': 1.2807360887527466, 'learning_rate': 0.00035562500000000004, 'epoch': 0.5777777777777777}
Step 2090: {'loss': 0.3512, 'grad_norm': 1.0232505798339844, 'learning_rate': 0.0003549305555555556, 'epoch': 0.5805555555555556}
Step 2100: {'loss': 0.3884, 'grad_norm': 0.7916234135627747, 'learning_rate': 0.0003542361111111111, 'epoch': 0.5833333333333334}
Step 2110: {'loss': 0.3704, 'grad_norm': 0.9240550994873047, 'learning_rate': 0.00035354166666666666, 'epoch': 0.5861111111111111}
Step 2120: {'loss': 0.3497, 'grad_norm': 0.996485710144043, 'learning_rate': 0.0003528472222222222, 'epoch': 0.5888888888888889}
Step 2130: {'loss': 0.3781, 'grad_norm': 0.8930094242095947, 'learning_rate': 0.0003521527777777778, 'epoch': 0.5916666666666667}
Step 2140: {'loss': 0.3642, 'grad_norm': 1.5608227252960205, 'learning_rate': 0.00035145833333333334, 'epoch': 0.5944444444444444}
Step 2150: {'loss': 0.3496, 'grad_norm': 0.8702371716499329, 'learning_rate': 0.0003507638888888889, 'epoch': 0.5972222222222222}
Step 2160: {'loss': 0.3511, 'grad_norm': 1.0153614282608032, 'learning_rate': 0.00035006944444444446, 'epoch': 0.6}
Step 2170: {'loss': 0.3281, 'grad_norm': 0.6581646800041199, 'learning_rate': 0.000349375, 'epoch': 0.6027777777777777}
Step 2180: {'loss': 0.3904, 'grad_norm': 0.6855140924453735, 'learning_rate': 0.0003486805555555556, 'epoch': 0.6055555555555555}
Step 2190: {'loss': 0.3703, 'grad_norm': 1.1826902627944946, 'learning_rate': 0.00034798611111111114, 'epoch': 0.6083333333333333}
Step 2200: {'loss': 0.3473, 'grad_norm': 1.0089176893234253, 'learning_rate': 0.00034729166666666665, 'epoch': 0.6111111111111112}
Step 2210: {'loss': 0.3881, 'grad_norm': 0.8591118454933167, 'learning_rate': 0.0003465972222222222, 'epoch': 0.6138888888888889}
Step 2220: {'loss': 0.3542, 'grad_norm': 0.7915061116218567, 'learning_rate': 0.00034590277777777777, 'epoch': 0.6166666666666667}
Step 2230: {'loss': 0.378, 'grad_norm': 0.8854325413703918, 'learning_rate': 0.0003452083333333333, 'epoch': 0.6194444444444445}
Step 2240: {'loss': 0.3447, 'grad_norm': 1.0311317443847656, 'learning_rate': 0.0003445138888888889, 'epoch': 0.6222222222222222}
Step 2250: {'loss': 0.3727, 'grad_norm': 0.8104013204574585, 'learning_rate': 0.00034381944444444445, 'epoch': 0.625}
Step 2260: {'loss': 0.3001, 'grad_norm': 0.8480514287948608, 'learning_rate': 0.000343125, 'epoch': 0.6277777777777778}
Step 2270: {'loss': 0.2925, 'grad_norm': 0.8111199140548706, 'learning_rate': 0.00034243055555555557, 'epoch': 0.6305555555555555}
Step 2280: {'loss': 0.4333, 'grad_norm': 1.1657048463821411, 'learning_rate': 0.0003417361111111111, 'epoch': 0.6333333333333333}
Step 2290: {'loss': 0.3619, 'grad_norm': 1.6530860662460327, 'learning_rate': 0.0003410416666666667, 'epoch': 0.6361111111111111}
Step 2300: {'loss': 0.3712, 'grad_norm': 1.0767959356307983, 'learning_rate': 0.00034034722222222224, 'epoch': 0.6388888888888888}
Step 2310: {'loss': 0.3447, 'grad_norm': 0.7224043011665344, 'learning_rate': 0.0003396527777777778, 'epoch': 0.6416666666666667}
Step 2320: {'loss': 0.3527, 'grad_norm': 0.7388277649879456, 'learning_rate': 0.0003389583333333333, 'epoch': 0.6444444444444445}
Step 2330: {'loss': 0.3596, 'grad_norm': 0.8114320039749146, 'learning_rate': 0.00033826388888888887, 'epoch': 0.6472222222222223}
Step 2340: {'loss': 0.3979, 'grad_norm': 1.0183889865875244, 'learning_rate': 0.00033756944444444443, 'epoch': 0.65}
Step 2350: {'loss': 0.3909, 'grad_norm': 0.9489858150482178, 'learning_rate': 0.000336875, 'epoch': 0.6527777777777778}
Step 2360: {'loss': 0.3583, 'grad_norm': 0.7341858744621277, 'learning_rate': 0.00033618055555555555, 'epoch': 0.6555555555555556}
Step 2370: {'loss': 0.3368, 'grad_norm': 0.7740371227264404, 'learning_rate': 0.0003354861111111111, 'epoch': 0.6583333333333333}
Step 2380: {'loss': 0.3798, 'grad_norm': 0.8386094570159912, 'learning_rate': 0.00033479166666666667, 'epoch': 0.6611111111111111}
Step 2390: {'loss': 0.3308, 'grad_norm': 1.0404618978500366, 'learning_rate': 0.00033409722222222223, 'epoch': 0.6638888888888889}
Step 2400: {'loss': 0.3078, 'grad_norm': 1.2577252388000488, 'learning_rate': 0.0003334027777777778, 'epoch': 0.6666666666666666}
Step 2410: {'loss': 0.3362, 'grad_norm': 0.9864920973777771, 'learning_rate': 0.00033270833333333335, 'epoch': 0.6694444444444444}
Step 2420: {'loss': 0.3904, 'grad_norm': 1.2836164236068726, 'learning_rate': 0.0003320138888888889, 'epoch': 0.6722222222222223}
Step 2430: {'loss': 0.3877, 'grad_norm': 0.7154744267463684, 'learning_rate': 0.00033131944444444447, 'epoch': 0.675}
Step 2440: {'loss': 0.352, 'grad_norm': 0.9600597023963928, 'learning_rate': 0.000330625, 'epoch': 0.6777777777777778}
Step 2450: {'loss': 0.3742, 'grad_norm': 1.0255532264709473, 'learning_rate': 0.0003299305555555556, 'epoch': 0.6805555555555556}
Step 2460: {'loss': 0.3839, 'grad_norm': 1.3276472091674805, 'learning_rate': 0.00032923611111111115, 'epoch': 0.6833333333333333}
Step 2470: {'loss': 0.3726, 'grad_norm': 0.7927356362342834, 'learning_rate': 0.0003285416666666667, 'epoch': 0.6861111111111111}
Step 2480: {'loss': 0.3699, 'grad_norm': 0.7133549451828003, 'learning_rate': 0.00032784722222222227, 'epoch': 0.6888888888888889}
Step 2490: {'loss': 0.3825, 'grad_norm': 0.9861924648284912, 'learning_rate': 0.0003271527777777778, 'epoch': 0.6916666666666667}
Step 2500: {'loss': 0.3943, 'grad_norm': 0.9549508690834045, 'learning_rate': 0.00032645833333333333, 'epoch': 0.6944444444444444}
Step 2510: {'loss': 0.3942, 'grad_norm': 0.983963131904602, 'learning_rate': 0.0003257638888888889, 'epoch': 0.6972222222222222}
Step 2520: {'loss': 0.3757, 'grad_norm': 1.0320814847946167, 'learning_rate': 0.00032506944444444445, 'epoch': 0.7}
Step 2530: {'loss': 0.3835, 'grad_norm': 0.7901421785354614, 'learning_rate': 0.000324375, 'epoch': 0.7027777777777777}
Step 2540: {'loss': 0.4121, 'grad_norm': 0.9490479230880737, 'learning_rate': 0.00032368055555555557, 'epoch': 0.7055555555555556}
Step 2550: {'loss': 0.4004, 'grad_norm': 1.0988205671310425, 'learning_rate': 0.0003229861111111111, 'epoch': 0.7083333333333334}
Step 2560: {'loss': 0.3882, 'grad_norm': 0.9593459963798523, 'learning_rate': 0.00032229166666666664, 'epoch': 0.7111111111111111}
Step 2570: {'loss': 0.3163, 'grad_norm': 0.9000091552734375, 'learning_rate': 0.0003215972222222222, 'epoch': 0.7138888888888889}
Step 2580: {'loss': 0.3338, 'grad_norm': 1.3824268579483032, 'learning_rate': 0.00032090277777777776, 'epoch': 0.7166666666666667}
Step 2590: {'loss': 0.4025, 'grad_norm': 1.256145715713501, 'learning_rate': 0.0003202083333333333, 'epoch': 0.7194444444444444}
Step 2600: {'loss': 0.3569, 'grad_norm': 0.798492431640625, 'learning_rate': 0.0003195138888888889, 'epoch': 0.7222222222222222}
Step 2610: {'loss': 0.3743, 'grad_norm': 0.9391863346099854, 'learning_rate': 0.00031881944444444443, 'epoch': 0.725}
Step 2620: {'loss': 0.3551, 'grad_norm': 0.8469956517219543, 'learning_rate': 0.000318125, 'epoch': 0.7277777777777777}
Step 2630: {'loss': 0.3984, 'grad_norm': 0.9822350740432739, 'learning_rate': 0.00031743055555555555, 'epoch': 0.7305555555555555}
Step 2640: {'loss': 0.3729, 'grad_norm': 1.0933927297592163, 'learning_rate': 0.0003167361111111111, 'epoch': 0.7333333333333333}
Step 2650: {'loss': 0.3207, 'grad_norm': 0.7817102074623108, 'learning_rate': 0.0003160416666666667, 'epoch': 0.7361111111111112}
Step 2660: {'loss': 0.3645, 'grad_norm': 0.8489531874656677, 'learning_rate': 0.00031534722222222223, 'epoch': 0.7388888888888889}
Step 2670: {'loss': 0.3929, 'grad_norm': 0.7940676212310791, 'learning_rate': 0.0003146527777777778, 'epoch': 0.7416666666666667}
Step 2680: {'loss': 0.3295, 'grad_norm': 1.0490814447402954, 'learning_rate': 0.00031395833333333335, 'epoch': 0.7444444444444445}
Step 2690: {'loss': 0.3901, 'grad_norm': 0.857442319393158, 'learning_rate': 0.0003132638888888889, 'epoch': 0.7472222222222222}
Step 2700: {'loss': 0.3308, 'grad_norm': 0.7971367239952087, 'learning_rate': 0.00031256944444444447, 'epoch': 0.75}
Step 2710: {'loss': 0.3606, 'grad_norm': 0.8807734847068787, 'learning_rate': 0.00031187500000000003, 'epoch': 0.7527777777777778}
Step 2720: {'loss': 0.3597, 'grad_norm': 0.6814056634902954, 'learning_rate': 0.0003111805555555556, 'epoch': 0.7555555555555555}
Step 2730: {'loss': 0.3822, 'grad_norm': 1.128536581993103, 'learning_rate': 0.00031048611111111115, 'epoch': 0.7583333333333333}
Step 2740: {'loss': 0.3787, 'grad_norm': 0.731015682220459, 'learning_rate': 0.0003097916666666667, 'epoch': 0.7611111111111111}
Step 2750: {'loss': 0.4392, 'grad_norm': 0.8435808420181274, 'learning_rate': 0.00030909722222222227, 'epoch': 0.7638888888888888}
Step 2760: {'loss': 0.3846, 'grad_norm': 0.8796563744544983, 'learning_rate': 0.00030840277777777783, 'epoch': 0.7666666666666667}
Step 2770: {'loss': 0.3962, 'grad_norm': 0.902008056640625, 'learning_rate': 0.00030770833333333334, 'epoch': 0.7694444444444445}
Step 2780: {'loss': 0.337, 'grad_norm': 0.6345598101615906, 'learning_rate': 0.0003070138888888889, 'epoch': 0.7722222222222223}
Step 2790: {'loss': 0.3689, 'grad_norm': 1.1901744604110718, 'learning_rate': 0.0003063194444444444, 'epoch': 0.775}
Step 2800: {'loss': 0.4169, 'grad_norm': 0.8463287949562073, 'learning_rate': 0.00030562499999999996, 'epoch': 0.7777777777777778}
Step 2810: {'loss': 0.3384, 'grad_norm': 0.9040129780769348, 'learning_rate': 0.0003049305555555555, 'epoch': 0.7805555555555556}
Step 2820: {'loss': 0.4148, 'grad_norm': 0.9590955376625061, 'learning_rate': 0.0003042361111111111, 'epoch': 0.7833333333333333}
Step 2830: {'loss': 0.3521, 'grad_norm': 0.693814754486084, 'learning_rate': 0.00030354166666666664, 'epoch': 0.7861111111111111}
Step 2840: {'loss': 0.3594, 'grad_norm': 0.8709492683410645, 'learning_rate': 0.0003028472222222222, 'epoch': 0.7888888888888889}
Step 2850: {'loss': 0.3298, 'grad_norm': 0.5708358287811279, 'learning_rate': 0.00030215277777777776, 'epoch': 0.7916666666666666}
Step 2860: {'loss': 0.3482, 'grad_norm': 0.9333485960960388, 'learning_rate': 0.0003014583333333333, 'epoch': 0.7944444444444444}
Step 2870: {'loss': 0.4089, 'grad_norm': 1.0128849744796753, 'learning_rate': 0.0003007638888888889, 'epoch': 0.7972222222222223}
Step 2880: {'loss': 0.3907, 'grad_norm': 0.5821235179901123, 'learning_rate': 0.00030006944444444444, 'epoch': 0.8}
Step 2890: {'loss': 0.418, 'grad_norm': 1.314296841621399, 'learning_rate': 0.000299375, 'epoch': 0.8027777777777778}
Step 2900: {'loss': 0.3368, 'grad_norm': 0.5200119018554688, 'learning_rate': 0.00029868055555555556, 'epoch': 0.8055555555555556}
Step 2910: {'loss': 0.349, 'grad_norm': 1.3467901945114136, 'learning_rate': 0.0002979861111111111, 'epoch': 0.8083333333333333}
Step 2920: {'loss': 0.3834, 'grad_norm': 1.5464160442352295, 'learning_rate': 0.0002972916666666667, 'epoch': 0.8111111111111111}
Step 2930: {'loss': 0.3448, 'grad_norm': 1.107317328453064, 'learning_rate': 0.00029659722222222224, 'epoch': 0.8138888888888889}
Step 2940: {'loss': 0.3472, 'grad_norm': 0.9457998275756836, 'learning_rate': 0.0002959027777777778, 'epoch': 0.8166666666666667}
Step 2950: {'loss': 0.3676, 'grad_norm': 1.0156643390655518, 'learning_rate': 0.00029520833333333336, 'epoch': 0.8194444444444444}
Step 2960: {'loss': 0.3791, 'grad_norm': 1.1799653768539429, 'learning_rate': 0.0002945138888888889, 'epoch': 0.8222222222222222}
Step 2970: {'loss': 0.3193, 'grad_norm': 0.9307745099067688, 'learning_rate': 0.0002938194444444445, 'epoch': 0.825}
Step 2980: {'loss': 0.39, 'grad_norm': 0.8947412967681885, 'learning_rate': 0.00029312500000000004, 'epoch': 0.8277777777777777}
Step 2990: {'loss': 0.3787, 'grad_norm': 0.9158574938774109, 'learning_rate': 0.0002924305555555556, 'epoch': 0.8305555555555556}
Step 3000: {'loss': 0.4077, 'grad_norm': 1.2266215085983276, 'learning_rate': 0.0002917361111111111, 'epoch': 0.8333333333333334}
Step 3010: {'loss': 0.4178, 'grad_norm': 0.8785902857780457, 'learning_rate': 0.00029104166666666666, 'epoch': 0.8361111111111111}
Step 3020: {'loss': 0.3889, 'grad_norm': 0.7662994861602783, 'learning_rate': 0.0002903472222222222, 'epoch': 0.8388888888888889}
Step 3030: {'loss': 0.3824, 'grad_norm': 1.1541175842285156, 'learning_rate': 0.0002896527777777778, 'epoch': 0.8416666666666667}
Step 3040: {'loss': 0.3819, 'grad_norm': 0.9727465510368347, 'learning_rate': 0.00028895833333333334, 'epoch': 0.8444444444444444}
Step 3050: {'loss': 0.383, 'grad_norm': 1.0377856492996216, 'learning_rate': 0.0002882638888888889, 'epoch': 0.8472222222222222}
Step 3060: {'loss': 0.3489, 'grad_norm': 0.9798476099967957, 'learning_rate': 0.00028756944444444446, 'epoch': 0.85}
Step 3070: {'loss': 0.3353, 'grad_norm': 1.104252815246582, 'learning_rate': 0.000286875, 'epoch': 0.8527777777777777}
Step 3080: {'loss': 0.3653, 'grad_norm': 1.150504231452942, 'learning_rate': 0.0002861805555555556, 'epoch': 0.8555555555555555}
Step 3090: {'loss': 0.3876, 'grad_norm': 0.7566519379615784, 'learning_rate': 0.00028548611111111114, 'epoch': 0.8583333333333333}
Step 3100: {'loss': 0.4138, 'grad_norm': 1.0326967239379883, 'learning_rate': 0.00028479166666666665, 'epoch': 0.8611111111111112}
Step 3110: {'loss': 0.364, 'grad_norm': 0.8283437490463257, 'learning_rate': 0.0002840972222222222, 'epoch': 0.8638888888888889}
Step 3120: {'loss': 0.3831, 'grad_norm': 0.832892119884491, 'learning_rate': 0.00028340277777777777, 'epoch': 0.8666666666666667}
Step 3130: {'loss': 0.3397, 'grad_norm': 1.459678292274475, 'learning_rate': 0.0002827083333333333, 'epoch': 0.8694444444444445}
Step 3140: {'loss': 0.3461, 'grad_norm': 0.7184412479400635, 'learning_rate': 0.0002820138888888889, 'epoch': 0.8722222222222222}
Step 3150: {'loss': 0.3408, 'grad_norm': 1.2094392776489258, 'learning_rate': 0.00028131944444444444, 'epoch': 0.875}
Step 3160: {'loss': 0.3387, 'grad_norm': 0.6768863201141357, 'learning_rate': 0.000280625, 'epoch': 0.8777777777777778}
Step 3170: {'loss': 0.3069, 'grad_norm': 0.8875123858451843, 'learning_rate': 0.00027993055555555556, 'epoch': 0.8805555555555555}
Step 3180: {'loss': 0.3912, 'grad_norm': 1.2964032888412476, 'learning_rate': 0.0002792361111111111, 'epoch': 0.8833333333333333}
Step 3190: {'loss': 0.3151, 'grad_norm': 0.7067863941192627, 'learning_rate': 0.0002785416666666667, 'epoch': 0.8861111111111111}
Step 3200: {'loss': 0.3477, 'grad_norm': 0.9742035269737244, 'learning_rate': 0.00027784722222222224, 'epoch': 0.8888888888888888}
Step 3210: {'loss': 0.452, 'grad_norm': 0.9558876156806946, 'learning_rate': 0.0002771527777777778, 'epoch': 0.8916666666666667}
Step 3220: {'loss': 0.3897, 'grad_norm': 1.0591477155685425, 'learning_rate': 0.0002764583333333333, 'epoch': 0.8944444444444445}
Step 3230: {'loss': 0.34, 'grad_norm': 1.3361389636993408, 'learning_rate': 0.00027576388888888887, 'epoch': 0.8972222222222223}
Step 3240: {'loss': 0.3011, 'grad_norm': 1.1074970960617065, 'learning_rate': 0.00027506944444444443, 'epoch': 0.9}
Step 3250: {'loss': 0.3331, 'grad_norm': 0.722775936126709, 'learning_rate': 0.000274375, 'epoch': 0.9027777777777778}
Step 3260: {'loss': 0.374, 'grad_norm': 0.8493661880493164, 'learning_rate': 0.00027368055555555555, 'epoch': 0.9055555555555556}
Step 3270: {'loss': 0.3849, 'grad_norm': 0.9100289940834045, 'learning_rate': 0.0002729861111111111, 'epoch': 0.9083333333333333}
Step 3280: {'loss': 0.3433, 'grad_norm': 0.8552045226097107, 'learning_rate': 0.00027229166666666667, 'epoch': 0.9111111111111111}
Step 3290: {'loss': 0.3751, 'grad_norm': 0.857826828956604, 'learning_rate': 0.0002715972222222222, 'epoch': 0.9138888888888889}
Step 3300: {'loss': 0.3558, 'grad_norm': 0.8157193660736084, 'learning_rate': 0.0002709027777777778, 'epoch': 0.9166666666666666}
Step 3310: {'loss': 0.4073, 'grad_norm': 0.9628279209136963, 'learning_rate': 0.00027020833333333335, 'epoch': 0.9194444444444444}
Step 3320: {'loss': 0.4021, 'grad_norm': 0.9609795808792114, 'learning_rate': 0.0002695138888888889, 'epoch': 0.9222222222222223}
Step 3330: {'loss': 0.3567, 'grad_norm': 1.3999781608581543, 'learning_rate': 0.00026881944444444447, 'epoch': 0.925}
Step 3340: {'loss': 0.3711, 'grad_norm': 1.2117531299591064, 'learning_rate': 0.000268125, 'epoch': 0.9277777777777778}
Step 3350: {'loss': 0.4122, 'grad_norm': 1.022240400314331, 'learning_rate': 0.0002674305555555556, 'epoch': 0.9305555555555556}
Step 3360: {'loss': 0.3659, 'grad_norm': 0.8866394758224487, 'learning_rate': 0.00026673611111111115, 'epoch': 0.9333333333333333}
Step 3370: {'loss': 0.3684, 'grad_norm': 1.3425043821334839, 'learning_rate': 0.0002660416666666667, 'epoch': 0.9361111111111111}
Step 3380: {'loss': 0.3905, 'grad_norm': 0.6576633453369141, 'learning_rate': 0.00026534722222222226, 'epoch': 0.9388888888888889}
Step 3390: {'loss': 0.369, 'grad_norm': 1.0501981973648071, 'learning_rate': 0.0002646527777777778, 'epoch': 0.9416666666666667}
Step 3400: {'loss': 0.3522, 'grad_norm': 1.098344087600708, 'learning_rate': 0.00026395833333333333, 'epoch': 0.9444444444444444}
Step 3410: {'loss': 0.3607, 'grad_norm': 1.1188414096832275, 'learning_rate': 0.0002632638888888889, 'epoch': 0.9472222222222222}
Step 3420: {'loss': 0.335, 'grad_norm': 1.3185241222381592, 'learning_rate': 0.00026256944444444445, 'epoch': 0.95}
Step 3430: {'loss': 0.38, 'grad_norm': 1.3552043437957764, 'learning_rate': 0.000261875, 'epoch': 0.9527777777777777}
Step 3440: {'loss': 0.3711, 'grad_norm': 0.9623088836669922, 'learning_rate': 0.00026118055555555557, 'epoch': 0.9555555555555556}
Step 3450: {'loss': 0.3606, 'grad_norm': 0.7621198892593384, 'learning_rate': 0.0002604861111111111, 'epoch': 0.9583333333333334}
Step 3460: {'loss': 0.3802, 'grad_norm': 0.6428835988044739, 'learning_rate': 0.00025979166666666663, 'epoch': 0.9611111111111111}
Step 3470: {'loss': 0.3614, 'grad_norm': 1.148672342300415, 'learning_rate': 0.0002590972222222222, 'epoch': 0.9638888888888889}
Step 3480: {'loss': 0.4225, 'grad_norm': 1.0350546836853027, 'learning_rate': 0.00025840277777777775, 'epoch': 0.9666666666666667}
Step 3490: {'loss': 0.3477, 'grad_norm': 0.5905433297157288, 'learning_rate': 0.0002577083333333333, 'epoch': 0.9694444444444444}
Step 3500: {'loss': 0.3907, 'grad_norm': 0.9461696743965149, 'learning_rate': 0.0002570138888888889, 'epoch': 0.9722222222222222}
Step 3510: {'loss': 0.3489, 'grad_norm': 1.0080838203430176, 'learning_rate': 0.00025631944444444443, 'epoch': 0.975}
Step 3520: {'loss': 0.3857, 'grad_norm': 0.8705554604530334, 'learning_rate': 0.000255625, 'epoch': 0.9777777777777777}
Step 3530: {'loss': 0.3803, 'grad_norm': 1.1292628049850464, 'learning_rate': 0.00025493055555555555, 'epoch': 0.9805555555555555}
Step 3540: {'loss': 0.3626, 'grad_norm': 1.0636334419250488, 'learning_rate': 0.0002542361111111111, 'epoch': 0.9833333333333333}
Step 3550: {'loss': 0.3501, 'grad_norm': 0.9988353252410889, 'learning_rate': 0.00025354166666666667, 'epoch': 0.9861111111111112}
Step 3560: {'loss': 0.4112, 'grad_norm': 0.7816382646560669, 'learning_rate': 0.00025284722222222223, 'epoch': 0.9888888888888889}
Step 3570: {'loss': 0.3368, 'grad_norm': 0.768824577331543, 'learning_rate': 0.0002521527777777778, 'epoch': 0.9916666666666667}
Step 3580: {'loss': 0.3834, 'grad_norm': 1.0084502696990967, 'learning_rate': 0.00025145833333333335, 'epoch': 0.9944444444444445}
Step 3590: {'loss': 0.428, 'grad_norm': 0.9396147727966309, 'learning_rate': 0.0002507638888888889, 'epoch': 0.9972222222222222}
Step 3600: {'loss': 0.3542, 'grad_norm': 0.8921627998352051, 'learning_rate': 0.00025006944444444447, 'epoch': 1.0}
Step 3610: {'loss': 0.3506, 'grad_norm': 0.8859854340553284, 'learning_rate': 0.00024937500000000003, 'epoch': 1.0027777777777778}
Step 3620: {'loss': 0.3531, 'grad_norm': 1.3428713083267212, 'learning_rate': 0.00024868055555555554, 'epoch': 1.0055555555555555}
Step 3630: {'loss': 0.3836, 'grad_norm': 0.8328580856323242, 'learning_rate': 0.0002479861111111111, 'epoch': 1.0083333333333333}
Step 3640: {'loss': 0.3415, 'grad_norm': 1.2534739971160889, 'learning_rate': 0.00024729166666666666, 'epoch': 1.011111111111111}
Step 3650: {'loss': 0.3797, 'grad_norm': 0.7718539834022522, 'learning_rate': 0.0002465972222222222, 'epoch': 1.0138888888888888}
Step 3660: {'loss': 0.3449, 'grad_norm': 0.8716961145401001, 'learning_rate': 0.0002459027777777778, 'epoch': 1.0166666666666666}
Step 3670: {'loss': 0.3803, 'grad_norm': 1.0124484300613403, 'learning_rate': 0.00024520833333333334, 'epoch': 1.0194444444444444}
Step 3680: {'loss': 0.3361, 'grad_norm': 0.8162761330604553, 'learning_rate': 0.0002445138888888889, 'epoch': 1.0222222222222221}
Step 3690: {'loss': 0.3203, 'grad_norm': 0.8454585671424866, 'learning_rate': 0.00024381944444444445, 'epoch': 1.025}
Step 3700: {'loss': 0.3378, 'grad_norm': 0.803736686706543, 'learning_rate': 0.00024312500000000001, 'epoch': 1.0277777777777777}
Step 3710: {'loss': 0.3801, 'grad_norm': 1.037105679512024, 'learning_rate': 0.00024243055555555557, 'epoch': 1.0305555555555554}
Step 3720: {'loss': 0.34, 'grad_norm': 0.8163091540336609, 'learning_rate': 0.00024173611111111113, 'epoch': 1.0333333333333334}
Step 3730: {'loss': 0.3709, 'grad_norm': 0.8226679563522339, 'learning_rate': 0.00024104166666666667, 'epoch': 1.0361111111111112}
Step 3740: {'loss': 0.3436, 'grad_norm': 0.971998929977417, 'learning_rate': 0.00024034722222222223, 'epoch': 1.038888888888889}
Step 3750: {'loss': 0.427, 'grad_norm': 0.7879049777984619, 'learning_rate': 0.00023965277777777779, 'epoch': 1.0416666666666667}
Step 3760: {'loss': 0.3107, 'grad_norm': 0.8107009530067444, 'learning_rate': 0.00023895833333333335, 'epoch': 1.0444444444444445}
Step 3770: {'loss': 0.3468, 'grad_norm': 0.7832186818122864, 'learning_rate': 0.00023826388888888888, 'epoch': 1.0472222222222223}
Step 3780: {'loss': 0.4007, 'grad_norm': 0.781257688999176, 'learning_rate': 0.00023756944444444444, 'epoch': 1.05}
Step 3790: {'loss': 0.3139, 'grad_norm': 1.0049653053283691, 'learning_rate': 0.000236875, 'epoch': 1.0527777777777778}
Step 3800: {'loss': 0.3323, 'grad_norm': 1.2994887828826904, 'learning_rate': 0.00023618055555555556, 'epoch': 1.0555555555555556}
Step 3810: {'loss': 0.3421, 'grad_norm': 0.8255516290664673, 'learning_rate': 0.00023548611111111112, 'epoch': 1.0583333333333333}
Step 3820: {'loss': 0.3754, 'grad_norm': 2.0970911979675293, 'learning_rate': 0.00023479166666666668, 'epoch': 1.0611111111111111}
Step 3830: {'loss': 0.3828, 'grad_norm': 1.1731005907058716, 'learning_rate': 0.00023409722222222224, 'epoch': 1.0638888888888889}
Step 3840: {'loss': 0.3991, 'grad_norm': 1.2053829431533813, 'learning_rate': 0.00023340277777777777, 'epoch': 1.0666666666666667}
Step 3850: {'loss': 0.3944, 'grad_norm': 1.0773425102233887, 'learning_rate': 0.00023270833333333333, 'epoch': 1.0694444444444444}
Step 3860: {'loss': 0.3101, 'grad_norm': 0.771128237247467, 'learning_rate': 0.0002320138888888889, 'epoch': 1.0722222222222222}
Step 3870: {'loss': 0.2803, 'grad_norm': 0.7973822951316833, 'learning_rate': 0.00023131944444444445, 'epoch': 1.075}
Step 3880: {'loss': 0.3418, 'grad_norm': 0.9426569938659668, 'learning_rate': 0.000230625, 'epoch': 1.0777777777777777}
Step 3890: {'loss': 0.3685, 'grad_norm': 1.2119535207748413, 'learning_rate': 0.00022993055555555557, 'epoch': 1.0805555555555555}
Step 3900: {'loss': 0.3694, 'grad_norm': 0.6173450946807861, 'learning_rate': 0.00022923611111111113, 'epoch': 1.0833333333333333}
Step 3910: {'loss': 0.3128, 'grad_norm': 0.980521023273468, 'learning_rate': 0.0002285416666666667, 'epoch': 1.086111111111111}
Step 3920: {'loss': 0.2919, 'grad_norm': 0.8003214001655579, 'learning_rate': 0.00022784722222222222, 'epoch': 1.0888888888888888}
Step 3930: {'loss': 0.3589, 'grad_norm': 0.8520367741584778, 'learning_rate': 0.00022715277777777778, 'epoch': 1.0916666666666666}
Step 3940: {'loss': 0.3961, 'grad_norm': 1.0289424657821655, 'learning_rate': 0.00022645833333333334, 'epoch': 1.0944444444444446}
Step 3950: {'loss': 0.3905, 'grad_norm': 0.8028462529182434, 'learning_rate': 0.00022576388888888887, 'epoch': 1.0972222222222223}
Step 3960: {'loss': 0.3381, 'grad_norm': 0.9631516337394714, 'learning_rate': 0.00022506944444444443, 'epoch': 1.1}
Step 3970: {'loss': 0.3346, 'grad_norm': 0.7749044895172119, 'learning_rate': 0.000224375, 'epoch': 1.1027777777777779}
Step 3980: {'loss': 0.3376, 'grad_norm': 0.8464378118515015, 'learning_rate': 0.00022368055555555555, 'epoch': 1.1055555555555556}
Step 3990: {'loss': 0.3355, 'grad_norm': 0.9873055219650269, 'learning_rate': 0.0002229861111111111, 'epoch': 1.1083333333333334}
Step 4000: {'loss': 0.3619, 'grad_norm': 1.080307960510254, 'learning_rate': 0.00022229166666666667, 'epoch': 1.1111111111111112}
Step 4010: {'loss': 0.3794, 'grad_norm': 1.3972293138504028, 'learning_rate': 0.00022159722222222223, 'epoch': 1.113888888888889}
Step 4020: {'loss': 0.3304, 'grad_norm': 1.0977786779403687, 'learning_rate': 0.0002209027777777778, 'epoch': 1.1166666666666667}
Step 4030: {'loss': 0.376, 'grad_norm': 0.8158265352249146, 'learning_rate': 0.00022020833333333335, 'epoch': 1.1194444444444445}
Step 4040: {'loss': 0.3648, 'grad_norm': 1.1149884462356567, 'learning_rate': 0.0002195138888888889, 'epoch': 1.1222222222222222}
Step 4050: {'loss': 0.3784, 'grad_norm': 0.9402377605438232, 'learning_rate': 0.00021881944444444447, 'epoch': 1.125}
Step 4060: {'loss': 0.3634, 'grad_norm': 1.045899748802185, 'learning_rate': 0.00021812500000000003, 'epoch': 1.1277777777777778}
Step 4070: {'loss': 0.3691, 'grad_norm': 1.4642329216003418, 'learning_rate': 0.00021743055555555554, 'epoch': 1.1305555555555555}
Step 4080: {'loss': 0.3742, 'grad_norm': 1.2382274866104126, 'learning_rate': 0.0002167361111111111, 'epoch': 1.1333333333333333}
Step 4090: {'loss': 0.357, 'grad_norm': 1.2162799835205078, 'learning_rate': 0.00021604166666666666, 'epoch': 1.136111111111111}
Step 4100: {'loss': 0.3564, 'grad_norm': 1.7792078256607056, 'learning_rate': 0.00021534722222222221, 'epoch': 1.1388888888888888}
Step 4110: {'loss': 0.3157, 'grad_norm': 0.7796129584312439, 'learning_rate': 0.00021465277777777777, 'epoch': 1.1416666666666666}
Step 4120: {'loss': 0.345, 'grad_norm': 0.6643263101577759, 'learning_rate': 0.00021395833333333333, 'epoch': 1.1444444444444444}
Step 4130: {'loss': 0.3544, 'grad_norm': 1.0279121398925781, 'learning_rate': 0.0002132638888888889, 'epoch': 1.1472222222222221}
Step 4140: {'loss': 0.3367, 'grad_norm': 1.2373069524765015, 'learning_rate': 0.00021256944444444445, 'epoch': 1.15}
Step 4150: {'loss': 0.4081, 'grad_norm': 0.8973654508590698, 'learning_rate': 0.00021187500000000001, 'epoch': 1.1527777777777777}
Step 4160: {'loss': 0.3087, 'grad_norm': 0.9051358103752136, 'learning_rate': 0.00021118055555555557, 'epoch': 1.1555555555555554}
Step 4170: {'loss': 0.3722, 'grad_norm': 0.7803297638893127, 'learning_rate': 0.00021048611111111113, 'epoch': 1.1583333333333332}
Step 4180: {'loss': 0.3571, 'grad_norm': 0.6003175973892212, 'learning_rate': 0.00020979166666666667, 'epoch': 1.1611111111111112}
Step 4190: {'loss': 0.3604, 'grad_norm': 0.8360638618469238, 'learning_rate': 0.00020909722222222223, 'epoch': 1.163888888888889}
Step 4200: {'loss': 0.3655, 'grad_norm': 1.0686113834381104, 'learning_rate': 0.00020840277777777779, 'epoch': 1.1666666666666667}
Step 4210: {'loss': 0.3847, 'grad_norm': 0.7714444398880005, 'learning_rate': 0.00020770833333333335, 'epoch': 1.1694444444444445}
Step 4220: {'loss': 0.4007, 'grad_norm': 1.184598684310913, 'learning_rate': 0.00020701388888888888, 'epoch': 1.1722222222222223}
Step 4230: {'loss': 0.37, 'grad_norm': 1.0426453351974487, 'learning_rate': 0.00020631944444444444, 'epoch': 1.175}
Step 4240: {'loss': 0.3444, 'grad_norm': 0.8462176322937012, 'learning_rate': 0.000205625, 'epoch': 1.1777777777777778}
Step 4250: {'loss': 0.3525, 'grad_norm': 0.6479933857917786, 'learning_rate': 0.00020493055555555556, 'epoch': 1.1805555555555556}
Step 4260: {'loss': 0.4023, 'grad_norm': 1.145336627960205, 'learning_rate': 0.00020423611111111112, 'epoch': 1.1833333333333333}
Step 4270: {'loss': 0.3336, 'grad_norm': 0.7819116115570068, 'learning_rate': 0.00020354166666666668, 'epoch': 1.1861111111111111}
Step 4280: {'loss': 0.3612, 'grad_norm': 1.7299307584762573, 'learning_rate': 0.00020284722222222224, 'epoch': 1.1888888888888889}
Step 4290: {'loss': 0.3666, 'grad_norm': 0.8103238344192505, 'learning_rate': 0.00020215277777777777, 'epoch': 1.1916666666666667}
Step 4300: {'loss': 0.333, 'grad_norm': 0.8873094916343689, 'learning_rate': 0.00020145833333333333, 'epoch': 1.1944444444444444}
Step 4310: {'loss': 0.4009, 'grad_norm': 0.982348620891571, 'learning_rate': 0.0002007638888888889, 'epoch': 1.1972222222222222}
Step 4320: {'loss': 0.3519, 'grad_norm': 0.9388111233711243, 'learning_rate': 0.00020006944444444445, 'epoch': 1.2}
Step 4330: {'loss': 0.3828, 'grad_norm': 1.0468776226043701, 'learning_rate': 0.000199375, 'epoch': 1.2027777777777777}
Step 4340: {'loss': 0.3669, 'grad_norm': 0.770270586013794, 'learning_rate': 0.00019868055555555557, 'epoch': 1.2055555555555555}
Step 4350: {'loss': 0.3189, 'grad_norm': 0.9708104729652405, 'learning_rate': 0.00019798611111111113, 'epoch': 1.2083333333333333}
Step 4360: {'loss': 0.3337, 'grad_norm': 0.8803644776344299, 'learning_rate': 0.0001972916666666667, 'epoch': 1.211111111111111}
Step 4370: {'loss': 0.3809, 'grad_norm': 0.9640840291976929, 'learning_rate': 0.00019659722222222222, 'epoch': 1.2138888888888888}
Step 4380: {'loss': 0.3167, 'grad_norm': 0.913031280040741, 'learning_rate': 0.00019590277777777778, 'epoch': 1.2166666666666668}
Step 4390: {'loss': 0.3368, 'grad_norm': 0.90055912733078, 'learning_rate': 0.00019520833333333334, 'epoch': 1.2194444444444446}
Step 4400: {'loss': 0.3454, 'grad_norm': 0.7960202097892761, 'learning_rate': 0.00019451388888888887, 'epoch': 1.2222222222222223}
Step 4410: {'loss': 0.374, 'grad_norm': 0.8005334734916687, 'learning_rate': 0.00019381944444444443, 'epoch': 1.225}
Step 4420: {'loss': 0.3578, 'grad_norm': 0.9576867818832397, 'learning_rate': 0.000193125, 'epoch': 1.2277777777777779}
Step 4430: {'loss': 0.343, 'grad_norm': 0.5703845024108887, 'learning_rate': 0.00019243055555555555, 'epoch': 1.2305555555555556}
Step 4440: {'loss': 0.3653, 'grad_norm': 0.7679778933525085, 'learning_rate': 0.0001917361111111111, 'epoch': 1.2333333333333334}
Step 4450: {'loss': 0.2661, 'grad_norm': 0.7979286313056946, 'learning_rate': 0.00019104166666666667, 'epoch': 1.2361111111111112}
Step 4460: {'loss': 0.3981, 'grad_norm': 1.1132190227508545, 'learning_rate': 0.00019034722222222223, 'epoch': 1.238888888888889}
Step 4470: {'loss': 0.3359, 'grad_norm': 1.2087671756744385, 'learning_rate': 0.0001896527777777778, 'epoch': 1.2416666666666667}
Step 4480: {'loss': 0.3676, 'grad_norm': 0.8415496349334717, 'learning_rate': 0.00018895833333333335, 'epoch': 1.2444444444444445}
Step 4490: {'loss': 0.3652, 'grad_norm': 0.796934187412262, 'learning_rate': 0.0001882638888888889, 'epoch': 1.2472222222222222}
Step 4500: {'loss': 0.4073, 'grad_norm': 0.7484904527664185, 'learning_rate': 0.00018756944444444447, 'epoch': 1.25}
Step 4510: {'loss': 0.3839, 'grad_norm': 1.3971506357192993, 'learning_rate': 0.00018687500000000003, 'epoch': 1.2527777777777778}
Step 4520: {'loss': 0.3985, 'grad_norm': 1.1106171607971191, 'learning_rate': 0.00018618055555555553, 'epoch': 1.2555555555555555}
Step 4530: {'loss': 0.3307, 'grad_norm': 0.726000964641571, 'learning_rate': 0.0001854861111111111, 'epoch': 1.2583333333333333}
Step 4540: {'loss': 0.3851, 'grad_norm': 0.8760451674461365, 'learning_rate': 0.00018479166666666665, 'epoch': 1.261111111111111}
Step 4550: {'loss': 0.3427, 'grad_norm': 0.5219600200653076, 'learning_rate': 0.00018409722222222221, 'epoch': 1.2638888888888888}
Step 4560: {'loss': 0.318, 'grad_norm': 0.7084457874298096, 'learning_rate': 0.00018340277777777777, 'epoch': 1.2666666666666666}
Step 4570: {'loss': 0.3342, 'grad_norm': 0.9438124299049377, 'learning_rate': 0.00018270833333333333, 'epoch': 1.2694444444444444}
Step 4580: {'loss': 0.3486, 'grad_norm': 1.0097692012786865, 'learning_rate': 0.0001820138888888889, 'epoch': 1.2722222222222221}
Step 4590: {'loss': 0.3885, 'grad_norm': 0.7854295969009399, 'learning_rate': 0.00018131944444444445, 'epoch': 1.275}
Step 4600: {'loss': 0.3959, 'grad_norm': 0.5837440490722656, 'learning_rate': 0.000180625, 'epoch': 1.2777777777777777}
Step 4610: {'loss': 0.3926, 'grad_norm': 0.5509504079818726, 'learning_rate': 0.00017993055555555557, 'epoch': 1.2805555555555554}
Step 4620: {'loss': 0.3754, 'grad_norm': 0.9418832063674927, 'learning_rate': 0.00017923611111111113, 'epoch': 1.2833333333333332}
Step 4630: {'loss': 0.3246, 'grad_norm': 0.8863547444343567, 'learning_rate': 0.00017854166666666667, 'epoch': 1.286111111111111}
Step 4640: {'loss': 0.3483, 'grad_norm': 0.7754102945327759, 'learning_rate': 0.00017784722222222222, 'epoch': 1.2888888888888888}
Step 4650: {'loss': 0.3833, 'grad_norm': 1.5061159133911133, 'learning_rate': 0.00017715277777777778, 'epoch': 1.2916666666666667}
Step 4660: {'loss': 0.3558, 'grad_norm': 0.47591641545295715, 'learning_rate': 0.00017645833333333334, 'epoch': 1.2944444444444445}
Step 4670: {'loss': 0.3516, 'grad_norm': 0.8976665139198303, 'learning_rate': 0.00017576388888888888, 'epoch': 1.2972222222222223}
Step 4680: {'loss': 0.35, 'grad_norm': 0.7774971127510071, 'learning_rate': 0.00017506944444444444, 'epoch': 1.3}
Step 4690: {'loss': 0.3638, 'grad_norm': 1.359769344329834, 'learning_rate': 0.000174375, 'epoch': 1.3027777777777778}
Step 4700: {'loss': 0.3473, 'grad_norm': 1.3779518604278564, 'learning_rate': 0.00017368055555555556, 'epoch': 1.3055555555555556}
Step 4710: {'loss': 0.359, 'grad_norm': 0.8527313470840454, 'learning_rate': 0.00017298611111111112, 'epoch': 1.3083333333333333}
Step 4720: {'loss': 0.3561, 'grad_norm': 0.8524971604347229, 'learning_rate': 0.00017229166666666668, 'epoch': 1.3111111111111111}
Step 4730: {'loss': 0.3525, 'grad_norm': 0.7389500737190247, 'learning_rate': 0.00017159722222222224, 'epoch': 1.3138888888888889}
Step 4740: {'loss': 0.3869, 'grad_norm': 0.828022837638855, 'learning_rate': 0.00017090277777777777, 'epoch': 1.3166666666666667}
Step 4750: {'loss': 0.3539, 'grad_norm': 0.816684901714325, 'learning_rate': 0.00017020833333333333, 'epoch': 1.3194444444444444}
Step 4760: {'loss': 0.3704, 'grad_norm': 1.1239229440689087, 'learning_rate': 0.0001695138888888889, 'epoch': 1.3222222222222222}
Step 4770: {'loss': 0.4153, 'grad_norm': 1.3787548542022705, 'learning_rate': 0.00016881944444444445, 'epoch': 1.325}
Step 4780: {'loss': 0.4624, 'grad_norm': 1.0065178871154785, 'learning_rate': 0.000168125, 'epoch': 1.3277777777777777}
Step 4790: {'loss': 0.3728, 'grad_norm': 1.0494624376296997, 'learning_rate': 0.00016743055555555557, 'epoch': 1.3305555555555555}
Step 4800: {'loss': 0.4178, 'grad_norm': 0.9695093035697937, 'learning_rate': 0.00016673611111111113, 'epoch': 1.3333333333333333}
Step 4810: {'loss': 0.3939, 'grad_norm': 0.8218809962272644, 'learning_rate': 0.0001660416666666667, 'epoch': 1.3361111111111112}
Step 4820: {'loss': 0.3918, 'grad_norm': 0.8516520261764526, 'learning_rate': 0.00016534722222222222, 'epoch': 1.338888888888889}
Step 4830: {'loss': 0.3491, 'grad_norm': 0.9054841995239258, 'learning_rate': 0.00016465277777777778, 'epoch': 1.3416666666666668}
Step 4840: {'loss': 0.3876, 'grad_norm': 0.8606035709381104, 'learning_rate': 0.00016395833333333334, 'epoch': 1.3444444444444446}
Step 4850: {'loss': 0.3552, 'grad_norm': 0.632196307182312, 'learning_rate': 0.00016326388888888887, 'epoch': 1.3472222222222223}
Step 4860: {'loss': 0.3742, 'grad_norm': 0.7745953798294067, 'learning_rate': 0.00016256944444444443, 'epoch': 1.35}
Step 4870: {'loss': 0.343, 'grad_norm': 0.8371674418449402, 'learning_rate': 0.000161875, 'epoch': 1.3527777777777779}
Step 4880: {'loss': 0.4122, 'grad_norm': 1.0833882093429565, 'learning_rate': 0.00016118055555555555, 'epoch': 1.3555555555555556}
Step 4890: {'loss': 0.3758, 'grad_norm': 0.7350378036499023, 'learning_rate': 0.0001604861111111111, 'epoch': 1.3583333333333334}
Step 4900: {'loss': 0.3175, 'grad_norm': 0.5855856537818909, 'learning_rate': 0.00015979166666666667, 'epoch': 1.3611111111111112}
Step 4910: {'loss': 0.3692, 'grad_norm': 1.0215426683425903, 'learning_rate': 0.00015909722222222223, 'epoch': 1.363888888888889}
Step 4920: {'loss': 0.3713, 'grad_norm': 0.8262746930122375, 'learning_rate': 0.0001584027777777778, 'epoch': 1.3666666666666667}
Step 4930: {'loss': 0.3511, 'grad_norm': 1.4491920471191406, 'learning_rate': 0.00015770833333333335, 'epoch': 1.3694444444444445}
Step 4940: {'loss': 0.3295, 'grad_norm': 1.1585856676101685, 'learning_rate': 0.0001570138888888889, 'epoch': 1.3722222222222222}
Step 4950: {'loss': 0.3665, 'grad_norm': 1.031739354133606, 'learning_rate': 0.00015631944444444447, 'epoch': 1.375}
Step 4960: {'loss': 0.3473, 'grad_norm': 0.4752075672149658, 'learning_rate': 0.00015562500000000003, 'epoch': 1.3777777777777778}
Step 4970: {'loss': 0.3455, 'grad_norm': 0.7579383850097656, 'learning_rate': 0.00015493055555555553, 'epoch': 1.3805555555555555}
Step 4980: {'loss': 0.3407, 'grad_norm': 0.7221474647521973, 'learning_rate': 0.0001542361111111111, 'epoch': 1.3833333333333333}
Step 4990: {'loss': 0.389, 'grad_norm': 1.090413212776184, 'learning_rate': 0.00015354166666666665, 'epoch': 1.386111111111111}
Step 5000: {'loss': 0.348, 'grad_norm': 1.1612117290496826, 'learning_rate': 0.0001528472222222222, 'epoch': 1.3888888888888888}
Step 5010: {'loss': 0.328, 'grad_norm': 0.502216637134552, 'learning_rate': 0.00015215277777777777, 'epoch': 1.3916666666666666}
Step 5020: {'loss': 0.3754, 'grad_norm': 0.8838213086128235, 'learning_rate': 0.00015145833333333333, 'epoch': 1.3944444444444444}
Step 5030: {'loss': 0.395, 'grad_norm': 0.912162184715271, 'learning_rate': 0.0001507638888888889, 'epoch': 1.3972222222222221}
Step 5040: {'loss': 0.3442, 'grad_norm': 1.622153401374817, 'learning_rate': 0.00015006944444444445, 'epoch': 1.4}
Step 5050: {'loss': 0.3199, 'grad_norm': 0.7381161451339722, 'learning_rate': 0.000149375, 'epoch': 1.4027777777777777}
Step 5060: {'loss': 0.3909, 'grad_norm': 0.9009467363357544, 'learning_rate': 0.00014868055555555557, 'epoch': 1.4055555555555554}
Step 5070: {'loss': 0.3849, 'grad_norm': 0.7018812894821167, 'learning_rate': 0.00014798611111111113, 'epoch': 1.4083333333333332}
Step 5080: {'loss': 0.3835, 'grad_norm': 0.9546132683753967, 'learning_rate': 0.00014729166666666666, 'epoch': 1.411111111111111}
Step 5090: {'loss': 0.3274, 'grad_norm': 1.1697124242782593, 'learning_rate': 0.00014659722222222222, 'epoch': 1.4138888888888888}
Step 5100: {'loss': 0.4052, 'grad_norm': 1.3565664291381836, 'learning_rate': 0.00014590277777777778, 'epoch': 1.4166666666666667}
Step 5110: {'loss': 0.3604, 'grad_norm': 1.1787910461425781, 'learning_rate': 0.00014520833333333334, 'epoch': 1.4194444444444445}
Step 5120: {'loss': 0.3559, 'grad_norm': 1.0595543384552002, 'learning_rate': 0.00014451388888888888, 'epoch': 1.4222222222222223}
Step 5130: {'loss': 0.3744, 'grad_norm': 0.9134190678596497, 'learning_rate': 0.00014381944444444444, 'epoch': 1.425}
Step 5140: {'loss': 0.4067, 'grad_norm': 1.0465103387832642, 'learning_rate': 0.000143125, 'epoch': 1.4277777777777778}
Step 5150: {'loss': 0.3462, 'grad_norm': 0.9561845660209656, 'learning_rate': 0.00014243055555555556, 'epoch': 1.4305555555555556}
Step 5160: {'loss': 0.3519, 'grad_norm': 1.109302282333374, 'learning_rate': 0.00014173611111111112, 'epoch': 1.4333333333333333}
Step 5170: {'loss': 0.3091, 'grad_norm': 0.7313287854194641, 'learning_rate': 0.00014104166666666668, 'epoch': 1.4361111111111111}
Step 5180: {'loss': 0.3191, 'grad_norm': 0.7655259966850281, 'learning_rate': 0.00014034722222222223, 'epoch': 1.4388888888888889}
Step 5190: {'loss': 0.4557, 'grad_norm': 1.1859838962554932, 'learning_rate': 0.00013965277777777777, 'epoch': 1.4416666666666667}
Step 5200: {'loss': 0.3647, 'grad_norm': 1.1753978729248047, 'learning_rate': 0.00013895833333333333, 'epoch': 1.4444444444444444}
Step 5210: {'loss': 0.3549, 'grad_norm': 1.3126381635665894, 'learning_rate': 0.0001382638888888889, 'epoch': 1.4472222222222222}
Step 5220: {'loss': 0.4042, 'grad_norm': 1.0486931800842285, 'learning_rate': 0.00013756944444444445, 'epoch': 1.45}
Step 5230: {'loss': 0.3582, 'grad_norm': 0.8072413206100464, 'learning_rate': 0.000136875, 'epoch': 1.4527777777777777}
Step 5240: {'loss': 0.3522, 'grad_norm': 0.8691003322601318, 'learning_rate': 0.00013618055555555557, 'epoch': 1.4555555555555555}
Step 5250: {'loss': 0.355, 'grad_norm': 0.9625279903411865, 'learning_rate': 0.00013548611111111113, 'epoch': 1.4583333333333333}
Step 5260: {'loss': 0.3888, 'grad_norm': 0.8138656616210938, 'learning_rate': 0.00013479166666666669, 'epoch': 1.4611111111111112}
Step 5270: {'loss': 0.3357, 'grad_norm': 1.9988906383514404, 'learning_rate': 0.00013409722222222222, 'epoch': 1.463888888888889}
Step 5280: {'loss': 0.377, 'grad_norm': 0.481717586517334, 'learning_rate': 0.00013340277777777778, 'epoch': 1.4666666666666668}
Step 5290: {'loss': 0.3725, 'grad_norm': 0.9918224215507507, 'learning_rate': 0.00013270833333333334, 'epoch': 1.4694444444444446}
Step 5300: {'loss': 0.3516, 'grad_norm': 1.0938483476638794, 'learning_rate': 0.00013201388888888887, 'epoch': 1.4722222222222223}
Step 5310: {'loss': 0.3863, 'grad_norm': 1.4641090631484985, 'learning_rate': 0.00013131944444444443, 'epoch': 1.475}
Step 5320: {'loss': 0.3295, 'grad_norm': 0.804721474647522, 'learning_rate': 0.000130625, 'epoch': 1.4777777777777779}
Step 5330: {'loss': 0.3876, 'grad_norm': 0.8012727499008179, 'learning_rate': 0.00012993055555555555, 'epoch': 1.4805555555555556}
Step 5340: {'loss': 0.3559, 'grad_norm': 1.1822549104690552, 'learning_rate': 0.0001292361111111111, 'epoch': 1.4833333333333334}
Step 5350: {'loss': 0.3377, 'grad_norm': 1.0906949043273926, 'learning_rate': 0.00012854166666666667, 'epoch': 1.4861111111111112}
Step 5360: {'loss': 0.286, 'grad_norm': 1.3319058418273926, 'learning_rate': 0.00012784722222222223, 'epoch': 1.488888888888889}
Step 5370: {'loss': 0.3698, 'grad_norm': 0.8560986518859863, 'learning_rate': 0.0001271527777777778, 'epoch': 1.4916666666666667}
Step 5380: {'loss': 0.38, 'grad_norm': 1.0357601642608643, 'learning_rate': 0.00012645833333333335, 'epoch': 1.4944444444444445}
Step 5390: {'loss': 0.3358, 'grad_norm': 0.7580131888389587, 'learning_rate': 0.0001257638888888889, 'epoch': 1.4972222222222222}
Step 5400: {'loss': 0.3221, 'grad_norm': 0.8353055119514465, 'learning_rate': 0.00012506944444444447, 'epoch': 1.5}
Step 5410: {'loss': 0.3427, 'grad_norm': 0.6895925998687744, 'learning_rate': 0.000124375, 'epoch': 1.5027777777777778}
Step 5420: {'loss': 0.3894, 'grad_norm': 0.9126948714256287, 'learning_rate': 0.00012368055555555556, 'epoch': 1.5055555555555555}
Step 5430: {'loss': 0.3458, 'grad_norm': 0.9382580518722534, 'learning_rate': 0.00012298611111111112, 'epoch': 1.5083333333333333}
Step 5440: {'loss': 0.3798, 'grad_norm': 0.898668646812439, 'learning_rate': 0.00012229166666666668, 'epoch': 1.511111111111111}
Step 5450: {'loss': 0.351, 'grad_norm': 0.830948531627655, 'learning_rate': 0.00012159722222222223, 'epoch': 1.5138888888888888}
Step 5460: {'loss': 0.3607, 'grad_norm': 0.8705616593360901, 'learning_rate': 0.00012090277777777777, 'epoch': 1.5166666666666666}
Step 5470: {'loss': 0.3699, 'grad_norm': 0.6962909698486328, 'learning_rate': 0.00012020833333333333, 'epoch': 1.5194444444444444}
Step 5480: {'loss': 0.3743, 'grad_norm': 1.0008952617645264, 'learning_rate': 0.00011951388888888889, 'epoch': 1.5222222222222221}
Step 5490: {'loss': 0.368, 'grad_norm': 0.6452656388282776, 'learning_rate': 0.00011881944444444445, 'epoch': 1.525}
Step 5500: {'loss': 0.3456, 'grad_norm': 1.0440870523452759, 'learning_rate': 0.000118125, 'epoch': 1.5277777777777777}
Step 5510: {'loss': 0.3291, 'grad_norm': 1.0869600772857666, 'learning_rate': 0.00011743055555555556, 'epoch': 1.5305555555555554}
Step 5520: {'loss': 0.3236, 'grad_norm': 0.8875594735145569, 'learning_rate': 0.00011673611111111112, 'epoch': 1.5333333333333332}
Step 5530: {'loss': 0.323, 'grad_norm': 0.7257322669029236, 'learning_rate': 0.00011604166666666666, 'epoch': 1.536111111111111}
Step 5540: {'loss': 0.3745, 'grad_norm': 1.7268681526184082, 'learning_rate': 0.00011534722222222222, 'epoch': 1.5388888888888888}
Step 5550: {'loss': 0.3217, 'grad_norm': 0.6470106244087219, 'learning_rate': 0.00011465277777777778, 'epoch': 1.5416666666666665}
Step 5560: {'loss': 0.3073, 'grad_norm': 0.6193144917488098, 'learning_rate': 0.00011395833333333333, 'epoch': 1.5444444444444443}
Step 5570: {'loss': 0.3747, 'grad_norm': 1.0123426914215088, 'learning_rate': 0.00011326388888888889, 'epoch': 1.5472222222222223}
Step 5580: {'loss': 0.3806, 'grad_norm': 0.978909969329834, 'learning_rate': 0.00011256944444444445, 'epoch': 1.55}
Step 5590: {'loss': 0.3817, 'grad_norm': 0.7009729146957397, 'learning_rate': 0.00011187500000000001, 'epoch': 1.5527777777777778}
Step 5600: {'loss': 0.3146, 'grad_norm': 0.698408305644989, 'learning_rate': 0.00011118055555555557, 'epoch': 1.5555555555555556}
Step 5610: {'loss': 0.3832, 'grad_norm': 0.902632474899292, 'learning_rate': 0.0001104861111111111, 'epoch': 1.5583333333333333}
Step 5620: {'loss': 0.3529, 'grad_norm': 0.6839684844017029, 'learning_rate': 0.00010979166666666666, 'epoch': 1.5611111111111111}
Step 5630: {'loss': 0.357, 'grad_norm': 0.5723890066146851, 'learning_rate': 0.00010909722222222222, 'epoch': 1.5638888888888889}
Step 5640: {'loss': 0.4109, 'grad_norm': 0.9357075691223145, 'learning_rate': 0.00010840277777777778, 'epoch': 1.5666666666666667}
Step 5650: {'loss': 0.3553, 'grad_norm': 0.8819712400436401, 'learning_rate': 0.00010770833333333334, 'epoch': 1.5694444444444444}
Step 5660: {'loss': 0.3663, 'grad_norm': 0.9105762839317322, 'learning_rate': 0.0001070138888888889, 'epoch': 1.5722222222222222}
Step 5670: {'loss': 0.3442, 'grad_norm': 1.0831583738327026, 'learning_rate': 0.00010631944444444445, 'epoch': 1.575}
Step 5680: {'loss': 0.3072, 'grad_norm': 0.735105574131012, 'learning_rate': 0.00010562499999999999, 'epoch': 1.5777777777777777}
Step 5690: {'loss': 0.3674, 'grad_norm': 0.7399554252624512, 'learning_rate': 0.00010493055555555555, 'epoch': 1.5805555555555557}
Step 5700: {'loss': 0.341, 'grad_norm': 0.8500544428825378, 'learning_rate': 0.00010423611111111111, 'epoch': 1.5833333333333335}
Step 5710: {'loss': 0.3554, 'grad_norm': 0.7692764401435852, 'learning_rate': 0.00010354166666666667, 'epoch': 1.5861111111111112}
Step 5720: {'loss': 0.3355, 'grad_norm': 1.1755496263504028, 'learning_rate': 0.00010284722222222223, 'epoch': 1.588888888888889}
Step 5730: {'loss': 0.3509, 'grad_norm': 0.8039107322692871, 'learning_rate': 0.00010215277777777778, 'epoch': 1.5916666666666668}
Step 5740: {'loss': 0.3688, 'grad_norm': 0.8833711743354797, 'learning_rate': 0.00010145833333333334, 'epoch': 1.5944444444444446}
Step 5750: {'loss': 0.3639, 'grad_norm': 1.129718542098999, 'learning_rate': 0.0001007638888888889, 'epoch': 1.5972222222222223}
Step 5760: {'loss': 0.3447, 'grad_norm': 1.591655969619751, 'learning_rate': 0.00010006944444444444, 'epoch': 1.6}
Step 5770: {'loss': 0.338, 'grad_norm': 0.951113224029541, 'learning_rate': 9.9375e-05, 'epoch': 1.6027777777777779}
Step 5780: {'loss': 0.3546, 'grad_norm': 0.8148602843284607, 'learning_rate': 9.868055555555555e-05, 'epoch': 1.6055555555555556}
Step 5790: {'loss': 0.3252, 'grad_norm': 0.6878256797790527, 'learning_rate': 9.798611111111111e-05, 'epoch': 1.6083333333333334}
Step 5800: {'loss': 0.2894, 'grad_norm': 0.6317640542984009, 'learning_rate': 9.729166666666667e-05, 'epoch': 1.6111111111111112}
Step 5810: {'loss': 0.3456, 'grad_norm': 0.7968037128448486, 'learning_rate': 9.659722222222223e-05, 'epoch': 1.613888888888889}
Step 5820: {'loss': 0.3368, 'grad_norm': 1.3732051849365234, 'learning_rate': 9.590277777777779e-05, 'epoch': 1.6166666666666667}
Step 5830: {'loss': 0.381, 'grad_norm': 1.6185022592544556, 'learning_rate': 9.520833333333333e-05, 'epoch': 1.6194444444444445}
Step 5840: {'loss': 0.3569, 'grad_norm': 0.871313750743866, 'learning_rate': 9.451388888888888e-05, 'epoch': 1.6222222222222222}
Step 5850: {'loss': 0.3442, 'grad_norm': 0.8805769085884094, 'learning_rate': 9.381944444444444e-05, 'epoch': 1.625}
Step 5860: {'loss': 0.4037, 'grad_norm': 1.0837019681930542, 'learning_rate': 9.3125e-05, 'epoch': 1.6277777777777778}
Step 5870: {'loss': 0.3879, 'grad_norm': 0.9946355223655701, 'learning_rate': 9.243055555555556e-05, 'epoch': 1.6305555555555555}
Step 5880: {'loss': 0.3187, 'grad_norm': 0.9012365341186523, 'learning_rate': 9.173611111111112e-05, 'epoch': 1.6333333333333333}
Step 5890: {'loss': 0.3828, 'grad_norm': 0.9754501581192017, 'learning_rate': 9.104166666666668e-05, 'epoch': 1.636111111111111}
Step 5900: {'loss': 0.3184, 'grad_norm': 0.8883254528045654, 'learning_rate': 9.034722222222223e-05, 'epoch': 1.6388888888888888}
Step 5910: {'loss': 0.3504, 'grad_norm': 0.8601677417755127, 'learning_rate': 8.965277777777777e-05, 'epoch': 1.6416666666666666}
Step 5920: {'loss': 0.3674, 'grad_norm': 0.7308903336524963, 'learning_rate': 8.895833333333333e-05, 'epoch': 1.6444444444444444}
Step 5930: {'loss': 0.3681, 'grad_norm': 0.7875720858573914, 'learning_rate': 8.826388888888889e-05, 'epoch': 1.6472222222222221}
Step 5940: {'loss': 0.334, 'grad_norm': 0.8521994948387146, 'learning_rate': 8.756944444444445e-05, 'epoch': 1.65}
Step 5950: {'loss': 0.3305, 'grad_norm': 0.8877090215682983, 'learning_rate': 8.6875e-05, 'epoch': 1.6527777777777777}
Step 5960: {'loss': 0.3927, 'grad_norm': 0.8691948056221008, 'learning_rate': 8.618055555555556e-05, 'epoch': 1.6555555555555554}
Step 5970: {'loss': 0.3468, 'grad_norm': 0.9388868808746338, 'learning_rate': 8.548611111111112e-05, 'epoch': 1.6583333333333332}
Step 5980: {'loss': 0.3139, 'grad_norm': 0.7326630353927612, 'learning_rate': 8.479166666666666e-05, 'epoch': 1.661111111111111}
Step 5990: {'loss': 0.3962, 'grad_norm': 1.3989348411560059, 'learning_rate': 8.409722222222222e-05, 'epoch': 1.6638888888888888}
Step 6000: {'loss': 0.3082, 'grad_norm': 0.6922260522842407, 'learning_rate': 8.340277777777778e-05, 'epoch': 1.6666666666666665}
Step 6010: {'loss': 0.3346, 'grad_norm': 1.0523561239242554, 'learning_rate': 8.270833333333333e-05, 'epoch': 1.6694444444444443}
Step 6020: {'loss': 0.3717, 'grad_norm': 1.172257423400879, 'learning_rate': 8.201388888888889e-05, 'epoch': 1.6722222222222223}
Step 6030: {'loss': 0.3609, 'grad_norm': 0.7818097472190857, 'learning_rate': 8.131944444444445e-05, 'epoch': 1.675}
Step 6040: {'loss': 0.3266, 'grad_norm': 0.8746528029441833, 'learning_rate': 8.062500000000001e-05, 'epoch': 1.6777777777777778}
Step 6050: {'loss': 0.3684, 'grad_norm': 0.8038381934165955, 'learning_rate': 7.993055555555557e-05, 'epoch': 1.6805555555555556}
Step 6060: {'loss': 0.3594, 'grad_norm': 1.1581289768218994, 'learning_rate': 7.92361111111111e-05, 'epoch': 1.6833333333333333}
Step 6070: {'loss': 0.3693, 'grad_norm': 0.7551894187927246, 'learning_rate': 7.854166666666666e-05, 'epoch': 1.6861111111111111}
Step 6080: {'loss': 0.3478, 'grad_norm': 0.6596279144287109, 'learning_rate': 7.784722222222222e-05, 'epoch': 1.6888888888888889}
Step 6090: {'loss': 0.3599, 'grad_norm': 0.6607614159584045, 'learning_rate': 7.715277777777778e-05, 'epoch': 1.6916666666666667}
Step 6100: {'loss': 0.337, 'grad_norm': 1.047293782234192, 'learning_rate': 7.645833333333334e-05, 'epoch': 1.6944444444444444}
Step 6110: {'loss': 0.3712, 'grad_norm': 1.5463846921920776, 'learning_rate': 7.57638888888889e-05, 'epoch': 1.6972222222222222}
Step 6120: {'loss': 0.3047, 'grad_norm': 0.8545289039611816, 'learning_rate': 7.506944444444445e-05, 'epoch': 1.7}
Step 6130: {'loss': 0.4002, 'grad_norm': 1.2308149337768555, 'learning_rate': 7.437499999999999e-05, 'epoch': 1.7027777777777777}
Step 6140: {'loss': 0.304, 'grad_norm': 0.875641405582428, 'learning_rate': 7.368055555555555e-05, 'epoch': 1.7055555555555557}
Step 6150: {'loss': 0.4342, 'grad_norm': 1.0619789361953735, 'learning_rate': 7.298611111111111e-05, 'epoch': 1.7083333333333335}
Step 6160: {'loss': 0.3922, 'grad_norm': 1.0468673706054688, 'learning_rate': 7.229166666666667e-05, 'epoch': 1.7111111111111112}
Step 6170: {'loss': 0.3621, 'grad_norm': 1.2391852140426636, 'learning_rate': 7.159722222222223e-05, 'epoch': 1.713888888888889}
Step 6180: {'loss': 0.3405, 'grad_norm': 0.6305778622627258, 'learning_rate': 7.090277777777778e-05, 'epoch': 1.7166666666666668}
Step 6190: {'loss': 0.3745, 'grad_norm': 1.0400415658950806, 'learning_rate': 7.020833333333334e-05, 'epoch': 1.7194444444444446}
Step 6200: {'loss': 0.3445, 'grad_norm': 0.996645450592041, 'learning_rate': 6.95138888888889e-05, 'epoch': 1.7222222222222223}
Step 6210: {'loss': 0.3676, 'grad_norm': 0.6342930793762207, 'learning_rate': 6.881944444444444e-05, 'epoch': 1.725}
Step 6220: {'loss': 0.3592, 'grad_norm': 0.9954935312271118, 'learning_rate': 6.8125e-05, 'epoch': 1.7277777777777779}
Step 6230: {'loss': 0.3911, 'grad_norm': 0.8731232285499573, 'learning_rate': 6.743055555555555e-05, 'epoch': 1.7305555555555556}
Step 6240: {'loss': 0.3534, 'grad_norm': 0.6985790729522705, 'learning_rate': 6.673611111111111e-05, 'epoch': 1.7333333333333334}
Step 6250: {'loss': 0.3548, 'grad_norm': 0.9659647345542908, 'learning_rate': 6.604166666666667e-05, 'epoch': 1.7361111111111112}
Step 6260: {'loss': 0.3315, 'grad_norm': 0.787177324295044, 'learning_rate': 6.534722222222223e-05, 'epoch': 1.738888888888889}
Step 6270: {'loss': 0.3546, 'grad_norm': 1.2728614807128906, 'learning_rate': 6.465277777777779e-05, 'epoch': 1.7416666666666667}
Step 6280: {'loss': 0.3259, 'grad_norm': 0.7969858646392822, 'learning_rate': 6.395833333333333e-05, 'epoch': 1.7444444444444445}
Step 6290: {'loss': 0.3959, 'grad_norm': 0.9237498044967651, 'learning_rate': 6.326388888888888e-05, 'epoch': 1.7472222222222222}
Step 6300: {'loss': 0.3154, 'grad_norm': 0.9005680680274963, 'learning_rate': 6.256944444444444e-05, 'epoch': 1.75}
Step 6310: {'loss': 0.298, 'grad_norm': 0.8617581129074097, 'learning_rate': 6.1875e-05, 'epoch': 1.7527777777777778}
Step 6320: {'loss': 0.3846, 'grad_norm': 1.0344029664993286, 'learning_rate': 6.118055555555556e-05, 'epoch': 1.7555555555555555}
Step 6330: {'loss': 0.3919, 'grad_norm': 0.9886870384216309, 'learning_rate': 6.048611111111111e-05, 'epoch': 1.7583333333333333}
Step 6340: {'loss': 0.3343, 'grad_norm': 0.8574046492576599, 'learning_rate': 5.9791666666666665e-05, 'epoch': 1.761111111111111}
Step 6350: {'loss': 0.3681, 'grad_norm': 0.9377538561820984, 'learning_rate': 5.9097222222222225e-05, 'epoch': 1.7638888888888888}
Step 6360: {'loss': 0.4495, 'grad_norm': 1.1367921829223633, 'learning_rate': 5.840277777777778e-05, 'epoch': 1.7666666666666666}
Step 6370: {'loss': 0.3626, 'grad_norm': 1.1414728164672852, 'learning_rate': 5.770833333333334e-05, 'epoch': 1.7694444444444444}
Step 6380: {'loss': 0.3435, 'grad_norm': 1.0849310159683228, 'learning_rate': 5.701388888888889e-05, 'epoch': 1.7722222222222221}
Step 6390: {'loss': 0.3787, 'grad_norm': 0.7493611574172974, 'learning_rate': 5.6319444444444444e-05, 'epoch': 1.775}
Step 6400: {'loss': 0.3295, 'grad_norm': 0.7983394861221313, 'learning_rate': 5.5625000000000004e-05, 'epoch': 1.7777777777777777}
Step 6410: {'loss': 0.3729, 'grad_norm': 0.8783614039421082, 'learning_rate': 5.493055555555556e-05, 'epoch': 1.7805555555555554}
Step 6420: {'loss': 0.3448, 'grad_norm': 0.5527877807617188, 'learning_rate': 5.423611111111111e-05, 'epoch': 1.7833333333333332}
Step 6430: {'loss': 0.3081, 'grad_norm': 1.0874789953231812, 'learning_rate': 5.354166666666667e-05, 'epoch': 1.786111111111111}
Step 6440: {'loss': 0.3965, 'grad_norm': 1.2598724365234375, 'learning_rate': 5.284722222222222e-05, 'epoch': 1.7888888888888888}
Step 6450: {'loss': 0.2987, 'grad_norm': 0.7261642813682556, 'learning_rate': 5.2152777777777775e-05, 'epoch': 1.7916666666666665}
Step 6460: {'loss': 0.3506, 'grad_norm': 1.1620192527770996, 'learning_rate': 5.1458333333333335e-05, 'epoch': 1.7944444444444443}
Step 6470: {'loss': 0.3367, 'grad_norm': 1.0198750495910645, 'learning_rate': 5.076388888888889e-05, 'epoch': 1.7972222222222223}
Step 6480: {'loss': 0.3424, 'grad_norm': 0.9107492566108704, 'learning_rate': 5.006944444444445e-05, 'epoch': 1.8}
Step 6490: {'loss': 0.3228, 'grad_norm': 1.257134199142456, 'learning_rate': 4.9375e-05, 'epoch': 1.8027777777777778}
Step 6500: {'loss': 0.317, 'grad_norm': 0.879673182964325, 'learning_rate': 4.8680555555555554e-05, 'epoch': 1.8055555555555556}
Step 6510: {'loss': 0.3512, 'grad_norm': 0.8475958704948425, 'learning_rate': 4.7986111111111113e-05, 'epoch': 1.8083333333333333}
Step 6520: {'loss': 0.3222, 'grad_norm': 0.8200752139091492, 'learning_rate': 4.729166666666667e-05, 'epoch': 1.8111111111111111}
Step 6530: {'loss': 0.351, 'grad_norm': 1.0571705102920532, 'learning_rate': 4.659722222222222e-05, 'epoch': 1.8138888888888889}
Step 6540: {'loss': 0.341, 'grad_norm': 0.8119821548461914, 'learning_rate': 4.590277777777778e-05, 'epoch': 1.8166666666666667}
Step 6550: {'loss': 0.4091, 'grad_norm': 0.7567364573478699, 'learning_rate': 4.520833333333334e-05, 'epoch': 1.8194444444444444}
Step 6560: {'loss': 0.3206, 'grad_norm': 0.9558236598968506, 'learning_rate': 4.4513888888888885e-05, 'epoch': 1.8222222222222222}
Step 6570: {'loss': 0.3357, 'grad_norm': 0.971173107624054, 'learning_rate': 4.3819444444444445e-05, 'epoch': 1.825}
Step 6580: {'loss': 0.3738, 'grad_norm': 0.7757169008255005, 'learning_rate': 4.3125e-05, 'epoch': 1.8277777777777777}
Step 6590: {'loss': 0.3499, 'grad_norm': 0.834693968296051, 'learning_rate': 4.243055555555556e-05, 'epoch': 1.8305555555555557}
Step 6600: {'loss': 0.3578, 'grad_norm': 0.5786476135253906, 'learning_rate': 4.173611111111111e-05, 'epoch': 1.8333333333333335}
Step 6610: {'loss': 0.3492, 'grad_norm': 0.9699791669845581, 'learning_rate': 4.1041666666666664e-05, 'epoch': 1.8361111111111112}
Step 6620: {'loss': 0.353, 'grad_norm': 0.830859363079071, 'learning_rate': 4.0347222222222223e-05, 'epoch': 1.838888888888889}
Step 6630: {'loss': 0.3258, 'grad_norm': 0.9570367932319641, 'learning_rate': 3.965277777777778e-05, 'epoch': 1.8416666666666668}
Step 6640: {'loss': 0.3426, 'grad_norm': 0.9664894938468933, 'learning_rate': 3.895833333333333e-05, 'epoch': 1.8444444444444446}
Step 6650: {'loss': 0.3609, 'grad_norm': 0.8651493787765503, 'learning_rate': 3.826388888888889e-05, 'epoch': 1.8472222222222223}
Step 6660: {'loss': 0.3462, 'grad_norm': 0.8625381588935852, 'learning_rate': 3.756944444444445e-05, 'epoch': 1.85}
Step 6670: {'loss': 0.3487, 'grad_norm': 1.152541160583496, 'learning_rate': 3.6875e-05, 'epoch': 1.8527777777777779}
Step 6680: {'loss': 0.3189, 'grad_norm': 0.7504462003707886, 'learning_rate': 3.6180555555555555e-05, 'epoch': 1.8555555555555556}
Step 6690: {'loss': 0.3723, 'grad_norm': 0.6049381494522095, 'learning_rate': 3.5486111111111115e-05, 'epoch': 1.8583333333333334}
Step 6700: {'loss': 0.3569, 'grad_norm': 0.9830672144889832, 'learning_rate': 3.479166666666667e-05, 'epoch': 1.8611111111111112}
Step 6710: {'loss': 0.3777, 'grad_norm': 1.0116746425628662, 'learning_rate': 3.409722222222222e-05, 'epoch': 1.863888888888889}
Step 6720: {'loss': 0.3712, 'grad_norm': 0.8378827571868896, 'learning_rate': 3.340277777777778e-05, 'epoch': 1.8666666666666667}
Step 6730: {'loss': 0.3646, 'grad_norm': 0.7129592895507812, 'learning_rate': 3.270833333333333e-05, 'epoch': 1.8694444444444445}
Step 6740: {'loss': 0.3198, 'grad_norm': 0.7492555975914001, 'learning_rate': 3.201388888888889e-05, 'epoch': 1.8722222222222222}
Step 6750: {'loss': 0.322, 'grad_norm': 0.5740299820899963, 'learning_rate': 3.131944444444444e-05, 'epoch': 1.875}
Step 6760: {'loss': 0.3339, 'grad_norm': 0.7519896626472473, 'learning_rate': 3.0625e-05, 'epoch': 1.8777777777777778}
Step 6770: {'loss': 0.3507, 'grad_norm': 0.9321101903915405, 'learning_rate': 2.9930555555555555e-05, 'epoch': 1.8805555555555555}
Step 6780: {'loss': 0.3241, 'grad_norm': 0.6216241717338562, 'learning_rate': 2.9236111111111112e-05, 'epoch': 1.8833333333333333}
Step 6790: {'loss': 0.427, 'grad_norm': 0.9473243355751038, 'learning_rate': 2.8541666666666668e-05, 'epoch': 1.886111111111111}
Step 6800: {'loss': 0.3694, 'grad_norm': 0.6644020676612854, 'learning_rate': 2.784722222222222e-05, 'epoch': 1.8888888888888888}
Step 6810: {'loss': 0.3376, 'grad_norm': 0.7716968655586243, 'learning_rate': 2.715277777777778e-05, 'epoch': 1.8916666666666666}
Step 6820: {'loss': 0.3327, 'grad_norm': 0.462411105632782, 'learning_rate': 2.6458333333333334e-05, 'epoch': 1.8944444444444444}
Step 6830: {'loss': 0.3299, 'grad_norm': 1.0546717643737793, 'learning_rate': 2.576388888888889e-05, 'epoch': 1.8972222222222221}
Step 6840: {'loss': 0.3269, 'grad_norm': 0.7580118775367737, 'learning_rate': 2.5069444444444443e-05, 'epoch': 1.9}
Step 6850: {'loss': 0.3413, 'grad_norm': 1.0205200910568237, 'learning_rate': 2.4375000000000003e-05, 'epoch': 1.9027777777777777}
Step 6860: {'loss': 0.3283, 'grad_norm': 0.8733226656913757, 'learning_rate': 2.3680555555555556e-05, 'epoch': 1.9055555555555554}
Step 6870: {'loss': 0.3572, 'grad_norm': 1.0683590173721313, 'learning_rate': 2.298611111111111e-05, 'epoch': 1.9083333333333332}
Step 6880: {'loss': 0.3141, 'grad_norm': 0.7762312293052673, 'learning_rate': 2.229166666666667e-05, 'epoch': 1.911111111111111}
Step 6890: {'loss': 0.3567, 'grad_norm': 1.0490330457687378, 'learning_rate': 2.159722222222222e-05, 'epoch': 1.9138888888888888}
Step 6900: {'loss': 0.3722, 'grad_norm': 0.6187660694122314, 'learning_rate': 2.0902777777777778e-05, 'epoch': 1.9166666666666665}
Step 6910: {'loss': 0.3206, 'grad_norm': 1.174342155456543, 'learning_rate': 2.020833333333333e-05, 'epoch': 1.9194444444444443}
Step 6920: {'loss': 0.3653, 'grad_norm': 0.9188172817230225, 'learning_rate': 1.951388888888889e-05, 'epoch': 1.9222222222222223}
Step 6930: {'loss': 0.3097, 'grad_norm': 0.9035581946372986, 'learning_rate': 1.8819444444444444e-05, 'epoch': 1.925}
Step 6940: {'loss': 0.3475, 'grad_norm': 0.977015495300293, 'learning_rate': 1.8125e-05, 'epoch': 1.9277777777777778}
Step 6950: {'loss': 0.2932, 'grad_norm': 0.7231382727622986, 'learning_rate': 1.7430555555555556e-05, 'epoch': 1.9305555555555556}
Step 6960: {'loss': 0.4172, 'grad_norm': 1.062969446182251, 'learning_rate': 1.6736111111111113e-05, 'epoch': 1.9333333333333333}
Step 6970: {'loss': 0.3376, 'grad_norm': 0.6699119806289673, 'learning_rate': 1.6041666666666666e-05, 'epoch': 1.9361111111111111}
Step 6980: {'loss': 0.3776, 'grad_norm': 0.9185771346092224, 'learning_rate': 1.5347222222222222e-05, 'epoch': 1.9388888888888889}
Step 6990: {'loss': 0.344, 'grad_norm': 0.956969141960144, 'learning_rate': 1.4652777777777779e-05, 'epoch': 1.9416666666666667}
Step 7000: {'loss': 0.3736, 'grad_norm': 0.6754063963890076, 'learning_rate': 1.3958333333333333e-05, 'epoch': 1.9444444444444444}
Step 7010: {'loss': 0.3821, 'grad_norm': 0.767859697341919, 'learning_rate': 1.326388888888889e-05, 'epoch': 1.9472222222222222}
Step 7020: {'loss': 0.3848, 'grad_norm': 0.5639698505401611, 'learning_rate': 1.2569444444444444e-05, 'epoch': 1.95}
Step 7030: {'loss': 0.3549, 'grad_norm': 1.5275342464447021, 'learning_rate': 1.1875e-05, 'epoch': 1.9527777777777777}
Step 7040: {'loss': 0.4047, 'grad_norm': 0.9424014091491699, 'learning_rate': 1.1180555555555555e-05, 'epoch': 1.9555555555555557}
Step 7050: {'loss': 0.3577, 'grad_norm': 1.0426874160766602, 'learning_rate': 1.0486111111111112e-05, 'epoch': 1.9583333333333335}
Step 7060: {'loss': 0.3443, 'grad_norm': 0.7226263880729675, 'learning_rate': 9.791666666666668e-06, 'epoch': 1.9611111111111112}
Step 7070: {'loss': 0.3035, 'grad_norm': 0.8320866227149963, 'learning_rate': 9.097222222222223e-06, 'epoch': 1.963888888888889}
Step 7080: {'loss': 0.3792, 'grad_norm': 0.9633474349975586, 'learning_rate': 8.402777777777779e-06, 'epoch': 1.9666666666666668}
Step 7090: {'loss': 0.3456, 'grad_norm': 0.9993703365325928, 'learning_rate': 7.708333333333334e-06, 'epoch': 1.9694444444444446}
Step 7100: {'loss': 0.3789, 'grad_norm': 0.7223721146583557, 'learning_rate': 7.013888888888889e-06, 'epoch': 1.9722222222222223}
Step 7110: {'loss': 0.3475, 'grad_norm': 0.9169549345970154, 'learning_rate': 6.319444444444445e-06, 'epoch': 1.975}
Step 7120: {'loss': 0.3302, 'grad_norm': 0.7624462842941284, 'learning_rate': 5.6249999999999995e-06, 'epoch': 1.9777777777777779}
Step 7130: {'loss': 0.3351, 'grad_norm': 0.6372274160385132, 'learning_rate': 4.930555555555555e-06, 'epoch': 1.9805555555555556}
Step 7140: {'loss': 0.2719, 'grad_norm': 0.7271193265914917, 'learning_rate': 4.236111111111111e-06, 'epoch': 1.9833333333333334}
Step 7150: {'loss': 0.3665, 'grad_norm': 0.8632153272628784, 'learning_rate': 3.5416666666666665e-06, 'epoch': 1.9861111111111112}
Step 7160: {'loss': 0.365, 'grad_norm': 0.730496346950531, 'learning_rate': 2.8472222222222224e-06, 'epoch': 1.988888888888889}
Step 7170: {'loss': 0.336, 'grad_norm': 0.9743202924728394, 'learning_rate': 2.152777777777778e-06, 'epoch': 1.9916666666666667}
Step 7180: {'loss': 0.3122, 'grad_norm': 0.7336352467536926, 'learning_rate': 1.4583333333333335e-06, 'epoch': 1.9944444444444445}
Step 7190: {'loss': 0.3879, 'grad_norm': 1.1854708194732666, 'learning_rate': 7.638888888888889e-07, 'epoch': 1.9972222222222222}
Step 7200: {'loss': 0.3287, 'grad_norm': 0.8962941765785217, 'learning_rate': 6.944444444444444e-08, 'epoch': 2.0}
Step 7200: {'train_runtime': 1040.7457, 'train_samples_per_second': 13.836, 'train_steps_per_second': 6.918, 'total_flos': 1.6759064382214963e+17, 'train_loss': 0.37178189446528753, 'epoch': 2.0}
