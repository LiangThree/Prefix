Step 10: {'loss': 1.4176, 'learning_rate': 0.0009981481481481482, 'epoch': 0.01}
Step 20: {'loss': 1.018, 'learning_rate': 0.0009962962962962963, 'epoch': 0.01}
Step 30: {'loss': 0.8651, 'learning_rate': 0.0009944444444444445, 'epoch': 0.02}
Step 40: {'loss': 0.8424, 'learning_rate': 0.0009925925925925927, 'epoch': 0.02}
Step 50: {'loss': 0.7857, 'learning_rate': 0.0009907407407407408, 'epoch': 0.03}
Step 60: {'loss': 0.7436, 'learning_rate': 0.000988888888888889, 'epoch': 0.03}
Step 70: {'loss': 0.7454, 'learning_rate': 0.0009870370370370371, 'epoch': 0.04}
Step 80: {'loss': 0.7181, 'learning_rate': 0.000985185185185185, 'epoch': 0.04}
Step 90: {'loss': 0.7187, 'learning_rate': 0.0009833333333333332, 'epoch': 0.05}
Step 100: {'loss': 0.7347, 'learning_rate': 0.0009814814814814816, 'epoch': 0.06}
Step 110: {'loss': 0.7391, 'learning_rate': 0.0009796296296296296, 'epoch': 0.06}
Step 120: {'loss': 0.7445, 'learning_rate': 0.0009777777777777777, 'epoch': 0.07}
Step 130: {'loss': 0.724, 'learning_rate': 0.000975925925925926, 'epoch': 0.07}
Step 140: {'loss': 0.7042, 'learning_rate': 0.0009740740740740741, 'epoch': 0.08}
Step 150: {'loss': 0.6991, 'learning_rate': 0.0009722222222222222, 'epoch': 0.08}
Step 160: {'loss': 0.6549, 'learning_rate': 0.0009703703703703704, 'epoch': 0.09}
Step 170: {'loss': 0.6757, 'learning_rate': 0.0009685185185185186, 'epoch': 0.09}
Step 180: {'loss': 0.6369, 'learning_rate': 0.0009666666666666667, 'epoch': 0.1}
Step 190: {'loss': 0.7107, 'learning_rate': 0.0009648148148148148, 'epoch': 0.11}
Step 200: {'loss': 0.6783, 'learning_rate': 0.0009629629629629629, 'epoch': 0.11}
Step 210: {'loss': 0.7178, 'learning_rate': 0.0009611111111111112, 'epoch': 0.12}
Step 220: {'loss': 0.6643, 'learning_rate': 0.0009592592592592593, 'epoch': 0.12}
Step 230: {'loss': 0.6737, 'learning_rate': 0.0009574074074074074, 'epoch': 0.13}
Step 240: {'loss': 0.7048, 'learning_rate': 0.0009555555555555556, 'epoch': 0.13}
Step 250: {'loss': 0.6695, 'learning_rate': 0.0009537037037037038, 'epoch': 0.14}
Step 260: {'loss': 0.6794, 'learning_rate': 0.0009518518518518518, 'epoch': 0.14}
Step 270: {'loss': 0.7282, 'learning_rate': 0.00095, 'epoch': 0.15}
Step 280: {'loss': 0.6901, 'learning_rate': 0.0009481481481481482, 'epoch': 0.16}
Step 290: {'loss': 0.6912, 'learning_rate': 0.0009462962962962963, 'epoch': 0.16}
Step 300: {'loss': 0.6886, 'learning_rate': 0.0009444444444444445, 'epoch': 0.17}
Step 310: {'loss': 0.6994, 'learning_rate': 0.0009425925925925925, 'epoch': 0.17}
Step 320: {'loss': 0.7023, 'learning_rate': 0.0009407407407407408, 'epoch': 0.18}
Step 330: {'loss': 0.7008, 'learning_rate': 0.000938888888888889, 'epoch': 0.18}
Step 340: {'loss': 0.7192, 'learning_rate': 0.000937037037037037, 'epoch': 0.19}
Step 350: {'loss': 0.7394, 'learning_rate': 0.0009351851851851853, 'epoch': 0.19}
Step 360: {'loss': 0.6864, 'learning_rate': 0.0009333333333333333, 'epoch': 0.2}
Step 370: {'loss': 0.6736, 'learning_rate': 0.0009314814814814815, 'epoch': 0.21}
Step 380: {'loss': 0.7497, 'learning_rate': 0.0009296296296296296, 'epoch': 0.21}
Step 390: {'loss': 0.649, 'learning_rate': 0.0009277777777777778, 'epoch': 0.22}
Step 400: {'loss': 0.6443, 'learning_rate': 0.000925925925925926, 'epoch': 0.22}
Step 410: {'loss': 0.6716, 'learning_rate': 0.0009240740740740741, 'epoch': 0.23}
Step 420: {'loss': 0.6775, 'learning_rate': 0.0009222222222222223, 'epoch': 0.23}
Step 430: {'loss': 0.6711, 'learning_rate': 0.0009203703703703704, 'epoch': 0.24}
Step 440: {'loss': 0.6227, 'learning_rate': 0.0009185185185185185, 'epoch': 0.24}
Step 450: {'loss': 0.6705, 'learning_rate': 0.0009166666666666666, 'epoch': 0.25}
Step 460: {'loss': 0.6806, 'learning_rate': 0.0009148148148148149, 'epoch': 0.26}
Step 470: {'loss': 0.6969, 'learning_rate': 0.000912962962962963, 'epoch': 0.26}
Step 480: {'loss': 0.6873, 'learning_rate': 0.0009111111111111111, 'epoch': 0.27}
Step 490: {'loss': 0.7207, 'learning_rate': 0.0009092592592592592, 'epoch': 0.27}
Step 500: {'loss': 0.7226, 'learning_rate': 0.0009074074074074074, 'epoch': 0.28}
Step 510: {'loss': 0.7054, 'learning_rate': 0.0009055555555555556, 'epoch': 0.28}
Step 520: {'loss': 0.6932, 'learning_rate': 0.0009037037037037037, 'epoch': 0.29}
Step 530: {'loss': 0.6735, 'learning_rate': 0.0009018518518518519, 'epoch': 0.29}
Step 540: {'loss': 0.7071, 'learning_rate': 0.0009000000000000001, 'epoch': 0.3}
Step 550: {'loss': 0.7405, 'learning_rate': 0.0008981481481481481, 'epoch': 0.31}
Step 560: {'loss': 0.7496, 'learning_rate': 0.0008962962962962963, 'epoch': 0.31}
Step 570: {'loss': 0.6775, 'learning_rate': 0.0008944444444444445, 'epoch': 0.32}
Step 580: {'loss': 0.6758, 'learning_rate': 0.0008925925925925926, 'epoch': 0.32}
Step 590: {'loss': 0.675, 'learning_rate': 0.0008907407407407408, 'epoch': 0.33}
Step 600: {'loss': 0.6684, 'learning_rate': 0.0008888888888888888, 'epoch': 0.33}
Step 610: {'loss': 0.6811, 'learning_rate': 0.0008870370370370371, 'epoch': 0.34}
Step 620: {'loss': 0.7125, 'learning_rate': 0.0008851851851851853, 'epoch': 0.34}
Step 630: {'loss': 0.685, 'learning_rate': 0.0008833333333333333, 'epoch': 0.35}
Step 640: {'loss': 0.6794, 'learning_rate': 0.0008814814814814816, 'epoch': 0.36}
Step 650: {'loss': 0.6657, 'learning_rate': 0.0008796296296296296, 'epoch': 0.36}
Step 660: {'loss': 0.7132, 'learning_rate': 0.0008777777777777778, 'epoch': 0.37}
Step 670: {'loss': 0.6983, 'learning_rate': 0.0008759259259259259, 'epoch': 0.37}
Step 680: {'loss': 0.7072, 'learning_rate': 0.0008740740740740741, 'epoch': 0.38}
Step 690: {'loss': 0.6873, 'learning_rate': 0.0008722222222222223, 'epoch': 0.38}
Step 700: {'loss': 0.7064, 'learning_rate': 0.0008703703703703704, 'epoch': 0.39}
Step 710: {'loss': 0.7061, 'learning_rate': 0.0008685185185185185, 'epoch': 0.39}
Step 720: {'loss': 0.7051, 'learning_rate': 0.0008666666666666667, 'epoch': 0.4}
Step 730: {'loss': 0.6933, 'learning_rate': 0.0008648148148148148, 'epoch': 0.41}
Step 740: {'loss': 0.6516, 'learning_rate': 0.0008629629629629629, 'epoch': 0.41}
Step 750: {'loss': 0.7441, 'learning_rate': 0.0008611111111111112, 'epoch': 0.42}
Step 760: {'loss': 0.6352, 'learning_rate': 0.0008592592592592593, 'epoch': 0.42}
Step 770: {'loss': 0.645, 'learning_rate': 0.0008574074074074074, 'epoch': 0.43}
Step 780: {'loss': 0.6437, 'learning_rate': 0.0008555555555555556, 'epoch': 0.43}
Step 790: {'loss': 0.7092, 'learning_rate': 0.0008537037037037037, 'epoch': 0.44}
Step 800: {'loss': 0.7178, 'learning_rate': 0.0008518518518518519, 'epoch': 0.44}
Step 810: {'loss': 0.6753, 'learning_rate': 0.00085, 'epoch': 0.45}
Step 820: {'loss': 0.7075, 'learning_rate': 0.0008481481481481481, 'epoch': 0.46}
Step 830: {'loss': 0.679, 'learning_rate': 0.0008462962962962964, 'epoch': 0.46}
Step 840: {'loss': 0.7242, 'learning_rate': 0.0008444444444444444, 'epoch': 0.47}
Step 850: {'loss': 0.6945, 'learning_rate': 0.0008425925925925926, 'epoch': 0.47}
Step 860: {'loss': 0.6648, 'learning_rate': 0.0008407407407407409, 'epoch': 0.48}
Step 870: {'loss': 0.6795, 'learning_rate': 0.0008388888888888889, 'epoch': 0.48}
Step 880: {'loss': 0.6907, 'learning_rate': 0.0008370370370370371, 'epoch': 0.49}
Step 890: {'loss': 0.7056, 'learning_rate': 0.0008351851851851851, 'epoch': 0.49}
Step 900: {'loss': 0.6073, 'learning_rate': 0.0008333333333333334, 'epoch': 0.5}
Step 910: {'loss': 0.6448, 'learning_rate': 0.0008314814814814815, 'epoch': 0.51}
Step 920: {'loss': 0.7027, 'learning_rate': 0.0008296296296296296, 'epoch': 0.51}
Step 930: {'loss': 0.7255, 'learning_rate': 0.0008277777777777778, 'epoch': 0.52}
Step 940: {'loss': 0.6525, 'learning_rate': 0.0008259259259259259, 'epoch': 0.52}
Step 950: {'loss': 0.6877, 'learning_rate': 0.0008240740740740741, 'epoch': 0.53}
Step 960: {'loss': 0.6965, 'learning_rate': 0.0008222222222222222, 'epoch': 0.53}
Step 970: {'loss': 0.6883, 'learning_rate': 0.0008203703703703704, 'epoch': 0.54}
Step 980: {'loss': 0.6575, 'learning_rate': 0.0008185185185185186, 'epoch': 0.54}
Step 990: {'loss': 0.6622, 'learning_rate': 0.0008166666666666667, 'epoch': 0.55}
Step 1000: {'loss': 0.6217, 'learning_rate': 0.0008148148148148148, 'epoch': 0.56}
Step 1010: {'loss': 0.7311, 'learning_rate': 0.000812962962962963, 'epoch': 0.56}
Step 1020: {'loss': 0.7184, 'learning_rate': 0.0008111111111111111, 'epoch': 0.57}
Step 1030: {'loss': 0.6555, 'learning_rate': 0.0008092592592592592, 'epoch': 0.57}
Step 1040: {'loss': 0.6878, 'learning_rate': 0.0008074074074074075, 'epoch': 0.58}
Step 1050: {'loss': 0.7119, 'learning_rate': 0.0008055555555555556, 'epoch': 0.58}
Step 1060: {'loss': 0.6358, 'learning_rate': 0.0008037037037037037, 'epoch': 0.59}
Step 1070: {'loss': 0.6601, 'learning_rate': 0.0008018518518518519, 'epoch': 0.59}
Step 1080: {'loss': 0.6982, 'learning_rate': 0.0008, 'epoch': 0.6}
Step 1090: {'loss': 0.6132, 'learning_rate': 0.0007981481481481482, 'epoch': 0.61}
Step 1100: {'loss': 0.7132, 'learning_rate': 0.0007962962962962962, 'epoch': 0.61}
Step 1110: {'loss': 0.6983, 'learning_rate': 0.0007944444444444444, 'epoch': 0.62}
Step 1120: {'loss': 0.6873, 'learning_rate': 0.0007925925925925927, 'epoch': 0.62}
Step 1130: {'loss': 0.6755, 'learning_rate': 0.0007907407407407407, 'epoch': 0.63}
Step 1140: {'loss': 0.6478, 'learning_rate': 0.0007888888888888889, 'epoch': 0.63}
Step 1150: {'loss': 0.6931, 'learning_rate': 0.0007870370370370372, 'epoch': 0.64}
Step 1160: {'loss': 0.6904, 'learning_rate': 0.0007851851851851852, 'epoch': 0.64}
Step 1170: {'loss': 0.6667, 'learning_rate': 0.0007833333333333334, 'epoch': 0.65}
Step 1180: {'loss': 0.697, 'learning_rate': 0.0007814814814814814, 'epoch': 0.66}
Step 1190: {'loss': 0.7085, 'learning_rate': 0.0007796296296296297, 'epoch': 0.66}
Step 1200: {'loss': 0.6595, 'learning_rate': 0.0007777777777777778, 'epoch': 0.67}
Step 1210: {'loss': 0.6803, 'learning_rate': 0.0007759259259259259, 'epoch': 0.67}
Step 1220: {'loss': 0.6701, 'learning_rate': 0.000774074074074074, 'epoch': 0.68}
Step 1230: {'loss': 0.6992, 'learning_rate': 0.0007722222222222223, 'epoch': 0.68}
Step 1240: {'loss': 0.7069, 'learning_rate': 0.0007703703703703704, 'epoch': 0.69}
Step 1250: {'loss': 0.6905, 'learning_rate': 0.0007685185185185185, 'epoch': 0.69}
Step 1260: {'loss': 0.6543, 'learning_rate': 0.0007666666666666667, 'epoch': 0.7}
Step 1270: {'loss': 0.7073, 'learning_rate': 0.0007648148148148148, 'epoch': 0.71}
Step 1280: {'loss': 0.7001, 'learning_rate': 0.000762962962962963, 'epoch': 0.71}
Step 1290: {'loss': 0.6873, 'learning_rate': 0.0007611111111111111, 'epoch': 0.72}
Step 1300: {'loss': 0.6719, 'learning_rate': 0.0007592592592592593, 'epoch': 0.72}
Step 1310: {'loss': 0.675, 'learning_rate': 0.0007574074074074075, 'epoch': 0.73}
Step 1320: {'loss': 0.673, 'learning_rate': 0.0007555555555555555, 'epoch': 0.73}
Step 1330: {'loss': 0.6975, 'learning_rate': 0.0007537037037037037, 'epoch': 0.74}
Step 1340: {'loss': 0.73, 'learning_rate': 0.0007518518518518519, 'epoch': 0.74}
Step 1350: {'loss': 0.6531, 'learning_rate': 0.00075, 'epoch': 0.75}
Step 1360: {'loss': 0.7035, 'learning_rate': 0.0007481481481481482, 'epoch': 0.76}
Step 1370: {'loss': 0.6715, 'learning_rate': 0.0007462962962962963, 'epoch': 0.76}
Step 1380: {'loss': 0.6494, 'learning_rate': 0.0007444444444444445, 'epoch': 0.77}
Step 1390: {'loss': 0.6912, 'learning_rate': 0.0007425925925925925, 'epoch': 0.77}
Step 1400: {'loss': 0.6904, 'learning_rate': 0.0007407407407407407, 'epoch': 0.78}
Step 1410: {'loss': 0.6515, 'learning_rate': 0.000738888888888889, 'epoch': 0.78}
Step 1420: {'loss': 0.6619, 'learning_rate': 0.000737037037037037, 'epoch': 0.79}
Step 1430: {'loss': 0.6962, 'learning_rate': 0.0007351851851851852, 'epoch': 0.79}
Step 1440: {'loss': 0.673, 'learning_rate': 0.0007333333333333333, 'epoch': 0.8}
Step 1450: {'loss': 0.6687, 'learning_rate': 0.0007314814814814815, 'epoch': 0.81}
Step 1460: {'loss': 0.6566, 'learning_rate': 0.0007296296296296297, 'epoch': 0.81}
Step 1470: {'loss': 0.6664, 'learning_rate': 0.0007277777777777777, 'epoch': 0.82}
Step 1480: {'loss': 0.6172, 'learning_rate': 0.000725925925925926, 'epoch': 0.82}
Step 1490: {'loss': 0.6309, 'learning_rate': 0.0007240740740740741, 'epoch': 0.83}
Step 1500: {'loss': 0.6449, 'learning_rate': 0.0007222222222222222, 'epoch': 0.83}
Step 1510: {'loss': 0.65, 'learning_rate': 0.0007203703703703703, 'epoch': 0.84}
Step 1520: {'loss': 0.6896, 'learning_rate': 0.0007185185185185186, 'epoch': 0.84}
Step 1530: {'loss': 0.698, 'learning_rate': 0.0007166666666666667, 'epoch': 0.85}
Step 1540: {'loss': 0.6829, 'learning_rate': 0.0007148148148148148, 'epoch': 0.86}
Step 1550: {'loss': 0.6781, 'learning_rate': 0.0007129629629629629, 'epoch': 0.86}
Step 1560: {'loss': 0.6867, 'learning_rate': 0.0007111111111111111, 'epoch': 0.87}
Step 1570: {'loss': 0.6578, 'learning_rate': 0.0007092592592592593, 'epoch': 0.87}
Step 1580: {'loss': 0.7025, 'learning_rate': 0.0007074074074074074, 'epoch': 0.88}
Step 1590: {'loss': 0.6707, 'learning_rate': 0.0007055555555555556, 'epoch': 0.88}
Step 1600: {'loss': 0.7064, 'learning_rate': 0.0007037037037037038, 'epoch': 0.89}
Step 1610: {'loss': 0.6558, 'learning_rate': 0.0007018518518518518, 'epoch': 0.89}
Step 1620: {'loss': 0.7004, 'learning_rate': 0.0007, 'epoch': 0.9}
Step 1630: {'loss': 0.7005, 'learning_rate': 0.0006981481481481482, 'epoch': 0.91}
Step 1640: {'loss': 0.6345, 'learning_rate': 0.0006962962962962963, 'epoch': 0.91}
Step 1650: {'loss': 0.6785, 'learning_rate': 0.0006944444444444445, 'epoch': 0.92}
Step 1660: {'loss': 0.6819, 'learning_rate': 0.0006925925925925925, 'epoch': 0.92}
Step 1670: {'loss': 0.7091, 'learning_rate': 0.0006907407407407408, 'epoch': 0.93}
Step 1680: {'loss': 0.7292, 'learning_rate': 0.000688888888888889, 'epoch': 0.93}
Step 1690: {'loss': 0.6858, 'learning_rate': 0.000687037037037037, 'epoch': 0.94}
Step 1700: {'loss': 0.6788, 'learning_rate': 0.0006851851851851853, 'epoch': 0.94}
Step 1710: {'loss': 0.6967, 'learning_rate': 0.0006833333333333333, 'epoch': 0.95}
Step 1720: {'loss': 0.6518, 'learning_rate': 0.0006814814814814815, 'epoch': 0.96}
Step 1730: {'loss': 0.6683, 'learning_rate': 0.0006796296296296296, 'epoch': 0.96}
Step 1740: {'loss': 0.7087, 'learning_rate': 0.0006777777777777778, 'epoch': 0.97}
Step 1750: {'loss': 0.6798, 'learning_rate': 0.000675925925925926, 'epoch': 0.97}
Step 1760: {'loss': 0.6579, 'learning_rate': 0.0006740740740740741, 'epoch': 0.98}
Step 1770: {'loss': 0.623, 'learning_rate': 0.0006722222222222223, 'epoch': 0.98}
Step 1780: {'loss': 0.6244, 'learning_rate': 0.0006703703703703704, 'epoch': 0.99}
Step 1790: {'loss': 0.6883, 'learning_rate': 0.0006685185185185185, 'epoch': 0.99}
Step 1800: {'loss': 0.7252, 'learning_rate': 0.0006666666666666666, 'epoch': 1.0}
Step 1800: {'eval_loss': 0.6772933602333069, 'eval_runtime': 69.8339, 'eval_samples_per_second': 25.775, 'eval_steps_per_second': 6.444, 'epoch': 1.0}
Step 1810: {'loss': 0.6231, 'learning_rate': 0.0006648148148148149, 'epoch': 1.01}
Step 1820: {'loss': 0.6507, 'learning_rate': 0.000662962962962963, 'epoch': 1.01}
Step 1830: {'loss': 0.6672, 'learning_rate': 0.0006611111111111111, 'epoch': 1.02}
Step 1840: {'loss': 0.6611, 'learning_rate': 0.0006592592592592592, 'epoch': 1.02}
Step 1850: {'loss': 0.6295, 'learning_rate': 0.0006574074074074074, 'epoch': 1.03}
Step 1860: {'loss': 0.6377, 'learning_rate': 0.0006555555555555556, 'epoch': 1.03}
Step 1870: {'loss': 0.645, 'learning_rate': 0.0006537037037037037, 'epoch': 1.04}
Step 1880: {'loss': 0.6678, 'learning_rate': 0.0006518518518518519, 'epoch': 1.04}
Step 1890: {'loss': 0.6401, 'learning_rate': 0.0006500000000000001, 'epoch': 1.05}
Step 1900: {'loss': 0.7013, 'learning_rate': 0.0006481481481481481, 'epoch': 1.06}
Step 1910: {'loss': 0.6585, 'learning_rate': 0.0006462962962962963, 'epoch': 1.06}
Step 1920: {'loss': 0.6742, 'learning_rate': 0.0006444444444444444, 'epoch': 1.07}
Step 1930: {'loss': 0.6539, 'learning_rate': 0.0006425925925925926, 'epoch': 1.07}
Step 1940: {'loss': 0.6732, 'learning_rate': 0.0006407407407407408, 'epoch': 1.08}
Step 1950: {'loss': 0.6572, 'learning_rate': 0.0006388888888888888, 'epoch': 1.08}
Step 1960: {'loss': 0.6742, 'learning_rate': 0.0006370370370370371, 'epoch': 1.09}
Step 1970: {'loss': 0.7074, 'learning_rate': 0.0006351851851851852, 'epoch': 1.09}
Step 1980: {'loss': 0.673, 'learning_rate': 0.0006333333333333333, 'epoch': 1.1}
Step 1990: {'loss': 0.7135, 'learning_rate': 0.0006314814814814816, 'epoch': 1.11}
Step 2000: {'loss': 0.6225, 'learning_rate': 0.0006296296296296296, 'epoch': 1.11}
Step 2010: {'loss': 0.6749, 'learning_rate': 0.0006277777777777778, 'epoch': 1.12}
Step 2020: {'loss': 0.6424, 'learning_rate': 0.0006259259259259259, 'epoch': 1.12}
Step 2030: {'loss': 0.6838, 'learning_rate': 0.0006240740740740741, 'epoch': 1.13}
Step 2040: {'loss': 0.6594, 'learning_rate': 0.0006222222222222223, 'epoch': 1.13}
Step 2050: {'loss': 0.6535, 'learning_rate': 0.0006203703703703704, 'epoch': 1.14}
Step 2060: {'loss': 0.6769, 'learning_rate': 0.0006185185185185185, 'epoch': 1.14}
Step 2070: {'loss': 0.655, 'learning_rate': 0.0006166666666666667, 'epoch': 1.15}
Step 2080: {'loss': 0.6932, 'learning_rate': 0.0006148148148148148, 'epoch': 1.16}
Step 2090: {'loss': 0.6354, 'learning_rate': 0.0006129629629629629, 'epoch': 1.16}
Step 2100: {'loss': 0.7044, 'learning_rate': 0.0006111111111111112, 'epoch': 1.17}
Step 2110: {'loss': 0.6569, 'learning_rate': 0.0006092592592592593, 'epoch': 1.17}
Step 2120: {'loss': 0.6843, 'learning_rate': 0.0006074074074074074, 'epoch': 1.18}
Step 2130: {'loss': 0.6184, 'learning_rate': 0.0006055555555555556, 'epoch': 1.18}
Step 2140: {'loss': 0.6764, 'learning_rate': 0.0006037037037037037, 'epoch': 1.19}
Step 2150: {'loss': 0.6441, 'learning_rate': 0.0006018518518518519, 'epoch': 1.19}
Step 2160: {'loss': 0.6822, 'learning_rate': 0.0006, 'epoch': 1.2}
Step 2170: {'loss': 0.688, 'learning_rate': 0.0005981481481481481, 'epoch': 1.21}
Step 2180: {'loss': 0.6363, 'learning_rate': 0.0005962962962962964, 'epoch': 1.21}
Step 2190: {'loss': 0.6414, 'learning_rate': 0.0005944444444444444, 'epoch': 1.22}
Step 2200: {'loss': 0.6955, 'learning_rate': 0.0005925925925925926, 'epoch': 1.22}
Step 2210: {'loss': 0.6736, 'learning_rate': 0.0005907407407407409, 'epoch': 1.23}
Step 2220: {'loss': 0.7205, 'learning_rate': 0.0005888888888888889, 'epoch': 1.23}
Step 2230: {'loss': 0.65, 'learning_rate': 0.0005870370370370371, 'epoch': 1.24}
Step 2240: {'loss': 0.6817, 'learning_rate': 0.0005851851851851851, 'epoch': 1.24}
Step 2250: {'loss': 0.625, 'learning_rate': 0.0005833333333333334, 'epoch': 1.25}
Step 2260: {'loss': 0.642, 'learning_rate': 0.0005814814814814815, 'epoch': 1.26}
Step 2270: {'loss': 0.7036, 'learning_rate': 0.0005796296296296296, 'epoch': 1.26}
Step 2280: {'loss': 0.6919, 'learning_rate': 0.0005777777777777778, 'epoch': 1.27}
Step 2290: {'loss': 0.6409, 'learning_rate': 0.0005759259259259259, 'epoch': 1.27}
Step 2300: {'loss': 0.6561, 'learning_rate': 0.0005740740740740741, 'epoch': 1.28}
Step 2310: {'loss': 0.6803, 'learning_rate': 0.0005722222222222222, 'epoch': 1.28}
Step 2320: {'loss': 0.6661, 'learning_rate': 0.0005703703703703704, 'epoch': 1.29}
Step 2330: {'loss': 0.6456, 'learning_rate': 0.0005685185185185185, 'epoch': 1.29}
Step 2340: {'loss': 0.6452, 'learning_rate': 0.0005666666666666667, 'epoch': 1.3}
Step 2350: {'loss': 0.6452, 'learning_rate': 0.0005648148148148148, 'epoch': 1.31}
Step 2360: {'loss': 0.6539, 'learning_rate': 0.000562962962962963, 'epoch': 1.31}
Step 2370: {'loss': 0.6449, 'learning_rate': 0.0005611111111111111, 'epoch': 1.32}
Step 2380: {'loss': 0.67, 'learning_rate': 0.0005592592592592592, 'epoch': 1.32}
Step 2390: {'loss': 0.6936, 'learning_rate': 0.0005574074074074075, 'epoch': 1.33}
Step 2400: {'loss': 0.6783, 'learning_rate': 0.0005555555555555556, 'epoch': 1.33}
Step 2410: {'loss': 0.6451, 'learning_rate': 0.0005537037037037037, 'epoch': 1.34}
Step 2420: {'loss': 0.6744, 'learning_rate': 0.0005518518518518519, 'epoch': 1.34}
Step 2430: {'loss': 0.6866, 'learning_rate': 0.00055, 'epoch': 1.35}
Step 2440: {'loss': 0.6899, 'learning_rate': 0.0005481481481481482, 'epoch': 1.36}
Step 2450: {'loss': 0.6459, 'learning_rate': 0.0005462962962962962, 'epoch': 1.36}
Step 2460: {'loss': 0.6461, 'learning_rate': 0.0005444444444444444, 'epoch': 1.37}
Step 2470: {'loss': 0.6748, 'learning_rate': 0.0005425925925925927, 'epoch': 1.37}
Step 2480: {'loss': 0.6335, 'learning_rate': 0.0005407407407407407, 'epoch': 1.38}
Step 2490: {'loss': 0.6416, 'learning_rate': 0.0005388888888888889, 'epoch': 1.38}
Step 2500: {'loss': 0.6712, 'learning_rate': 0.0005370370370370371, 'epoch': 1.39}
Step 2510: {'loss': 0.6311, 'learning_rate': 0.0005351851851851852, 'epoch': 1.39}
Step 2520: {'loss': 0.7051, 'learning_rate': 0.0005333333333333334, 'epoch': 1.4}
Step 2530: {'loss': 0.7012, 'learning_rate': 0.0005314814814814814, 'epoch': 1.41}
Step 2540: {'loss': 0.6492, 'learning_rate': 0.0005296296296296297, 'epoch': 1.41}
Step 2550: {'loss': 0.7289, 'learning_rate': 0.0005277777777777778, 'epoch': 1.42}
Step 2560: {'loss': 0.6923, 'learning_rate': 0.0005259259259259259, 'epoch': 1.42}
Step 2570: {'loss': 0.651, 'learning_rate': 0.000524074074074074, 'epoch': 1.43}
Step 2580: {'loss': 0.635, 'learning_rate': 0.0005222222222222223, 'epoch': 1.43}
Step 2590: {'loss': 0.6686, 'learning_rate': 0.0005203703703703704, 'epoch': 1.44}
Step 2600: {'loss': 0.6404, 'learning_rate': 0.0005185185185185185, 'epoch': 1.44}
Step 2610: {'loss': 0.6705, 'learning_rate': 0.0005166666666666667, 'epoch': 1.45}
Step 2620: {'loss': 0.675, 'learning_rate': 0.0005148148148148148, 'epoch': 1.46}
Step 2630: {'loss': 0.6867, 'learning_rate': 0.000512962962962963, 'epoch': 1.46}
Step 2640: {'loss': 0.6793, 'learning_rate': 0.0005111111111111111, 'epoch': 1.47}
Step 2650: {'loss': 0.6671, 'learning_rate': 0.0005092592592592593, 'epoch': 1.47}
Step 2660: {'loss': 0.6331, 'learning_rate': 0.0005074074074074075, 'epoch': 1.48}
Step 2670: {'loss': 0.6735, 'learning_rate': 0.0005055555555555555, 'epoch': 1.48}
Step 2680: {'loss': 0.7116, 'learning_rate': 0.0005037037037037037, 'epoch': 1.49}
Step 2690: {'loss': 0.6804, 'learning_rate': 0.0005018518518518519, 'epoch': 1.49}
Step 2700: {'loss': 0.6628, 'learning_rate': 0.0005, 'epoch': 1.5}
Step 2710: {'loss': 0.6414, 'learning_rate': 0.0004981481481481482, 'epoch': 1.51}
Step 2720: {'loss': 0.6802, 'learning_rate': 0.0004962962962962963, 'epoch': 1.51}
Step 2730: {'loss': 0.6569, 'learning_rate': 0.0004944444444444445, 'epoch': 1.52}
Step 2740: {'loss': 0.6752, 'learning_rate': 0.0004925925925925925, 'epoch': 1.52}
Step 2750: {'loss': 0.628, 'learning_rate': 0.0004907407407407408, 'epoch': 1.53}
Step 2760: {'loss': 0.6852, 'learning_rate': 0.0004888888888888889, 'epoch': 1.53}
Step 2770: {'loss': 0.6847, 'learning_rate': 0.00048703703703703707, 'epoch': 1.54}
Step 2780: {'loss': 0.6432, 'learning_rate': 0.0004851851851851852, 'epoch': 1.54}
Step 2790: {'loss': 0.6941, 'learning_rate': 0.00048333333333333334, 'epoch': 1.55}
Step 2800: {'loss': 0.6566, 'learning_rate': 0.00048148148148148144, 'epoch': 1.56}
Step 2810: {'loss': 0.7003, 'learning_rate': 0.00047962962962962965, 'epoch': 1.56}
Step 2820: {'loss': 0.7016, 'learning_rate': 0.0004777777777777778, 'epoch': 1.57}
Step 2830: {'loss': 0.6577, 'learning_rate': 0.0004759259259259259, 'epoch': 1.57}
Step 2840: {'loss': 0.6671, 'learning_rate': 0.0004740740740740741, 'epoch': 1.58}
Step 2850: {'loss': 0.6452, 'learning_rate': 0.00047222222222222224, 'epoch': 1.58}
Step 2860: {'loss': 0.7592, 'learning_rate': 0.0004703703703703704, 'epoch': 1.59}
Step 2870: {'loss': 0.6488, 'learning_rate': 0.0004685185185185185, 'epoch': 1.59}
Step 2880: {'loss': 0.6831, 'learning_rate': 0.00046666666666666666, 'epoch': 1.6}
Step 2890: {'loss': 0.6251, 'learning_rate': 0.0004648148148148148, 'epoch': 1.61}
Step 2900: {'loss': 0.6783, 'learning_rate': 0.000462962962962963, 'epoch': 1.61}
Step 2910: {'loss': 0.6299, 'learning_rate': 0.00046111111111111114, 'epoch': 1.62}
Step 2920: {'loss': 0.6573, 'learning_rate': 0.00045925925925925925, 'epoch': 1.62}
Step 2930: {'loss': 0.6271, 'learning_rate': 0.00045740740740740746, 'epoch': 1.63}
Step 2940: {'loss': 0.6659, 'learning_rate': 0.00045555555555555556, 'epoch': 1.63}
Step 2950: {'loss': 0.6408, 'learning_rate': 0.0004537037037037037, 'epoch': 1.64}
Step 2960: {'loss': 0.6768, 'learning_rate': 0.00045185185185185183, 'epoch': 1.64}
Step 2970: {'loss': 0.6682, 'learning_rate': 0.00045000000000000004, 'epoch': 1.65}
Step 2980: {'loss': 0.6799, 'learning_rate': 0.00044814814814814815, 'epoch': 1.66}
Step 2990: {'loss': 0.7211, 'learning_rate': 0.0004462962962962963, 'epoch': 1.66}
Step 3000: {'loss': 0.696, 'learning_rate': 0.0004444444444444444, 'epoch': 1.67}
Step 3010: {'loss': 0.6687, 'learning_rate': 0.0004425925925925926, 'epoch': 1.67}
Step 3020: {'loss': 0.6426, 'learning_rate': 0.0004407407407407408, 'epoch': 1.68}
Step 3030: {'loss': 0.6489, 'learning_rate': 0.0004388888888888889, 'epoch': 1.68}
Step 3040: {'loss': 0.6674, 'learning_rate': 0.00043703703703703705, 'epoch': 1.69}
Step 3050: {'loss': 0.6662, 'learning_rate': 0.0004351851851851852, 'epoch': 1.69}
Step 3060: {'loss': 0.6816, 'learning_rate': 0.00043333333333333337, 'epoch': 1.7}
Step 3070: {'loss': 0.6878, 'learning_rate': 0.00043148148148148147, 'epoch': 1.71}
Step 3080: {'loss': 0.6393, 'learning_rate': 0.00042962962962962963, 'epoch': 1.71}
Step 3090: {'loss': 0.6506, 'learning_rate': 0.0004277777777777778, 'epoch': 1.72}
Step 3100: {'loss': 0.6632, 'learning_rate': 0.00042592592592592595, 'epoch': 1.72}
Step 3110: {'loss': 0.625, 'learning_rate': 0.00042407407407407406, 'epoch': 1.73}
Step 3120: {'loss': 0.6364, 'learning_rate': 0.0004222222222222222, 'epoch': 1.73}
Step 3130: {'loss': 0.6769, 'learning_rate': 0.00042037037037037043, 'epoch': 1.74}
Step 3140: {'loss': 0.674, 'learning_rate': 0.00041851851851851853, 'epoch': 1.74}
Step 3150: {'loss': 0.6742, 'learning_rate': 0.0004166666666666667, 'epoch': 1.75}
Step 3160: {'loss': 0.6829, 'learning_rate': 0.0004148148148148148, 'epoch': 1.76}
Step 3170: {'loss': 0.6696, 'learning_rate': 0.00041296296296296296, 'epoch': 1.76}
Step 3180: {'loss': 0.7017, 'learning_rate': 0.0004111111111111111, 'epoch': 1.77}
Step 3190: {'loss': 0.6569, 'learning_rate': 0.0004092592592592593, 'epoch': 1.77}
Step 3200: {'loss': 0.6618, 'learning_rate': 0.0004074074074074074, 'epoch': 1.78}
Step 3210: {'loss': 0.6637, 'learning_rate': 0.00040555555555555554, 'epoch': 1.78}
Step 3220: {'loss': 0.6635, 'learning_rate': 0.00040370370370370375, 'epoch': 1.79}
Step 3230: {'loss': 0.662, 'learning_rate': 0.00040185185185185186, 'epoch': 1.79}
Step 3240: {'loss': 0.6449, 'learning_rate': 0.0004, 'epoch': 1.8}
Step 3250: {'loss': 0.6614, 'learning_rate': 0.0003981481481481481, 'epoch': 1.81}
Step 3260: {'loss': 0.6562, 'learning_rate': 0.00039629629629629634, 'epoch': 1.81}
Step 3270: {'loss': 0.6588, 'learning_rate': 0.00039444444444444444, 'epoch': 1.82}
Step 3280: {'loss': 0.671, 'learning_rate': 0.0003925925925925926, 'epoch': 1.82}
Step 3290: {'loss': 0.661, 'learning_rate': 0.0003907407407407407, 'epoch': 1.83}
Step 3300: {'loss': 0.6546, 'learning_rate': 0.0003888888888888889, 'epoch': 1.83}
Step 3310: {'loss': 0.6463, 'learning_rate': 0.000387037037037037, 'epoch': 1.84}
Step 3320: {'loss': 0.6964, 'learning_rate': 0.0003851851851851852, 'epoch': 1.84}
Step 3330: {'loss': 0.6782, 'learning_rate': 0.00038333333333333334, 'epoch': 1.85}
Step 3340: {'loss': 0.6517, 'learning_rate': 0.0003814814814814815, 'epoch': 1.86}
Step 3350: {'loss': 0.6671, 'learning_rate': 0.00037962962962962966, 'epoch': 1.86}
Step 3360: {'loss': 0.6632, 'learning_rate': 0.00037777777777777777, 'epoch': 1.87}
Step 3370: {'loss': 0.6323, 'learning_rate': 0.00037592592592592593, 'epoch': 1.87}
Step 3380: {'loss': 0.6818, 'learning_rate': 0.0003740740740740741, 'epoch': 1.88}
Step 3390: {'loss': 0.6549, 'learning_rate': 0.00037222222222222225, 'epoch': 1.88}
Step 3400: {'loss': 0.6758, 'learning_rate': 0.00037037037037037035, 'epoch': 1.89}
Step 3410: {'loss': 0.6766, 'learning_rate': 0.0003685185185185185, 'epoch': 1.89}
Step 3420: {'loss': 0.6492, 'learning_rate': 0.00036666666666666667, 'epoch': 1.9}
Step 3430: {'loss': 0.6524, 'learning_rate': 0.00036481481481481483, 'epoch': 1.91}
Step 3440: {'loss': 0.7279, 'learning_rate': 0.000362962962962963, 'epoch': 1.91}
Step 3450: {'loss': 0.6058, 'learning_rate': 0.0003611111111111111, 'epoch': 1.92}
Step 3460: {'loss': 0.6516, 'learning_rate': 0.0003592592592592593, 'epoch': 1.92}
Step 3470: {'loss': 0.6947, 'learning_rate': 0.0003574074074074074, 'epoch': 1.93}
Step 3480: {'loss': 0.6521, 'learning_rate': 0.00035555555555555557, 'epoch': 1.93}
Step 3490: {'loss': 0.6943, 'learning_rate': 0.0003537037037037037, 'epoch': 1.94}
Step 3500: {'loss': 0.6639, 'learning_rate': 0.0003518518518518519, 'epoch': 1.94}
Step 3510: {'loss': 0.6466, 'learning_rate': 0.00035, 'epoch': 1.95}
Step 3520: {'loss': 0.6945, 'learning_rate': 0.00034814814814814816, 'epoch': 1.96}
Step 3530: {'loss': 0.6629, 'learning_rate': 0.00034629629629629626, 'epoch': 1.96}
Step 3540: {'loss': 0.6737, 'learning_rate': 0.0003444444444444445, 'epoch': 1.97}
Step 3550: {'loss': 0.6272, 'learning_rate': 0.00034259259259259263, 'epoch': 1.97}
Step 3560: {'loss': 0.6468, 'learning_rate': 0.00034074074074074074, 'epoch': 1.98}
Step 3570: {'loss': 0.67, 'learning_rate': 0.0003388888888888889, 'epoch': 1.98}
Step 3580: {'loss': 0.6286, 'learning_rate': 0.00033703703703703706, 'epoch': 1.99}
Step 3590: {'loss': 0.6617, 'learning_rate': 0.0003351851851851852, 'epoch': 1.99}
Step 3600: {'loss': 0.653, 'learning_rate': 0.0003333333333333333, 'epoch': 2.0}
Step 3600: {'eval_loss': 0.6638187170028687, 'eval_runtime': 69.7558, 'eval_samples_per_second': 25.804, 'eval_steps_per_second': 6.451, 'epoch': 2.0}
Step 3610: {'loss': 0.6413, 'learning_rate': 0.0003314814814814815, 'epoch': 2.01}
Step 3620: {'loss': 0.6545, 'learning_rate': 0.0003296296296296296, 'epoch': 2.01}
Step 3630: {'loss': 0.6597, 'learning_rate': 0.0003277777777777778, 'epoch': 2.02}
Step 3640: {'loss': 0.6499, 'learning_rate': 0.00032592592592592596, 'epoch': 2.02}
Step 3650: {'loss': 0.6483, 'learning_rate': 0.00032407407407407406, 'epoch': 2.03}
Step 3660: {'loss': 0.6463, 'learning_rate': 0.0003222222222222222, 'epoch': 2.03}
Step 3670: {'loss': 0.7013, 'learning_rate': 0.0003203703703703704, 'epoch': 2.04}
Step 3680: {'loss': 0.6415, 'learning_rate': 0.00031851851851851854, 'epoch': 2.04}
Step 3690: {'loss': 0.7029, 'learning_rate': 0.00031666666666666665, 'epoch': 2.05}
Step 3700: {'loss': 0.6825, 'learning_rate': 0.0003148148148148148, 'epoch': 2.06}
Step 3710: {'loss': 0.6773, 'learning_rate': 0.00031296296296296297, 'epoch': 2.06}
Step 3720: {'loss': 0.6497, 'learning_rate': 0.0003111111111111111, 'epoch': 2.07}
Step 3730: {'loss': 0.6457, 'learning_rate': 0.00030925925925925923, 'epoch': 2.07}
Step 3740: {'loss': 0.6794, 'learning_rate': 0.0003074074074074074, 'epoch': 2.08}
Step 3750: {'loss': 0.6674, 'learning_rate': 0.0003055555555555556, 'epoch': 2.08}
Step 3760: {'loss': 0.6119, 'learning_rate': 0.0003037037037037037, 'epoch': 2.09}
Step 3770: {'loss': 0.6567, 'learning_rate': 0.00030185185185185187, 'epoch': 2.09}
Step 3780: {'loss': 0.6158, 'learning_rate': 0.0003, 'epoch': 2.1}
Step 3790: {'loss': 0.6179, 'learning_rate': 0.0002981481481481482, 'epoch': 2.11}
Step 3800: {'loss': 0.6586, 'learning_rate': 0.0002962962962962963, 'epoch': 2.11}
Step 3810: {'loss': 0.6601, 'learning_rate': 0.00029444444444444445, 'epoch': 2.12}
Step 3820: {'loss': 0.6802, 'learning_rate': 0.00029259259259259256, 'epoch': 2.12}
Step 3830: {'loss': 0.7118, 'learning_rate': 0.00029074074074074077, 'epoch': 2.13}
Step 3840: {'loss': 0.636, 'learning_rate': 0.0002888888888888889, 'epoch': 2.13}
Step 3850: {'loss': 0.6324, 'learning_rate': 0.00028703703703703703, 'epoch': 2.14}
Step 3860: {'loss': 0.6795, 'learning_rate': 0.0002851851851851852, 'epoch': 2.14}
Step 3870: {'loss': 0.6512, 'learning_rate': 0.00028333333333333335, 'epoch': 2.15}
Step 3880: {'loss': 0.6514, 'learning_rate': 0.0002814814814814815, 'epoch': 2.16}
Step 3890: {'loss': 0.6448, 'learning_rate': 0.0002796296296296296, 'epoch': 2.16}
Step 3900: {'loss': 0.6699, 'learning_rate': 0.0002777777777777778, 'epoch': 2.17}
Step 3910: {'loss': 0.6283, 'learning_rate': 0.00027592592592592594, 'epoch': 2.17}
Step 3920: {'loss': 0.6868, 'learning_rate': 0.0002740740740740741, 'epoch': 2.18}
Step 3930: {'loss': 0.6946, 'learning_rate': 0.0002722222222222222, 'epoch': 2.18}
Step 3940: {'loss': 0.6958, 'learning_rate': 0.00027037037037037036, 'epoch': 2.19}
Step 3950: {'loss': 0.664, 'learning_rate': 0.0002685185185185186, 'epoch': 2.19}
Step 3960: {'loss': 0.6544, 'learning_rate': 0.0002666666666666667, 'epoch': 2.2}
Step 3970: {'loss': 0.6149, 'learning_rate': 0.00026481481481481484, 'epoch': 2.21}
Step 3980: {'loss': 0.6394, 'learning_rate': 0.00026296296296296294, 'epoch': 2.21}
Step 3990: {'loss': 0.616, 'learning_rate': 0.00026111111111111116, 'epoch': 2.22}
Step 4000: {'loss': 0.6828, 'learning_rate': 0.00025925925925925926, 'epoch': 2.22}
Step 4010: {'loss': 0.6813, 'learning_rate': 0.0002574074074074074, 'epoch': 2.23}
Step 4020: {'loss': 0.6258, 'learning_rate': 0.00025555555555555553, 'epoch': 2.23}
Step 4030: {'loss': 0.6927, 'learning_rate': 0.00025370370370370374, 'epoch': 2.24}
Step 4040: {'loss': 0.6798, 'learning_rate': 0.00025185185185185185, 'epoch': 2.24}
Step 4050: {'loss': 0.7176, 'learning_rate': 0.00025, 'epoch': 2.25}
Step 4060: {'loss': 0.6407, 'learning_rate': 0.00024814814814814816, 'epoch': 2.26}
Step 4070: {'loss': 0.6699, 'learning_rate': 0.00024629629629629627, 'epoch': 2.26}
Step 4080: {'loss': 0.688, 'learning_rate': 0.00024444444444444443, 'epoch': 2.27}
Step 4090: {'loss': 0.6613, 'learning_rate': 0.0002425925925925926, 'epoch': 2.27}
Step 4100: {'loss': 0.6551, 'learning_rate': 0.00024074074074074072, 'epoch': 2.28}
Step 4110: {'loss': 0.6932, 'learning_rate': 0.0002388888888888889, 'epoch': 2.28}
Step 4120: {'loss': 0.6283, 'learning_rate': 0.00023703703703703704, 'epoch': 2.29}
Step 4130: {'loss': 0.6407, 'learning_rate': 0.0002351851851851852, 'epoch': 2.29}
Step 4140: {'loss': 0.6826, 'learning_rate': 0.00023333333333333333, 'epoch': 2.3}
Step 4150: {'loss': 0.6972, 'learning_rate': 0.0002314814814814815, 'epoch': 2.31}
Step 4160: {'loss': 0.6143, 'learning_rate': 0.00022962962962962962, 'epoch': 2.31}
Step 4170: {'loss': 0.6182, 'learning_rate': 0.00022777777777777778, 'epoch': 2.32}
Step 4180: {'loss': 0.6837, 'learning_rate': 0.00022592592592592591, 'epoch': 2.32}
Step 4190: {'loss': 0.6503, 'learning_rate': 0.00022407407407407407, 'epoch': 2.33}
Step 4200: {'loss': 0.7094, 'learning_rate': 0.0002222222222222222, 'epoch': 2.33}
Step 4210: {'loss': 0.6759, 'learning_rate': 0.0002203703703703704, 'epoch': 2.34}
Step 4220: {'loss': 0.6148, 'learning_rate': 0.00021851851851851852, 'epoch': 2.34}
Step 4230: {'loss': 0.6619, 'learning_rate': 0.00021666666666666668, 'epoch': 2.35}
Step 4240: {'loss': 0.6715, 'learning_rate': 0.00021481481481481482, 'epoch': 2.36}
Step 4250: {'loss': 0.6423, 'learning_rate': 0.00021296296296296298, 'epoch': 2.36}
Step 4260: {'loss': 0.708, 'learning_rate': 0.0002111111111111111, 'epoch': 2.37}
Step 4270: {'loss': 0.6227, 'learning_rate': 0.00020925925925925927, 'epoch': 2.37}
Step 4280: {'loss': 0.6597, 'learning_rate': 0.0002074074074074074, 'epoch': 2.38}
Step 4290: {'loss': 0.6699, 'learning_rate': 0.00020555555555555556, 'epoch': 2.38}
Step 4300: {'loss': 0.6506, 'learning_rate': 0.0002037037037037037, 'epoch': 2.39}
Step 4310: {'loss': 0.6297, 'learning_rate': 0.00020185185185185188, 'epoch': 2.39}
Step 4320: {'loss': 0.6422, 'learning_rate': 0.0002, 'epoch': 2.4}
Step 4330: {'loss': 0.6687, 'learning_rate': 0.00019814814814814817, 'epoch': 2.41}
Step 4340: {'loss': 0.6645, 'learning_rate': 0.0001962962962962963, 'epoch': 2.41}
Step 4350: {'loss': 0.6347, 'learning_rate': 0.00019444444444444446, 'epoch': 2.42}
Step 4360: {'loss': 0.6333, 'learning_rate': 0.0001925925925925926, 'epoch': 2.42}
Step 4370: {'loss': 0.6204, 'learning_rate': 0.00019074074074074075, 'epoch': 2.43}
Step 4380: {'loss': 0.6567, 'learning_rate': 0.00018888888888888888, 'epoch': 2.43}
Step 4390: {'loss': 0.6799, 'learning_rate': 0.00018703703703703704, 'epoch': 2.44}
Step 4400: {'loss': 0.6607, 'learning_rate': 0.00018518518518518518, 'epoch': 2.44}
Step 4410: {'loss': 0.6466, 'learning_rate': 0.00018333333333333334, 'epoch': 2.45}
Step 4420: {'loss': 0.611, 'learning_rate': 0.0001814814814814815, 'epoch': 2.46}
Step 4430: {'loss': 0.6845, 'learning_rate': 0.00017962962962962965, 'epoch': 2.46}
Step 4440: {'loss': 0.6148, 'learning_rate': 0.00017777777777777779, 'epoch': 2.47}
Step 4450: {'loss': 0.7131, 'learning_rate': 0.00017592592592592595, 'epoch': 2.47}
Step 4460: {'loss': 0.6614, 'learning_rate': 0.00017407407407407408, 'epoch': 2.48}
Step 4470: {'loss': 0.6713, 'learning_rate': 0.00017222222222222224, 'epoch': 2.48}
Step 4480: {'loss': 0.6554, 'learning_rate': 0.00017037037037037037, 'epoch': 2.49}
Step 4490: {'loss': 0.6195, 'learning_rate': 0.00016851851851851853, 'epoch': 2.49}
Step 4500: {'loss': 0.6241, 'learning_rate': 0.00016666666666666666, 'epoch': 2.5}
Step 4510: {'loss': 0.7341, 'learning_rate': 0.0001648148148148148, 'epoch': 2.51}
Step 4520: {'loss': 0.6517, 'learning_rate': 0.00016296296296296298, 'epoch': 2.51}
Step 4530: {'loss': 0.6716, 'learning_rate': 0.0001611111111111111, 'epoch': 2.52}
Step 4540: {'loss': 0.6871, 'learning_rate': 0.00015925925925925927, 'epoch': 2.52}
Step 4550: {'loss': 0.6192, 'learning_rate': 0.0001574074074074074, 'epoch': 2.53}
Step 4560: {'loss': 0.6304, 'learning_rate': 0.00015555555555555556, 'epoch': 2.53}
Step 4570: {'loss': 0.6549, 'learning_rate': 0.0001537037037037037, 'epoch': 2.54}
Step 4580: {'loss': 0.6335, 'learning_rate': 0.00015185185185185185, 'epoch': 2.54}
Step 4590: {'loss': 0.663, 'learning_rate': 0.00015, 'epoch': 2.55}
Step 4600: {'loss': 0.6336, 'learning_rate': 0.00014814814814814815, 'epoch': 2.56}
Step 4610: {'loss': 0.6391, 'learning_rate': 0.00014629629629629628, 'epoch': 2.56}
Step 4620: {'loss': 0.6538, 'learning_rate': 0.00014444444444444444, 'epoch': 2.57}
Step 4630: {'loss': 0.6637, 'learning_rate': 0.0001425925925925926, 'epoch': 2.57}
Step 4640: {'loss': 0.6345, 'learning_rate': 0.00014074074074074076, 'epoch': 2.58}
Step 4650: {'loss': 0.6709, 'learning_rate': 0.0001388888888888889, 'epoch': 2.58}
Step 4660: {'loss': 0.6322, 'learning_rate': 0.00013703703703703705, 'epoch': 2.59}
Step 4670: {'loss': 0.6628, 'learning_rate': 0.00013518518518518518, 'epoch': 2.59}
Step 4680: {'loss': 0.6487, 'learning_rate': 0.00013333333333333334, 'epoch': 2.6}
Step 4690: {'loss': 0.6187, 'learning_rate': 0.00013148148148148147, 'epoch': 2.61}
Step 4700: {'loss': 0.6662, 'learning_rate': 0.00012962962962962963, 'epoch': 2.61}
Step 4710: {'loss': 0.6573, 'learning_rate': 0.00012777777777777776, 'epoch': 2.62}
Step 4720: {'loss': 0.7074, 'learning_rate': 0.00012592592592592592, 'epoch': 2.62}
Step 4730: {'loss': 0.6733, 'learning_rate': 0.00012407407407407408, 'epoch': 2.63}
Step 4740: {'loss': 0.5995, 'learning_rate': 0.00012222222222222221, 'epoch': 2.63}
Step 4750: {'loss': 0.6157, 'learning_rate': 0.00012037037037037036, 'epoch': 2.64}
Step 4760: {'loss': 0.6415, 'learning_rate': 0.00011851851851851852, 'epoch': 2.64}
Step 4770: {'loss': 0.6837, 'learning_rate': 0.00011666666666666667, 'epoch': 2.65}
Step 4780: {'loss': 0.6394, 'learning_rate': 0.00011481481481481481, 'epoch': 2.66}
Step 4790: {'loss': 0.6352, 'learning_rate': 0.00011296296296296296, 'epoch': 2.66}
Step 4800: {'loss': 0.6534, 'learning_rate': 0.0001111111111111111, 'epoch': 2.67}
Step 4810: {'loss': 0.6374, 'learning_rate': 0.00010925925925925926, 'epoch': 2.67}
Step 4820: {'loss': 0.6486, 'learning_rate': 0.00010740740740740741, 'epoch': 2.68}
Step 4830: {'loss': 0.6363, 'learning_rate': 0.00010555555555555555, 'epoch': 2.68}
Step 4840: {'loss': 0.6489, 'learning_rate': 0.0001037037037037037, 'epoch': 2.69}
Step 4850: {'loss': 0.6743, 'learning_rate': 0.00010185185185185185, 'epoch': 2.69}
Step 4860: {'loss': 0.6556, 'learning_rate': 0.0001, 'epoch': 2.7}
Step 4870: {'loss': 0.676, 'learning_rate': 9.814814814814815e-05, 'epoch': 2.71}
Step 4880: {'loss': 0.6641, 'learning_rate': 9.62962962962963e-05, 'epoch': 2.71}
Step 4890: {'loss': 0.6535, 'learning_rate': 9.444444444444444e-05, 'epoch': 2.72}
Step 4900: {'loss': 0.6121, 'learning_rate': 9.259259259259259e-05, 'epoch': 2.72}
Step 4910: {'loss': 0.6606, 'learning_rate': 9.074074074074075e-05, 'epoch': 2.73}
Step 4920: {'loss': 0.6068, 'learning_rate': 8.888888888888889e-05, 'epoch': 2.73}
Step 4930: {'loss': 0.7106, 'learning_rate': 8.703703703703704e-05, 'epoch': 2.74}
Step 4940: {'loss': 0.6516, 'learning_rate': 8.518518518518518e-05, 'epoch': 2.74}
Step 4950: {'loss': 0.653, 'learning_rate': 8.333333333333333e-05, 'epoch': 2.75}
Step 4960: {'loss': 0.6419, 'learning_rate': 8.148148148148149e-05, 'epoch': 2.76}
Step 4970: {'loss': 0.6243, 'learning_rate': 7.962962962962964e-05, 'epoch': 2.76}
Step 4980: {'loss': 0.6734, 'learning_rate': 7.777777777777778e-05, 'epoch': 2.77}
Step 4990: {'loss': 0.6737, 'learning_rate': 7.592592592592593e-05, 'epoch': 2.77}
Step 5000: {'loss': 0.5855, 'learning_rate': 7.407407407407407e-05, 'epoch': 2.78}
Step 5010: {'loss': 0.664, 'learning_rate': 7.222222222222222e-05, 'epoch': 2.78}
Step 5020: {'loss': 0.6188, 'learning_rate': 7.037037037037038e-05, 'epoch': 2.79}
Step 5030: {'loss': 0.6429, 'learning_rate': 6.851851851851852e-05, 'epoch': 2.79}
Step 5040: {'loss': 0.6266, 'learning_rate': 6.666666666666667e-05, 'epoch': 2.8}
Step 5050: {'loss': 0.6397, 'learning_rate': 6.481481481481482e-05, 'epoch': 2.81}
Step 5060: {'loss': 0.6522, 'learning_rate': 6.296296296296296e-05, 'epoch': 2.81}
Step 5070: {'loss': 0.6497, 'learning_rate': 6.111111111111111e-05, 'epoch': 2.82}
Step 5080: {'loss': 0.6583, 'learning_rate': 5.925925925925926e-05, 'epoch': 2.82}
Step 5090: {'loss': 0.6319, 'learning_rate': 5.7407407407407406e-05, 'epoch': 2.83}
Step 5100: {'loss': 0.6673, 'learning_rate': 5.555555555555555e-05, 'epoch': 2.83}
Step 5110: {'loss': 0.6472, 'learning_rate': 5.3703703703703704e-05, 'epoch': 2.84}
Step 5120: {'loss': 0.6255, 'learning_rate': 5.185185185185185e-05, 'epoch': 2.84}
Step 5130: {'loss': 0.6465, 'learning_rate': 5e-05, 'epoch': 2.85}
Step 5140: {'loss': 0.6561, 'learning_rate': 4.814814814814815e-05, 'epoch': 2.86}
Step 5150: {'loss': 0.624, 'learning_rate': 4.6296296296296294e-05, 'epoch': 2.86}
Step 5160: {'loss': 0.6463, 'learning_rate': 4.4444444444444447e-05, 'epoch': 2.87}
Step 5170: {'loss': 0.6462, 'learning_rate': 4.259259259259259e-05, 'epoch': 2.87}
Step 5180: {'loss': 0.7165, 'learning_rate': 4.0740740740740745e-05, 'epoch': 2.88}
Step 5190: {'loss': 0.664, 'learning_rate': 3.888888888888889e-05, 'epoch': 2.88}
Step 5200: {'loss': 0.6688, 'learning_rate': 3.7037037037037037e-05, 'epoch': 2.89}
Step 5210: {'loss': 0.592, 'learning_rate': 3.518518518518519e-05, 'epoch': 2.89}
Step 5220: {'loss': 0.6148, 'learning_rate': 3.3333333333333335e-05, 'epoch': 2.9}
Step 5230: {'loss': 0.6359, 'learning_rate': 3.148148148148148e-05, 'epoch': 2.91}
Step 5240: {'loss': 0.6414, 'learning_rate': 2.962962962962963e-05, 'epoch': 2.91}
Step 5250: {'loss': 0.6307, 'learning_rate': 2.7777777777777776e-05, 'epoch': 2.92}
Step 5260: {'loss': 0.6748, 'learning_rate': 2.5925925925925925e-05, 'epoch': 2.92}
Step 5270: {'loss': 0.6437, 'learning_rate': 2.4074074074074074e-05, 'epoch': 2.93}
Step 5280: {'loss': 0.6375, 'learning_rate': 2.2222222222222223e-05, 'epoch': 2.93}
Step 5290: {'loss': 0.6173, 'learning_rate': 2.0370370370370372e-05, 'epoch': 2.94}
Step 5300: {'loss': 0.6593, 'learning_rate': 1.8518518518518518e-05, 'epoch': 2.94}
Step 5310: {'loss': 0.6509, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.95}
Step 5320: {'loss': 0.6769, 'learning_rate': 1.4814814814814815e-05, 'epoch': 2.96}
Step 5330: {'loss': 0.7025, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.96}
Step 5340: {'loss': 0.607, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.97}
Step 5350: {'loss': 0.6037, 'learning_rate': 9.259259259259259e-06, 'epoch': 2.97}
Step 5360: {'loss': 0.6706, 'learning_rate': 7.4074074074074075e-06, 'epoch': 2.98}
Step 5370: {'loss': 0.6697, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.98}
Step 5380: {'loss': 0.6453, 'learning_rate': 3.7037037037037037e-06, 'epoch': 2.99}
Step 5390: {'loss': 0.6499, 'learning_rate': 1.8518518518518519e-06, 'epoch': 2.99}
Step 5400: {'loss': 0.6465, 'learning_rate': 0.0, 'epoch': 3.0}
Step 5400: {'eval_loss': 0.6574286818504333, 'eval_runtime': 69.7333, 'eval_samples_per_second': 25.813, 'eval_steps_per_second': 6.453, 'epoch': 3.0}
Step 5400: {'train_runtime': 1510.1535, 'train_samples_per_second': 14.303, 'train_steps_per_second': 3.576, 'total_flos': 2.9481352065662976e+17, 'train_loss': 0.670821838290603, 'epoch': 3.0}
Step 10: {'loss': 1.4434, 'grad_norm': 1.253375768661499, 'learning_rate': 0.0009983333333333333, 'epoch': 0.005555555555555556}
Step 20: {'loss': 1.022, 'grad_norm': 1.4386268854141235, 'learning_rate': 0.0009964814814814814, 'epoch': 0.011111111111111112}
Step 30: {'loss': 0.9136, 'grad_norm': 1.1403379440307617, 'learning_rate': 0.0009946296296296296, 'epoch': 0.016666666666666666}
Step 40: {'loss': 0.8707, 'grad_norm': 1.0450160503387451, 'learning_rate': 0.0009927777777777778, 'epoch': 0.022222222222222223}
Step 50: {'loss': 0.771, 'grad_norm': 0.9723523855209351, 'learning_rate': 0.000990925925925926, 'epoch': 0.027777777777777776}
Step 60: {'loss': 0.6977, 'grad_norm': 0.740486741065979, 'learning_rate': 0.000989074074074074, 'epoch': 0.03333333333333333}
Step 70: {'loss': 0.7436, 'grad_norm': 0.774289071559906, 'learning_rate': 0.0009872222222222222, 'epoch': 0.03888888888888889}
Step 80: {'loss': 0.7167, 'grad_norm': 0.7907611131668091, 'learning_rate': 0.0009853703703703704, 'epoch': 0.044444444444444446}
Step 90: {'loss': 0.7021, 'grad_norm': 0.8185099363327026, 'learning_rate': 0.0009835185185185186, 'epoch': 0.05}
Step 100: {'loss': 0.7527, 'grad_norm': 0.8182543516159058, 'learning_rate': 0.0009816666666666667, 'epoch': 0.05555555555555555}
Step 110: {'loss': 0.6866, 'grad_norm': 0.6531724333763123, 'learning_rate': 0.0009798148148148149, 'epoch': 0.06111111111111111}
Step 120: {'loss': 0.7183, 'grad_norm': 0.9268796443939209, 'learning_rate': 0.000977962962962963, 'epoch': 0.06666666666666667}
Step 130: {'loss': 0.6904, 'grad_norm': 0.6421839594841003, 'learning_rate': 0.0009761111111111112, 'epoch': 0.07222222222222222}
Step 140: {'loss': 0.6798, 'grad_norm': 0.9229457974433899, 'learning_rate': 0.0009742592592592592, 'epoch': 0.07777777777777778}
Step 150: {'loss': 0.7096, 'grad_norm': 0.5985835194587708, 'learning_rate': 0.0009724074074074074, 'epoch': 0.08333333333333333}
Step 160: {'loss': 0.6526, 'grad_norm': 0.8017398118972778, 'learning_rate': 0.0009705555555555556, 'epoch': 0.08888888888888889}
Step 170: {'loss': 0.7346, 'grad_norm': 0.7006151676177979, 'learning_rate': 0.0009687037037037037, 'epoch': 0.09444444444444444}
Step 180: {'loss': 0.7083, 'grad_norm': 0.8023908734321594, 'learning_rate': 0.0009668518518518519, 'epoch': 0.1}
Step 190: {'loss': 0.7452, 'grad_norm': 0.8033954501152039, 'learning_rate': 0.000965, 'epoch': 0.10555555555555556}
Step 200: {'loss': 0.7212, 'grad_norm': 0.710238516330719, 'learning_rate': 0.0009631481481481482, 'epoch': 0.1111111111111111}
Step 210: {'loss': 0.7088, 'grad_norm': 0.8188366293907166, 'learning_rate': 0.0009612962962962964, 'epoch': 0.11666666666666667}
Step 220: {'loss': 0.6918, 'grad_norm': 0.8032983541488647, 'learning_rate': 0.0009594444444444444, 'epoch': 0.12222222222222222}
Step 230: {'loss': 0.6994, 'grad_norm': 0.5808964967727661, 'learning_rate': 0.0009575925925925926, 'epoch': 0.12777777777777777}
Step 240: {'loss': 0.6811, 'grad_norm': 0.6687936186790466, 'learning_rate': 0.0009557407407407408, 'epoch': 0.13333333333333333}
Step 250: {'loss': 0.6956, 'grad_norm': 0.7166041135787964, 'learning_rate': 0.0009538888888888889, 'epoch': 0.1388888888888889}
Step 260: {'loss': 0.7039, 'grad_norm': 0.6682332158088684, 'learning_rate': 0.000952037037037037, 'epoch': 0.14444444444444443}
Step 270: {'loss': 0.6944, 'grad_norm': 0.6642886400222778, 'learning_rate': 0.0009501851851851852, 'epoch': 0.15}
Step 280: {'loss': 0.7239, 'grad_norm': 0.5212097764015198, 'learning_rate': 0.0009483333333333334, 'epoch': 0.15555555555555556}
Step 290: {'loss': 0.7216, 'grad_norm': 0.6650060415267944, 'learning_rate': 0.0009464814814814815, 'epoch': 0.16111111111111112}
Step 300: {'loss': 0.7101, 'grad_norm': 0.7055570483207703, 'learning_rate': 0.0009446296296296296, 'epoch': 0.16666666666666666}
Step 310: {'loss': 0.7592, 'grad_norm': 0.688845157623291, 'learning_rate': 0.0009427777777777778, 'epoch': 0.17222222222222222}
Step 320: {'loss': 0.6966, 'grad_norm': 0.5606459975242615, 'learning_rate': 0.000940925925925926, 'epoch': 0.17777777777777778}
Step 330: {'loss': 0.6817, 'grad_norm': 0.601313054561615, 'learning_rate': 0.000939074074074074, 'epoch': 0.18333333333333332}
Step 340: {'loss': 0.7289, 'grad_norm': 0.5967966914176941, 'learning_rate': 0.0009372222222222222, 'epoch': 0.18888888888888888}
Step 350: {'loss': 0.7602, 'grad_norm': 0.5733615159988403, 'learning_rate': 0.0009353703703703705, 'epoch': 0.19444444444444445}
Step 360: {'loss': 0.6997, 'grad_norm': 0.6916326284408569, 'learning_rate': 0.0009335185185185185, 'epoch': 0.2}
Step 370: {'loss': 0.6878, 'grad_norm': 0.7806985378265381, 'learning_rate': 0.0009316666666666667, 'epoch': 0.20555555555555555}
Step 380: {'loss': 0.7222, 'grad_norm': 0.8728587627410889, 'learning_rate': 0.0009298148148148147, 'epoch': 0.2111111111111111}
Step 390: {'loss': 0.6582, 'grad_norm': 0.531636118888855, 'learning_rate': 0.000927962962962963, 'epoch': 0.21666666666666667}
Step 400: {'loss': 0.7269, 'grad_norm': 0.5489881038665771, 'learning_rate': 0.0009261111111111112, 'epoch': 0.2222222222222222}
Step 410: {'loss': 0.6758, 'grad_norm': 0.5253226161003113, 'learning_rate': 0.0009242592592592592, 'epoch': 0.22777777777777777}
Step 420: {'loss': 0.7264, 'grad_norm': 0.5675041079521179, 'learning_rate': 0.0009224074074074075, 'epoch': 0.23333333333333334}
Step 430: {'loss': 0.7155, 'grad_norm': 0.7292743921279907, 'learning_rate': 0.0009205555555555555, 'epoch': 0.2388888888888889}
Step 440: {'loss': 0.7256, 'grad_norm': 0.5805851221084595, 'learning_rate': 0.0009187037037037037, 'epoch': 0.24444444444444444}
Step 450: {'loss': 0.6651, 'grad_norm': 0.7232912182807922, 'learning_rate': 0.0009168518518518519, 'epoch': 0.25}
Step 460: {'loss': 0.683, 'grad_norm': 0.47682619094848633, 'learning_rate': 0.000915, 'epoch': 0.25555555555555554}
Step 470: {'loss': 0.73, 'grad_norm': 0.7023460865020752, 'learning_rate': 0.0009131481481481482, 'epoch': 0.2611111111111111}
Step 480: {'loss': 0.6949, 'grad_norm': 0.6403785943984985, 'learning_rate': 0.0009112962962962963, 'epoch': 0.26666666666666666}
Step 490: {'loss': 0.6935, 'grad_norm': 0.7795379161834717, 'learning_rate': 0.0009094444444444445, 'epoch': 0.2722222222222222}
Step 500: {'loss': 0.7069, 'grad_norm': 0.6425089240074158, 'learning_rate': 0.0009075925925925927, 'epoch': 0.2777777777777778}
Step 510: {'loss': 0.6854, 'grad_norm': 0.5523149371147156, 'learning_rate': 0.0009057407407407407, 'epoch': 0.2833333333333333}
Step 520: {'loss': 0.7078, 'grad_norm': 0.5572990775108337, 'learning_rate': 0.0009038888888888889, 'epoch': 0.28888888888888886}
Step 530: {'loss': 0.6945, 'grad_norm': 0.7368336319923401, 'learning_rate': 0.0009020370370370371, 'epoch': 0.29444444444444445}
Step 540: {'loss': 0.6728, 'grad_norm': 0.8133772015571594, 'learning_rate': 0.0009001851851851852, 'epoch': 0.3}
Step 550: {'loss': 0.6696, 'grad_norm': 0.6614447236061096, 'learning_rate': 0.0008983333333333333, 'epoch': 0.3055555555555556}
Step 560: {'loss': 0.6756, 'grad_norm': 0.5749937891960144, 'learning_rate': 0.0008964814814814815, 'epoch': 0.3111111111111111}
Step 570: {'loss': 0.7427, 'grad_norm': 0.5842587351799011, 'learning_rate': 0.0008946296296296297, 'epoch': 0.31666666666666665}
Step 580: {'loss': 0.6663, 'grad_norm': 0.5779374241828918, 'learning_rate': 0.0008927777777777778, 'epoch': 0.32222222222222224}
Step 590: {'loss': 0.7134, 'grad_norm': 0.5275846719741821, 'learning_rate': 0.0008909259259259259, 'epoch': 0.3277777777777778}
Step 600: {'loss': 0.6837, 'grad_norm': 0.5853307843208313, 'learning_rate': 0.0008890740740740741, 'epoch': 0.3333333333333333}
Step 610: {'loss': 0.7017, 'grad_norm': 0.6789016127586365, 'learning_rate': 0.0008872222222222223, 'epoch': 0.3388888888888889}
Step 620: {'loss': 0.6828, 'grad_norm': 0.6377775073051453, 'learning_rate': 0.0008853703703703703, 'epoch': 0.34444444444444444}
Step 630: {'loss': 0.6323, 'grad_norm': 0.6353005170822144, 'learning_rate': 0.0008835185185185185, 'epoch': 0.35}
Step 640: {'loss': 0.7024, 'grad_norm': 0.6296694278717041, 'learning_rate': 0.0008816666666666668, 'epoch': 0.35555555555555557}
Step 650: {'loss': 0.6568, 'grad_norm': 0.7696701288223267, 'learning_rate': 0.0008798148148148148, 'epoch': 0.3611111111111111}
Step 660: {'loss': 0.6724, 'grad_norm': 0.7816658616065979, 'learning_rate': 0.000877962962962963, 'epoch': 0.36666666666666664}
Step 670: {'loss': 0.6582, 'grad_norm': 0.6229236125946045, 'learning_rate': 0.000876111111111111, 'epoch': 0.37222222222222223}
Step 680: {'loss': 0.6759, 'grad_norm': 0.6061961054801941, 'learning_rate': 0.0008742592592592593, 'epoch': 0.37777777777777777}
Step 690: {'loss': 0.6554, 'grad_norm': 0.6135115623474121, 'learning_rate': 0.0008724074074074075, 'epoch': 0.38333333333333336}
Step 700: {'loss': 0.7051, 'grad_norm': 0.6433917284011841, 'learning_rate': 0.0008705555555555555, 'epoch': 0.3888888888888889}
Step 710: {'loss': 0.6941, 'grad_norm': 0.8552817106246948, 'learning_rate': 0.0008687037037037038, 'epoch': 0.39444444444444443}
Step 720: {'loss': 0.735, 'grad_norm': 0.6119270324707031, 'learning_rate': 0.0008668518518518519, 'epoch': 0.4}
Step 730: {'loss': 0.7094, 'grad_norm': 0.4699738919734955, 'learning_rate': 0.000865, 'epoch': 0.40555555555555556}
Step 740: {'loss': 0.652, 'grad_norm': 0.5005632638931274, 'learning_rate': 0.0008631481481481482, 'epoch': 0.4111111111111111}
Step 750: {'loss': 0.704, 'grad_norm': 0.5941410660743713, 'learning_rate': 0.0008612962962962963, 'epoch': 0.4166666666666667}
Step 760: {'loss': 0.7132, 'grad_norm': 0.652804434299469, 'learning_rate': 0.0008594444444444445, 'epoch': 0.4222222222222222}
Step 770: {'loss': 0.6754, 'grad_norm': 0.8922988772392273, 'learning_rate': 0.0008575925925925926, 'epoch': 0.42777777777777776}
Step 780: {'loss': 0.6635, 'grad_norm': 0.8471359610557556, 'learning_rate': 0.0008557407407407407, 'epoch': 0.43333333333333335}
Step 790: {'loss': 0.7363, 'grad_norm': 0.6676560640335083, 'learning_rate': 0.000853888888888889, 'epoch': 0.4388888888888889}
Step 800: {'loss': 0.6768, 'grad_norm': 0.5771393179893494, 'learning_rate': 0.0008520370370370371, 'epoch': 0.4444444444444444}
Step 810: {'loss': 0.6373, 'grad_norm': 0.5011053681373596, 'learning_rate': 0.0008501851851851852, 'epoch': 0.45}
Step 820: {'loss': 0.6769, 'grad_norm': 0.8290781378746033, 'learning_rate': 0.0008483333333333334, 'epoch': 0.45555555555555555}
Step 830: {'loss': 0.6614, 'grad_norm': 0.5304519534111023, 'learning_rate': 0.0008464814814814815, 'epoch': 0.46111111111111114}
Step 840: {'loss': 0.6684, 'grad_norm': 0.7151890993118286, 'learning_rate': 0.0008446296296296296, 'epoch': 0.4666666666666667}
Step 850: {'loss': 0.6431, 'grad_norm': 0.514968752861023, 'learning_rate': 0.0008427777777777778, 'epoch': 0.4722222222222222}
Step 860: {'loss': 0.6559, 'grad_norm': 0.5156329274177551, 'learning_rate': 0.000840925925925926, 'epoch': 0.4777777777777778}
Step 870: {'loss': 0.6791, 'grad_norm': 0.556818962097168, 'learning_rate': 0.0008390740740740741, 'epoch': 0.48333333333333334}
Step 880: {'loss': 0.6723, 'grad_norm': 0.5901644229888916, 'learning_rate': 0.0008372222222222222, 'epoch': 0.4888888888888889}
Step 890: {'loss': 0.6999, 'grad_norm': 0.7257574200630188, 'learning_rate': 0.0008353703703703703, 'epoch': 0.49444444444444446}
Step 900: {'loss': 0.6481, 'grad_norm': 0.665376603603363, 'learning_rate': 0.0008335185185185186, 'epoch': 0.5}
Step 910: {'loss': 0.6306, 'grad_norm': 0.6364867091178894, 'learning_rate': 0.0008316666666666666, 'epoch': 0.5055555555555555}
Step 920: {'loss': 0.7065, 'grad_norm': 0.7526056170463562, 'learning_rate': 0.0008298148148148148, 'epoch': 0.5111111111111111}
Step 930: {'loss': 0.6793, 'grad_norm': 0.5308705568313599, 'learning_rate': 0.0008279629629629631, 'epoch': 0.5166666666666667}
Step 940: {'loss': 0.6674, 'grad_norm': 0.5538775324821472, 'learning_rate': 0.0008261111111111111, 'epoch': 0.5222222222222223}
Step 950: {'loss': 0.6708, 'grad_norm': 0.6803678274154663, 'learning_rate': 0.0008242592592592593, 'epoch': 0.5277777777777778}
Step 960: {'loss': 0.6795, 'grad_norm': 0.5681134462356567, 'learning_rate': 0.0008224074074074073, 'epoch': 0.5333333333333333}
Step 970: {'loss': 0.6424, 'grad_norm': 0.6401285529136658, 'learning_rate': 0.0008205555555555556, 'epoch': 0.5388888888888889}
Step 980: {'loss': 0.6909, 'grad_norm': 0.5616291761398315, 'learning_rate': 0.0008187037037037038, 'epoch': 0.5444444444444444}
Step 990: {'loss': 0.6418, 'grad_norm': 0.4914694130420685, 'learning_rate': 0.0008168518518518518, 'epoch': 0.55}
Step 1000: {'loss': 0.662, 'grad_norm': 0.6479924321174622, 'learning_rate': 0.000815, 'epoch': 0.5555555555555556}
Step 1010: {'loss': 0.6857, 'grad_norm': 0.5321085453033447, 'learning_rate': 0.0008131481481481482, 'epoch': 0.5611111111111111}
Step 1020: {'loss': 0.7074, 'grad_norm': 0.6363521218299866, 'learning_rate': 0.0008112962962962963, 'epoch': 0.5666666666666667}
Step 1030: {'loss': 0.6254, 'grad_norm': 0.6691738963127136, 'learning_rate': 0.0008094444444444444, 'epoch': 0.5722222222222222}
Step 1040: {'loss': 0.64, 'grad_norm': 0.5583109259605408, 'learning_rate': 0.0008075925925925926, 'epoch': 0.5777777777777777}
Step 1050: {'loss': 0.6856, 'grad_norm': 0.7036234736442566, 'learning_rate': 0.0008057407407407408, 'epoch': 0.5833333333333334}
Step 1060: {'loss': 0.6955, 'grad_norm': 0.48843830823898315, 'learning_rate': 0.0008038888888888889, 'epoch': 0.5888888888888889}
Step 1070: {'loss': 0.6853, 'grad_norm': 0.7725180387496948, 'learning_rate': 0.000802037037037037, 'epoch': 0.5944444444444444}
Step 1080: {'loss': 0.6777, 'grad_norm': 0.5827479362487793, 'learning_rate': 0.0008001851851851852, 'epoch': 0.6}
Step 1090: {'loss': 0.6772, 'grad_norm': 0.5124014616012573, 'learning_rate': 0.0007983333333333334, 'epoch': 0.6055555555555555}
Step 1100: {'loss': 0.6604, 'grad_norm': 0.6604494452476501, 'learning_rate': 0.0007964814814814815, 'epoch': 0.6111111111111112}
Step 1110: {'loss': 0.6753, 'grad_norm': 0.5897396206855774, 'learning_rate': 0.0007946296296296296, 'epoch': 0.6166666666666667}
Step 1120: {'loss': 0.64, 'grad_norm': 0.6147085428237915, 'learning_rate': 0.0007927777777777778, 'epoch': 0.6222222222222222}
Step 1130: {'loss': 0.6319, 'grad_norm': 0.42357632517814636, 'learning_rate': 0.0007909259259259259, 'epoch': 0.6277777777777778}
Step 1140: {'loss': 0.652, 'grad_norm': 0.6573575735092163, 'learning_rate': 0.0007890740740740741, 'epoch': 0.6333333333333333}
Step 1150: {'loss': 0.6796, 'grad_norm': 0.84578537940979, 'learning_rate': 0.0007872222222222223, 'epoch': 0.6388888888888888}
Step 1160: {'loss': 0.6891, 'grad_norm': 0.7682648301124573, 'learning_rate': 0.0007853703703703704, 'epoch': 0.6444444444444445}
Step 1170: {'loss': 0.6812, 'grad_norm': 0.5508800148963928, 'learning_rate': 0.0007835185185185186, 'epoch': 0.65}
Step 1180: {'loss': 0.6918, 'grad_norm': 0.5673089027404785, 'learning_rate': 0.0007816666666666666, 'epoch': 0.6555555555555556}
Step 1190: {'loss': 0.6506, 'grad_norm': 0.6378026604652405, 'learning_rate': 0.0007798148148148149, 'epoch': 0.6611111111111111}
Step 1200: {'loss': 0.7002, 'grad_norm': 0.558413565158844, 'learning_rate': 0.0007779629629629629, 'epoch': 0.6666666666666666}
Step 1210: {'loss': 0.6749, 'grad_norm': 0.5077846646308899, 'learning_rate': 0.0007761111111111111, 'epoch': 0.6722222222222223}
Step 1220: {'loss': 0.6299, 'grad_norm': 0.6936260461807251, 'learning_rate': 0.0007742592592592594, 'epoch': 0.6777777777777778}
Step 1230: {'loss': 0.7123, 'grad_norm': 0.6332815885543823, 'learning_rate': 0.0007724074074074074, 'epoch': 0.6833333333333333}
Step 1240: {'loss': 0.6669, 'grad_norm': 0.6421427130699158, 'learning_rate': 0.0007705555555555556, 'epoch': 0.6888888888888889}
Step 1250: {'loss': 0.6743, 'grad_norm': 0.6425850987434387, 'learning_rate': 0.0007687037037037037, 'epoch': 0.6944444444444444}
Step 1260: {'loss': 0.6786, 'grad_norm': 0.5328369736671448, 'learning_rate': 0.0007668518518518519, 'epoch': 0.7}
Step 1270: {'loss': 0.6667, 'grad_norm': 0.51949143409729, 'learning_rate': 0.0007650000000000001, 'epoch': 0.7055555555555556}
Step 1280: {'loss': 0.6037, 'grad_norm': 0.6699501872062683, 'learning_rate': 0.0007631481481481481, 'epoch': 0.7111111111111111}
Step 1290: {'loss': 0.6561, 'grad_norm': 0.5870910286903381, 'learning_rate': 0.0007612962962962963, 'epoch': 0.7166666666666667}
Step 1300: {'loss': 0.6723, 'grad_norm': 0.8787160515785217, 'learning_rate': 0.0007594444444444445, 'epoch': 0.7222222222222222}
Step 1310: {'loss': 0.6782, 'grad_norm': 0.6295564770698547, 'learning_rate': 0.0007575925925925926, 'epoch': 0.7277777777777777}
Step 1320: {'loss': 0.6294, 'grad_norm': 0.618606686592102, 'learning_rate': 0.0007557407407407407, 'epoch': 0.7333333333333333}
Step 1330: {'loss': 0.6528, 'grad_norm': 0.697856605052948, 'learning_rate': 0.000753888888888889, 'epoch': 0.7388888888888889}
Step 1340: {'loss': 0.6903, 'grad_norm': 0.6837623715400696, 'learning_rate': 0.0007520370370370371, 'epoch': 0.7444444444444445}
Step 1350: {'loss': 0.6448, 'grad_norm': 0.558910071849823, 'learning_rate': 0.0007501851851851852, 'epoch': 0.75}
Step 1360: {'loss': 0.6772, 'grad_norm': 0.5786346793174744, 'learning_rate': 0.0007483333333333333, 'epoch': 0.7555555555555555}
Step 1370: {'loss': 0.6696, 'grad_norm': 0.5753886699676514, 'learning_rate': 0.0007464814814814815, 'epoch': 0.7611111111111111}
Step 1380: {'loss': 0.6834, 'grad_norm': 0.5243350267410278, 'learning_rate': 0.0007446296296296297, 'epoch': 0.7666666666666667}
Step 1390: {'loss': 0.6559, 'grad_norm': 0.5484777688980103, 'learning_rate': 0.0007427777777777778, 'epoch': 0.7722222222222223}
Step 1400: {'loss': 0.7011, 'grad_norm': 0.5450450778007507, 'learning_rate': 0.0007409259259259259, 'epoch': 0.7777777777777778}
Step 1410: {'loss': 0.6849, 'grad_norm': 0.6029413342475891, 'learning_rate': 0.0007390740740740741, 'epoch': 0.7833333333333333}
Step 1420: {'loss': 0.6608, 'grad_norm': 0.43278515338897705, 'learning_rate': 0.0007372222222222222, 'epoch': 0.7888888888888889}
Step 1430: {'loss': 0.6161, 'grad_norm': 0.45626071095466614, 'learning_rate': 0.0007353703703703704, 'epoch': 0.7944444444444444}
Step 1440: {'loss': 0.6615, 'grad_norm': 0.7803824543952942, 'learning_rate': 0.0007335185185185185, 'epoch': 0.8}
Step 1450: {'loss': 0.7193, 'grad_norm': 0.5364481806755066, 'learning_rate': 0.0007316666666666667, 'epoch': 0.8055555555555556}
Step 1460: {'loss': 0.661, 'grad_norm': 0.9069743752479553, 'learning_rate': 0.0007298148148148149, 'epoch': 0.8111111111111111}
Step 1470: {'loss': 0.6692, 'grad_norm': 0.712385892868042, 'learning_rate': 0.0007279629629629629, 'epoch': 0.8166666666666667}
Step 1480: {'loss': 0.6292, 'grad_norm': 0.6000635623931885, 'learning_rate': 0.0007261111111111112, 'epoch': 0.8222222222222222}
Step 1490: {'loss': 0.6762, 'grad_norm': 0.6006329655647278, 'learning_rate': 0.0007242592592592592, 'epoch': 0.8277777777777777}
Step 1500: {'loss': 0.6916, 'grad_norm': 0.5144730806350708, 'learning_rate': 0.0007224074074074074, 'epoch': 0.8333333333333334}
Step 1510: {'loss': 0.7384, 'grad_norm': 0.5919799208641052, 'learning_rate': 0.0007205555555555556, 'epoch': 0.8388888888888889}
Step 1520: {'loss': 0.7127, 'grad_norm': 0.5427912473678589, 'learning_rate': 0.0007187037037037037, 'epoch': 0.8444444444444444}
Step 1530: {'loss': 0.686, 'grad_norm': 0.6782852411270142, 'learning_rate': 0.0007168518518518519, 'epoch': 0.85}
Step 1540: {'loss': 0.685, 'grad_norm': 0.5337863564491272, 'learning_rate': 0.000715, 'epoch': 0.8555555555555555}
Step 1550: {'loss': 0.666, 'grad_norm': 0.6752694249153137, 'learning_rate': 0.0007131481481481482, 'epoch': 0.8611111111111112}
Step 1560: {'loss': 0.6365, 'grad_norm': 0.6102226972579956, 'learning_rate': 0.0007112962962962964, 'epoch': 0.8666666666666667}
Step 10: {'loss': 1.5006, 'grad_norm': 1.3646637201309204, 'learning_rate': 0.0009991666666666666, 'epoch': 0.002777777777777778}
Step 20: {'loss': 1.0381, 'grad_norm': 2.2633583545684814, 'learning_rate': 0.0009982407407407407, 'epoch': 0.005555555555555556}
Step 30: {'loss': 0.9622, 'grad_norm': 1.4447152614593506, 'learning_rate': 0.0009973148148148148, 'epoch': 0.008333333333333333}
Step 40: {'loss': 0.9188, 'grad_norm': 1.1228680610656738, 'learning_rate': 0.0009963888888888889, 'epoch': 0.011111111111111112}
Step 50: {'loss': 0.8637, 'grad_norm': 1.0205663442611694, 'learning_rate': 0.000995462962962963, 'epoch': 0.013888888888888888}
Step 60: {'loss': 0.7828, 'grad_norm': 1.1172573566436768, 'learning_rate': 0.000994537037037037, 'epoch': 0.016666666666666666}
Step 70: {'loss': 0.7635, 'grad_norm': 1.0591667890548706, 'learning_rate': 0.0009936111111111111, 'epoch': 0.019444444444444445}
Step 80: {'loss': 0.7675, 'grad_norm': 0.9845139980316162, 'learning_rate': 0.0009926851851851852, 'epoch': 0.022222222222222223}
Step 90: {'loss': 0.7605, 'grad_norm': 0.9529698491096497, 'learning_rate': 0.0009917592592592593, 'epoch': 0.025}
Step 100: {'loss': 0.7166, 'grad_norm': 0.8700059056282043, 'learning_rate': 0.0009908333333333334, 'epoch': 0.027777777777777776}
Step 110: {'loss': 0.6949, 'grad_norm': 0.978961169719696, 'learning_rate': 0.0009899074074074074, 'epoch': 0.030555555555555555}
Step 120: {'loss': 0.6988, 'grad_norm': 0.9509324431419373, 'learning_rate': 0.0009889814814814815, 'epoch': 0.03333333333333333}
Step 130: {'loss': 0.7318, 'grad_norm': 1.085640788078308, 'learning_rate': 0.0009880555555555556, 'epoch': 0.03611111111111111}
Step 140: {'loss': 0.7009, 'grad_norm': 1.0989278554916382, 'learning_rate': 0.0009871296296296297, 'epoch': 0.03888888888888889}
Step 150: {'loss': 0.7076, 'grad_norm': 0.6398680210113525, 'learning_rate': 0.0009862037037037038, 'epoch': 0.041666666666666664}
Step 160: {'loss': 0.6809, 'grad_norm': 0.6305155158042908, 'learning_rate': 0.0009852777777777778, 'epoch': 0.044444444444444446}
Step 170: {'loss': 0.6905, 'grad_norm': 0.8347012400627136, 'learning_rate': 0.000984351851851852, 'epoch': 0.04722222222222222}
Step 180: {'loss': 0.6829, 'grad_norm': 1.073115348815918, 'learning_rate': 0.000983425925925926, 'epoch': 0.05}
Step 190: {'loss': 0.7541, 'grad_norm': 1.1931475400924683, 'learning_rate': 0.0009825, 'epoch': 0.05277777777777778}
Step 200: {'loss': 0.7679, 'grad_norm': 0.8968724012374878, 'learning_rate': 0.0009815740740740742, 'epoch': 0.05555555555555555}
Step 210: {'loss': 0.7279, 'grad_norm': 0.8510154485702515, 'learning_rate': 0.0009806481481481482, 'epoch': 0.058333333333333334}
Step 220: {'loss': 0.7439, 'grad_norm': 0.9143465161323547, 'learning_rate': 0.0009797222222222223, 'epoch': 0.06111111111111111}
Step 230: {'loss': 0.7193, 'grad_norm': 1.3280261754989624, 'learning_rate': 0.0009787962962962964, 'epoch': 0.06388888888888888}
Step 240: {'loss': 0.76, 'grad_norm': 0.8915621638298035, 'learning_rate': 0.0009778703703703705, 'epoch': 0.06666666666666667}
Step 250: {'loss': 0.6844, 'grad_norm': 1.009490966796875, 'learning_rate': 0.0009769444444444443, 'epoch': 0.06944444444444445}
Step 260: {'loss': 0.6877, 'grad_norm': 0.8763436079025269, 'learning_rate': 0.0009760185185185185, 'epoch': 0.07222222222222222}
Step 270: {'loss': 0.6705, 'grad_norm': 0.8498031497001648, 'learning_rate': 0.0009750925925925926, 'epoch': 0.075}
Step 280: {'loss': 0.6673, 'grad_norm': 0.6960461735725403, 'learning_rate': 0.0009741666666666667, 'epoch': 0.07777777777777778}
Step 290: {'loss': 0.711, 'grad_norm': 1.0569826364517212, 'learning_rate': 0.0009732407407407408, 'epoch': 0.08055555555555556}
Step 300: {'loss': 0.6799, 'grad_norm': 0.7677145600318909, 'learning_rate': 0.0009723148148148149, 'epoch': 0.08333333333333333}
Step 310: {'loss': 0.6802, 'grad_norm': 0.6910646557807922, 'learning_rate': 0.0009713888888888889, 'epoch': 0.08611111111111111}
Step 320: {'loss': 0.7392, 'grad_norm': 0.7834469079971313, 'learning_rate': 0.000970462962962963, 'epoch': 0.08888888888888889}
Step 330: {'loss': 0.7297, 'grad_norm': 0.7790639996528625, 'learning_rate': 0.0009695370370370371, 'epoch': 0.09166666666666666}
Step 340: {'loss': 0.7517, 'grad_norm': 0.733124315738678, 'learning_rate': 0.0009686111111111111, 'epoch': 0.09444444444444444}
Step 350: {'loss': 0.7387, 'grad_norm': 0.9016074538230896, 'learning_rate': 0.0009676851851851852, 'epoch': 0.09722222222222222}
Step 360: {'loss': 0.7433, 'grad_norm': 0.7723226547241211, 'learning_rate': 0.0009667592592592592, 'epoch': 0.1}
Step 370: {'loss': 0.7487, 'grad_norm': 0.7677344679832458, 'learning_rate': 0.0009658333333333333, 'epoch': 0.10277777777777777}
Step 380: {'loss': 0.6342, 'grad_norm': 1.224940538406372, 'learning_rate': 0.0009649074074074075, 'epoch': 0.10555555555555556}
Step 390: {'loss': 0.6333, 'grad_norm': 0.9706849455833435, 'learning_rate': 0.0009639814814814815, 'epoch': 0.10833333333333334}
Step 400: {'loss': 0.6969, 'grad_norm': 0.821990966796875, 'learning_rate': 0.0009630555555555555, 'epoch': 0.1111111111111111}
Step 410: {'loss': 0.7013, 'grad_norm': 0.9705829620361328, 'learning_rate': 0.0009621296296296297, 'epoch': 0.11388888888888889}
Step 420: {'loss': 0.6606, 'grad_norm': 0.7091352939605713, 'learning_rate': 0.0009612037037037037, 'epoch': 0.11666666666666667}
Step 430: {'loss': 0.7361, 'grad_norm': 0.9513391256332397, 'learning_rate': 0.0009602777777777778, 'epoch': 0.11944444444444445}
Step 440: {'loss': 0.7273, 'grad_norm': 0.936480700969696, 'learning_rate': 0.000959351851851852, 'epoch': 0.12222222222222222}
Step 450: {'loss': 0.7241, 'grad_norm': 0.8534360527992249, 'learning_rate': 0.0009584259259259259, 'epoch': 0.125}
Step 460: {'loss': 0.7403, 'grad_norm': 1.2680965662002563, 'learning_rate': 0.0009575, 'epoch': 0.12777777777777777}
Step 470: {'loss': 0.7391, 'grad_norm': 0.6379629969596863, 'learning_rate': 0.000956574074074074, 'epoch': 0.13055555555555556}
Step 480: {'loss': 0.814, 'grad_norm': 0.8655158877372742, 'learning_rate': 0.0009556481481481482, 'epoch': 0.13333333333333333}
Step 490: {'loss': 0.7217, 'grad_norm': 0.8139844536781311, 'learning_rate': 0.0009547222222222223, 'epoch': 0.1361111111111111}
Step 500: {'loss': 0.7739, 'grad_norm': 0.6981763243675232, 'learning_rate': 0.0009537962962962962, 'epoch': 0.1388888888888889}
Step 510: {'loss': 0.7524, 'grad_norm': 0.745084822177887, 'learning_rate': 0.0009528703703703704, 'epoch': 0.14166666666666666}
Step 520: {'loss': 0.6938, 'grad_norm': 1.3644664287567139, 'learning_rate': 0.0009519444444444445, 'epoch': 0.14444444444444443}
Step 530: {'loss': 0.7109, 'grad_norm': 1.0668871402740479, 'learning_rate': 0.0009510185185185185, 'epoch': 0.14722222222222223}
Step 540: {'loss': 0.699, 'grad_norm': 1.1568529605865479, 'learning_rate': 0.0009500925925925927, 'epoch': 0.15}
Step 550: {'loss': 0.681, 'grad_norm': 0.5247589349746704, 'learning_rate': 0.0009491666666666667, 'epoch': 0.1527777777777778}
Step 560: {'loss': 0.6835, 'grad_norm': 1.010379672050476, 'learning_rate': 0.0009482407407407407, 'epoch': 0.15555555555555556}
Step 570: {'loss': 0.7043, 'grad_norm': 1.4328421354293823, 'learning_rate': 0.0009473148148148149, 'epoch': 0.15833333333333333}
Step 580: {'loss': 0.7004, 'grad_norm': 1.0717788934707642, 'learning_rate': 0.0009463888888888889, 'epoch': 0.16111111111111112}
Step 590: {'loss': 0.7099, 'grad_norm': 0.7910663485527039, 'learning_rate': 0.0009454629629629629, 'epoch': 0.1638888888888889}
Step 600: {'loss': 0.7119, 'grad_norm': 0.6872370839118958, 'learning_rate': 0.0009445370370370371, 'epoch': 0.16666666666666666}
Step 610: {'loss': 0.6682, 'grad_norm': 0.8352832198143005, 'learning_rate': 0.0009436111111111111, 'epoch': 0.16944444444444445}
Step 620: {'loss': 0.6562, 'grad_norm': 1.0273610353469849, 'learning_rate': 0.0009426851851851852, 'epoch': 0.17222222222222222}
Step 630: {'loss': 0.7214, 'grad_norm': 1.2212239503860474, 'learning_rate': 0.0009417592592592593, 'epoch': 0.175}
Step 640: {'loss': 0.6778, 'grad_norm': 1.0145097970962524, 'learning_rate': 0.0009408333333333333, 'epoch': 0.17777777777777778}
Step 650: {'loss': 0.6851, 'grad_norm': 1.0324647426605225, 'learning_rate': 0.0009399074074074074, 'epoch': 0.18055555555555555}
Step 660: {'loss': 0.6015, 'grad_norm': 0.9535157680511475, 'learning_rate': 0.0009389814814814815, 'epoch': 0.18333333333333332}
Step 670: {'loss': 0.712, 'grad_norm': 0.7893326282501221, 'learning_rate': 0.0009380555555555556, 'epoch': 0.18611111111111112}
Step 680: {'loss': 0.6663, 'grad_norm': 0.7986326813697815, 'learning_rate': 0.0009371296296296297, 'epoch': 0.18888888888888888}
Step 690: {'loss': 0.6858, 'grad_norm': 1.227538824081421, 'learning_rate': 0.0009362037037037036, 'epoch': 0.19166666666666668}
Step 700: {'loss': 0.6738, 'grad_norm': 0.790931224822998, 'learning_rate': 0.0009352777777777778, 'epoch': 0.19444444444444445}
Step 710: {'loss': 0.7033, 'grad_norm': 0.7651138305664062, 'learning_rate': 0.0009343518518518519, 'epoch': 0.19722222222222222}
Step 720: {'loss': 0.6887, 'grad_norm': 0.9118724465370178, 'learning_rate': 0.0009334259259259259, 'epoch': 0.2}
Step 730: {'loss': 0.7254, 'grad_norm': 0.6003547310829163, 'learning_rate': 0.0009325000000000001, 'epoch': 0.20277777777777778}
Step 740: {'loss': 0.6379, 'grad_norm': 0.6581688523292542, 'learning_rate': 0.0009315740740740741, 'epoch': 0.20555555555555555}
Step 750: {'loss': 0.6796, 'grad_norm': 1.010275959968567, 'learning_rate': 0.0009306481481481481, 'epoch': 0.20833333333333334}
Step 760: {'loss': 0.6981, 'grad_norm': 0.6791630983352661, 'learning_rate': 0.0009297222222222223, 'epoch': 0.2111111111111111}
Step 770: {'loss': 0.7657, 'grad_norm': 0.9261451959609985, 'learning_rate': 0.0009287962962962964, 'epoch': 0.21388888888888888}
Step 780: {'loss': 0.7153, 'grad_norm': 1.0421253442764282, 'learning_rate': 0.0009278703703703704, 'epoch': 0.21666666666666667}
Step 790: {'loss': 0.6745, 'grad_norm': 1.0338128805160522, 'learning_rate': 0.0009269444444444444, 'epoch': 0.21944444444444444}
Step 800: {'loss': 0.6466, 'grad_norm': 0.9029507040977478, 'learning_rate': 0.0009260185185185185, 'epoch': 0.2222222222222222}
Step 810: {'loss': 0.6509, 'grad_norm': 0.685720682144165, 'learning_rate': 0.0009250925925925926, 'epoch': 0.225}
Step 820: {'loss': 0.7432, 'grad_norm': 0.8216124773025513, 'learning_rate': 0.0009241666666666667, 'epoch': 0.22777777777777777}
Step 830: {'loss': 0.6271, 'grad_norm': 0.6657191514968872, 'learning_rate': 0.0009232407407407407, 'epoch': 0.23055555555555557}
Step 840: {'loss': 0.6976, 'grad_norm': 1.224473237991333, 'learning_rate': 0.0009223148148148148, 'epoch': 0.23333333333333334}
Step 850: {'loss': 0.6936, 'grad_norm': 0.9883789420127869, 'learning_rate': 0.0009213888888888889, 'epoch': 0.2361111111111111}
Step 860: {'loss': 0.6906, 'grad_norm': 0.8042855262756348, 'learning_rate': 0.000920462962962963, 'epoch': 0.2388888888888889}
Step 870: {'loss': 0.6819, 'grad_norm': 0.8943239450454712, 'learning_rate': 0.0009195370370370371, 'epoch': 0.24166666666666667}
Step 880: {'loss': 0.7177, 'grad_norm': 0.7819314002990723, 'learning_rate': 0.0009186111111111111, 'epoch': 0.24444444444444444}
Step 890: {'loss': 0.6992, 'grad_norm': 0.8986078500747681, 'learning_rate': 0.0009176851851851852, 'epoch': 0.24722222222222223}
Step 900: {'loss': 0.6862, 'grad_norm': 0.6348748803138733, 'learning_rate': 0.0009167592592592593, 'epoch': 0.25}
Step 910: {'loss': 0.6515, 'grad_norm': 0.8045545816421509, 'learning_rate': 0.0009158333333333334, 'epoch': 0.25277777777777777}
Step 920: {'loss': 0.5993, 'grad_norm': 1.13197660446167, 'learning_rate': 0.0009149074074074074, 'epoch': 0.25555555555555554}
Step 930: {'loss': 0.759, 'grad_norm': 0.7497161030769348, 'learning_rate': 0.0009139814814814815, 'epoch': 0.25833333333333336}
Step 940: {'loss': 0.6762, 'grad_norm': 0.7518277764320374, 'learning_rate': 0.0009130555555555555, 'epoch': 0.2611111111111111}
Step 950: {'loss': 0.7055, 'grad_norm': 0.8486589193344116, 'learning_rate': 0.0009121296296296296, 'epoch': 0.2638888888888889}
Step 960: {'loss': 0.6666, 'grad_norm': 0.8357183337211609, 'learning_rate': 0.0009112037037037038, 'epoch': 0.26666666666666666}
Step 970: {'loss': 0.678, 'grad_norm': 0.5309781432151794, 'learning_rate': 0.0009102777777777778, 'epoch': 0.26944444444444443}
Step 980: {'loss': 0.682, 'grad_norm': 0.9047258496284485, 'learning_rate': 0.0009093518518518518, 'epoch': 0.2722222222222222}
Step 990: {'loss': 0.6781, 'grad_norm': 0.7202913165092468, 'learning_rate': 0.000908425925925926, 'epoch': 0.275}
Step 1000: {'loss': 0.732, 'grad_norm': 0.608462929725647, 'learning_rate': 0.0009075, 'epoch': 0.2777777777777778}
Step 1010: {'loss': 0.7431, 'grad_norm': 1.01364266872406, 'learning_rate': 0.0009065740740740741, 'epoch': 0.28055555555555556}
Step 1020: {'loss': 0.7566, 'grad_norm': 0.9747775793075562, 'learning_rate': 0.0009056481481481483, 'epoch': 0.2833333333333333}
Step 1030: {'loss': 0.6275, 'grad_norm': 0.8714442849159241, 'learning_rate': 0.0009047222222222222, 'epoch': 0.2861111111111111}
Step 1040: {'loss': 0.6709, 'grad_norm': 0.7275382280349731, 'learning_rate': 0.0009037962962962963, 'epoch': 0.28888888888888886}
Step 1050: {'loss': 0.7286, 'grad_norm': 0.7881795167922974, 'learning_rate': 0.0009028703703703704, 'epoch': 0.2916666666666667}
Step 1060: {'loss': 0.7456, 'grad_norm': 0.8571773171424866, 'learning_rate': 0.0009019444444444445, 'epoch': 0.29444444444444445}
Step 1070: {'loss': 0.6424, 'grad_norm': 0.988713800907135, 'learning_rate': 0.0009010185185185186, 'epoch': 0.2972222222222222}
Step 1080: {'loss': 0.6435, 'grad_norm': 0.8775952458381653, 'learning_rate': 0.0009000925925925925, 'epoch': 0.3}
Step 1090: {'loss': 0.7543, 'grad_norm': 0.9935707449913025, 'learning_rate': 0.0008991666666666667, 'epoch': 0.30277777777777776}
Step 1100: {'loss': 0.6219, 'grad_norm': 0.8240730166435242, 'learning_rate': 0.0008982407407407408, 'epoch': 0.3055555555555556}
Step 1110: {'loss': 0.7001, 'grad_norm': 0.7550655603408813, 'learning_rate': 0.0008973148148148148, 'epoch': 0.30833333333333335}
Step 1120: {'loss': 0.6956, 'grad_norm': 0.8300790786743164, 'learning_rate': 0.000896388888888889, 'epoch': 0.3111111111111111}
Step 1130: {'loss': 0.7147, 'grad_norm': 0.9162051677703857, 'learning_rate': 0.000895462962962963, 'epoch': 0.3138888888888889}
Step 1140: {'loss': 0.6677, 'grad_norm': 0.8615278005599976, 'learning_rate': 0.000894537037037037, 'epoch': 0.31666666666666665}
Step 1150: {'loss': 0.6206, 'grad_norm': 0.7417134642601013, 'learning_rate': 0.0008936111111111112, 'epoch': 0.3194444444444444}
Step 1160: {'loss': 0.6474, 'grad_norm': 0.8901429176330566, 'learning_rate': 0.0008926851851851852, 'epoch': 0.32222222222222224}
Step 1170: {'loss': 0.7478, 'grad_norm': 0.999198853969574, 'learning_rate': 0.0008917592592592592, 'epoch': 0.325}
Step 1180: {'loss': 0.7308, 'grad_norm': 1.1924114227294922, 'learning_rate': 0.0008908333333333334, 'epoch': 0.3277777777777778}
Step 1190: {'loss': 0.7, 'grad_norm': 0.9461113810539246, 'learning_rate': 0.0008899074074074074, 'epoch': 0.33055555555555555}
Step 1200: {'loss': 0.7312, 'grad_norm': 0.8053910136222839, 'learning_rate': 0.0008889814814814815, 'epoch': 0.3333333333333333}
Step 1210: {'loss': 0.6989, 'grad_norm': 0.9713849425315857, 'learning_rate': 0.0008880555555555557, 'epoch': 0.33611111111111114}
Step 1220: {'loss': 0.712, 'grad_norm': 0.8679209351539612, 'learning_rate': 0.0008871296296296296, 'epoch': 0.3388888888888889}
Step 1230: {'loss': 0.6413, 'grad_norm': 0.931260347366333, 'learning_rate': 0.0008862037037037037, 'epoch': 0.3416666666666667}
Step 1240: {'loss': 0.7499, 'grad_norm': 0.7671078443527222, 'learning_rate': 0.0008852777777777778, 'epoch': 0.34444444444444444}
Step 1250: {'loss': 0.6819, 'grad_norm': 1.0536764860153198, 'learning_rate': 0.0008843518518518519, 'epoch': 0.3472222222222222}
Step 1260: {'loss': 0.7148, 'grad_norm': 0.8113176226615906, 'learning_rate': 0.000883425925925926, 'epoch': 0.35}
Step 1270: {'loss': 0.6224, 'grad_norm': 1.0317027568817139, 'learning_rate': 0.0008824999999999999, 'epoch': 0.3527777777777778}
Step 1280: {'loss': 0.6543, 'grad_norm': 0.7590470314025879, 'learning_rate': 0.0008815740740740741, 'epoch': 0.35555555555555557}
Step 1290: {'loss': 0.6905, 'grad_norm': 0.9285265207290649, 'learning_rate': 0.0008806481481481482, 'epoch': 0.35833333333333334}
Step 1300: {'loss': 0.6909, 'grad_norm': 0.9198168516159058, 'learning_rate': 0.0008797222222222222, 'epoch': 0.3611111111111111}
Step 1310: {'loss': 0.6886, 'grad_norm': 0.9222473502159119, 'learning_rate': 0.0008787962962962964, 'epoch': 0.3638888888888889}
Step 1320: {'loss': 0.7469, 'grad_norm': 0.6956414580345154, 'learning_rate': 0.0008778703703703704, 'epoch': 0.36666666666666664}
Step 1330: {'loss': 0.7031, 'grad_norm': 0.9112398624420166, 'learning_rate': 0.0008769444444444444, 'epoch': 0.36944444444444446}
Step 1340: {'loss': 0.6912, 'grad_norm': 0.8467714190483093, 'learning_rate': 0.0008760185185185186, 'epoch': 0.37222222222222223}
Step 1350: {'loss': 0.708, 'grad_norm': 1.0607943534851074, 'learning_rate': 0.0008750925925925927, 'epoch': 0.375}
Step 1360: {'loss': 0.7171, 'grad_norm': 0.8482407331466675, 'learning_rate': 0.0008741666666666666, 'epoch': 0.37777777777777777}
Step 1370: {'loss': 0.6897, 'grad_norm': 0.9696671962738037, 'learning_rate': 0.0008732407407407407, 'epoch': 0.38055555555555554}
Step 1380: {'loss': 0.7606, 'grad_norm': 0.6786556243896484, 'learning_rate': 0.0008723148148148148, 'epoch': 0.38333333333333336}
Step 1390: {'loss': 0.724, 'grad_norm': 0.936271607875824, 'learning_rate': 0.0008713888888888889, 'epoch': 0.3861111111111111}
Step 1400: {'loss': 0.7941, 'grad_norm': 0.8665652871131897, 'learning_rate': 0.000870462962962963, 'epoch': 0.3888888888888889}
Step 1410: {'loss': 0.6766, 'grad_norm': 0.9571622014045715, 'learning_rate': 0.000869537037037037, 'epoch': 0.39166666666666666}
Step 1420: {'loss': 0.7116, 'grad_norm': 0.7981716394424438, 'learning_rate': 0.0008686111111111111, 'epoch': 0.39444444444444443}
Step 1430: {'loss': 0.6733, 'grad_norm': 0.7893588542938232, 'learning_rate': 0.0008676851851851852, 'epoch': 0.3972222222222222}
Step 1440: {'loss': 0.6744, 'grad_norm': 0.8095383644104004, 'learning_rate': 0.0008667592592592593, 'epoch': 0.4}
Step 1450: {'loss': 0.703, 'grad_norm': 0.836571455001831, 'learning_rate': 0.0008658333333333334, 'epoch': 0.4027777777777778}
Step 1460: {'loss': 0.6325, 'grad_norm': 0.6901267170906067, 'learning_rate': 0.0008649074074074074, 'epoch': 0.40555555555555556}
Step 1470: {'loss': 0.6368, 'grad_norm': 0.9689679145812988, 'learning_rate': 0.0008639814814814815, 'epoch': 0.4083333333333333}
Step 1480: {'loss': 0.7265, 'grad_norm': 0.7726849317550659, 'learning_rate': 0.0008630555555555556, 'epoch': 0.4111111111111111}
Step 1490: {'loss': 0.7014, 'grad_norm': 0.6463975310325623, 'learning_rate': 0.0008621296296296296, 'epoch': 0.41388888888888886}
Step 1500: {'loss': 0.7062, 'grad_norm': 1.0164313316345215, 'learning_rate': 0.0008612037037037038, 'epoch': 0.4166666666666667}
Step 1510: {'loss': 0.7036, 'grad_norm': 0.7203904390335083, 'learning_rate': 0.0008602777777777778, 'epoch': 0.41944444444444445}
Step 1520: {'loss': 0.6904, 'grad_norm': 0.8440589308738708, 'learning_rate': 0.0008593518518518518, 'epoch': 0.4222222222222222}
Step 1530: {'loss': 0.6393, 'grad_norm': 0.9501042366027832, 'learning_rate': 0.0008584259259259259, 'epoch': 0.425}
Step 1540: {'loss': 0.6745, 'grad_norm': 1.0439116954803467, 'learning_rate': 0.0008575000000000001, 'epoch': 0.42777777777777776}
Step 1550: {'loss': 0.656, 'grad_norm': 0.5469887256622314, 'learning_rate': 0.000856574074074074, 'epoch': 0.4305555555555556}
Step 1560: {'loss': 0.7171, 'grad_norm': 0.9001676440238953, 'learning_rate': 0.0008556481481481481, 'epoch': 0.43333333333333335}
Step 1570: {'loss': 0.6943, 'grad_norm': 0.9030395150184631, 'learning_rate': 0.0008547222222222223, 'epoch': 0.4361111111111111}
Step 1580: {'loss': 0.7162, 'grad_norm': 1.1467463970184326, 'learning_rate': 0.0008537962962962963, 'epoch': 0.4388888888888889}
Step 1590: {'loss': 0.6993, 'grad_norm': 1.1946802139282227, 'learning_rate': 0.0008528703703703704, 'epoch': 0.44166666666666665}
Step 1600: {'loss': 0.6664, 'grad_norm': 0.733849048614502, 'learning_rate': 0.0008519444444444445, 'epoch': 0.4444444444444444}
Step 1610: {'loss': 0.6613, 'grad_norm': 0.7737716436386108, 'learning_rate': 0.0008510185185185185, 'epoch': 0.44722222222222224}
Step 1620: {'loss': 0.7033, 'grad_norm': 1.1619197130203247, 'learning_rate': 0.0008500925925925926, 'epoch': 0.45}
Step 1630: {'loss': 0.683, 'grad_norm': 1.0779178142547607, 'learning_rate': 0.0008491666666666667, 'epoch': 0.4527777777777778}
Step 1640: {'loss': 0.712, 'grad_norm': 0.825573205947876, 'learning_rate': 0.0008482407407407408, 'epoch': 0.45555555555555555}
Step 1650: {'loss': 0.7398, 'grad_norm': 0.5772960186004639, 'learning_rate': 0.0008473148148148148, 'epoch': 0.4583333333333333}
Step 1660: {'loss': 0.6927, 'grad_norm': 0.744476854801178, 'learning_rate': 0.0008463888888888889, 'epoch': 0.46111111111111114}
Step 1670: {'loss': 0.6415, 'grad_norm': 0.8541237115859985, 'learning_rate': 0.000845462962962963, 'epoch': 0.4638888888888889}
Step 1680: {'loss': 0.6252, 'grad_norm': 0.6738374829292297, 'learning_rate': 0.0008445370370370371, 'epoch': 0.4666666666666667}
Step 1690: {'loss': 0.6649, 'grad_norm': 1.2844691276550293, 'learning_rate': 0.0008436111111111111, 'epoch': 0.46944444444444444}
Step 1700: {'loss': 0.7167, 'grad_norm': 0.8025264739990234, 'learning_rate': 0.0008426851851851852, 'epoch': 0.4722222222222222}
Step 1710: {'loss': 0.7196, 'grad_norm': 0.89445960521698, 'learning_rate': 0.0008417592592592592, 'epoch': 0.475}
Step 1720: {'loss': 0.6959, 'grad_norm': 0.8562153577804565, 'learning_rate': 0.0008408333333333333, 'epoch': 0.4777777777777778}
Step 1730: {'loss': 0.6841, 'grad_norm': 0.7725589871406555, 'learning_rate': 0.0008399074074074075, 'epoch': 0.48055555555555557}
Step 1740: {'loss': 0.7478, 'grad_norm': 1.0921369791030884, 'learning_rate': 0.0008389814814814815, 'epoch': 0.48333333333333334}
Step 1750: {'loss': 0.6933, 'grad_norm': 0.8325185775756836, 'learning_rate': 0.0008380555555555555, 'epoch': 0.4861111111111111}
Step 1760: {'loss': 0.6936, 'grad_norm': 0.797316312789917, 'learning_rate': 0.0008371296296296297, 'epoch': 0.4888888888888889}
Step 1770: {'loss': 0.6859, 'grad_norm': 1.0323692560195923, 'learning_rate': 0.0008362037037037037, 'epoch': 0.49166666666666664}
Step 1780: {'loss': 0.6981, 'grad_norm': 0.541163980960846, 'learning_rate': 0.0008352777777777778, 'epoch': 0.49444444444444446}
Step 1790: {'loss': 0.6858, 'grad_norm': 1.0141786336898804, 'learning_rate': 0.000834351851851852, 'epoch': 0.49722222222222223}
Step 1800: {'loss': 0.7167, 'grad_norm': 0.9139230847358704, 'learning_rate': 0.0008334259259259259, 'epoch': 0.5}
Step 1810: {'loss': 0.6976, 'grad_norm': 0.5941638350486755, 'learning_rate': 0.0008325, 'epoch': 0.5027777777777778}
Step 1820: {'loss': 0.6388, 'grad_norm': 0.9973217248916626, 'learning_rate': 0.000831574074074074, 'epoch': 0.5055555555555555}
Step 1830: {'loss': 0.6684, 'grad_norm': 0.8174107670783997, 'learning_rate': 0.0008306481481481482, 'epoch': 0.5083333333333333}
Step 1840: {'loss': 0.7925, 'grad_norm': 1.1359399557113647, 'learning_rate': 0.0008297222222222223, 'epoch': 0.5111111111111111}
Step 1850: {'loss': 0.6442, 'grad_norm': 0.8350860476493835, 'learning_rate': 0.0008287962962962962, 'epoch': 0.5138888888888888}
Step 1860: {'loss': 0.7111, 'grad_norm': 0.8281328678131104, 'learning_rate': 0.0008278703703703704, 'epoch': 0.5166666666666667}
Step 1870: {'loss': 0.6437, 'grad_norm': 0.9522933959960938, 'learning_rate': 0.0008269444444444445, 'epoch': 0.5194444444444445}
Step 1880: {'loss': 0.7234, 'grad_norm': 1.0319139957427979, 'learning_rate': 0.0008260185185185185, 'epoch': 0.5222222222222223}
Step 1890: {'loss': 0.6909, 'grad_norm': 0.8113915920257568, 'learning_rate': 0.0008250925925925927, 'epoch': 0.525}
Step 1900: {'loss': 0.6772, 'grad_norm': 0.7912397384643555, 'learning_rate': 0.0008241666666666667, 'epoch': 0.5277777777777778}
Step 1910: {'loss': 0.6853, 'grad_norm': 0.7304309010505676, 'learning_rate': 0.0008232407407407407, 'epoch': 0.5305555555555556}
Step 1920: {'loss': 0.6474, 'grad_norm': 0.8564749360084534, 'learning_rate': 0.0008223148148148149, 'epoch': 0.5333333333333333}
Step 1930: {'loss': 0.6407, 'grad_norm': 0.8567033410072327, 'learning_rate': 0.0008213888888888889, 'epoch': 0.5361111111111111}
Step 1940: {'loss': 0.6784, 'grad_norm': 0.8370447158813477, 'learning_rate': 0.0008204629629629629, 'epoch': 0.5388888888888889}
Step 1950: {'loss': 0.7179, 'grad_norm': 0.8003085255622864, 'learning_rate': 0.0008195370370370371, 'epoch': 0.5416666666666666}
Step 1960: {'loss': 0.652, 'grad_norm': 0.6814853549003601, 'learning_rate': 0.0008186111111111111, 'epoch': 0.5444444444444444}
Step 1970: {'loss': 0.6492, 'grad_norm': 0.6098477840423584, 'learning_rate': 0.0008176851851851852, 'epoch': 0.5472222222222223}
Step 1980: {'loss': 0.625, 'grad_norm': 0.6909766793251038, 'learning_rate': 0.0008167592592592593, 'epoch': 0.55}
Step 1990: {'loss': 0.6375, 'grad_norm': 0.9273913502693176, 'learning_rate': 0.0008158333333333333, 'epoch': 0.5527777777777778}
Step 2000: {'loss': 0.7132, 'grad_norm': 1.2538963556289673, 'learning_rate': 0.0008149074074074074, 'epoch': 0.5555555555555556}
Step 2010: {'loss': 0.6722, 'grad_norm': 0.8182616829872131, 'learning_rate': 0.0008139814814814815, 'epoch': 0.5583333333333333}
Step 2020: {'loss': 0.692, 'grad_norm': 0.8736388683319092, 'learning_rate': 0.0008130555555555556, 'epoch': 0.5611111111111111}
Step 2030: {'loss': 0.6647, 'grad_norm': 0.8339471817016602, 'learning_rate': 0.0008121296296296297, 'epoch': 0.5638888888888889}
Step 2040: {'loss': 0.7169, 'grad_norm': 0.7035650014877319, 'learning_rate': 0.0008112037037037036, 'epoch': 0.5666666666666667}
Step 2050: {'loss': 0.7278, 'grad_norm': 1.0696932077407837, 'learning_rate': 0.0008102777777777778, 'epoch': 0.5694444444444444}
Step 2060: {'loss': 0.6694, 'grad_norm': 0.7773990035057068, 'learning_rate': 0.0008093518518518519, 'epoch': 0.5722222222222222}
Step 2070: {'loss': 0.6827, 'grad_norm': 0.8075864315032959, 'learning_rate': 0.0008084259259259259, 'epoch': 0.575}
Step 2080: {'loss': 0.6522, 'grad_norm': 0.529029905796051, 'learning_rate': 0.0008075000000000001, 'epoch': 0.5777777777777777}
Step 2090: {'loss': 0.6504, 'grad_norm': 0.7407407760620117, 'learning_rate': 0.0008065740740740741, 'epoch': 0.5805555555555556}
Step 2100: {'loss': 0.658, 'grad_norm': 0.8342859745025635, 'learning_rate': 0.0008056481481481481, 'epoch': 0.5833333333333334}
Step 2110: {'loss': 0.7326, 'grad_norm': 1.0816569328308105, 'learning_rate': 0.0008047222222222223, 'epoch': 0.5861111111111111}
Step 2120: {'loss': 0.7241, 'grad_norm': 0.6651383638381958, 'learning_rate': 0.0008037962962962964, 'epoch': 0.5888888888888889}
Step 2130: {'loss': 0.7316, 'grad_norm': 0.8995113968849182, 'learning_rate': 0.0008028703703703703, 'epoch': 0.5916666666666667}
Step 2140: {'loss': 0.6274, 'grad_norm': 0.8805766701698303, 'learning_rate': 0.0008019444444444444, 'epoch': 0.5944444444444444}
Step 2150: {'loss': 0.7107, 'grad_norm': 0.6865440607070923, 'learning_rate': 0.0008010185185185185, 'epoch': 0.5972222222222222}
Step 2160: {'loss': 0.673, 'grad_norm': 0.677648663520813, 'learning_rate': 0.0008000925925925926, 'epoch': 0.6}
Step 2170: {'loss': 0.67, 'grad_norm': 0.6228123903274536, 'learning_rate': 0.0007991666666666667, 'epoch': 0.6027777777777777}
Step 2180: {'loss': 0.6752, 'grad_norm': 0.9210924506187439, 'learning_rate': 0.0007982407407407407, 'epoch': 0.6055555555555555}
Step 2190: {'loss': 0.7381, 'grad_norm': 0.8251059055328369, 'learning_rate': 0.0007973148148148148, 'epoch': 0.6083333333333333}
Step 2200: {'loss': 0.7111, 'grad_norm': 0.8085007667541504, 'learning_rate': 0.0007963888888888889, 'epoch': 0.6111111111111112}
Step 2210: {'loss': 0.6762, 'grad_norm': 0.6548811793327332, 'learning_rate': 0.000795462962962963, 'epoch': 0.6138888888888889}
Step 2220: {'loss': 0.6893, 'grad_norm': 0.7476052641868591, 'learning_rate': 0.0007945370370370371, 'epoch': 0.6166666666666667}
Step 2230: {'loss': 0.6977, 'grad_norm': 0.8826360702514648, 'learning_rate': 0.0007936111111111111, 'epoch': 0.6194444444444445}
Step 2240: {'loss': 0.6785, 'grad_norm': 0.48438313603401184, 'learning_rate': 0.0007926851851851852, 'epoch': 0.6222222222222222}
Step 2250: {'loss': 0.6585, 'grad_norm': 0.7510474920272827, 'learning_rate': 0.0007917592592592593, 'epoch': 0.625}
Step 2260: {'loss': 0.6972, 'grad_norm': 0.6487661600112915, 'learning_rate': 0.0007908333333333334, 'epoch': 0.6277777777777778}
Step 2270: {'loss': 0.614, 'grad_norm': 0.9229776859283447, 'learning_rate': 0.0007899074074074074, 'epoch': 0.6305555555555555}
Step 2280: {'loss': 0.6773, 'grad_norm': 0.9189867377281189, 'learning_rate': 0.0007889814814814815, 'epoch': 0.6333333333333333}
Step 2290: {'loss': 0.7064, 'grad_norm': 0.7389242649078369, 'learning_rate': 0.0007880555555555555, 'epoch': 0.6361111111111111}
Step 2300: {'loss': 0.6847, 'grad_norm': 0.6427311301231384, 'learning_rate': 0.0007871296296296296, 'epoch': 0.6388888888888888}
Step 2310: {'loss': 0.6846, 'grad_norm': 0.7678201198577881, 'learning_rate': 0.0007862037037037038, 'epoch': 0.6416666666666667}
Step 2320: {'loss': 0.6688, 'grad_norm': 0.7449121475219727, 'learning_rate': 0.0007852777777777778, 'epoch': 0.6444444444444445}
Step 2330: {'loss': 0.6261, 'grad_norm': 0.6285765767097473, 'learning_rate': 0.0007843518518518518, 'epoch': 0.6472222222222223}
Step 2340: {'loss': 0.693, 'grad_norm': 0.966930091381073, 'learning_rate': 0.000783425925925926, 'epoch': 0.65}
Step 2350: {'loss': 0.6586, 'grad_norm': 0.637708842754364, 'learning_rate': 0.0007825, 'epoch': 0.6527777777777778}
Step 2360: {'loss': 0.6877, 'grad_norm': 0.8814504742622375, 'learning_rate': 0.0007815740740740741, 'epoch': 0.6555555555555556}
Step 2370: {'loss': 0.7009, 'grad_norm': 0.7636117339134216, 'learning_rate': 0.0007806481481481483, 'epoch': 0.6583333333333333}
Step 2380: {'loss': 0.654, 'grad_norm': 0.8890866637229919, 'learning_rate': 0.0007797222222222222, 'epoch': 0.6611111111111111}
Step 2390: {'loss': 0.602, 'grad_norm': 0.9517114162445068, 'learning_rate': 0.0007787962962962963, 'epoch': 0.6638888888888889}
Step 2400: {'loss': 0.6769, 'grad_norm': 0.8763775825500488, 'learning_rate': 0.0007778703703703704, 'epoch': 0.6666666666666666}
Step 2410: {'loss': 0.6774, 'grad_norm': 0.9389059543609619, 'learning_rate': 0.0007769444444444445, 'epoch': 0.6694444444444444}
Step 2420: {'loss': 0.6981, 'grad_norm': 0.8260912895202637, 'learning_rate': 0.0007760185185185186, 'epoch': 0.6722222222222223}
Step 2430: {'loss': 0.6537, 'grad_norm': 1.1356886625289917, 'learning_rate': 0.0007750925925925925, 'epoch': 0.675}
Step 2440: {'loss': 0.6354, 'grad_norm': 1.406122088432312, 'learning_rate': 0.0007741666666666667, 'epoch': 0.6777777777777778}
Step 2450: {'loss': 0.6627, 'grad_norm': 0.937326967716217, 'learning_rate': 0.0007732407407407408, 'epoch': 0.6805555555555556}
Step 2460: {'loss': 0.6474, 'grad_norm': 1.1104679107666016, 'learning_rate': 0.0007723148148148148, 'epoch': 0.6833333333333333}
Step 2470: {'loss': 0.6442, 'grad_norm': 0.8476929068565369, 'learning_rate': 0.000771388888888889, 'epoch': 0.6861111111111111}
Step 2480: {'loss': 0.6381, 'grad_norm': 0.6749787330627441, 'learning_rate': 0.000770462962962963, 'epoch': 0.6888888888888889}
Step 2490: {'loss': 0.7534, 'grad_norm': 0.7528630495071411, 'learning_rate': 0.000769537037037037, 'epoch': 0.6916666666666667}
Step 2500: {'loss': 0.6599, 'grad_norm': 0.8061642646789551, 'learning_rate': 0.0007686111111111112, 'epoch': 0.6944444444444444}
Step 2510: {'loss': 0.6207, 'grad_norm': 0.7472395300865173, 'learning_rate': 0.0007676851851851852, 'epoch': 0.6972222222222222}
Step 2520: {'loss': 0.7267, 'grad_norm': 1.2436944246292114, 'learning_rate': 0.0007667592592592592, 'epoch': 0.7}
Step 2530: {'loss': 0.7011, 'grad_norm': 1.02713143825531, 'learning_rate': 0.0007658333333333334, 'epoch': 0.7027777777777777}
Step 2540: {'loss': 0.7076, 'grad_norm': 0.8127556443214417, 'learning_rate': 0.0007649074074074074, 'epoch': 0.7055555555555556}
Step 2550: {'loss': 0.6755, 'grad_norm': 0.790130078792572, 'learning_rate': 0.0007639814814814815, 'epoch': 0.7083333333333334}
Step 2560: {'loss': 0.7074, 'grad_norm': 1.1339976787567139, 'learning_rate': 0.0007630555555555557, 'epoch': 0.7111111111111111}
Step 2570: {'loss': 0.6651, 'grad_norm': 0.7098342180252075, 'learning_rate': 0.0007621296296296296, 'epoch': 0.7138888888888889}
Step 2580: {'loss': 0.6187, 'grad_norm': 0.670978844165802, 'learning_rate': 0.0007612037037037037, 'epoch': 0.7166666666666667}
Step 2590: {'loss': 0.6637, 'grad_norm': 1.2447218894958496, 'learning_rate': 0.0007602777777777778, 'epoch': 0.7194444444444444}
Step 2600: {'loss': 0.6595, 'grad_norm': 1.8707138299942017, 'learning_rate': 0.0007593518518518519, 'epoch': 0.7222222222222222}
Step 2610: {'loss': 0.6385, 'grad_norm': 0.8858618140220642, 'learning_rate': 0.000758425925925926, 'epoch': 0.725}
Step 2620: {'loss': 0.5808, 'grad_norm': 1.0731604099273682, 'learning_rate': 0.0007574999999999999, 'epoch': 0.7277777777777777}
Step 2630: {'loss': 0.67, 'grad_norm': 0.6042941212654114, 'learning_rate': 0.0007565740740740741, 'epoch': 0.7305555555555555}
Step 2640: {'loss': 0.6929, 'grad_norm': 0.7615246772766113, 'learning_rate': 0.0007556481481481482, 'epoch': 0.7333333333333333}
Step 2650: {'loss': 0.6665, 'grad_norm': 0.8294219374656677, 'learning_rate': 0.0007547222222222222, 'epoch': 0.7361111111111112}
Step 2660: {'loss': 0.6227, 'grad_norm': 0.9022091627120972, 'learning_rate': 0.0007537962962962964, 'epoch': 0.7388888888888889}
Step 2670: {'loss': 0.6906, 'grad_norm': 0.7582359910011292, 'learning_rate': 0.0007528703703703704, 'epoch': 0.7416666666666667}
Step 2680: {'loss': 0.6867, 'grad_norm': 0.6809619069099426, 'learning_rate': 0.0007519444444444444, 'epoch': 0.7444444444444445}
Step 2690: {'loss': 0.6843, 'grad_norm': 0.8401224613189697, 'learning_rate': 0.0007510185185185186, 'epoch': 0.7472222222222222}
Step 2700: {'loss': 0.716, 'grad_norm': 0.838631808757782, 'learning_rate': 0.0007500925925925927, 'epoch': 0.75}
Step 2710: {'loss': 0.7794, 'grad_norm': 0.852379560470581, 'learning_rate': 0.0007491666666666666, 'epoch': 0.7527777777777778}
Step 2720: {'loss': 0.6966, 'grad_norm': 0.578640341758728, 'learning_rate': 0.0007482407407407407, 'epoch': 0.7555555555555555}
Step 2730: {'loss': 0.6947, 'grad_norm': 1.1447043418884277, 'learning_rate': 0.0007473148148148148, 'epoch': 0.7583333333333333}
Step 2740: {'loss': 0.6333, 'grad_norm': 0.8034425377845764, 'learning_rate': 0.0007463888888888889, 'epoch': 0.7611111111111111}
Step 2750: {'loss': 0.6787, 'grad_norm': 0.7218250036239624, 'learning_rate': 0.000745462962962963, 'epoch': 0.7638888888888888}
Step 2760: {'loss': 0.7029, 'grad_norm': 0.8103365898132324, 'learning_rate': 0.000744537037037037, 'epoch': 0.7666666666666667}
Step 2770: {'loss': 0.6654, 'grad_norm': 0.5711478590965271, 'learning_rate': 0.0007436111111111111, 'epoch': 0.7694444444444445}
Step 2780: {'loss': 0.6756, 'grad_norm': 0.6830101013183594, 'learning_rate': 0.0007426851851851852, 'epoch': 0.7722222222222223}
Step 2790: {'loss': 0.6522, 'grad_norm': 1.044830083847046, 'learning_rate': 0.0007417592592592593, 'epoch': 0.775}
Step 2800: {'loss': 0.6775, 'grad_norm': 0.8648367524147034, 'learning_rate': 0.0007408333333333334, 'epoch': 0.7777777777777778}
Step 2810: {'loss': 0.619, 'grad_norm': 0.731927752494812, 'learning_rate': 0.0007399074074074074, 'epoch': 0.7805555555555556}
Step 2820: {'loss': 0.7238, 'grad_norm': 0.6753863096237183, 'learning_rate': 0.0007389814814814815, 'epoch': 0.7833333333333333}
Step 2830: {'loss': 0.6737, 'grad_norm': 1.1416605710983276, 'learning_rate': 0.0007380555555555556, 'epoch': 0.7861111111111111}
Step 2840: {'loss': 0.7112, 'grad_norm': 0.8685232400894165, 'learning_rate': 0.0007371296296296296, 'epoch': 0.7888888888888889}
Step 2850: {'loss': 0.6316, 'grad_norm': 0.7710384726524353, 'learning_rate': 0.0007362037037037038, 'epoch': 0.7916666666666666}
Step 2860: {'loss': 0.6783, 'grad_norm': 0.8660411834716797, 'learning_rate': 0.0007352777777777778, 'epoch': 0.7944444444444444}
Step 2870: {'loss': 0.6449, 'grad_norm': 1.0011610984802246, 'learning_rate': 0.0007343518518518518, 'epoch': 0.7972222222222223}
Step 2880: {'loss': 0.6429, 'grad_norm': 0.9081866145133972, 'learning_rate': 0.0007334259259259259, 'epoch': 0.8}
Step 2890: {'loss': 0.6856, 'grad_norm': 1.26024329662323, 'learning_rate': 0.0007325000000000001, 'epoch': 0.8027777777777778}
Step 2900: {'loss': 0.7507, 'grad_norm': 0.8061937689781189, 'learning_rate': 0.000731574074074074, 'epoch': 0.8055555555555556}
Step 2910: {'loss': 0.7399, 'grad_norm': 1.1111164093017578, 'learning_rate': 0.0007306481481481481, 'epoch': 0.8083333333333333}
Step 2920: {'loss': 0.6577, 'grad_norm': 0.6415600180625916, 'learning_rate': 0.0007297222222222223, 'epoch': 0.8111111111111111}
Step 2930: {'loss': 0.6411, 'grad_norm': 0.8814072012901306, 'learning_rate': 0.0007287962962962963, 'epoch': 0.8138888888888889}
Step 2940: {'loss': 0.6394, 'grad_norm': 0.7201979756355286, 'learning_rate': 0.0007278703703703704, 'epoch': 0.8166666666666667}
Step 2950: {'loss': 0.6336, 'grad_norm': 0.6112795472145081, 'learning_rate': 0.0007269444444444444, 'epoch': 0.8194444444444444}
Step 2960: {'loss': 0.7631, 'grad_norm': 0.8614532947540283, 'learning_rate': 0.0007260185185185185, 'epoch': 0.8222222222222222}
Step 2970: {'loss': 0.6567, 'grad_norm': 0.9692746996879578, 'learning_rate': 0.0007250925925925926, 'epoch': 0.825}
Step 2980: {'loss': 0.6882, 'grad_norm': 0.9216079711914062, 'learning_rate': 0.0007241666666666667, 'epoch': 0.8277777777777777}
Step 2990: {'loss': 0.667, 'grad_norm': 0.7451750040054321, 'learning_rate': 0.0007232407407407408, 'epoch': 0.8305555555555556}
Step 3000: {'loss': 0.7081, 'grad_norm': 0.6608121395111084, 'learning_rate': 0.0007223148148148148, 'epoch': 0.8333333333333334}
Step 3010: {'loss': 0.6218, 'grad_norm': 1.236409306526184, 'learning_rate': 0.0007213888888888889, 'epoch': 0.8361111111111111}
Step 3020: {'loss': 0.65, 'grad_norm': 1.1464053392410278, 'learning_rate': 0.000720462962962963, 'epoch': 0.8388888888888889}
Step 3030: {'loss': 0.6734, 'grad_norm': 0.9375758171081543, 'learning_rate': 0.0007195370370370371, 'epoch': 0.8416666666666667}
Step 3040: {'loss': 0.6334, 'grad_norm': 1.0821932554244995, 'learning_rate': 0.0007186111111111111, 'epoch': 0.8444444444444444}
Step 3050: {'loss': 0.641, 'grad_norm': 0.7025777101516724, 'learning_rate': 0.0007176851851851852, 'epoch': 0.8472222222222222}
Step 3060: {'loss': 0.649, 'grad_norm': 0.7106415629386902, 'learning_rate': 0.0007167592592592592, 'epoch': 0.85}
Step 3070: {'loss': 0.6554, 'grad_norm': 0.9013742208480835, 'learning_rate': 0.0007158333333333333, 'epoch': 0.8527777777777777}
Step 3080: {'loss': 0.6818, 'grad_norm': 0.7096195220947266, 'learning_rate': 0.0007149074074074075, 'epoch': 0.8555555555555555}
Step 3090: {'loss': 0.671, 'grad_norm': 0.6662688851356506, 'learning_rate': 0.0007139814814814815, 'epoch': 0.8583333333333333}
Step 3100: {'loss': 0.6856, 'grad_norm': 0.9373258948326111, 'learning_rate': 0.0007130555555555555, 'epoch': 0.8611111111111112}
Step 3110: {'loss': 0.651, 'grad_norm': 0.8544309139251709, 'learning_rate': 0.0007121296296296297, 'epoch': 0.8638888888888889}
Step 3120: {'loss': 0.6977, 'grad_norm': 0.8205552697181702, 'learning_rate': 0.0007112037037037037, 'epoch': 0.8666666666666667}
Step 3130: {'loss': 0.6792, 'grad_norm': 0.6242661476135254, 'learning_rate': 0.0007102777777777778, 'epoch': 0.8694444444444445}
Step 3140: {'loss': 0.6814, 'grad_norm': 1.0161932706832886, 'learning_rate': 0.000709351851851852, 'epoch': 0.8722222222222222}
Step 3150: {'loss': 0.6607, 'grad_norm': 0.9396870136260986, 'learning_rate': 0.0007084259259259259, 'epoch': 0.875}
Step 3160: {'loss': 0.6695, 'grad_norm': 0.5871210694313049, 'learning_rate': 0.0007075, 'epoch': 0.8777777777777778}
Step 3170: {'loss': 0.7234, 'grad_norm': 0.5081318020820618, 'learning_rate': 0.000706574074074074, 'epoch': 0.8805555555555555}
Step 3180: {'loss': 0.6337, 'grad_norm': 0.7840694189071655, 'learning_rate': 0.0007056481481481482, 'epoch': 0.8833333333333333}
Step 3190: {'loss': 0.638, 'grad_norm': 0.9218876957893372, 'learning_rate': 0.0007047222222222223, 'epoch': 0.8861111111111111}
Step 3200: {'loss': 0.667, 'grad_norm': 1.020723581314087, 'learning_rate': 0.0007037962962962962, 'epoch': 0.8888888888888888}
Step 3210: {'loss': 0.7089, 'grad_norm': 0.8794745802879333, 'learning_rate': 0.0007028703703703704, 'epoch': 0.8916666666666667}
Step 3220: {'loss': 0.6785, 'grad_norm': 0.7994971871376038, 'learning_rate': 0.0007019444444444445, 'epoch': 0.8944444444444445}
Step 3230: {'loss': 0.6, 'grad_norm': 1.1277843713760376, 'learning_rate': 0.0007010185185185185, 'epoch': 0.8972222222222223}
Step 3240: {'loss': 0.6213, 'grad_norm': 0.7815968990325928, 'learning_rate': 0.0007000925925925926, 'epoch': 0.9}
Step 3250: {'loss': 0.6763, 'grad_norm': 1.1071186065673828, 'learning_rate': 0.0006991666666666667, 'epoch': 0.9027777777777778}
Step 3260: {'loss': 0.7147, 'grad_norm': 0.8119691610336304, 'learning_rate': 0.0006982407407407407, 'epoch': 0.9055555555555556}
Step 3270: {'loss': 0.6605, 'grad_norm': 0.952329158782959, 'learning_rate': 0.0006973148148148149, 'epoch': 0.9083333333333333}
Step 3280: {'loss': 0.7563, 'grad_norm': 0.8626884818077087, 'learning_rate': 0.0006963888888888889, 'epoch': 0.9111111111111111}
Step 3290: {'loss': 0.7165, 'grad_norm': 0.701650083065033, 'learning_rate': 0.0006954629629629629, 'epoch': 0.9138888888888889}
Step 3300: {'loss': 0.6775, 'grad_norm': 0.6875571012496948, 'learning_rate': 0.0006945370370370371, 'epoch': 0.9166666666666666}
Step 3310: {'loss': 0.7184, 'grad_norm': 1.0268772840499878, 'learning_rate': 0.0006936111111111111, 'epoch': 0.9194444444444444}
Step 3320: {'loss': 0.6533, 'grad_norm': 0.66936194896698, 'learning_rate': 0.0006926851851851852, 'epoch': 0.9222222222222223}
Step 3330: {'loss': 0.6519, 'grad_norm': 0.7280157208442688, 'learning_rate': 0.0006917592592592593, 'epoch': 0.925}
Step 3340: {'loss': 0.6315, 'grad_norm': 0.6538472175598145, 'learning_rate': 0.0006908333333333333, 'epoch': 0.9277777777777778}
Step 3350: {'loss': 0.6727, 'grad_norm': 0.915672779083252, 'learning_rate': 0.0006899074074074074, 'epoch': 0.9305555555555556}
Step 3360: {'loss': 0.7164, 'grad_norm': 0.5858809947967529, 'learning_rate': 0.0006889814814814815, 'epoch': 0.9333333333333333}
Step 3370: {'loss': 0.6916, 'grad_norm': 0.965141773223877, 'learning_rate': 0.0006880555555555556, 'epoch': 0.9361111111111111}
Step 3380: {'loss': 0.6036, 'grad_norm': 0.8176212310791016, 'learning_rate': 0.0006871296296296297, 'epoch': 0.9388888888888889}
Step 3390: {'loss': 0.648, 'grad_norm': 0.5799514651298523, 'learning_rate': 0.0006862037037037036, 'epoch': 0.9416666666666667}
Step 3400: {'loss': 0.7196, 'grad_norm': 0.9002702236175537, 'learning_rate': 0.0006852777777777778, 'epoch': 0.9444444444444444}
Step 3410: {'loss': 0.7109, 'grad_norm': 0.900326132774353, 'learning_rate': 0.0006843518518518519, 'epoch': 0.9472222222222222}
Step 3420: {'loss': 0.6708, 'grad_norm': 0.8400514721870422, 'learning_rate': 0.0006834259259259259, 'epoch': 0.95}
Step 3430: {'loss': 0.6488, 'grad_norm': 0.7998694181442261, 'learning_rate': 0.0006825000000000001, 'epoch': 0.9527777777777777}
Step 3440: {'loss': 0.7252, 'grad_norm': 1.0283623933792114, 'learning_rate': 0.0006815740740740741, 'epoch': 0.9555555555555556}
Step 3450: {'loss': 0.6315, 'grad_norm': 1.1121437549591064, 'learning_rate': 0.0006806481481481481, 'epoch': 0.9583333333333334}
Step 3460: {'loss': 0.6845, 'grad_norm': 0.9159902334213257, 'learning_rate': 0.0006797222222222223, 'epoch': 0.9611111111111111}
Step 3470: {'loss': 0.6141, 'grad_norm': 0.5951553583145142, 'learning_rate': 0.0006787962962962964, 'epoch': 0.9638888888888889}
Step 3480: {'loss': 0.627, 'grad_norm': 0.6755774021148682, 'learning_rate': 0.0006778703703703703, 'epoch': 0.9666666666666667}
Step 3490: {'loss': 0.6861, 'grad_norm': 0.9305242300033569, 'learning_rate': 0.0006769444444444444, 'epoch': 0.9694444444444444}
Step 3500: {'loss': 0.6698, 'grad_norm': 1.1216723918914795, 'learning_rate': 0.0006760185185185185, 'epoch': 0.9722222222222222}
Step 3510: {'loss': 0.6873, 'grad_norm': 0.781022310256958, 'learning_rate': 0.0006750925925925926, 'epoch': 0.975}
Step 3520: {'loss': 0.7385, 'grad_norm': 0.6395853757858276, 'learning_rate': 0.0006741666666666667, 'epoch': 0.9777777777777777}
Step 3530: {'loss': 0.7197, 'grad_norm': 1.1706647872924805, 'learning_rate': 0.0006732407407407407, 'epoch': 0.9805555555555555}
Step 3540: {'loss': 0.645, 'grad_norm': 1.0363777875900269, 'learning_rate': 0.0006723148148148148, 'epoch': 0.9833333333333333}
Step 3550: {'loss': 0.6541, 'grad_norm': 0.804283618927002, 'learning_rate': 0.0006713888888888889, 'epoch': 0.9861111111111112}
Step 3560: {'loss': 0.6295, 'grad_norm': 0.9420791268348694, 'learning_rate': 0.000670462962962963, 'epoch': 0.9888888888888889}
Step 3570: {'loss': 0.6579, 'grad_norm': 0.7115967869758606, 'learning_rate': 0.0006695370370370371, 'epoch': 0.9916666666666667}
Step 3580: {'loss': 0.6837, 'grad_norm': 1.0584174394607544, 'learning_rate': 0.0006686111111111111, 'epoch': 0.9944444444444445}
Step 3590: {'loss': 0.6726, 'grad_norm': 0.8890631794929504, 'learning_rate': 0.0006676851851851852, 'epoch': 0.9972222222222222}
Step 3600: {'loss': 0.7153, 'grad_norm': 0.7633700966835022, 'learning_rate': 0.0006667592592592593, 'epoch': 1.0}
Step 3610: {'loss': 0.6769, 'grad_norm': 0.8012357950210571, 'learning_rate': 0.0006658333333333334, 'epoch': 1.0027777777777778}
Step 3620: {'loss': 0.6955, 'grad_norm': 0.8593760132789612, 'learning_rate': 0.0006649074074074074, 'epoch': 1.0055555555555555}
Step 3630: {'loss': 0.6837, 'grad_norm': 1.052349328994751, 'learning_rate': 0.0006639814814814815, 'epoch': 1.0083333333333333}
Step 3640: {'loss': 0.6144, 'grad_norm': 0.8060346245765686, 'learning_rate': 0.0006630555555555555, 'epoch': 1.011111111111111}
Step 3650: {'loss': 0.6894, 'grad_norm': 0.9091729521751404, 'learning_rate': 0.0006621296296296296, 'epoch': 1.0138888888888888}
Step 3660: {'loss': 0.7269, 'grad_norm': 0.9605087637901306, 'learning_rate': 0.0006612037037037038, 'epoch': 1.0166666666666666}
Step 3670: {'loss': 0.6779, 'grad_norm': 1.0500035285949707, 'learning_rate': 0.0006602777777777778, 'epoch': 1.0194444444444444}
Step 3680: {'loss': 0.7014, 'grad_norm': 0.7277320027351379, 'learning_rate': 0.0006593518518518518, 'epoch': 1.0222222222222221}
Step 3690: {'loss': 0.6977, 'grad_norm': 0.7897562384605408, 'learning_rate': 0.000658425925925926, 'epoch': 1.025}
Step 3700: {'loss': 0.6679, 'grad_norm': 1.0681993961334229, 'learning_rate': 0.0006575, 'epoch': 1.0277777777777777}
Step 3710: {'loss': 0.6736, 'grad_norm': 0.9829151034355164, 'learning_rate': 0.0006565740740740741, 'epoch': 1.0305555555555554}
Step 3720: {'loss': 0.6691, 'grad_norm': 0.8468822240829468, 'learning_rate': 0.0006556481481481483, 'epoch': 1.0333333333333334}
Step 3730: {'loss': 0.6316, 'grad_norm': 0.4698442816734314, 'learning_rate': 0.0006547222222222222, 'epoch': 1.0361111111111112}
Step 3740: {'loss': 0.6072, 'grad_norm': 0.9127992391586304, 'learning_rate': 0.0006537962962962963, 'epoch': 1.038888888888889}
Step 3750: {'loss': 0.7177, 'grad_norm': 0.9243505001068115, 'learning_rate': 0.0006528703703703704, 'epoch': 1.0416666666666667}
Step 3760: {'loss': 0.677, 'grad_norm': 0.9450393915176392, 'learning_rate': 0.0006519444444444445, 'epoch': 1.0444444444444445}
Step 3770: {'loss': 0.6281, 'grad_norm': 0.6861655712127686, 'learning_rate': 0.0006510185185185185, 'epoch': 1.0472222222222223}
Step 3780: {'loss': 0.6659, 'grad_norm': 0.5636876821517944, 'learning_rate': 0.0006500925925925925, 'epoch': 1.05}
Step 3790: {'loss': 0.633, 'grad_norm': 1.4105453491210938, 'learning_rate': 0.0006491666666666667, 'epoch': 1.0527777777777778}
Step 3800: {'loss': 0.6339, 'grad_norm': 0.7840680480003357, 'learning_rate': 0.0006482407407407408, 'epoch': 1.0555555555555556}
Step 3810: {'loss': 0.6892, 'grad_norm': 0.8549970388412476, 'learning_rate': 0.0006473148148148148, 'epoch': 1.0583333333333333}
Step 3820: {'loss': 0.7345, 'grad_norm': 0.7710251808166504, 'learning_rate': 0.000646388888888889, 'epoch': 1.0611111111111111}
Step 3830: {'loss': 0.5913, 'grad_norm': 0.7447915077209473, 'learning_rate': 0.000645462962962963, 'epoch': 1.0638888888888889}
Step 3840: {'loss': 0.6497, 'grad_norm': 1.0246039628982544, 'learning_rate': 0.000644537037037037, 'epoch': 1.0666666666666667}
Step 3850: {'loss': 0.6912, 'grad_norm': 1.0091071128845215, 'learning_rate': 0.0006436111111111112, 'epoch': 1.0694444444444444}
Step 3860: {'loss': 0.6875, 'grad_norm': 0.8032788038253784, 'learning_rate': 0.0006426851851851852, 'epoch': 1.0722222222222222}
Step 3870: {'loss': 0.7063, 'grad_norm': 0.9453672170639038, 'learning_rate': 0.0006417592592592592, 'epoch': 1.075}
Step 3880: {'loss': 0.6929, 'grad_norm': 0.9810822606086731, 'learning_rate': 0.0006408333333333334, 'epoch': 1.0777777777777777}
Step 3890: {'loss': 0.6411, 'grad_norm': 0.9158861041069031, 'learning_rate': 0.0006399074074074074, 'epoch': 1.0805555555555555}
Step 3900: {'loss': 0.7194, 'grad_norm': 0.6153334975242615, 'learning_rate': 0.0006389814814814815, 'epoch': 1.0833333333333333}
Step 3910: {'loss': 0.6552, 'grad_norm': 0.7933943271636963, 'learning_rate': 0.0006380555555555557, 'epoch': 1.086111111111111}
Step 3920: {'loss': 0.7223, 'grad_norm': 0.5456054210662842, 'learning_rate': 0.0006371296296296296, 'epoch': 1.0888888888888888}
Step 3930: {'loss': 0.6604, 'grad_norm': 0.8346977233886719, 'learning_rate': 0.0006362037037037037, 'epoch': 1.0916666666666666}
Step 3940: {'loss': 0.5911, 'grad_norm': 0.8889461755752563, 'learning_rate': 0.0006352777777777778, 'epoch': 1.0944444444444446}
Step 3950: {'loss': 0.6742, 'grad_norm': 0.7215882539749146, 'learning_rate': 0.0006343518518518519, 'epoch': 1.0972222222222223}
Step 3960: {'loss': 0.7258, 'grad_norm': 0.9262710213661194, 'learning_rate': 0.000633425925925926, 'epoch': 1.1}
Step 3970: {'loss': 0.6482, 'grad_norm': 0.7411662936210632, 'learning_rate': 0.0006324999999999999, 'epoch': 1.1027777777777779}
Step 3980: {'loss': 0.629, 'grad_norm': 1.099053144454956, 'learning_rate': 0.0006315740740740741, 'epoch': 1.1055555555555556}
Step 3990: {'loss': 0.6617, 'grad_norm': 0.9512221813201904, 'learning_rate': 0.0006306481481481482, 'epoch': 1.1083333333333334}
Step 4000: {'loss': 0.7, 'grad_norm': 0.9373720288276672, 'learning_rate': 0.0006297222222222222, 'epoch': 1.1111111111111112}
Step 4010: {'loss': 0.6395, 'grad_norm': 0.6966601610183716, 'learning_rate': 0.0006287962962962964, 'epoch': 1.113888888888889}
Step 4020: {'loss': 0.6741, 'grad_norm': 0.8329615592956543, 'learning_rate': 0.0006278703703703704, 'epoch': 1.1166666666666667}
Step 4030: {'loss': 0.6813, 'grad_norm': 1.0705628395080566, 'learning_rate': 0.0006269444444444444, 'epoch': 1.1194444444444445}
Step 4040: {'loss': 0.6661, 'grad_norm': 0.7066503167152405, 'learning_rate': 0.0006260185185185186, 'epoch': 1.1222222222222222}
Step 4050: {'loss': 0.664, 'grad_norm': 0.7140580415725708, 'learning_rate': 0.0006250925925925927, 'epoch': 1.125}
Step 4060: {'loss': 0.6611, 'grad_norm': 1.1443275213241577, 'learning_rate': 0.0006241666666666666, 'epoch': 1.1277777777777778}
Step 4070: {'loss': 0.6995, 'grad_norm': 0.8026610612869263, 'learning_rate': 0.0006232407407407407, 'epoch': 1.1305555555555555}
Step 4080: {'loss': 0.7123, 'grad_norm': 0.7138727903366089, 'learning_rate': 0.0006223148148148148, 'epoch': 1.1333333333333333}
Step 4090: {'loss': 0.6575, 'grad_norm': 0.9055011868476868, 'learning_rate': 0.0006213888888888889, 'epoch': 1.136111111111111}
Step 4100: {'loss': 0.609, 'grad_norm': 0.6901528239250183, 'learning_rate': 0.000620462962962963, 'epoch': 1.1388888888888888}
Step 4110: {'loss': 0.7049, 'grad_norm': 0.8768789768218994, 'learning_rate': 0.000619537037037037, 'epoch': 1.1416666666666666}
Step 4120: {'loss': 0.6649, 'grad_norm': 0.5124434232711792, 'learning_rate': 0.0006186111111111111, 'epoch': 1.1444444444444444}
Step 4130: {'loss': 0.6842, 'grad_norm': 1.25178062915802, 'learning_rate': 0.0006176851851851852, 'epoch': 1.1472222222222221}
Step 4140: {'loss': 0.6412, 'grad_norm': 0.6875889301300049, 'learning_rate': 0.0006167592592592593, 'epoch': 1.15}
Step 4150: {'loss': 0.6804, 'grad_norm': 1.0612276792526245, 'learning_rate': 0.0006158333333333334, 'epoch': 1.1527777777777777}
Step 4160: {'loss': 0.6151, 'grad_norm': 0.8807951211929321, 'learning_rate': 0.0006149074074074074, 'epoch': 1.1555555555555554}
Step 4170: {'loss': 0.6837, 'grad_norm': 0.7031263709068298, 'learning_rate': 0.0006139814814814815, 'epoch': 1.1583333333333332}
Step 4180: {'loss': 0.6485, 'grad_norm': 0.6244300603866577, 'learning_rate': 0.0006130555555555556, 'epoch': 1.1611111111111112}
Step 4190: {'loss': 0.6241, 'grad_norm': 0.8426876664161682, 'learning_rate': 0.0006121296296296296, 'epoch': 1.163888888888889}
Step 4200: {'loss': 0.5883, 'grad_norm': 0.7485365271568298, 'learning_rate': 0.0006112037037037038, 'epoch': 1.1666666666666667}
Step 4210: {'loss': 0.6824, 'grad_norm': 0.8774537444114685, 'learning_rate': 0.0006102777777777778, 'epoch': 1.1694444444444445}
Step 4220: {'loss': 0.6331, 'grad_norm': 0.903041660785675, 'learning_rate': 0.0006093518518518518, 'epoch': 1.1722222222222223}
Step 4230: {'loss': 0.6092, 'grad_norm': 0.8075701594352722, 'learning_rate': 0.0006084259259259259, 'epoch': 1.175}
Step 4240: {'loss': 0.608, 'grad_norm': 0.7298506498336792, 'learning_rate': 0.0006075000000000001, 'epoch': 1.1777777777777778}
Step 4250: {'loss': 0.6707, 'grad_norm': 0.9761049747467041, 'learning_rate': 0.000606574074074074, 'epoch': 1.1805555555555556}
Step 4260: {'loss': 0.646, 'grad_norm': 0.8767765164375305, 'learning_rate': 0.0006056481481481481, 'epoch': 1.1833333333333333}
Step 4270: {'loss': 0.6417, 'grad_norm': 1.0487598180770874, 'learning_rate': 0.0006047222222222223, 'epoch': 1.1861111111111111}
Step 4280: {'loss': 0.6575, 'grad_norm': 0.7658440470695496, 'learning_rate': 0.0006037962962962963, 'epoch': 1.1888888888888889}
Step 4290: {'loss': 0.7154, 'grad_norm': 0.8017815947532654, 'learning_rate': 0.0006028703703703704, 'epoch': 1.1916666666666667}
Step 4300: {'loss': 0.751, 'grad_norm': 0.9671195149421692, 'learning_rate': 0.0006019444444444444, 'epoch': 1.1944444444444444}
Step 4310: {'loss': 0.6415, 'grad_norm': 0.728670060634613, 'learning_rate': 0.0006010185185185185, 'epoch': 1.1972222222222222}
Step 4320: {'loss': 0.8027, 'grad_norm': 0.7214349508285522, 'learning_rate': 0.0006000925925925926, 'epoch': 1.2}
Step 4330: {'loss': 0.6532, 'grad_norm': 0.6227733492851257, 'learning_rate': 0.0005991666666666667, 'epoch': 1.2027777777777777}
Step 4340: {'loss': 0.7003, 'grad_norm': 0.7385109663009644, 'learning_rate': 0.0005982407407407408, 'epoch': 1.2055555555555555}
Step 4350: {'loss': 0.6709, 'grad_norm': 0.7525941729545593, 'learning_rate': 0.0005973148148148148, 'epoch': 1.2083333333333333}
Step 4360: {'loss': 0.7473, 'grad_norm': 1.1481285095214844, 'learning_rate': 0.0005963888888888889, 'epoch': 1.211111111111111}
Step 4370: {'loss': 0.6694, 'grad_norm': 0.7420151233673096, 'learning_rate': 0.000595462962962963, 'epoch': 1.2138888888888888}
Step 4380: {'loss': 0.6646, 'grad_norm': 0.5767298340797424, 'learning_rate': 0.0005945370370370371, 'epoch': 1.2166666666666668}
Step 4390: {'loss': 0.7115, 'grad_norm': 1.0914499759674072, 'learning_rate': 0.000593611111111111, 'epoch': 1.2194444444444446}
Step 4400: {'loss': 0.6791, 'grad_norm': 0.7995210886001587, 'learning_rate': 0.0005926851851851852, 'epoch': 1.2222222222222223}
Step 4410: {'loss': 0.6796, 'grad_norm': 1.2343465089797974, 'learning_rate': 0.0005917592592592592, 'epoch': 1.225}
Step 4420: {'loss': 0.6837, 'grad_norm': 0.9017381072044373, 'learning_rate': 0.0005908333333333333, 'epoch': 1.2277777777777779}
Step 4430: {'loss': 0.633, 'grad_norm': 0.5717770457267761, 'learning_rate': 0.0005899074074074075, 'epoch': 1.2305555555555556}
Step 4440: {'loss': 0.6757, 'grad_norm': 1.0514498949050903, 'learning_rate': 0.0005889814814814815, 'epoch': 1.2333333333333334}
Step 4450: {'loss': 0.6589, 'grad_norm': 0.8121827840805054, 'learning_rate': 0.0005880555555555555, 'epoch': 1.2361111111111112}
Step 4460: {'loss': 0.6342, 'grad_norm': 1.0105528831481934, 'learning_rate': 0.0005871296296296297, 'epoch': 1.238888888888889}
Step 4470: {'loss': 0.6104, 'grad_norm': 0.7419048547744751, 'learning_rate': 0.0005862037037037037, 'epoch': 1.2416666666666667}
Step 4480: {'loss': 0.6878, 'grad_norm': 0.8780317306518555, 'learning_rate': 0.0005852777777777778, 'epoch': 1.2444444444444445}
Step 4490: {'loss': 0.7101, 'grad_norm': 0.8749815821647644, 'learning_rate': 0.000584351851851852, 'epoch': 1.2472222222222222}
Step 4500: {'loss': 0.6436, 'grad_norm': 0.8359644412994385, 'learning_rate': 0.0005834259259259259, 'epoch': 1.25}
Step 4510: {'loss': 0.6751, 'grad_norm': 0.925256073474884, 'learning_rate': 0.0005825, 'epoch': 1.2527777777777778}
Step 4520: {'loss': 0.6497, 'grad_norm': 0.7925516366958618, 'learning_rate': 0.000581574074074074, 'epoch': 1.2555555555555555}
Step 4530: {'loss': 0.69, 'grad_norm': 0.7969954609870911, 'learning_rate': 0.0005806481481481482, 'epoch': 1.2583333333333333}
Step 4540: {'loss': 0.6882, 'grad_norm': 0.7109329104423523, 'learning_rate': 0.0005797222222222222, 'epoch': 1.261111111111111}
Step 4550: {'loss': 0.6329, 'grad_norm': 0.5900425314903259, 'learning_rate': 0.0005787962962962962, 'epoch': 1.2638888888888888}
Step 4560: {'loss': 0.6665, 'grad_norm': 0.9872959852218628, 'learning_rate': 0.0005778703703703704, 'epoch': 1.2666666666666666}
Step 4570: {'loss': 0.6718, 'grad_norm': 0.7064030766487122, 'learning_rate': 0.0005769444444444445, 'epoch': 1.2694444444444444}
Step 4580: {'loss': 0.6912, 'grad_norm': 0.6342254281044006, 'learning_rate': 0.0005760185185185185, 'epoch': 1.2722222222222221}
Step 4590: {'loss': 0.6849, 'grad_norm': 0.7528260946273804, 'learning_rate': 0.0005750925925925926, 'epoch': 1.275}
Step 4600: {'loss': 0.6608, 'grad_norm': 0.9314285516738892, 'learning_rate': 0.0005741666666666667, 'epoch': 1.2777777777777777}
Step 4610: {'loss': 0.6767, 'grad_norm': 0.8722109794616699, 'learning_rate': 0.0005732407407407407, 'epoch': 1.2805555555555554}
Step 4620: {'loss': 0.6082, 'grad_norm': 0.871670126914978, 'learning_rate': 0.0005723148148148149, 'epoch': 1.2833333333333332}
Step 4630: {'loss': 0.6368, 'grad_norm': 0.8981574773788452, 'learning_rate': 0.0005713888888888889, 'epoch': 1.286111111111111}
Step 4640: {'loss': 0.5882, 'grad_norm': 1.0577600002288818, 'learning_rate': 0.0005704629629629629, 'epoch': 1.2888888888888888}
Step 4650: {'loss': 0.6702, 'grad_norm': 0.7606303095817566, 'learning_rate': 0.0005695370370370371, 'epoch': 1.2916666666666667}
Step 4660: {'loss': 0.6545, 'grad_norm': 1.0649607181549072, 'learning_rate': 0.0005686111111111111, 'epoch': 1.2944444444444445}
Step 4670: {'loss': 0.6391, 'grad_norm': 0.69991534948349, 'learning_rate': 0.0005676851851851852, 'epoch': 1.2972222222222223}
Step 4680: {'loss': 0.6936, 'grad_norm': 0.7988398671150208, 'learning_rate': 0.0005667592592592593, 'epoch': 1.3}
Step 4690: {'loss': 0.612, 'grad_norm': 0.6144318580627441, 'learning_rate': 0.0005658333333333333, 'epoch': 1.3027777777777778}
Step 4700: {'loss': 0.7259, 'grad_norm': 0.8909952640533447, 'learning_rate': 0.0005649074074074074, 'epoch': 1.3055555555555556}
Step 4710: {'loss': 0.6181, 'grad_norm': 0.7053846120834351, 'learning_rate': 0.0005639814814814815, 'epoch': 1.3083333333333333}
Step 4720: {'loss': 0.6468, 'grad_norm': 0.8253353238105774, 'learning_rate': 0.0005630555555555556, 'epoch': 1.3111111111111111}
Step 4730: {'loss': 0.6927, 'grad_norm': 0.9574330449104309, 'learning_rate': 0.0005621296296296297, 'epoch': 1.3138888888888889}
Step 4740: {'loss': 0.6934, 'grad_norm': 0.7606011629104614, 'learning_rate': 0.0005612037037037036, 'epoch': 1.3166666666666667}
Step 4750: {'loss': 0.5809, 'grad_norm': 1.0815399885177612, 'learning_rate': 0.0005602777777777778, 'epoch': 1.3194444444444444}
Step 4760: {'loss': 0.6512, 'grad_norm': 0.6944469213485718, 'learning_rate': 0.0005593518518518519, 'epoch': 1.3222222222222222}
Step 4770: {'loss': 0.6789, 'grad_norm': 0.8247124552726746, 'learning_rate': 0.0005584259259259259, 'epoch': 1.325}
Step 4780: {'loss': 0.6335, 'grad_norm': 0.6368785500526428, 'learning_rate': 0.0005575, 'epoch': 1.3277777777777777}
Step 4790: {'loss': 0.674, 'grad_norm': 0.8198631405830383, 'learning_rate': 0.0005565740740740741, 'epoch': 1.3305555555555555}
Step 4800: {'loss': 0.6212, 'grad_norm': 0.799633264541626, 'learning_rate': 0.0005556481481481481, 'epoch': 1.3333333333333333}
Step 4810: {'loss': 0.6684, 'grad_norm': 1.0572912693023682, 'learning_rate': 0.0005547222222222223, 'epoch': 1.3361111111111112}
Step 4820: {'loss': 0.6615, 'grad_norm': 0.6499459743499756, 'learning_rate': 0.0005537962962962964, 'epoch': 1.338888888888889}
Step 4830: {'loss': 0.68, 'grad_norm': 0.6837276220321655, 'learning_rate': 0.0005528703703703703, 'epoch': 1.3416666666666668}
Step 4840: {'loss': 0.6694, 'grad_norm': 0.7166235446929932, 'learning_rate': 0.0005519444444444444, 'epoch': 1.3444444444444446}
Step 4850: {'loss': 0.6684, 'grad_norm': 0.7076424956321716, 'learning_rate': 0.0005510185185185185, 'epoch': 1.3472222222222223}
Step 4860: {'loss': 0.7041, 'grad_norm': 0.7271551489830017, 'learning_rate': 0.0005500925925925926, 'epoch': 1.35}
Step 4870: {'loss': 0.6743, 'grad_norm': 1.1176894903182983, 'learning_rate': 0.0005491666666666667, 'epoch': 1.3527777777777779}
Step 4880: {'loss': 0.6553, 'grad_norm': 0.9908235669136047, 'learning_rate': 0.0005482407407407407, 'epoch': 1.3555555555555556}
Step 4890: {'loss': 0.7083, 'grad_norm': 2.7999885082244873, 'learning_rate': 0.0005473148148148148, 'epoch': 1.3583333333333334}
Step 4900: {'loss': 0.656, 'grad_norm': 1.0052769184112549, 'learning_rate': 0.0005463888888888889, 'epoch': 1.3611111111111112}
Step 4910: {'loss': 0.7107, 'grad_norm': 0.9931519627571106, 'learning_rate': 0.000545462962962963, 'epoch': 1.363888888888889}
Step 4920: {'loss': 0.6568, 'grad_norm': 0.9186360836029053, 'learning_rate': 0.0005445370370370371, 'epoch': 1.3666666666666667}
Step 4930: {'loss': 0.6426, 'grad_norm': 1.2231621742248535, 'learning_rate': 0.0005436111111111111, 'epoch': 1.3694444444444445}
Step 4940: {'loss': 0.6649, 'grad_norm': 0.655144989490509, 'learning_rate': 0.0005426851851851852, 'epoch': 1.3722222222222222}
Step 4950: {'loss': 0.7022, 'grad_norm': 0.7580323815345764, 'learning_rate': 0.0005417592592592593, 'epoch': 1.375}
Step 4960: {'loss': 0.6581, 'grad_norm': 0.7489679455757141, 'learning_rate': 0.0005408333333333334, 'epoch': 1.3777777777777778}
Step 4970: {'loss': 0.7091, 'grad_norm': 0.9802636504173279, 'learning_rate': 0.0005399074074074073, 'epoch': 1.3805555555555555}
Step 4980: {'loss': 0.6869, 'grad_norm': 0.9058273434638977, 'learning_rate': 0.0005389814814814815, 'epoch': 1.3833333333333333}
Step 4990: {'loss': 0.5882, 'grad_norm': 0.692432701587677, 'learning_rate': 0.0005380555555555555, 'epoch': 1.386111111111111}
Step 5000: {'loss': 0.6775, 'grad_norm': 0.8727784156799316, 'learning_rate': 0.0005371296296296296, 'epoch': 1.3888888888888888}
Step 5010: {'loss': 0.6617, 'grad_norm': 0.8522828817367554, 'learning_rate': 0.0005362037037037038, 'epoch': 1.3916666666666666}
Step 5020: {'loss': 0.6439, 'grad_norm': 0.8529783487319946, 'learning_rate': 0.0005352777777777777, 'epoch': 1.3944444444444444}
Step 5030: {'loss': 0.6578, 'grad_norm': 0.4783572852611542, 'learning_rate': 0.0005343518518518518, 'epoch': 1.3972222222222221}
Step 5040: {'loss': 0.6443, 'grad_norm': 0.6778289675712585, 'learning_rate': 0.000533425925925926, 'epoch': 1.4}
Step 5050: {'loss': 0.6294, 'grad_norm': 0.8232460021972656, 'learning_rate': 0.0005325, 'epoch': 1.4027777777777777}
Step 5060: {'loss': 0.7062, 'grad_norm': 0.5993192195892334, 'learning_rate': 0.0005315740740740741, 'epoch': 1.4055555555555554}
Step 5070: {'loss': 0.6396, 'grad_norm': 0.9731958508491516, 'learning_rate': 0.0005306481481481483, 'epoch': 1.4083333333333332}
Step 5080: {'loss': 0.6549, 'grad_norm': 1.0089653730392456, 'learning_rate': 0.0005297222222222222, 'epoch': 1.411111111111111}
Step 5090: {'loss': 0.6306, 'grad_norm': 0.8316560983657837, 'learning_rate': 0.0005287962962962963, 'epoch': 1.4138888888888888}
Step 5100: {'loss': 0.7303, 'grad_norm': 1.0404976606369019, 'learning_rate': 0.0005278703703703704, 'epoch': 1.4166666666666667}
Step 5110: {'loss': 0.6805, 'grad_norm': 0.7751266956329346, 'learning_rate': 0.0005269444444444445, 'epoch': 1.4194444444444445}
Step 5120: {'loss': 0.6721, 'grad_norm': 0.9376856684684753, 'learning_rate': 0.0005260185185185185, 'epoch': 1.4222222222222223}
Step 5130: {'loss': 0.6477, 'grad_norm': 0.7129948139190674, 'learning_rate': 0.0005250925925925925, 'epoch': 1.425}
Step 5140: {'loss': 0.6758, 'grad_norm': 0.6546798944473267, 'learning_rate': 0.0005241666666666667, 'epoch': 1.4277777777777778}
Step 5150: {'loss': 0.675, 'grad_norm': 0.9557648301124573, 'learning_rate': 0.0005232407407407408, 'epoch': 1.4305555555555556}
Step 5160: {'loss': 0.6842, 'grad_norm': 0.7654586434364319, 'learning_rate': 0.0005223148148148148, 'epoch': 1.4333333333333333}
Step 5170: {'loss': 0.6971, 'grad_norm': 0.8821561336517334, 'learning_rate': 0.0005213888888888889, 'epoch': 1.4361111111111111}
Step 5180: {'loss': 0.6787, 'grad_norm': 0.8374277949333191, 'learning_rate': 0.000520462962962963, 'epoch': 1.4388888888888889}
Step 5190: {'loss': 0.652, 'grad_norm': 0.9154159426689148, 'learning_rate': 0.000519537037037037, 'epoch': 1.4416666666666667}
Step 5200: {'loss': 0.6605, 'grad_norm': 0.6274651885032654, 'learning_rate': 0.0005186111111111112, 'epoch': 1.4444444444444444}
Step 5210: {'loss': 0.6981, 'grad_norm': 0.8664298057556152, 'learning_rate': 0.0005176851851851852, 'epoch': 1.4472222222222222}
Step 5220: {'loss': 0.6924, 'grad_norm': 0.8986976146697998, 'learning_rate': 0.0005167592592592592, 'epoch': 1.45}
Step 5230: {'loss': 0.6465, 'grad_norm': 0.8925490975379944, 'learning_rate': 0.0005158333333333334, 'epoch': 1.4527777777777777}
Step 5240: {'loss': 0.633, 'grad_norm': 1.1454598903656006, 'learning_rate': 0.0005149074074074074, 'epoch': 1.4555555555555555}
Step 5250: {'loss': 0.6159, 'grad_norm': 0.6690759658813477, 'learning_rate': 0.0005139814814814815, 'epoch': 1.4583333333333333}
Step 5260: {'loss': 0.6876, 'grad_norm': 0.8379626870155334, 'learning_rate': 0.0005130555555555557, 'epoch': 1.4611111111111112}
Step 5270: {'loss': 0.6526, 'grad_norm': 0.9023770689964294, 'learning_rate': 0.0005121296296296296, 'epoch': 1.463888888888889}
Step 5280: {'loss': 0.651, 'grad_norm': 0.9456700086593628, 'learning_rate': 0.0005112037037037037, 'epoch': 1.4666666666666668}
Step 5290: {'loss': 0.6592, 'grad_norm': 0.8038197159767151, 'learning_rate': 0.0005102777777777778, 'epoch': 1.4694444444444446}
Step 5300: {'loss': 0.6372, 'grad_norm': 0.8458607196807861, 'learning_rate': 0.0005093518518518519, 'epoch': 1.4722222222222223}
Step 5310: {'loss': 0.5756, 'grad_norm': 0.9492864608764648, 'learning_rate': 0.000508425925925926, 'epoch': 1.475}
Step 5320: {'loss': 0.6495, 'grad_norm': 0.9699934720993042, 'learning_rate': 0.0005074999999999999, 'epoch': 1.4777777777777779}
Step 5330: {'loss': 0.6653, 'grad_norm': 0.8484638333320618, 'learning_rate': 0.0005065740740740741, 'epoch': 1.4805555555555556}
Step 5340: {'loss': 0.6698, 'grad_norm': 0.6704410314559937, 'learning_rate': 0.0005056481481481482, 'epoch': 1.4833333333333334}
Step 5350: {'loss': 0.7016, 'grad_norm': 0.5556963086128235, 'learning_rate': 0.0005047222222222222, 'epoch': 1.4861111111111112}
Step 5360: {'loss': 0.6524, 'grad_norm': 0.7101222276687622, 'learning_rate': 0.0005037962962962963, 'epoch': 1.488888888888889}
Step 5370: {'loss': 0.6886, 'grad_norm': 1.2335193157196045, 'learning_rate': 0.0005028703703703704, 'epoch': 1.4916666666666667}
Step 5380: {'loss': 0.6626, 'grad_norm': 0.6530622243881226, 'learning_rate': 0.0005019444444444444, 'epoch': 1.4944444444444445}
Step 5390: {'loss': 0.6356, 'grad_norm': 0.6993422508239746, 'learning_rate': 0.0005010185185185186, 'epoch': 1.4972222222222222}
Step 5400: {'loss': 0.7454, 'grad_norm': 1.223759651184082, 'learning_rate': 0.0005000925925925927, 'epoch': 1.5}
Step 5410: {'loss': 0.6197, 'grad_norm': 0.7525629997253418, 'learning_rate': 0.0004991666666666666, 'epoch': 1.5027777777777778}
Step 5420: {'loss': 0.7316, 'grad_norm': 0.97254478931427, 'learning_rate': 0.0004982407407407407, 'epoch': 1.5055555555555555}
Step 5430: {'loss': 0.6571, 'grad_norm': 1.2393642663955688, 'learning_rate': 0.0004973148148148148, 'epoch': 1.5083333333333333}
Step 5440: {'loss': 0.6308, 'grad_norm': 0.6996080279350281, 'learning_rate': 0.0004963888888888889, 'epoch': 1.511111111111111}
Step 5450: {'loss': 0.7301, 'grad_norm': 0.8958896994590759, 'learning_rate': 0.000495462962962963, 'epoch': 1.5138888888888888}
Step 5460: {'loss': 0.7445, 'grad_norm': 0.9483970403671265, 'learning_rate': 0.000494537037037037, 'epoch': 1.5166666666666666}
Step 5470: {'loss': 0.6641, 'grad_norm': 0.8344494700431824, 'learning_rate': 0.0004936111111111111, 'epoch': 1.5194444444444444}
Step 5480: {'loss': 0.7023, 'grad_norm': 1.424513816833496, 'learning_rate': 0.0004926851851851852, 'epoch': 1.5222222222222221}
Step 5490: {'loss': 0.598, 'grad_norm': 0.995017409324646, 'learning_rate': 0.0004917592592592593, 'epoch': 1.525}
Step 5500: {'loss': 0.662, 'grad_norm': 0.7937155365943909, 'learning_rate': 0.0004908333333333334, 'epoch': 1.5277777777777777}
Step 5510: {'loss': 0.667, 'grad_norm': 1.205254316329956, 'learning_rate': 0.0004899074074074074, 'epoch': 1.5305555555555554}
Step 5520: {'loss': 0.682, 'grad_norm': 0.8039038181304932, 'learning_rate': 0.0004889814814814815, 'epoch': 1.5333333333333332}
Step 5530: {'loss': 0.6895, 'grad_norm': 0.9800747632980347, 'learning_rate': 0.0004880555555555556, 'epoch': 1.536111111111111}
Step 5540: {'loss': 0.6396, 'grad_norm': 0.8423120379447937, 'learning_rate': 0.0004871296296296296, 'epoch': 1.5388888888888888}
Step 5550: {'loss': 0.6558, 'grad_norm': 1.0107064247131348, 'learning_rate': 0.0004862037037037037, 'epoch': 1.5416666666666665}
Step 5560: {'loss': 0.6882, 'grad_norm': 0.7567182779312134, 'learning_rate': 0.0004852777777777778, 'epoch': 1.5444444444444443}
Step 5570: {'loss': 0.6792, 'grad_norm': 0.9844295382499695, 'learning_rate': 0.00048435185185185186, 'epoch': 1.5472222222222223}
Step 5580: {'loss': 0.6045, 'grad_norm': 0.7146309614181519, 'learning_rate': 0.00048342592592592594, 'epoch': 1.55}
Step 5590: {'loss': 0.6452, 'grad_norm': 0.5634483695030212, 'learning_rate': 0.0004825, 'epoch': 1.5527777777777778}
Step 5600: {'loss': 0.6688, 'grad_norm': 0.6840879321098328, 'learning_rate': 0.0004815740740740741, 'epoch': 1.5555555555555556}
Step 5610: {'loss': 0.6971, 'grad_norm': 0.6579753756523132, 'learning_rate': 0.0004806481481481482, 'epoch': 1.5583333333333333}
Step 5620: {'loss': 0.739, 'grad_norm': 0.8605349659919739, 'learning_rate': 0.0004797222222222222, 'epoch': 1.5611111111111111}
Step 5630: {'loss': 0.7003, 'grad_norm': 0.6660901308059692, 'learning_rate': 0.0004787962962962963, 'epoch': 1.5638888888888889}
Step 5640: {'loss': 0.678, 'grad_norm': 1.0727459192276, 'learning_rate': 0.0004778703703703704, 'epoch': 1.5666666666666667}
Step 5650: {'loss': 0.6886, 'grad_norm': 0.9436613917350769, 'learning_rate': 0.00047694444444444444, 'epoch': 1.5694444444444444}
Step 5660: {'loss': 0.7091, 'grad_norm': 0.9999890327453613, 'learning_rate': 0.0004760185185185185, 'epoch': 1.5722222222222222}
Step 5670: {'loss': 0.6109, 'grad_norm': 0.591140866279602, 'learning_rate': 0.0004750925925925926, 'epoch': 1.575}
Step 5680: {'loss': 0.6956, 'grad_norm': 1.1433593034744263, 'learning_rate': 0.0004741666666666667, 'epoch': 1.5777777777777777}
Step 5690: {'loss': 0.6856, 'grad_norm': 0.623332679271698, 'learning_rate': 0.00047324074074074076, 'epoch': 1.5805555555555557}
Step 5700: {'loss': 0.6329, 'grad_norm': 1.5503895282745361, 'learning_rate': 0.0004723148148148148, 'epoch': 1.5833333333333335}
Step 5710: {'loss': 0.7199, 'grad_norm': 1.2354941368103027, 'learning_rate': 0.0004713888888888889, 'epoch': 1.5861111111111112}
Step 5720: {'loss': 0.6675, 'grad_norm': 0.5233092308044434, 'learning_rate': 0.000470462962962963, 'epoch': 1.588888888888889}
Step 5730: {'loss': 0.6542, 'grad_norm': 0.9832479953765869, 'learning_rate': 0.000469537037037037, 'epoch': 1.5916666666666668}
Step 5740: {'loss': 0.631, 'grad_norm': 0.7461268901824951, 'learning_rate': 0.0004686111111111111, 'epoch': 1.5944444444444446}
Step 5750: {'loss': 0.6694, 'grad_norm': 0.5535038709640503, 'learning_rate': 0.00046768518518518524, 'epoch': 1.5972222222222223}
Step 5760: {'loss': 0.7066, 'grad_norm': 0.7378057241439819, 'learning_rate': 0.00046675925925925926, 'epoch': 1.6}
Step 5770: {'loss': 0.685, 'grad_norm': 0.8947925567626953, 'learning_rate': 0.00046583333333333334, 'epoch': 1.6027777777777779}
Step 5780: {'loss': 0.6779, 'grad_norm': 0.7875657677650452, 'learning_rate': 0.00046490740740740737, 'epoch': 1.6055555555555556}
Step 5790: {'loss': 0.6704, 'grad_norm': 0.5704609751701355, 'learning_rate': 0.0004639814814814815, 'epoch': 1.6083333333333334}
Step 5800: {'loss': 0.6858, 'grad_norm': 1.0042650699615479, 'learning_rate': 0.0004630555555555556, 'epoch': 1.6111111111111112}
Step 5810: {'loss': 0.6978, 'grad_norm': 0.848146915435791, 'learning_rate': 0.0004621296296296296, 'epoch': 1.613888888888889}
Step 5820: {'loss': 0.6483, 'grad_norm': 0.7240785956382751, 'learning_rate': 0.00046120370370370374, 'epoch': 1.6166666666666667}
Step 5830: {'loss': 0.6522, 'grad_norm': 1.0826921463012695, 'learning_rate': 0.00046027777777777777, 'epoch': 1.6194444444444445}
Step 5840: {'loss': 0.6472, 'grad_norm': 0.8191800713539124, 'learning_rate': 0.00045935185185185185, 'epoch': 1.6222222222222222}
Step 5850: {'loss': 0.6535, 'grad_norm': 0.7070503830909729, 'learning_rate': 0.00045842592592592593, 'epoch': 1.625}
Step 5860: {'loss': 0.6499, 'grad_norm': 0.6633242964744568, 'learning_rate': 0.0004575, 'epoch': 1.6277777777777778}
Step 5870: {'loss': 0.6579, 'grad_norm': 0.6710444688796997, 'learning_rate': 0.0004565740740740741, 'epoch': 1.6305555555555555}
Step 5880: {'loss': 0.6273, 'grad_norm': 0.78597092628479, 'learning_rate': 0.00045564814814814817, 'epoch': 1.6333333333333333}
Step 5890: {'loss': 0.651, 'grad_norm': 0.7327778339385986, 'learning_rate': 0.00045472222222222225, 'epoch': 1.636111111111111}
Step 5900: {'loss': 0.6413, 'grad_norm': 0.6821173429489136, 'learning_rate': 0.0004537962962962963, 'epoch': 1.6388888888888888}
Step 5910: {'loss': 0.6298, 'grad_norm': 0.6523887515068054, 'learning_rate': 0.00045287037037037035, 'epoch': 1.6416666666666666}
Step 5920: {'loss': 0.6519, 'grad_norm': 0.8434454798698425, 'learning_rate': 0.00045194444444444443, 'epoch': 1.6444444444444444}
Step 5930: {'loss': 0.6232, 'grad_norm': 0.740850567817688, 'learning_rate': 0.00045101851851851857, 'epoch': 1.6472222222222221}
Step 5940: {'loss': 0.6075, 'grad_norm': 0.9450167417526245, 'learning_rate': 0.0004500925925925926, 'epoch': 1.65}
Step 5950: {'loss': 0.6833, 'grad_norm': 0.8156110048294067, 'learning_rate': 0.00044916666666666667, 'epoch': 1.6527777777777777}
Step 5960: {'loss': 0.683, 'grad_norm': 0.8275836706161499, 'learning_rate': 0.00044824074074074075, 'epoch': 1.6555555555555554}
Step 5970: {'loss': 0.6822, 'grad_norm': 0.946735680103302, 'learning_rate': 0.00044731481481481483, 'epoch': 1.6583333333333332}
Step 5980: {'loss': 0.6258, 'grad_norm': 0.6421032547950745, 'learning_rate': 0.0004463888888888889, 'epoch': 1.661111111111111}
Step 5990: {'loss': 0.6943, 'grad_norm': 1.1735155582427979, 'learning_rate': 0.00044546296296296293, 'epoch': 1.6638888888888888}
Step 6000: {'loss': 0.6306, 'grad_norm': 0.6392925977706909, 'learning_rate': 0.00044453703703703707, 'epoch': 1.6666666666666665}
Step 6010: {'loss': 0.6979, 'grad_norm': 0.936419665813446, 'learning_rate': 0.00044361111111111115, 'epoch': 1.6694444444444443}
Step 6020: {'loss': 0.6385, 'grad_norm': 0.8971812725067139, 'learning_rate': 0.0004426851851851852, 'epoch': 1.6722222222222223}
Step 6030: {'loss': 0.6342, 'grad_norm': 0.9162484407424927, 'learning_rate': 0.00044175925925925925, 'epoch': 1.675}
Step 6040: {'loss': 0.6657, 'grad_norm': 0.6801207661628723, 'learning_rate': 0.0004408333333333334, 'epoch': 1.6777777777777778}
Step 6050: {'loss': 0.7107, 'grad_norm': 0.8208082318305969, 'learning_rate': 0.0004399074074074074, 'epoch': 1.6805555555555556}
Step 6060: {'loss': 0.6487, 'grad_norm': 0.7587725520133972, 'learning_rate': 0.0004389814814814815, 'epoch': 1.6833333333333333}
Step 6070: {'loss': 0.6828, 'grad_norm': 0.623006284236908, 'learning_rate': 0.0004380555555555555, 'epoch': 1.6861111111111111}
Step 6080: {'loss': 0.6177, 'grad_norm': 1.277563214302063, 'learning_rate': 0.00043712962962962965, 'epoch': 1.6888888888888889}
Step 6090: {'loss': 0.6046, 'grad_norm': 1.1492069959640503, 'learning_rate': 0.00043620370370370373, 'epoch': 1.6916666666666667}
Step 6100: {'loss': 0.667, 'grad_norm': 0.9354274272918701, 'learning_rate': 0.00043527777777777776, 'epoch': 1.6944444444444444}
Step 6110: {'loss': 0.6599, 'grad_norm': 0.7527644038200378, 'learning_rate': 0.0004343518518518519, 'epoch': 1.6972222222222222}
Step 6120: {'loss': 0.6893, 'grad_norm': 0.84090656042099, 'learning_rate': 0.00043342592592592597, 'epoch': 1.7}
Step 6130: {'loss': 0.7147, 'grad_norm': 1.06031334400177, 'learning_rate': 0.0004325, 'epoch': 1.7027777777777777}
Step 6140: {'loss': 0.7328, 'grad_norm': 1.1216185092926025, 'learning_rate': 0.0004315740740740741, 'epoch': 1.7055555555555557}
Step 6150: {'loss': 0.6249, 'grad_norm': 0.7386803030967712, 'learning_rate': 0.00043064814814814816, 'epoch': 1.7083333333333335}
Step 6160: {'loss': 0.6549, 'grad_norm': 0.9012212157249451, 'learning_rate': 0.00042972222222222224, 'epoch': 1.7111111111111112}
Step 6170: {'loss': 0.6446, 'grad_norm': 1.1869195699691772, 'learning_rate': 0.0004287962962962963, 'epoch': 1.713888888888889}
Step 6180: {'loss': 0.6658, 'grad_norm': 1.1268506050109863, 'learning_rate': 0.00042787037037037034, 'epoch': 1.7166666666666668}
Step 6190: {'loss': 0.6638, 'grad_norm': 0.8265566825866699, 'learning_rate': 0.0004269444444444445, 'epoch': 1.7194444444444446}
Step 6200: {'loss': 0.6181, 'grad_norm': 0.9547301530838013, 'learning_rate': 0.00042601851851851855, 'epoch': 1.7222222222222223}
Step 6210: {'loss': 0.6077, 'grad_norm': 0.8963481187820435, 'learning_rate': 0.0004250925925925926, 'epoch': 1.725}
Step 6220: {'loss': 0.7323, 'grad_norm': 0.9910904765129089, 'learning_rate': 0.0004241666666666667, 'epoch': 1.7277777777777779}
Step 6230: {'loss': 0.6868, 'grad_norm': 1.1232529878616333, 'learning_rate': 0.00042324074074074074, 'epoch': 1.7305555555555556}
Step 6240: {'loss': 0.6556, 'grad_norm': 1.465880036354065, 'learning_rate': 0.0004223148148148148, 'epoch': 1.7333333333333334}
Step 6250: {'loss': 0.6332, 'grad_norm': 0.9093559980392456, 'learning_rate': 0.0004213888888888889, 'epoch': 1.7361111111111112}
Step 6260: {'loss': 0.6651, 'grad_norm': 1.1729856729507446, 'learning_rate': 0.000420462962962963, 'epoch': 1.738888888888889}
Step 6270: {'loss': 0.667, 'grad_norm': 0.7019344568252563, 'learning_rate': 0.00041953703703703706, 'epoch': 1.7416666666666667}
Step 6280: {'loss': 0.6586, 'grad_norm': 1.0680739879608154, 'learning_rate': 0.0004186111111111111, 'epoch': 1.7444444444444445}
Step 6290: {'loss': 0.6369, 'grad_norm': 0.8994818329811096, 'learning_rate': 0.00041768518518518516, 'epoch': 1.7472222222222222}
Step 6300: {'loss': 0.6616, 'grad_norm': 0.9343961477279663, 'learning_rate': 0.0004167592592592593, 'epoch': 1.75}
Step 6310: {'loss': 0.6127, 'grad_norm': 1.2975378036499023, 'learning_rate': 0.0004158333333333333, 'epoch': 1.7527777777777778}
Step 6320: {'loss': 0.5776, 'grad_norm': 0.6876413226127625, 'learning_rate': 0.0004149074074074074, 'epoch': 1.7555555555555555}
Step 6330: {'loss': 0.698, 'grad_norm': 0.9556054472923279, 'learning_rate': 0.00041398148148148154, 'epoch': 1.7583333333333333}
Step 6340: {'loss': 0.5712, 'grad_norm': 0.8229528665542603, 'learning_rate': 0.00041305555555555556, 'epoch': 1.761111111111111}
Step 6350: {'loss': 0.6151, 'grad_norm': 0.6317466497421265, 'learning_rate': 0.00041212962962962964, 'epoch': 1.7638888888888888}
Step 6360: {'loss': 0.6263, 'grad_norm': 0.5619044899940491, 'learning_rate': 0.00041120370370370367, 'epoch': 1.7666666666666666}
Step 6370: {'loss': 0.6496, 'grad_norm': 0.8539053797721863, 'learning_rate': 0.0004102777777777778, 'epoch': 1.7694444444444444}
Step 6380: {'loss': 0.6301, 'grad_norm': 0.9548324346542358, 'learning_rate': 0.0004093518518518519, 'epoch': 1.7722222222222221}
Step 6390: {'loss': 0.6566, 'grad_norm': 0.9795403480529785, 'learning_rate': 0.0004084259259259259, 'epoch': 1.775}
Step 6400: {'loss': 0.7041, 'grad_norm': 0.6470170617103577, 'learning_rate': 0.0004075, 'epoch': 1.7777777777777777}
Step 6410: {'loss': 0.6623, 'grad_norm': 1.2394591569900513, 'learning_rate': 0.0004065740740740741, 'epoch': 1.7805555555555554}
Step 6420: {'loss': 0.698, 'grad_norm': 0.9661957621574402, 'learning_rate': 0.00040564814814814814, 'epoch': 1.7833333333333332}
Step 6430: {'loss': 0.6576, 'grad_norm': 1.2290047407150269, 'learning_rate': 0.0004047222222222222, 'epoch': 1.786111111111111}
Step 6440: {'loss': 0.6177, 'grad_norm': 0.6131457686424255, 'learning_rate': 0.0004037962962962963, 'epoch': 1.7888888888888888}
Step 6450: {'loss': 0.7153, 'grad_norm': 1.4154112339019775, 'learning_rate': 0.0004028703703703704, 'epoch': 1.7916666666666665}
Step 6460: {'loss': 0.6584, 'grad_norm': 0.9063841104507446, 'learning_rate': 0.00040194444444444446, 'epoch': 1.7944444444444443}
Step 6470: {'loss': 0.6707, 'grad_norm': 1.0580825805664062, 'learning_rate': 0.0004010185185185185, 'epoch': 1.7972222222222223}
Step 6480: {'loss': 0.6357, 'grad_norm': 0.6053555011749268, 'learning_rate': 0.0004000925925925926, 'epoch': 1.8}
Step 6490: {'loss': 0.6362, 'grad_norm': 0.7668417692184448, 'learning_rate': 0.0003991666666666667, 'epoch': 1.8027777777777778}
Step 6500: {'loss': 0.6657, 'grad_norm': 0.8272584080696106, 'learning_rate': 0.00039824074074074073, 'epoch': 1.8055555555555556}
Step 6510: {'loss': 0.6624, 'grad_norm': 0.828779935836792, 'learning_rate': 0.0003973148148148148, 'epoch': 1.8083333333333333}
Step 6520: {'loss': 0.6618, 'grad_norm': 1.0005333423614502, 'learning_rate': 0.0003963888888888889, 'epoch': 1.8111111111111111}
Step 6530: {'loss': 0.5887, 'grad_norm': 0.976422131061554, 'learning_rate': 0.00039546296296296297, 'epoch': 1.8138888888888889}
Step 6540: {'loss': 0.6158, 'grad_norm': 0.851580023765564, 'learning_rate': 0.00039453703703703705, 'epoch': 1.8166666666666667}
Step 6550: {'loss': 0.6223, 'grad_norm': 0.7833102941513062, 'learning_rate': 0.0003936111111111111, 'epoch': 1.8194444444444444}
Step 6560: {'loss': 0.766, 'grad_norm': 0.839853048324585, 'learning_rate': 0.0003926851851851852, 'epoch': 1.8222222222222222}
Step 6570: {'loss': 0.6135, 'grad_norm': 0.7812259197235107, 'learning_rate': 0.0003917592592592593, 'epoch': 1.825}
Step 6580: {'loss': 0.6571, 'grad_norm': 0.870988667011261, 'learning_rate': 0.0003908333333333333, 'epoch': 1.8277777777777777}
Step 6590: {'loss': 0.6515, 'grad_norm': 1.0428218841552734, 'learning_rate': 0.00038990740740740744, 'epoch': 1.8305555555555557}
Step 6600: {'loss': 0.634, 'grad_norm': 1.0392398834228516, 'learning_rate': 0.00038898148148148147, 'epoch': 1.8333333333333335}
Step 6610: {'loss': 0.6192, 'grad_norm': 0.6494820713996887, 'learning_rate': 0.00038805555555555555, 'epoch': 1.8361111111111112}
Step 6620: {'loss': 0.6195, 'grad_norm': 0.6136878728866577, 'learning_rate': 0.0003871296296296297, 'epoch': 1.838888888888889}
Step 6630: {'loss': 0.6718, 'grad_norm': 0.8439782857894897, 'learning_rate': 0.0003862037037037037, 'epoch': 1.8416666666666668}
Step 6640: {'loss': 0.6843, 'grad_norm': 0.8369672894477844, 'learning_rate': 0.0003852777777777778, 'epoch': 1.8444444444444446}
Step 6650: {'loss': 0.6864, 'grad_norm': 0.9636010527610779, 'learning_rate': 0.00038435185185185187, 'epoch': 1.8472222222222223}
Step 6660: {'loss': 0.7205, 'grad_norm': 0.7401886582374573, 'learning_rate': 0.00038342592592592595, 'epoch': 1.85}
Step 6670: {'loss': 0.6337, 'grad_norm': 1.0007972717285156, 'learning_rate': 0.00038250000000000003, 'epoch': 1.8527777777777779}
Step 6680: {'loss': 0.6443, 'grad_norm': 0.6300867199897766, 'learning_rate': 0.00038157407407407405, 'epoch': 1.8555555555555556}
Step 6690: {'loss': 0.6503, 'grad_norm': 1.0280852317810059, 'learning_rate': 0.00038064814814814813, 'epoch': 1.8583333333333334}
Step 6700: {'loss': 0.6983, 'grad_norm': 0.6702498197555542, 'learning_rate': 0.00037972222222222227, 'epoch': 1.8611111111111112}
Step 6710: {'loss': 0.641, 'grad_norm': 0.8043636679649353, 'learning_rate': 0.0003787962962962963, 'epoch': 1.863888888888889}
Step 6720: {'loss': 0.6603, 'grad_norm': 0.7377650141716003, 'learning_rate': 0.00037787037037037037, 'epoch': 1.8666666666666667}
Step 6730: {'loss': 0.6965, 'grad_norm': 0.8904222846031189, 'learning_rate': 0.0003769444444444445, 'epoch': 1.8694444444444445}
Step 6740: {'loss': 0.6815, 'grad_norm': 1.0322117805480957, 'learning_rate': 0.00037601851851851853, 'epoch': 1.8722222222222222}
Step 6750: {'loss': 0.6381, 'grad_norm': 0.571058452129364, 'learning_rate': 0.0003750925925925926, 'epoch': 1.875}
Step 6760: {'loss': 0.6863, 'grad_norm': 0.9526206254959106, 'learning_rate': 0.00037416666666666664, 'epoch': 1.8777777777777778}
Step 6770: {'loss': 0.6982, 'grad_norm': 0.9805837273597717, 'learning_rate': 0.00037324074074074077, 'epoch': 1.8805555555555555}
Step 6780: {'loss': 0.6685, 'grad_norm': 0.9622757434844971, 'learning_rate': 0.00037231481481481485, 'epoch': 1.8833333333333333}
Step 6790: {'loss': 0.6544, 'grad_norm': 0.6750184893608093, 'learning_rate': 0.0003713888888888889, 'epoch': 1.886111111111111}
Step 6800: {'loss': 0.6641, 'grad_norm': 1.0317413806915283, 'learning_rate': 0.00037046296296296295, 'epoch': 1.8888888888888888}
Step 6810: {'loss': 0.654, 'grad_norm': 0.8630368709564209, 'learning_rate': 0.00036953703703703703, 'epoch': 1.8916666666666666}
Step 6820: {'loss': 0.6811, 'grad_norm': 0.8361721038818359, 'learning_rate': 0.0003686111111111111, 'epoch': 1.8944444444444444}
Step 6830: {'loss': 0.5884, 'grad_norm': 0.8275496363639832, 'learning_rate': 0.0003676851851851852, 'epoch': 1.8972222222222221}
Step 6840: {'loss': 0.6679, 'grad_norm': 0.9249782562255859, 'learning_rate': 0.0003667592592592593, 'epoch': 1.9}
Step 6850: {'loss': 0.7123, 'grad_norm': 0.6554723381996155, 'learning_rate': 0.00036583333333333335, 'epoch': 1.9027777777777777}
Step 6860: {'loss': 0.6216, 'grad_norm': 0.8973800539970398, 'learning_rate': 0.00036490740740740743, 'epoch': 1.9055555555555554}
Step 6870: {'loss': 0.7095, 'grad_norm': 1.0843521356582642, 'learning_rate': 0.00036398148148148146, 'epoch': 1.9083333333333332}
Step 6880: {'loss': 0.6718, 'grad_norm': 0.9629547595977783, 'learning_rate': 0.0003630555555555556, 'epoch': 1.911111111111111}
Step 6890: {'loss': 0.6401, 'grad_norm': 0.8433547616004944, 'learning_rate': 0.0003621296296296296, 'epoch': 1.9138888888888888}
Step 6900: {'loss': 0.6355, 'grad_norm': 0.7846249938011169, 'learning_rate': 0.0003612037037037037, 'epoch': 1.9166666666666665}
Step 6910: {'loss': 0.7036, 'grad_norm': 0.6804269552230835, 'learning_rate': 0.0003602777777777778, 'epoch': 1.9194444444444443}
Step 6920: {'loss': 0.6955, 'grad_norm': 0.9360872507095337, 'learning_rate': 0.00035935185185185186, 'epoch': 1.9222222222222223}
Step 6930: {'loss': 0.6654, 'grad_norm': 0.8714174628257751, 'learning_rate': 0.00035842592592592594, 'epoch': 1.925}
Step 6940: {'loss': 0.6743, 'grad_norm': 0.6834771037101746, 'learning_rate': 0.0003575, 'epoch': 1.9277777777777778}
Step 6950: {'loss': 0.6941, 'grad_norm': 0.5913121104240417, 'learning_rate': 0.0003565740740740741, 'epoch': 1.9305555555555556}
Step 6960: {'loss': 0.6428, 'grad_norm': 0.8518291115760803, 'learning_rate': 0.0003556481481481482, 'epoch': 1.9333333333333333}
Step 6970: {'loss': 0.6184, 'grad_norm': 0.9376748204231262, 'learning_rate': 0.0003547222222222222, 'epoch': 1.9361111111111111}
Step 6980: {'loss': 0.6265, 'grad_norm': 0.8067630529403687, 'learning_rate': 0.0003537962962962963, 'epoch': 1.9388888888888889}
Step 6990: {'loss': 0.654, 'grad_norm': 0.8818400502204895, 'learning_rate': 0.0003528703703703704, 'epoch': 1.9416666666666667}
Step 7000: {'loss': 0.6891, 'grad_norm': 1.205554723739624, 'learning_rate': 0.00035194444444444444, 'epoch': 1.9444444444444444}
Step 7010: {'loss': 0.652, 'grad_norm': 1.1295552253723145, 'learning_rate': 0.0003510185185185185, 'epoch': 1.9472222222222222}
Step 7020: {'loss': 0.6554, 'grad_norm': 0.9850383996963501, 'learning_rate': 0.0003500925925925926, 'epoch': 1.95}
Step 7030: {'loss': 0.6206, 'grad_norm': 0.625298023223877, 'learning_rate': 0.0003491666666666667, 'epoch': 1.9527777777777777}
Step 7040: {'loss': 0.7107, 'grad_norm': 1.1532297134399414, 'learning_rate': 0.00034824074074074076, 'epoch': 1.9555555555555557}
Step 7050: {'loss': 0.6555, 'grad_norm': 0.7612976431846619, 'learning_rate': 0.0003473148148148148, 'epoch': 1.9583333333333335}
Step 7060: {'loss': 0.6405, 'grad_norm': 0.8715088367462158, 'learning_rate': 0.0003463888888888889, 'epoch': 1.9611111111111112}
Step 7070: {'loss': 0.7197, 'grad_norm': 0.8833218216896057, 'learning_rate': 0.000345462962962963, 'epoch': 1.963888888888889}
Step 7080: {'loss': 0.6107, 'grad_norm': 0.8303826451301575, 'learning_rate': 0.000344537037037037, 'epoch': 1.9666666666666668}
Step 7090: {'loss': 0.6088, 'grad_norm': 0.8735034465789795, 'learning_rate': 0.0003436111111111111, 'epoch': 1.9694444444444446}
Step 7100: {'loss': 0.6472, 'grad_norm': 0.6857428550720215, 'learning_rate': 0.00034268518518518524, 'epoch': 1.9722222222222223}
Step 7110: {'loss': 0.6948, 'grad_norm': 1.027235507965088, 'learning_rate': 0.00034175925925925926, 'epoch': 1.975}
Step 7120: {'loss': 0.6567, 'grad_norm': 0.8557460904121399, 'learning_rate': 0.00034083333333333334, 'epoch': 1.9777777777777779}
Step 7130: {'loss': 0.6426, 'grad_norm': 0.8750475645065308, 'learning_rate': 0.00033990740740740737, 'epoch': 1.9805555555555556}
Step 7140: {'loss': 0.6643, 'grad_norm': 1.1007928848266602, 'learning_rate': 0.0003389814814814815, 'epoch': 1.9833333333333334}
Step 7150: {'loss': 0.6747, 'grad_norm': 0.7002270221710205, 'learning_rate': 0.0003380555555555556, 'epoch': 1.9861111111111112}
Step 7160: {'loss': 0.6482, 'grad_norm': 0.8989475965499878, 'learning_rate': 0.0003371296296296296, 'epoch': 1.988888888888889}
Step 7170: {'loss': 0.634, 'grad_norm': 1.21580171585083, 'learning_rate': 0.00033620370370370374, 'epoch': 1.9916666666666667}
Step 7180: {'loss': 0.6959, 'grad_norm': 0.6980317234992981, 'learning_rate': 0.00033527777777777777, 'epoch': 1.9944444444444445}
Step 7190: {'loss': 0.6819, 'grad_norm': 0.6902672648429871, 'learning_rate': 0.00033435185185185185, 'epoch': 1.9972222222222222}
Step 7200: {'loss': 0.6423, 'grad_norm': 0.6166542172431946, 'learning_rate': 0.0003334259259259259, 'epoch': 2.0}
Step 7210: {'loss': 0.6364, 'grad_norm': 0.6027740240097046, 'learning_rate': 0.0003325, 'epoch': 2.0027777777777778}
Step 7220: {'loss': 0.6363, 'grad_norm': 0.6221803426742554, 'learning_rate': 0.0003315740740740741, 'epoch': 2.0055555555555555}
Step 7230: {'loss': 0.653, 'grad_norm': 0.6565884351730347, 'learning_rate': 0.00033064814814814816, 'epoch': 2.0083333333333333}
Step 7240: {'loss': 0.6316, 'grad_norm': 0.6461201906204224, 'learning_rate': 0.00032972222222222224, 'epoch': 2.011111111111111}
Step 7250: {'loss': 0.638, 'grad_norm': 0.9108869433403015, 'learning_rate': 0.0003287962962962963, 'epoch': 2.013888888888889}
Step 7260: {'loss': 0.6488, 'grad_norm': 0.7917641997337341, 'learning_rate': 0.00032787037037037035, 'epoch': 2.0166666666666666}
Step 7270: {'loss': 0.6709, 'grad_norm': 0.9323894381523132, 'learning_rate': 0.00032694444444444443, 'epoch': 2.0194444444444444}
Step 7280: {'loss': 0.6587, 'grad_norm': 0.7125295996665955, 'learning_rate': 0.00032601851851851856, 'epoch': 2.022222222222222}
Step 7290: {'loss': 0.6712, 'grad_norm': 0.8595399260520935, 'learning_rate': 0.0003250925925925926, 'epoch': 2.025}
Step 7300: {'loss': 0.6493, 'grad_norm': 0.9113875031471252, 'learning_rate': 0.00032416666666666667, 'epoch': 2.0277777777777777}
Step 7310: {'loss': 0.6112, 'grad_norm': 1.0050921440124512, 'learning_rate': 0.00032324074074074075, 'epoch': 2.0305555555555554}
Step 7320: {'loss': 0.6918, 'grad_norm': 0.6612622737884521, 'learning_rate': 0.0003223148148148148, 'epoch': 2.033333333333333}
Step 7330: {'loss': 0.6428, 'grad_norm': 0.7247720956802368, 'learning_rate': 0.0003213888888888889, 'epoch': 2.036111111111111}
Step 7340: {'loss': 0.6726, 'grad_norm': 0.9164385199546814, 'learning_rate': 0.00032046296296296293, 'epoch': 2.0388888888888888}
Step 7350: {'loss': 0.6237, 'grad_norm': 0.7618330121040344, 'learning_rate': 0.00031953703703703707, 'epoch': 2.0416666666666665}
Step 7360: {'loss': 0.6611, 'grad_norm': 0.866963267326355, 'learning_rate': 0.00031861111111111115, 'epoch': 2.0444444444444443}
Step 7370: {'loss': 0.6243, 'grad_norm': 0.8436292409896851, 'learning_rate': 0.00031768518518518517, 'epoch': 2.047222222222222}
Step 7380: {'loss': 0.6656, 'grad_norm': 0.7061527371406555, 'learning_rate': 0.00031675925925925925, 'epoch': 2.05}
Step 7390: {'loss': 0.5826, 'grad_norm': 0.6495361328125, 'learning_rate': 0.0003158333333333334, 'epoch': 2.0527777777777776}
Step 7400: {'loss': 0.6327, 'grad_norm': 0.6849541664123535, 'learning_rate': 0.0003149074074074074, 'epoch': 2.0555555555555554}
Step 7410: {'loss': 0.7169, 'grad_norm': 0.6871500611305237, 'learning_rate': 0.0003139814814814815, 'epoch': 2.058333333333333}
Step 7420: {'loss': 0.6779, 'grad_norm': 0.7146351933479309, 'learning_rate': 0.0003130555555555555, 'epoch': 2.061111111111111}
Step 7430: {'loss': 0.6842, 'grad_norm': 0.6658228039741516, 'learning_rate': 0.00031212962962962965, 'epoch': 2.063888888888889}
Step 7440: {'loss': 0.637, 'grad_norm': 0.9088107943534851, 'learning_rate': 0.00031120370370370373, 'epoch': 2.066666666666667}
Step 7450: {'loss': 0.637, 'grad_norm': 0.8657804727554321, 'learning_rate': 0.00031027777777777775, 'epoch': 2.0694444444444446}
Step 7460: {'loss': 0.644, 'grad_norm': 0.8595170378684998, 'learning_rate': 0.0003093518518518519, 'epoch': 2.0722222222222224}
Step 7470: {'loss': 0.6885, 'grad_norm': 0.7222751975059509, 'learning_rate': 0.00030842592592592597, 'epoch': 2.075}
Step 7480: {'loss': 0.7108, 'grad_norm': 0.9629441499710083, 'learning_rate': 0.0003075, 'epoch': 2.077777777777778}
Step 7490: {'loss': 0.6349, 'grad_norm': 0.8137246966362, 'learning_rate': 0.0003065740740740741, 'epoch': 2.0805555555555557}
Step 7500: {'loss': 0.6846, 'grad_norm': 0.7343961596488953, 'learning_rate': 0.00030564814814814815, 'epoch': 2.0833333333333335}
Step 7510: {'loss': 0.6229, 'grad_norm': 0.6918264031410217, 'learning_rate': 0.00030472222222222223, 'epoch': 2.0861111111111112}
Step 7520: {'loss': 0.6438, 'grad_norm': 0.9458678364753723, 'learning_rate': 0.0003037962962962963, 'epoch': 2.088888888888889}
Step 7530: {'loss': 0.6385, 'grad_norm': 0.8217737674713135, 'learning_rate': 0.00030287037037037034, 'epoch': 2.091666666666667}
Step 7540: {'loss': 0.6664, 'grad_norm': 1.3029754161834717, 'learning_rate': 0.00030194444444444447, 'epoch': 2.0944444444444446}
Step 7550: {'loss': 0.6326, 'grad_norm': 0.8674917221069336, 'learning_rate': 0.00030101851851851855, 'epoch': 2.0972222222222223}
Step 7560: {'loss': 0.6261, 'grad_norm': 0.8362931609153748, 'learning_rate': 0.0003000925925925926, 'epoch': 2.1}
Step 7570: {'loss': 0.7233, 'grad_norm': 0.9607522487640381, 'learning_rate': 0.0002991666666666667, 'epoch': 2.102777777777778}
Step 7580: {'loss': 0.6742, 'grad_norm': 0.9918955564498901, 'learning_rate': 0.00029824074074074074, 'epoch': 2.1055555555555556}
Step 7590: {'loss': 0.6448, 'grad_norm': 0.6746413111686707, 'learning_rate': 0.0002973148148148148, 'epoch': 2.1083333333333334}
Step 7600: {'loss': 0.6087, 'grad_norm': 0.7688090801239014, 'learning_rate': 0.0002963888888888889, 'epoch': 2.111111111111111}
Step 7610: {'loss': 0.6561, 'grad_norm': 0.916523277759552, 'learning_rate': 0.000295462962962963, 'epoch': 2.113888888888889}
Step 7620: {'loss': 0.6339, 'grad_norm': 1.095767855644226, 'learning_rate': 0.00029453703703703705, 'epoch': 2.1166666666666667}
Step 7630: {'loss': 0.6791, 'grad_norm': 0.9369708299636841, 'learning_rate': 0.0002936111111111111, 'epoch': 2.1194444444444445}
Step 7640: {'loss': 0.6311, 'grad_norm': 0.7339886426925659, 'learning_rate': 0.00029268518518518516, 'epoch': 2.1222222222222222}
Step 7650: {'loss': 0.6825, 'grad_norm': 0.9407808184623718, 'learning_rate': 0.0002917592592592593, 'epoch': 2.125}
Step 7660: {'loss': 0.6884, 'grad_norm': 0.6571096777915955, 'learning_rate': 0.0002908333333333333, 'epoch': 2.1277777777777778}
Step 7670: {'loss': 0.6369, 'grad_norm': 0.8173349499702454, 'learning_rate': 0.0002899074074074074, 'epoch': 2.1305555555555555}
Step 7680: {'loss': 0.6488, 'grad_norm': 0.6034359931945801, 'learning_rate': 0.00028898148148148153, 'epoch': 2.1333333333333333}
Step 7690: {'loss': 0.6745, 'grad_norm': 0.7065454125404358, 'learning_rate': 0.00028805555555555556, 'epoch': 2.136111111111111}
Step 7700: {'loss': 0.6273, 'grad_norm': 0.8368133306503296, 'learning_rate': 0.00028712962962962964, 'epoch': 2.138888888888889}
Step 7710: {'loss': 0.7214, 'grad_norm': 1.2411588430404663, 'learning_rate': 0.00028620370370370366, 'epoch': 2.1416666666666666}
Step 7720: {'loss': 0.6538, 'grad_norm': 0.8041263818740845, 'learning_rate': 0.0002852777777777778, 'epoch': 2.1444444444444444}
Step 7730: {'loss': 0.7219, 'grad_norm': 0.8899211883544922, 'learning_rate': 0.0002843518518518519, 'epoch': 2.147222222222222}
Step 7740: {'loss': 0.6611, 'grad_norm': 1.034676194190979, 'learning_rate': 0.0002834259259259259, 'epoch': 2.15}
Step 7750: {'loss': 0.6237, 'grad_norm': 1.050476312637329, 'learning_rate': 0.0002825, 'epoch': 2.1527777777777777}
Step 7760: {'loss': 0.6425, 'grad_norm': 0.8561080098152161, 'learning_rate': 0.0002815740740740741, 'epoch': 2.1555555555555554}
Step 7770: {'loss': 0.6721, 'grad_norm': 1.0322657823562622, 'learning_rate': 0.00028064814814814814, 'epoch': 2.158333333333333}
Step 7780: {'loss': 0.6178, 'grad_norm': 0.8291898369789124, 'learning_rate': 0.0002797222222222222, 'epoch': 2.161111111111111}
Step 7790: {'loss': 0.6626, 'grad_norm': 0.7371812462806702, 'learning_rate': 0.0002787962962962963, 'epoch': 2.1638888888888888}
Step 7800: {'loss': 0.7112, 'grad_norm': 0.6828094720840454, 'learning_rate': 0.0002778703703703704, 'epoch': 2.1666666666666665}
Step 7810: {'loss': 0.6028, 'grad_norm': 0.8196917176246643, 'learning_rate': 0.00027694444444444446, 'epoch': 2.1694444444444443}
Step 7820: {'loss': 0.5631, 'grad_norm': 1.012894868850708, 'learning_rate': 0.0002760185185185185, 'epoch': 2.172222222222222}
Step 7830: {'loss': 0.6294, 'grad_norm': 0.8883897662162781, 'learning_rate': 0.0002750925925925926, 'epoch': 2.175}
Step 7840: {'loss': 0.5811, 'grad_norm': 0.8652555346488953, 'learning_rate': 0.0002741666666666667, 'epoch': 2.1777777777777776}
Step 7850: {'loss': 0.6795, 'grad_norm': 1.1235532760620117, 'learning_rate': 0.0002732407407407407, 'epoch': 2.1805555555555554}
Step 7860: {'loss': 0.6759, 'grad_norm': 0.7417702674865723, 'learning_rate': 0.0002723148148148148, 'epoch': 2.183333333333333}
Step 7870: {'loss': 0.6855, 'grad_norm': 0.7338894605636597, 'learning_rate': 0.0002713888888888889, 'epoch': 2.186111111111111}
Step 7880: {'loss': 0.6608, 'grad_norm': 0.6519621014595032, 'learning_rate': 0.00027046296296296296, 'epoch': 2.188888888888889}
Step 7890: {'loss': 0.6664, 'grad_norm': 0.7383039593696594, 'learning_rate': 0.00026953703703703704, 'epoch': 2.191666666666667}
Step 7900: {'loss': 0.5826, 'grad_norm': 0.7553963661193848, 'learning_rate': 0.0002686111111111111, 'epoch': 2.1944444444444446}
Step 7910: {'loss': 0.6817, 'grad_norm': 0.8632181882858276, 'learning_rate': 0.0002676851851851852, 'epoch': 2.1972222222222224}
Step 7920: {'loss': 0.6642, 'grad_norm': 1.169599175453186, 'learning_rate': 0.0002667592592592593, 'epoch': 2.2}
Step 7930: {'loss': 0.6212, 'grad_norm': 0.7276008725166321, 'learning_rate': 0.0002658333333333333, 'epoch': 2.202777777777778}
Step 7940: {'loss': 0.667, 'grad_norm': 0.7959561347961426, 'learning_rate': 0.00026490740740740744, 'epoch': 2.2055555555555557}
Step 7950: {'loss': 0.6013, 'grad_norm': 1.2067234516143799, 'learning_rate': 0.00026398148148148147, 'epoch': 2.2083333333333335}
Step 7960: {'loss': 0.6514, 'grad_norm': 1.0632140636444092, 'learning_rate': 0.00026305555555555555, 'epoch': 2.2111111111111112}
Step 7970: {'loss': 0.6767, 'grad_norm': 1.3525073528289795, 'learning_rate': 0.0002621296296296297, 'epoch': 2.213888888888889}
Step 7980: {'loss': 0.6807, 'grad_norm': 0.7848325371742249, 'learning_rate': 0.0002612037037037037, 'epoch': 2.216666666666667}
Step 7990: {'loss': 0.622, 'grad_norm': 0.7079116702079773, 'learning_rate': 0.0002602777777777778, 'epoch': 2.2194444444444446}
Step 8000: {'loss': 0.6546, 'grad_norm': 1.0982197523117065, 'learning_rate': 0.00025935185185185187, 'epoch': 2.2222222222222223}
Step 8010: {'loss': 0.6194, 'grad_norm': 0.755702018737793, 'learning_rate': 0.00025842592592592595, 'epoch': 2.225}
Step 8020: {'loss': 0.5962, 'grad_norm': 1.092740774154663, 'learning_rate': 0.0002575, 'epoch': 2.227777777777778}
Step 8030: {'loss': 0.6387, 'grad_norm': 0.916525661945343, 'learning_rate': 0.00025657407407407405, 'epoch': 2.2305555555555556}
Step 8040: {'loss': 0.704, 'grad_norm': 0.7761074304580688, 'learning_rate': 0.00025564814814814813, 'epoch': 2.2333333333333334}
Step 8050: {'loss': 0.5929, 'grad_norm': 1.0710538625717163, 'learning_rate': 0.00025472222222222226, 'epoch': 2.236111111111111}
Step 8060: {'loss': 0.6518, 'grad_norm': 1.0164364576339722, 'learning_rate': 0.0002537962962962963, 'epoch': 2.238888888888889}
Step 8070: {'loss': 0.6418, 'grad_norm': 0.8721168041229248, 'learning_rate': 0.00025287037037037037, 'epoch': 2.2416666666666667}
Step 8080: {'loss': 0.6635, 'grad_norm': 1.0121592283248901, 'learning_rate': 0.0002519444444444445, 'epoch': 2.2444444444444445}
Step 8090: {'loss': 0.6667, 'grad_norm': 1.3487926721572876, 'learning_rate': 0.00025101851851851853, 'epoch': 2.2472222222222222}
Step 8100: {'loss': 0.6, 'grad_norm': 0.8367531895637512, 'learning_rate': 0.0002500925925925926, 'epoch': 2.25}
Step 8110: {'loss': 0.6559, 'grad_norm': 0.9123650789260864, 'learning_rate': 0.0002491666666666667, 'epoch': 2.2527777777777778}
Step 8120: {'loss': 0.6613, 'grad_norm': 0.7615652680397034, 'learning_rate': 0.0002482407407407407, 'epoch': 2.2555555555555555}
Step 8130: {'loss': 0.6392, 'grad_norm': 0.9595277905464172, 'learning_rate': 0.00024731481481481485, 'epoch': 2.2583333333333333}
Step 8140: {'loss': 0.6937, 'grad_norm': 0.6071203351020813, 'learning_rate': 0.00024638888888888887, 'epoch': 2.261111111111111}
Step 8150: {'loss': 0.6818, 'grad_norm': 0.6326594948768616, 'learning_rate': 0.00024546296296296295, 'epoch': 2.263888888888889}
Step 8160: {'loss': 0.5669, 'grad_norm': 1.0255450010299683, 'learning_rate': 0.00024453703703703703, 'epoch': 2.2666666666666666}
Step 8170: {'loss': 0.5959, 'grad_norm': 0.899308443069458, 'learning_rate': 0.0002436111111111111, 'epoch': 2.2694444444444444}
Step 8180: {'loss': 0.74, 'grad_norm': 0.8165244460105896, 'learning_rate': 0.0002426851851851852, 'epoch': 2.272222222222222}
Step 8190: {'loss': 0.6237, 'grad_norm': 0.792343258857727, 'learning_rate': 0.00024175925925925927, 'epoch': 2.275}
Step 8200: {'loss': 0.6861, 'grad_norm': 1.081024408340454, 'learning_rate': 0.00024083333333333335, 'epoch': 2.2777777777777777}
Step 8210: {'loss': 0.5949, 'grad_norm': 0.6623978614807129, 'learning_rate': 0.0002399074074074074, 'epoch': 2.2805555555555554}
Step 8220: {'loss': 0.6593, 'grad_norm': 0.9741553664207458, 'learning_rate': 0.00023898148148148148, 'epoch': 2.283333333333333}
Step 8230: {'loss': 0.6081, 'grad_norm': 0.7216392755508423, 'learning_rate': 0.00023805555555555556, 'epoch': 2.286111111111111}
Step 8240: {'loss': 0.6586, 'grad_norm': 0.9268730878829956, 'learning_rate': 0.00023712962962962964, 'epoch': 2.2888888888888888}
Step 8250: {'loss': 0.721, 'grad_norm': 0.8645563721656799, 'learning_rate': 0.0002362037037037037, 'epoch': 2.2916666666666665}
Step 8260: {'loss': 0.6308, 'grad_norm': 0.9519358277320862, 'learning_rate': 0.00023527777777777777, 'epoch': 2.2944444444444443}
Step 8270: {'loss': 0.5933, 'grad_norm': 0.8050002455711365, 'learning_rate': 0.00023435185185185185, 'epoch': 2.297222222222222}
Step 8280: {'loss': 0.6496, 'grad_norm': 0.8151520490646362, 'learning_rate': 0.00023342592592592593, 'epoch': 2.3}
Step 8290: {'loss': 0.6305, 'grad_norm': 0.825842559337616, 'learning_rate': 0.0002325, 'epoch': 2.3027777777777776}
Step 8300: {'loss': 0.72, 'grad_norm': 0.8755457401275635, 'learning_rate': 0.00023157407407407407, 'epoch': 2.3055555555555554}
Step 8310: {'loss': 0.6417, 'grad_norm': 0.9934532642364502, 'learning_rate': 0.00023064814814814817, 'epoch': 2.3083333333333336}
Step 8320: {'loss': 0.6375, 'grad_norm': 0.7030176520347595, 'learning_rate': 0.00022972222222222223, 'epoch': 2.311111111111111}
Step 8330: {'loss': 0.5977, 'grad_norm': 0.6408690810203552, 'learning_rate': 0.0002287962962962963, 'epoch': 2.313888888888889}
Step 8340: {'loss': 0.6432, 'grad_norm': 0.6182078719139099, 'learning_rate': 0.00022787037037037036, 'epoch': 2.3166666666666664}
Step 8350: {'loss': 0.6367, 'grad_norm': 0.8826351165771484, 'learning_rate': 0.00022694444444444446, 'epoch': 2.3194444444444446}
Step 8360: {'loss': 0.6776, 'grad_norm': 1.065885305404663, 'learning_rate': 0.00022601851851851852, 'epoch': 2.3222222222222224}
Step 8370: {'loss': 0.6625, 'grad_norm': 0.9458047747612, 'learning_rate': 0.0002250925925925926, 'epoch': 2.325}
Step 8380: {'loss': 0.6441, 'grad_norm': 0.6056820154190063, 'learning_rate': 0.00022416666666666665, 'epoch': 2.327777777777778}
Step 8390: {'loss': 0.659, 'grad_norm': 0.8523105382919312, 'learning_rate': 0.00022324074074074076, 'epoch': 2.3305555555555557}
Step 8400: {'loss': 0.6395, 'grad_norm': 0.8057348132133484, 'learning_rate': 0.00022231481481481484, 'epoch': 2.3333333333333335}
Step 8410: {'loss': 0.6483, 'grad_norm': 0.731612503528595, 'learning_rate': 0.0002213888888888889, 'epoch': 2.3361111111111112}
Step 8420: {'loss': 0.6519, 'grad_norm': 1.0495482683181763, 'learning_rate': 0.00022046296296296297, 'epoch': 2.338888888888889}
Step 8430: {'loss': 0.6524, 'grad_norm': 0.890943706035614, 'learning_rate': 0.00021953703703703705, 'epoch': 2.341666666666667}
Step 8440: {'loss': 0.6677, 'grad_norm': 1.1430824995040894, 'learning_rate': 0.00021861111111111113, 'epoch': 2.3444444444444446}
Step 8450: {'loss': 0.6215, 'grad_norm': 1.0607374906539917, 'learning_rate': 0.00021768518518518518, 'epoch': 2.3472222222222223}
Step 8460: {'loss': 0.6603, 'grad_norm': 0.6057912707328796, 'learning_rate': 0.00021675925925925926, 'epoch': 2.35}
Step 8470: {'loss': 0.6939, 'grad_norm': 0.6688693165779114, 'learning_rate': 0.00021583333333333334, 'epoch': 2.352777777777778}
Step 8480: {'loss': 0.643, 'grad_norm': 0.7363706231117249, 'learning_rate': 0.00021490740740740742, 'epoch': 2.3555555555555556}
Step 8490: {'loss': 0.6586, 'grad_norm': 0.8775315284729004, 'learning_rate': 0.0002139814814814815, 'epoch': 2.3583333333333334}
Step 8500: {'loss': 0.6644, 'grad_norm': 0.8881251811981201, 'learning_rate': 0.00021305555555555555, 'epoch': 2.361111111111111}
Step 8510: {'loss': 0.6057, 'grad_norm': 0.9592494368553162, 'learning_rate': 0.00021212962962962966, 'epoch': 2.363888888888889}
Step 8520: {'loss': 0.6272, 'grad_norm': 0.7461987733840942, 'learning_rate': 0.0002112037037037037, 'epoch': 2.3666666666666667}
Step 8530: {'loss': 0.6047, 'grad_norm': 0.9227849841117859, 'learning_rate': 0.0002102777777777778, 'epoch': 2.3694444444444445}
Step 8540: {'loss': 0.6429, 'grad_norm': 1.1975998878479004, 'learning_rate': 0.00020935185185185184, 'epoch': 2.3722222222222222}
Step 8550: {'loss': 0.6244, 'grad_norm': 0.7250137329101562, 'learning_rate': 0.00020842592592592592, 'epoch': 2.375}
Step 8560: {'loss': 0.7048, 'grad_norm': 0.661571204662323, 'learning_rate': 0.0002075, 'epoch': 2.3777777777777778}
Step 8570: {'loss': 0.6393, 'grad_norm': 0.8413398861885071, 'learning_rate': 0.00020657407407407408, 'epoch': 2.3805555555555555}
Step 8580: {'loss': 0.6433, 'grad_norm': 0.9316430687904358, 'learning_rate': 0.00020564814814814813, 'epoch': 2.3833333333333333}
Step 8590: {'loss': 0.6862, 'grad_norm': 0.8575371503829956, 'learning_rate': 0.00020472222222222221, 'epoch': 2.386111111111111}
Step 8600: {'loss': 0.6504, 'grad_norm': 0.6986721754074097, 'learning_rate': 0.00020379629629629632, 'epoch': 2.388888888888889}
Step 8610: {'loss': 0.6442, 'grad_norm': 0.7000221014022827, 'learning_rate': 0.00020287037037037037, 'epoch': 2.3916666666666666}
Step 8620: {'loss': 0.6796, 'grad_norm': 0.923176109790802, 'learning_rate': 0.00020194444444444445, 'epoch': 2.3944444444444444}
Step 8630: {'loss': 0.6358, 'grad_norm': 0.9430415630340576, 'learning_rate': 0.0002010185185185185, 'epoch': 2.397222222222222}
Step 8640: {'loss': 0.6863, 'grad_norm': 0.9716782569885254, 'learning_rate': 0.0002000925925925926, 'epoch': 2.4}
Step 8650: {'loss': 0.6619, 'grad_norm': 0.8911420106887817, 'learning_rate': 0.00019916666666666667, 'epoch': 2.4027777777777777}
Step 8660: {'loss': 0.6056, 'grad_norm': 0.8187595009803772, 'learning_rate': 0.00019824074074074074, 'epoch': 2.4055555555555554}
Step 8670: {'loss': 0.7373, 'grad_norm': 1.1227723360061646, 'learning_rate': 0.0001973148148148148, 'epoch': 2.408333333333333}
Step 8680: {'loss': 0.6336, 'grad_norm': 0.7446385622024536, 'learning_rate': 0.0001963888888888889, 'epoch': 2.411111111111111}
Step 8690: {'loss': 0.6617, 'grad_norm': 0.8283422589302063, 'learning_rate': 0.00019546296296296296, 'epoch': 2.4138888888888888}
Step 8700: {'loss': 0.6642, 'grad_norm': 1.1163064241409302, 'learning_rate': 0.00019453703703703704, 'epoch': 2.4166666666666665}
Step 8710: {'loss': 0.6297, 'grad_norm': 0.8272415995597839, 'learning_rate': 0.00019361111111111112, 'epoch': 2.4194444444444443}
Step 8720: {'loss': 0.7186, 'grad_norm': 1.3340659141540527, 'learning_rate': 0.0001926851851851852, 'epoch': 2.422222222222222}
Step 8730: {'loss': 0.5867, 'grad_norm': 0.7728515267372131, 'learning_rate': 0.00019175925925925928, 'epoch': 2.425}
Step 8740: {'loss': 0.616, 'grad_norm': 0.9356815218925476, 'learning_rate': 0.00019083333333333333, 'epoch': 2.4277777777777776}
Step 8750: {'loss': 0.6116, 'grad_norm': 0.6609739661216736, 'learning_rate': 0.0001899074074074074, 'epoch': 2.4305555555555554}
Step 8760: {'loss': 0.594, 'grad_norm': 0.7094157934188843, 'learning_rate': 0.0001889814814814815, 'epoch': 2.4333333333333336}
Step 8770: {'loss': 0.6317, 'grad_norm': 0.9318602085113525, 'learning_rate': 0.00018805555555555557, 'epoch': 2.436111111111111}
Step 8780: {'loss': 0.6253, 'grad_norm': 1.1752047538757324, 'learning_rate': 0.00018712962962962962, 'epoch': 2.438888888888889}
Step 8790: {'loss': 0.6413, 'grad_norm': 1.02479088306427, 'learning_rate': 0.0001862037037037037, 'epoch': 2.4416666666666664}
Step 8800: {'loss': 0.6194, 'grad_norm': 0.75111323595047, 'learning_rate': 0.0001852777777777778, 'epoch': 2.4444444444444446}
Step 8810: {'loss': 0.6339, 'grad_norm': 1.042405605316162, 'learning_rate': 0.00018435185185185186, 'epoch': 2.4472222222222224}
Step 8820: {'loss': 0.6541, 'grad_norm': 0.9296086430549622, 'learning_rate': 0.00018342592592592594, 'epoch': 2.45}
Step 8830: {'loss': 0.6412, 'grad_norm': 0.9529194831848145, 'learning_rate': 0.0001825, 'epoch': 2.452777777777778}
Step 8840: {'loss': 0.629, 'grad_norm': 0.8338598608970642, 'learning_rate': 0.0001815740740740741, 'epoch': 2.4555555555555557}
Step 8850: {'loss': 0.6533, 'grad_norm': 1.154700756072998, 'learning_rate': 0.00018064814814814815, 'epoch': 2.4583333333333335}
Step 8860: {'loss': 0.6097, 'grad_norm': 0.84744793176651, 'learning_rate': 0.00017972222222222223, 'epoch': 2.4611111111111112}
Step 8870: {'loss': 0.6687, 'grad_norm': 1.1238470077514648, 'learning_rate': 0.00017879629629629628, 'epoch': 2.463888888888889}
Step 8880: {'loss': 0.6161, 'grad_norm': 0.8148899674415588, 'learning_rate': 0.0001778703703703704, 'epoch': 2.466666666666667}
Step 8890: {'loss': 0.6512, 'grad_norm': 0.8065722584724426, 'learning_rate': 0.00017694444444444444, 'epoch': 2.4694444444444446}
Step 8900: {'loss': 0.6417, 'grad_norm': 0.9418556094169617, 'learning_rate': 0.00017601851851851852, 'epoch': 2.4722222222222223}
Step 8910: {'loss': 0.6199, 'grad_norm': 0.8176323175430298, 'learning_rate': 0.0001750925925925926, 'epoch': 2.475}
Step 8920: {'loss': 0.6028, 'grad_norm': 1.1242438554763794, 'learning_rate': 0.00017416666666666668, 'epoch': 2.477777777777778}
Step 8930: {'loss': 0.6518, 'grad_norm': 0.6255040764808655, 'learning_rate': 0.00017324074074074076, 'epoch': 2.4805555555555556}
Step 8940: {'loss': 0.6519, 'grad_norm': 0.5772543549537659, 'learning_rate': 0.0001723148148148148, 'epoch': 2.4833333333333334}
Step 8950: {'loss': 0.6683, 'grad_norm': 0.6943292021751404, 'learning_rate': 0.0001713888888888889, 'epoch': 2.486111111111111}
Step 8960: {'loss': 0.6531, 'grad_norm': 0.7209966778755188, 'learning_rate': 0.00017046296296296295, 'epoch': 2.488888888888889}
Step 8970: {'loss': 0.6266, 'grad_norm': 0.677564799785614, 'learning_rate': 0.00016953703703703705, 'epoch': 2.4916666666666667}
Step 8980: {'loss': 0.6858, 'grad_norm': 0.7244075536727905, 'learning_rate': 0.0001686111111111111, 'epoch': 2.4944444444444445}
Step 8990: {'loss': 0.6542, 'grad_norm': 0.9138884544372559, 'learning_rate': 0.00016768518518518518, 'epoch': 2.4972222222222222}
Step 9000: {'loss': 0.658, 'grad_norm': 0.6544936895370483, 'learning_rate': 0.00016675925925925924, 'epoch': 2.5}
Step 9010: {'loss': 0.6281, 'grad_norm': 0.6846991181373596, 'learning_rate': 0.00016583333333333334, 'epoch': 2.5027777777777778}
Step 9020: {'loss': 0.6113, 'grad_norm': 0.7331544160842896, 'learning_rate': 0.00016490740740740742, 'epoch': 2.5055555555555555}
Step 9030: {'loss': 0.6771, 'grad_norm': 0.8820978999137878, 'learning_rate': 0.00016398148148148148, 'epoch': 2.5083333333333333}
Step 9040: {'loss': 0.6669, 'grad_norm': 0.7686078548431396, 'learning_rate': 0.00016305555555555556, 'epoch': 2.511111111111111}
Step 9050: {'loss': 0.6449, 'grad_norm': 0.6719695329666138, 'learning_rate': 0.00016212962962962964, 'epoch': 2.513888888888889}
Step 9060: {'loss': 0.6377, 'grad_norm': 0.674659013748169, 'learning_rate': 0.00016120370370370371, 'epoch': 2.5166666666666666}
Step 9070: {'loss': 0.6688, 'grad_norm': 0.759141206741333, 'learning_rate': 0.00016027777777777777, 'epoch': 2.5194444444444444}
Step 9080: {'loss': 0.6412, 'grad_norm': 1.4427350759506226, 'learning_rate': 0.00015935185185185185, 'epoch': 2.522222222222222}
Step 9090: {'loss': 0.6277, 'grad_norm': 0.8935546278953552, 'learning_rate': 0.00015842592592592593, 'epoch': 2.525}
Step 9100: {'loss': 0.6553, 'grad_norm': 0.5671314597129822, 'learning_rate': 0.0001575, 'epoch': 2.5277777777777777}
Step 9110: {'loss': 0.5833, 'grad_norm': 0.6973909139633179, 'learning_rate': 0.00015657407407407409, 'epoch': 2.5305555555555554}
Step 9120: {'loss': 0.6323, 'grad_norm': 0.8242277503013611, 'learning_rate': 0.00015564814814814814, 'epoch': 2.533333333333333}
Step 9130: {'loss': 0.6784, 'grad_norm': 0.714238166809082, 'learning_rate': 0.00015472222222222225, 'epoch': 2.536111111111111}
Step 9140: {'loss': 0.6725, 'grad_norm': 1.073338270187378, 'learning_rate': 0.0001537962962962963, 'epoch': 2.5388888888888888}
Step 9150: {'loss': 0.5821, 'grad_norm': 0.8647600412368774, 'learning_rate': 0.00015287037037037038, 'epoch': 2.5416666666666665}
Step 9160: {'loss': 0.7345, 'grad_norm': 0.8614096641540527, 'learning_rate': 0.00015194444444444443, 'epoch': 2.5444444444444443}
Step 9170: {'loss': 0.6398, 'grad_norm': 0.72278892993927, 'learning_rate': 0.00015101851851851854, 'epoch': 2.5472222222222225}
Step 9180: {'loss': 0.5916, 'grad_norm': 0.8470430970191956, 'learning_rate': 0.0001500925925925926, 'epoch': 2.55}
Step 9190: {'loss': 0.6308, 'grad_norm': 0.5825691819190979, 'learning_rate': 0.00014916666666666667, 'epoch': 2.552777777777778}
Step 9200: {'loss': 0.6506, 'grad_norm': 0.6866015791893005, 'learning_rate': 0.00014824074074074072, 'epoch': 2.5555555555555554}
Step 9210: {'loss': 0.6565, 'grad_norm': 1.6225001811981201, 'learning_rate': 0.00014731481481481483, 'epoch': 2.5583333333333336}
Step 9220: {'loss': 0.6657, 'grad_norm': 1.312111496925354, 'learning_rate': 0.0001463888888888889, 'epoch': 2.561111111111111}
Step 9230: {'loss': 0.6969, 'grad_norm': 0.7908146977424622, 'learning_rate': 0.00014546296296296296, 'epoch': 2.563888888888889}
Step 9240: {'loss': 0.7044, 'grad_norm': 0.7105810642242432, 'learning_rate': 0.00014453703703703704, 'epoch': 2.5666666666666664}
Step 9250: {'loss': 0.6936, 'grad_norm': 0.6428616046905518, 'learning_rate': 0.00014361111111111112, 'epoch': 2.5694444444444446}
Step 9260: {'loss': 0.6559, 'grad_norm': 1.0248098373413086, 'learning_rate': 0.0001426851851851852, 'epoch': 2.572222222222222}
Step 9270: {'loss': 0.6561, 'grad_norm': 0.94749516248703, 'learning_rate': 0.00014175925925925925, 'epoch': 2.575}
Step 9280: {'loss': 0.5723, 'grad_norm': 0.7776075005531311, 'learning_rate': 0.00014083333333333333, 'epoch': 2.5777777777777775}
Step 9290: {'loss': 0.6928, 'grad_norm': 0.9576674103736877, 'learning_rate': 0.0001399074074074074, 'epoch': 2.5805555555555557}
Step 9300: {'loss': 0.6899, 'grad_norm': 0.6350960731506348, 'learning_rate': 0.0001389814814814815, 'epoch': 2.5833333333333335}
Step 9310: {'loss': 0.5897, 'grad_norm': 1.0900702476501465, 'learning_rate': 0.00013805555555555554, 'epoch': 2.5861111111111112}
Step 9320: {'loss': 0.6455, 'grad_norm': 0.9881388545036316, 'learning_rate': 0.00013712962962962962, 'epoch': 2.588888888888889}
Step 9330: {'loss': 0.6563, 'grad_norm': 1.038124680519104, 'learning_rate': 0.00013620370370370373, 'epoch': 2.591666666666667}
Step 9340: {'loss': 0.6656, 'grad_norm': 1.1608684062957764, 'learning_rate': 0.00013527777777777778, 'epoch': 2.5944444444444446}
Step 9350: {'loss': 0.6474, 'grad_norm': 1.0360956192016602, 'learning_rate': 0.00013435185185185186, 'epoch': 2.5972222222222223}
Step 9360: {'loss': 0.6846, 'grad_norm': 0.6169044971466064, 'learning_rate': 0.00013342592592592592, 'epoch': 2.6}
Step 9370: {'loss': 0.6579, 'grad_norm': 0.8780419826507568, 'learning_rate': 0.00013250000000000002, 'epoch': 2.602777777777778}
Step 9380: {'loss': 0.6583, 'grad_norm': 1.0376157760620117, 'learning_rate': 0.00013157407407407407, 'epoch': 2.6055555555555556}
Step 9390: {'loss': 0.6539, 'grad_norm': 1.2516518831253052, 'learning_rate': 0.00013064814814814815, 'epoch': 2.6083333333333334}
Step 9400: {'loss': 0.6058, 'grad_norm': 0.7279247641563416, 'learning_rate': 0.0001297222222222222, 'epoch': 2.611111111111111}
Step 9410: {'loss': 0.6237, 'grad_norm': 0.8876640796661377, 'learning_rate': 0.0001287962962962963, 'epoch': 2.613888888888889}
Step 9420: {'loss': 0.6791, 'grad_norm': 1.1247690916061401, 'learning_rate': 0.0001278703703703704, 'epoch': 2.6166666666666667}
Step 9430: {'loss': 0.5814, 'grad_norm': 0.551413357257843, 'learning_rate': 0.00012694444444444445, 'epoch': 2.6194444444444445}
Step 9440: {'loss': 0.6649, 'grad_norm': 0.9070808291435242, 'learning_rate': 0.00012601851851851853, 'epoch': 2.6222222222222222}
Step 9450: {'loss': 0.7074, 'grad_norm': 1.1422224044799805, 'learning_rate': 0.00012509259259259258, 'epoch': 2.625}
Step 9460: {'loss': 0.6911, 'grad_norm': 1.0755705833435059, 'learning_rate': 0.00012416666666666666, 'epoch': 2.6277777777777778}
Step 9470: {'loss': 0.6668, 'grad_norm': 0.695010781288147, 'learning_rate': 0.00012324074074074074, 'epoch': 2.6305555555555555}
Step 9480: {'loss': 0.6807, 'grad_norm': 0.8815183639526367, 'learning_rate': 0.00012231481481481482, 'epoch': 2.6333333333333333}
Step 9490: {'loss': 0.6193, 'grad_norm': 0.6921142935752869, 'learning_rate': 0.0001213888888888889, 'epoch': 2.636111111111111}
Step 9500: {'loss': 0.6682, 'grad_norm': 1.0983202457427979, 'learning_rate': 0.00012046296296296296, 'epoch': 2.638888888888889}
Step 9510: {'loss': 0.6116, 'grad_norm': 0.9126554727554321, 'learning_rate': 0.00011953703703703704, 'epoch': 2.6416666666666666}
Step 9520: {'loss': 0.6348, 'grad_norm': 0.8078356981277466, 'learning_rate': 0.00011861111111111111, 'epoch': 2.6444444444444444}
Step 9530: {'loss': 0.6333, 'grad_norm': 0.75094074010849, 'learning_rate': 0.00011768518518518519, 'epoch': 2.647222222222222}
Step 9540: {'loss': 0.6634, 'grad_norm': 0.9085268378257751, 'learning_rate': 0.00011675925925925925, 'epoch': 2.65}
Step 9550: {'loss': 0.5898, 'grad_norm': 1.0298490524291992, 'learning_rate': 0.00011583333333333333, 'epoch': 2.6527777777777777}
Step 9560: {'loss': 0.5948, 'grad_norm': 0.9835603833198547, 'learning_rate': 0.0001149074074074074, 'epoch': 2.6555555555555554}
Step 9570: {'loss': 0.5505, 'grad_norm': 0.7210650444030762, 'learning_rate': 0.00011398148148148148, 'epoch': 2.658333333333333}
Step 9580: {'loss': 0.6733, 'grad_norm': 0.8050153255462646, 'learning_rate': 0.00011305555555555556, 'epoch': 2.661111111111111}
Step 9590: {'loss': 0.6564, 'grad_norm': 0.8918493390083313, 'learning_rate': 0.00011212962962962964, 'epoch': 2.6638888888888888}
Step 9600: {'loss': 0.6539, 'grad_norm': 0.634711742401123, 'learning_rate': 0.0001112037037037037, 'epoch': 2.6666666666666665}
Step 9610: {'loss': 0.6218, 'grad_norm': 1.02634859085083, 'learning_rate': 0.00011027777777777779, 'epoch': 2.6694444444444443}
Step 9620: {'loss': 0.6389, 'grad_norm': 1.3819375038146973, 'learning_rate': 0.00010935185185185185, 'epoch': 2.6722222222222225}
Step 9630: {'loss': 0.7038, 'grad_norm': 1.0344873666763306, 'learning_rate': 0.00010842592592592593, 'epoch': 2.675}
Step 9640: {'loss': 0.6699, 'grad_norm': 1.0678025484085083, 'learning_rate': 0.0001075, 'epoch': 2.677777777777778}
Step 9650: {'loss': 0.739, 'grad_norm': 0.9627734422683716, 'learning_rate': 0.00010657407407407408, 'epoch': 2.6805555555555554}
Step 9660: {'loss': 0.6234, 'grad_norm': 0.6107382774353027, 'learning_rate': 0.00010564814814814814, 'epoch': 2.6833333333333336}
Step 9670: {'loss': 0.6749, 'grad_norm': 0.5591232180595398, 'learning_rate': 0.00010472222222222222, 'epoch': 2.686111111111111}
Step 9680: {'loss': 0.6475, 'grad_norm': 1.2011096477508545, 'learning_rate': 0.0001037962962962963, 'epoch': 2.688888888888889}
Step 9690: {'loss': 0.6172, 'grad_norm': 0.6563103199005127, 'learning_rate': 0.00010287037037037038, 'epoch': 2.6916666666666664}
Step 9700: {'loss': 0.5901, 'grad_norm': 0.6295058727264404, 'learning_rate': 0.00010194444444444445, 'epoch': 2.6944444444444446}
Step 9710: {'loss': 0.6505, 'grad_norm': 1.3093944787979126, 'learning_rate': 0.00010101851851851853, 'epoch': 2.697222222222222}
Step 9720: {'loss': 0.6406, 'grad_norm': 1.0224390029907227, 'learning_rate': 0.0001000925925925926, 'epoch': 2.7}
Step 9730: {'loss': 0.6367, 'grad_norm': 0.6153217554092407, 'learning_rate': 9.916666666666667e-05, 'epoch': 2.7027777777777775}
Step 9740: {'loss': 0.6274, 'grad_norm': 0.8994601964950562, 'learning_rate': 9.824074074074074e-05, 'epoch': 2.7055555555555557}
Step 9750: {'loss': 0.6333, 'grad_norm': 0.6915463805198669, 'learning_rate': 9.731481481481482e-05, 'epoch': 2.7083333333333335}
Step 9760: {'loss': 0.649, 'grad_norm': 0.8578041791915894, 'learning_rate': 9.638888888888889e-05, 'epoch': 2.7111111111111112}
Step 9770: {'loss': 0.6064, 'grad_norm': 1.1414952278137207, 'learning_rate': 9.546296296296297e-05, 'epoch': 2.713888888888889}
Step 9780: {'loss': 0.6493, 'grad_norm': 0.8181298971176147, 'learning_rate': 9.453703703703703e-05, 'epoch': 2.716666666666667}
Step 9790: {'loss': 0.6776, 'grad_norm': 0.9403446316719055, 'learning_rate': 9.361111111111112e-05, 'epoch': 2.7194444444444446}
Step 9800: {'loss': 0.6665, 'grad_norm': 0.8236411809921265, 'learning_rate': 9.268518518518519e-05, 'epoch': 2.7222222222222223}
Step 9810: {'loss': 0.6487, 'grad_norm': 0.8162099719047546, 'learning_rate': 9.175925925925927e-05, 'epoch': 2.725}
Step 9820: {'loss': 0.6805, 'grad_norm': 0.8114904761314392, 'learning_rate': 9.083333333333334e-05, 'epoch': 2.727777777777778}
Step 9830: {'loss': 0.623, 'grad_norm': 0.8526946902275085, 'learning_rate': 8.990740740740742e-05, 'epoch': 2.7305555555555556}
Step 9840: {'loss': 0.6383, 'grad_norm': 0.7603975534439087, 'learning_rate': 8.898148148148148e-05, 'epoch': 2.7333333333333334}
Step 9850: {'loss': 0.6135, 'grad_norm': 0.6679796576499939, 'learning_rate': 8.805555555555556e-05, 'epoch': 2.736111111111111}
Step 9860: {'loss': 0.6365, 'grad_norm': 1.1395595073699951, 'learning_rate': 8.712962962962963e-05, 'epoch': 2.738888888888889}
Step 9870: {'loss': 0.6087, 'grad_norm': 0.96330326795578, 'learning_rate': 8.62037037037037e-05, 'epoch': 2.7416666666666667}
Step 9880: {'loss': 0.6626, 'grad_norm': 1.264084815979004, 'learning_rate': 8.527777777777777e-05, 'epoch': 2.7444444444444445}
Step 9890: {'loss': 0.7315, 'grad_norm': 0.9069048762321472, 'learning_rate': 8.435185185185185e-05, 'epoch': 2.7472222222222222}
Step 9900: {'loss': 0.6389, 'grad_norm': 0.9443334341049194, 'learning_rate': 8.342592592592593e-05, 'epoch': 2.75}
Step 9910: {'loss': 0.6451, 'grad_norm': 1.0864311456680298, 'learning_rate': 8.25e-05, 'epoch': 2.7527777777777778}
Step 9920: {'loss': 0.6646, 'grad_norm': 0.93599534034729, 'learning_rate': 8.157407407407408e-05, 'epoch': 2.7555555555555555}
Step 9930: {'loss': 0.6928, 'grad_norm': 1.0423814058303833, 'learning_rate': 8.064814814814815e-05, 'epoch': 2.7583333333333333}
Step 9940: {'loss': 0.6514, 'grad_norm': 0.9809893369674683, 'learning_rate': 7.972222222222223e-05, 'epoch': 2.761111111111111}
Step 9950: {'loss': 0.6471, 'grad_norm': 0.9876667857170105, 'learning_rate': 7.879629629629629e-05, 'epoch': 2.763888888888889}
Step 9960: {'loss': 0.677, 'grad_norm': 0.7586153149604797, 'learning_rate': 7.787037037037037e-05, 'epoch': 2.7666666666666666}
Step 9970: {'loss': 0.5891, 'grad_norm': 1.1444607973098755, 'learning_rate': 7.694444444444444e-05, 'epoch': 2.7694444444444444}
Step 9980: {'loss': 0.6441, 'grad_norm': 0.654060959815979, 'learning_rate': 7.601851851851852e-05, 'epoch': 2.772222222222222}
Step 9990: {'loss': 0.6229, 'grad_norm': 0.6490415930747986, 'learning_rate': 7.509259259259258e-05, 'epoch': 2.775}
Step 10000: {'loss': 0.6808, 'grad_norm': 1.2302539348602295, 'learning_rate': 7.416666666666668e-05, 'epoch': 2.7777777777777777}
Step 10010: {'loss': 0.6684, 'grad_norm': 0.8710448741912842, 'learning_rate': 7.324074074074074e-05, 'epoch': 2.7805555555555554}
Step 10020: {'loss': 0.6168, 'grad_norm': 0.7045026421546936, 'learning_rate': 7.231481481481482e-05, 'epoch': 2.783333333333333}
Step 10030: {'loss': 0.6439, 'grad_norm': 0.9137087464332581, 'learning_rate': 7.138888888888889e-05, 'epoch': 2.786111111111111}
Step 10040: {'loss': 0.7072, 'grad_norm': 0.7226964831352234, 'learning_rate': 7.046296296296297e-05, 'epoch': 2.7888888888888888}
Step 10050: {'loss': 0.6334, 'grad_norm': 1.0325461626052856, 'learning_rate': 6.953703703703703e-05, 'epoch': 2.7916666666666665}
Step 10060: {'loss': 0.6785, 'grad_norm': 0.7541915774345398, 'learning_rate': 6.861111111111111e-05, 'epoch': 2.7944444444444443}
Step 10070: {'loss': 0.6359, 'grad_norm': 0.9341352581977844, 'learning_rate': 6.768518518518518e-05, 'epoch': 2.7972222222222225}
Step 10080: {'loss': 0.6608, 'grad_norm': 0.7336277365684509, 'learning_rate': 6.675925925925926e-05, 'epoch': 2.8}
Step 10090: {'loss': 0.6304, 'grad_norm': 0.7189087867736816, 'learning_rate': 6.583333333333333e-05, 'epoch': 2.802777777777778}
Step 10100: {'loss': 0.6753, 'grad_norm': 1.123694658279419, 'learning_rate': 6.490740740740742e-05, 'epoch': 2.8055555555555554}
Step 10110: {'loss': 0.665, 'grad_norm': 0.8550098538398743, 'learning_rate': 6.398148148148148e-05, 'epoch': 2.8083333333333336}
Step 10120: {'loss': 0.5644, 'grad_norm': 1.3974106311798096, 'learning_rate': 6.305555555555556e-05, 'epoch': 2.811111111111111}
Step 10130: {'loss': 0.6752, 'grad_norm': 0.7515254616737366, 'learning_rate': 6.212962962962963e-05, 'epoch': 2.813888888888889}
Step 10140: {'loss': 0.6253, 'grad_norm': 0.9781531691551208, 'learning_rate': 6.120370370370371e-05, 'epoch': 2.8166666666666664}
Step 10150: {'loss': 0.6356, 'grad_norm': 0.6660248041152954, 'learning_rate': 6.0277777777777776e-05, 'epoch': 2.8194444444444446}
Step 10160: {'loss': 0.6348, 'grad_norm': 0.726143479347229, 'learning_rate': 5.935185185185185e-05, 'epoch': 2.822222222222222}
Step 10170: {'loss': 0.6182, 'grad_norm': 0.8310552835464478, 'learning_rate': 5.842592592592592e-05, 'epoch': 2.825}
Step 10180: {'loss': 0.6392, 'grad_norm': 0.7529642581939697, 'learning_rate': 5.75e-05, 'epoch': 2.8277777777777775}
Step 10190: {'loss': 0.6547, 'grad_norm': 0.7952353358268738, 'learning_rate': 5.6574074074074075e-05, 'epoch': 2.8305555555555557}
Step 10200: {'loss': 0.6532, 'grad_norm': 0.8846012353897095, 'learning_rate': 5.564814814814815e-05, 'epoch': 2.8333333333333335}
Step 10210: {'loss': 0.6202, 'grad_norm': 0.6043882966041565, 'learning_rate': 5.472222222222222e-05, 'epoch': 2.8361111111111112}
Step 10220: {'loss': 0.6547, 'grad_norm': 0.9002484083175659, 'learning_rate': 5.379629629629629e-05, 'epoch': 2.838888888888889}
Step 10230: {'loss': 0.5781, 'grad_norm': 0.9310476183891296, 'learning_rate': 5.287037037037037e-05, 'epoch': 2.841666666666667}
Step 10240: {'loss': 0.6081, 'grad_norm': 0.6198385953903198, 'learning_rate': 5.1944444444444446e-05, 'epoch': 2.8444444444444446}
Step 10250: {'loss': 0.6825, 'grad_norm': 0.7880304455757141, 'learning_rate': 5.101851851851852e-05, 'epoch': 2.8472222222222223}
Step 10260: {'loss': 0.6692, 'grad_norm': 0.947138249874115, 'learning_rate': 5.009259259259259e-05, 'epoch': 2.85}
Step 10270: {'loss': 0.6394, 'grad_norm': 0.9850090742111206, 'learning_rate': 4.9166666666666665e-05, 'epoch': 2.852777777777778}
Step 10280: {'loss': 0.6262, 'grad_norm': 0.698903501033783, 'learning_rate': 4.8240740740740744e-05, 'epoch': 2.8555555555555556}
Step 10290: {'loss': 0.658, 'grad_norm': 0.7441518306732178, 'learning_rate': 4.731481481481482e-05, 'epoch': 2.8583333333333334}
Step 10300: {'loss': 0.6493, 'grad_norm': 0.5472598075866699, 'learning_rate': 4.638888888888889e-05, 'epoch': 2.861111111111111}
Step 10310: {'loss': 0.6363, 'grad_norm': 1.0346746444702148, 'learning_rate': 4.546296296296296e-05, 'epoch': 2.863888888888889}
Step 10320: {'loss': 0.7028, 'grad_norm': 0.9020096659660339, 'learning_rate': 4.4537037037037036e-05, 'epoch': 2.8666666666666667}
Step 10330: {'loss': 0.6423, 'grad_norm': 0.9344401955604553, 'learning_rate': 4.3611111111111116e-05, 'epoch': 2.8694444444444445}
Step 10340: {'loss': 0.6758, 'grad_norm': 0.729332447052002, 'learning_rate': 4.268518518518519e-05, 'epoch': 2.8722222222222222}
Step 10350: {'loss': 0.5784, 'grad_norm': 0.8338464498519897, 'learning_rate': 4.175925925925926e-05, 'epoch': 2.875}
Step 10360: {'loss': 0.6085, 'grad_norm': 0.6770678162574768, 'learning_rate': 4.0833333333333334e-05, 'epoch': 2.8777777777777778}
Step 10370: {'loss': 0.6517, 'grad_norm': 0.7636075019836426, 'learning_rate': 3.990740740740741e-05, 'epoch': 2.8805555555555555}
Step 10380: {'loss': 0.6396, 'grad_norm': 0.9931038022041321, 'learning_rate': 3.898148148148148e-05, 'epoch': 2.8833333333333333}
Step 10390: {'loss': 0.6141, 'grad_norm': 0.8109070658683777, 'learning_rate': 3.805555555555556e-05, 'epoch': 2.886111111111111}
Step 10400: {'loss': 0.674, 'grad_norm': 0.9230614900588989, 'learning_rate': 3.712962962962963e-05, 'epoch': 2.888888888888889}
Step 10410: {'loss': 0.6262, 'grad_norm': 0.8123881220817566, 'learning_rate': 3.6203703703703706e-05, 'epoch': 2.8916666666666666}
Step 10420: {'loss': 0.6356, 'grad_norm': 0.7840829491615295, 'learning_rate': 3.527777777777778e-05, 'epoch': 2.8944444444444444}
Step 10430: {'loss': 0.638, 'grad_norm': 0.7446093559265137, 'learning_rate': 3.435185185185185e-05, 'epoch': 2.897222222222222}
Step 10440: {'loss': 0.6809, 'grad_norm': 1.0398024320602417, 'learning_rate': 3.342592592592593e-05, 'epoch': 2.9}
Step 10450: {'loss': 0.641, 'grad_norm': 0.7738614082336426, 'learning_rate': 3.2500000000000004e-05, 'epoch': 2.9027777777777777}
Step 10460: {'loss': 0.636, 'grad_norm': 1.0060904026031494, 'learning_rate': 3.157407407407408e-05, 'epoch': 2.9055555555555554}
Step 10470: {'loss': 0.6505, 'grad_norm': 0.7762778997421265, 'learning_rate': 3.064814814814815e-05, 'epoch': 2.908333333333333}
Step 10480: {'loss': 0.6203, 'grad_norm': 0.9538207054138184, 'learning_rate': 2.9722222222222223e-05, 'epoch': 2.911111111111111}
Step 10490: {'loss': 0.6115, 'grad_norm': 0.910980224609375, 'learning_rate': 2.8796296296296296e-05, 'epoch': 2.9138888888888888}
Step 10500: {'loss': 0.6624, 'grad_norm': 0.9019673466682434, 'learning_rate': 2.7870370370370372e-05, 'epoch': 2.9166666666666665}
Step 10510: {'loss': 0.6429, 'grad_norm': 0.8169827461242676, 'learning_rate': 2.6944444444444445e-05, 'epoch': 2.9194444444444443}
Step 10520: {'loss': 0.6984, 'grad_norm': 1.0495641231536865, 'learning_rate': 2.6018518518518518e-05, 'epoch': 2.9222222222222225}
Step 10530: {'loss': 0.6438, 'grad_norm': 1.0303914546966553, 'learning_rate': 2.5092592592592594e-05, 'epoch': 2.925}
Step 10540: {'loss': 0.6522, 'grad_norm': 0.9589970111846924, 'learning_rate': 2.4166666666666667e-05, 'epoch': 2.927777777777778}
Step 10550: {'loss': 0.7046, 'grad_norm': 0.9696404933929443, 'learning_rate': 2.3240740740740743e-05, 'epoch': 2.9305555555555554}
Step 10560: {'loss': 0.666, 'grad_norm': 0.7259945273399353, 'learning_rate': 2.2314814814814816e-05, 'epoch': 2.9333333333333336}
Step 10570: {'loss': 0.6833, 'grad_norm': 0.7026296257972717, 'learning_rate': 2.138888888888889e-05, 'epoch': 2.936111111111111}
Step 10580: {'loss': 0.6315, 'grad_norm': 0.8301868438720703, 'learning_rate': 2.0462962962962965e-05, 'epoch': 2.938888888888889}
Step 10590: {'loss': 0.6475, 'grad_norm': 0.8649994134902954, 'learning_rate': 1.9537037037037038e-05, 'epoch': 2.9416666666666664}
Step 10600: {'loss': 0.654, 'grad_norm': 1.3940683603286743, 'learning_rate': 1.861111111111111e-05, 'epoch': 2.9444444444444446}
Step 10610: {'loss': 0.6831, 'grad_norm': 0.579914927482605, 'learning_rate': 1.7685185185185187e-05, 'epoch': 2.947222222222222}
Step 10620: {'loss': 0.639, 'grad_norm': 0.7532121539115906, 'learning_rate': 1.675925925925926e-05, 'epoch': 2.95}
Step 10630: {'loss': 0.5932, 'grad_norm': 0.7103831768035889, 'learning_rate': 1.5833333333333336e-05, 'epoch': 2.9527777777777775}
Step 10640: {'loss': 0.679, 'grad_norm': 1.141414761543274, 'learning_rate': 1.4907407407407408e-05, 'epoch': 2.9555555555555557}
Step 10650: {'loss': 0.6384, 'grad_norm': 1.1112457513809204, 'learning_rate': 1.3981481481481482e-05, 'epoch': 2.9583333333333335}
Step 10660: {'loss': 0.7027, 'grad_norm': 0.8169615268707275, 'learning_rate': 1.3055555555555557e-05, 'epoch': 2.9611111111111112}
Step 10670: {'loss': 0.6339, 'grad_norm': 0.9001653790473938, 'learning_rate': 1.212962962962963e-05, 'epoch': 2.963888888888889}
Step 10680: {'loss': 0.7036, 'grad_norm': 1.3955168724060059, 'learning_rate': 1.1203703703703704e-05, 'epoch': 2.966666666666667}
Step 10690: {'loss': 0.6663, 'grad_norm': 0.7760140299797058, 'learning_rate': 1.0277777777777779e-05, 'epoch': 2.9694444444444446}
Step 10700: {'loss': 0.6133, 'grad_norm': 0.7000696659088135, 'learning_rate': 9.351851851851854e-06, 'epoch': 2.9722222222222223}
Step 10710: {'loss': 0.6584, 'grad_norm': 0.7220127582550049, 'learning_rate': 8.425925925925925e-06, 'epoch': 2.975}
Step 10720: {'loss': 0.6615, 'grad_norm': 0.99390709400177, 'learning_rate': 7.5e-06, 'epoch': 2.977777777777778}
Step 10730: {'loss': 0.6434, 'grad_norm': 0.6664393544197083, 'learning_rate': 6.574074074074074e-06, 'epoch': 2.9805555555555556}
Step 10740: {'loss': 0.6357, 'grad_norm': 0.9499270915985107, 'learning_rate': 5.648148148148148e-06, 'epoch': 2.9833333333333334}
Step 10750: {'loss': 0.597, 'grad_norm': 0.7623863220214844, 'learning_rate': 4.722222222222222e-06, 'epoch': 2.986111111111111}
Step 10760: {'loss': 0.681, 'grad_norm': 0.861004114151001, 'learning_rate': 3.7962962962962964e-06, 'epoch': 2.988888888888889}
Step 10770: {'loss': 0.649, 'grad_norm': 0.8869550824165344, 'learning_rate': 2.8703703703703706e-06, 'epoch': 2.9916666666666667}
Step 10780: {'loss': 0.6773, 'grad_norm': 0.917582631111145, 'learning_rate': 1.9444444444444444e-06, 'epoch': 2.9944444444444445}
Step 10790: {'loss': 0.6151, 'grad_norm': 1.0878525972366333, 'learning_rate': 1.0185185185185185e-06, 'epoch': 2.9972222222222222}
Step 10800: {'loss': 0.6686, 'grad_norm': 0.9119032025337219, 'learning_rate': 9.259259259259259e-08, 'epoch': 3.0}
Step 10800: {'train_runtime': 1628.0989, 'train_samples_per_second': 13.267, 'train_steps_per_second': 6.634, 'total_flos': 2.5316235775696896e+17, 'train_loss': 0.6679583474883327, 'epoch': 3.0}
