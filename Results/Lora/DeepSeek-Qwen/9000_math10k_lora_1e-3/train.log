Step 10: {'loss': 1.3884, 'grad_norm': 1.137596607208252, 'learning_rate': 0.0009991666666666666, 'epoch': 0.002777777777777778}
Step 20: {'loss': 0.9672, 'grad_norm': 1.92319917678833, 'learning_rate': 0.0009982407407407407, 'epoch': 0.005555555555555556}
Step 30: {'loss': 0.8328, 'grad_norm': 1.392852544784546, 'learning_rate': 0.0009973148148148148, 'epoch': 0.008333333333333333}
Step 40: {'loss': 0.8043, 'grad_norm': 0.755594789981842, 'learning_rate': 0.0009963888888888889, 'epoch': 0.011111111111111112}
Step 50: {'loss': 0.6758, 'grad_norm': 0.6838133335113525, 'learning_rate': 0.000995462962962963, 'epoch': 0.013888888888888888}
Step 60: {'loss': 0.746, 'grad_norm': 0.8249536752700806, 'learning_rate': 0.000994537037037037, 'epoch': 0.016666666666666666}
Step 70: {'loss': 0.6765, 'grad_norm': 0.6590256094932556, 'learning_rate': 0.0009936111111111111, 'epoch': 0.019444444444444445}
Step 80: {'loss': 0.6648, 'grad_norm': 0.5352673530578613, 'learning_rate': 0.0009926851851851852, 'epoch': 0.022222222222222223}
Step 90: {'loss': 0.6509, 'grad_norm': 0.919703483581543, 'learning_rate': 0.0009917592592592593, 'epoch': 0.025}
Step 100: {'loss': 0.675, 'grad_norm': 0.8908982276916504, 'learning_rate': 0.0009908333333333334, 'epoch': 0.027777777777777776}
Step 110: {'loss': 0.6341, 'grad_norm': 0.768118679523468, 'learning_rate': 0.0009899074074074074, 'epoch': 0.030555555555555555}
Step 120: {'loss': 0.603, 'grad_norm': 0.5735469460487366, 'learning_rate': 0.0009889814814814815, 'epoch': 0.03333333333333333}
Step 130: {'loss': 0.6406, 'grad_norm': 0.8929343223571777, 'learning_rate': 0.0009880555555555556, 'epoch': 0.03611111111111111}
Step 140: {'loss': 0.5987, 'grad_norm': 0.7568646669387817, 'learning_rate': 0.0009871296296296297, 'epoch': 0.03888888888888889}
Step 150: {'loss': 0.5915, 'grad_norm': 0.8262506723403931, 'learning_rate': 0.0009862037037037038, 'epoch': 0.041666666666666664}
Step 160: {'loss': 0.6694, 'grad_norm': 0.9186010956764221, 'learning_rate': 0.0009852777777777778, 'epoch': 0.044444444444444446}
Step 170: {'loss': 0.5986, 'grad_norm': 0.7930593490600586, 'learning_rate': 0.000984351851851852, 'epoch': 0.04722222222222222}
Step 180: {'loss': 0.6687, 'grad_norm': 0.8671921491622925, 'learning_rate': 0.000983425925925926, 'epoch': 0.05}
Step 190: {'loss': 0.5934, 'grad_norm': 0.7469523549079895, 'learning_rate': 0.0009825, 'epoch': 0.05277777777777778}
Step 200: {'loss': 0.5575, 'grad_norm': 0.8477131128311157, 'learning_rate': 0.0009815740740740742, 'epoch': 0.05555555555555555}
Step 210: {'loss': 0.5464, 'grad_norm': 0.6830127239227295, 'learning_rate': 0.0009806481481481482, 'epoch': 0.058333333333333334}
Step 220: {'loss': 0.5987, 'grad_norm': 0.8399839997291565, 'learning_rate': 0.0009797222222222223, 'epoch': 0.06111111111111111}
Step 230: {'loss': 0.5537, 'grad_norm': 0.6434714794158936, 'learning_rate': 0.0009787962962962964, 'epoch': 0.06388888888888888}
Step 10: {'loss': 1.5705, 'grad_norm': 1.238773226737976, 'learning_rate': 0.0009991666666666666, 'epoch': 0.002777777777777778}
Step 20: {'loss': 1.1781, 'grad_norm': 1.3269470930099487, 'learning_rate': 0.0009982407407407407, 'epoch': 0.005555555555555556}
Step 30: {'loss': 1.1812, 'grad_norm': 0.901052713394165, 'learning_rate': 0.0009973148148148148, 'epoch': 0.008333333333333333}
Step 40: {'loss': 1.1002, 'grad_norm': 1.2027873992919922, 'learning_rate': 0.0009963888888888889, 'epoch': 0.011111111111111112}
Step 50: {'loss': 1.2141, 'grad_norm': 1.5290861129760742, 'learning_rate': 0.000995462962962963, 'epoch': 0.013888888888888888}
Step 60: {'loss': 0.9718, 'grad_norm': 1.0795872211456299, 'learning_rate': 0.000994537037037037, 'epoch': 0.016666666666666666}
Step 70: {'loss': 0.9984, 'grad_norm': 1.2219723463058472, 'learning_rate': 0.0009936111111111111, 'epoch': 0.019444444444444445}
Step 80: {'loss': 0.8617, 'grad_norm': 0.4499532878398895, 'learning_rate': 0.0009926851851851852, 'epoch': 0.022222222222222223}
Step 90: {'loss': 1.0796, 'grad_norm': 0.6468902826309204, 'learning_rate': 0.0009917592592592593, 'epoch': 0.025}
Step 100: {'loss': 0.9418, 'grad_norm': 1.056768774986267, 'learning_rate': 0.0009908333333333334, 'epoch': 0.027777777777777776}
Step 110: {'loss': 0.9647, 'grad_norm': 1.084182620048523, 'learning_rate': 0.0009899074074074074, 'epoch': 0.030555555555555555}
Step 120: {'loss': 1.0291, 'grad_norm': 0.8010030388832092, 'learning_rate': 0.0009889814814814815, 'epoch': 0.03333333333333333}
Step 130: {'loss': 1.0021, 'grad_norm': 1.4194326400756836, 'learning_rate': 0.0009880555555555556, 'epoch': 0.03611111111111111}
Step 140: {'loss': 0.9913, 'grad_norm': 0.9305849671363831, 'learning_rate': 0.0009871296296296297, 'epoch': 0.03888888888888889}
Step 150: {'loss': 0.9908, 'grad_norm': 1.0166187286376953, 'learning_rate': 0.0009862037037037038, 'epoch': 0.041666666666666664}
Step 160: {'loss': 1.0063, 'grad_norm': 1.1120647192001343, 'learning_rate': 0.0009852777777777778, 'epoch': 0.044444444444444446}
Step 170: {'loss': 0.9709, 'grad_norm': 0.8671045899391174, 'learning_rate': 0.000984351851851852, 'epoch': 0.04722222222222222}
Step 180: {'loss': 0.9477, 'grad_norm': 1.147214651107788, 'learning_rate': 0.000983425925925926, 'epoch': 0.05}
Step 190: {'loss': 1.028, 'grad_norm': 1.3763597011566162, 'learning_rate': 0.0009825, 'epoch': 0.05277777777777778}
Step 200: {'loss': 0.9124, 'grad_norm': 1.3272852897644043, 'learning_rate': 0.0009815740740740742, 'epoch': 0.05555555555555555}
Step 210: {'loss': 1.0417, 'grad_norm': 1.2237554788589478, 'learning_rate': 0.0009806481481481482, 'epoch': 0.058333333333333334}
Step 220: {'loss': 0.9428, 'grad_norm': 1.0998016595840454, 'learning_rate': 0.0009797222222222223, 'epoch': 0.06111111111111111}
Step 230: {'loss': 0.9037, 'grad_norm': 1.2570655345916748, 'learning_rate': 0.0009787962962962964, 'epoch': 0.06388888888888888}
Step 240: {'loss': 0.9705, 'grad_norm': 1.2111964225769043, 'learning_rate': 0.0009778703703703705, 'epoch': 0.06666666666666667}
Step 250: {'loss': 0.9122, 'grad_norm': 0.9442682862281799, 'learning_rate': 0.0009769444444444443, 'epoch': 0.06944444444444445}
Step 260: {'loss': 0.9998, 'grad_norm': 0.9503730535507202, 'learning_rate': 0.0009760185185185185, 'epoch': 0.07222222222222222}
Step 270: {'loss': 0.9044, 'grad_norm': 1.230414628982544, 'learning_rate': 0.0009750925925925926, 'epoch': 0.075}
Step 280: {'loss': 0.9983, 'grad_norm': 1.275768756866455, 'learning_rate': 0.0009741666666666667, 'epoch': 0.07777777777777778}
Step 290: {'loss': 1.0203, 'grad_norm': 0.8914201259613037, 'learning_rate': 0.0009732407407407408, 'epoch': 0.08055555555555556}
Step 300: {'loss': 0.9027, 'grad_norm': 0.6895507574081421, 'learning_rate': 0.0009723148148148149, 'epoch': 0.08333333333333333}
Step 310: {'loss': 0.9627, 'grad_norm': 1.2431998252868652, 'learning_rate': 0.0009713888888888889, 'epoch': 0.08611111111111111}
Step 320: {'loss': 1.0249, 'grad_norm': 1.2896960973739624, 'learning_rate': 0.000970462962962963, 'epoch': 0.08888888888888889}
Step 330: {'loss': 0.8488, 'grad_norm': 1.0320008993148804, 'learning_rate': 0.0009695370370370371, 'epoch': 0.09166666666666666}
Step 340: {'loss': 0.8966, 'grad_norm': 0.7327704429626465, 'learning_rate': 0.0009686111111111111, 'epoch': 0.09444444444444444}
Step 350: {'loss': 0.9312, 'grad_norm': 0.8020442128181458, 'learning_rate': 0.0009676851851851852, 'epoch': 0.09722222222222222}
Step 360: {'loss': 0.9228, 'grad_norm': 1.234554648399353, 'learning_rate': 0.0009667592592592592, 'epoch': 0.1}
Step 370: {'loss': 0.9027, 'grad_norm': 1.2432907819747925, 'learning_rate': 0.0009658333333333333, 'epoch': 0.10277777777777777}
Step 380: {'loss': 0.9502, 'grad_norm': 0.9888966679573059, 'learning_rate': 0.0009649074074074075, 'epoch': 0.10555555555555556}
Step 390: {'loss': 0.9348, 'grad_norm': 0.6041784882545471, 'learning_rate': 0.0009639814814814815, 'epoch': 0.10833333333333334}
Step 400: {'loss': 1.1441, 'grad_norm': 1.068894386291504, 'learning_rate': 0.0009630555555555555, 'epoch': 0.1111111111111111}
Step 410: {'loss': 0.9336, 'grad_norm': 0.8284893035888672, 'learning_rate': 0.0009621296296296297, 'epoch': 0.11388888888888889}
Step 420: {'loss': 0.8836, 'grad_norm': 0.8066319823265076, 'learning_rate': 0.0009612037037037037, 'epoch': 0.11666666666666667}
Step 430: {'loss': 0.9651, 'grad_norm': 1.4122616052627563, 'learning_rate': 0.0009602777777777778, 'epoch': 0.11944444444444445}
Step 440: {'loss': 0.9501, 'grad_norm': 1.0117321014404297, 'learning_rate': 0.000959351851851852, 'epoch': 0.12222222222222222}
Step 450: {'loss': 0.9585, 'grad_norm': 0.9800297021865845, 'learning_rate': 0.0009584259259259259, 'epoch': 0.125}
Step 460: {'loss': 0.9754, 'grad_norm': 1.184921145439148, 'learning_rate': 0.0009575, 'epoch': 0.12777777777777777}
Step 470: {'loss': 0.9379, 'grad_norm': 0.7322402000427246, 'learning_rate': 0.000956574074074074, 'epoch': 0.13055555555555556}
Step 480: {'loss': 0.9759, 'grad_norm': 0.8829019665718079, 'learning_rate': 0.0009556481481481482, 'epoch': 0.13333333333333333}
Step 490: {'loss': 0.8771, 'grad_norm': 1.0051387548446655, 'learning_rate': 0.0009547222222222223, 'epoch': 0.1361111111111111}
Step 500: {'loss': 0.9083, 'grad_norm': 1.3912544250488281, 'learning_rate': 0.0009537962962962962, 'epoch': 0.1388888888888889}
Step 510: {'loss': 0.9643, 'grad_norm': 1.1616065502166748, 'learning_rate': 0.0009528703703703704, 'epoch': 0.14166666666666666}
Step 520: {'loss': 0.9272, 'grad_norm': 0.9596391916275024, 'learning_rate': 0.0009519444444444445, 'epoch': 0.14444444444444443}
Step 530: {'loss': 0.9224, 'grad_norm': 1.1443911790847778, 'learning_rate': 0.0009510185185185185, 'epoch': 0.14722222222222223}
Step 540: {'loss': 0.9819, 'grad_norm': 0.8085744976997375, 'learning_rate': 0.0009500925925925927, 'epoch': 0.15}
Step 550: {'loss': 0.9061, 'grad_norm': 1.2199629545211792, 'learning_rate': 0.0009491666666666667, 'epoch': 0.1527777777777778}
Step 560: {'loss': 0.9885, 'grad_norm': 1.0808335542678833, 'learning_rate': 0.0009482407407407407, 'epoch': 0.15555555555555556}
Step 570: {'loss': 0.9028, 'grad_norm': 0.781554102897644, 'learning_rate': 0.0009473148148148149, 'epoch': 0.15833333333333333}
Step 580: {'loss': 0.8283, 'grad_norm': 0.7914939522743225, 'learning_rate': 0.0009463888888888889, 'epoch': 0.16111111111111112}
Step 590: {'loss': 0.9376, 'grad_norm': 1.2648719549179077, 'learning_rate': 0.0009454629629629629, 'epoch': 0.1638888888888889}
Step 600: {'loss': 0.9232, 'grad_norm': 1.2876217365264893, 'learning_rate': 0.0009445370370370371, 'epoch': 0.16666666666666666}
Step 610: {'loss': 0.9044, 'grad_norm': 0.8054521679878235, 'learning_rate': 0.0009436111111111111, 'epoch': 0.16944444444444445}
Step 620: {'loss': 0.8515, 'grad_norm': 1.2079861164093018, 'learning_rate': 0.0009426851851851852, 'epoch': 0.17222222222222222}
Step 630: {'loss': 0.809, 'grad_norm': 0.907983124256134, 'learning_rate': 0.0009417592592592593, 'epoch': 0.175}
Step 640: {'loss': 0.9393, 'grad_norm': 0.9489096999168396, 'learning_rate': 0.0009408333333333333, 'epoch': 0.17777777777777778}
Step 650: {'loss': 0.8634, 'grad_norm': 1.0175871849060059, 'learning_rate': 0.0009399074074074074, 'epoch': 0.18055555555555555}
Step 660: {'loss': 0.8731, 'grad_norm': 0.7764352560043335, 'learning_rate': 0.0009389814814814815, 'epoch': 0.18333333333333332}
Step 670: {'loss': 0.8363, 'grad_norm': 0.8097610473632812, 'learning_rate': 0.0009380555555555556, 'epoch': 0.18611111111111112}
Step 680: {'loss': 0.9482, 'grad_norm': 1.1039241552352905, 'learning_rate': 0.0009371296296296297, 'epoch': 0.18888888888888888}
Step 690: {'loss': 0.9286, 'grad_norm': 0.9640945196151733, 'learning_rate': 0.0009362037037037036, 'epoch': 0.19166666666666668}
Step 700: {'loss': 0.9479, 'grad_norm': 1.3491812944412231, 'learning_rate': 0.0009352777777777778, 'epoch': 0.19444444444444445}
Step 710: {'loss': 0.8952, 'grad_norm': 0.7691694498062134, 'learning_rate': 0.0009343518518518519, 'epoch': 0.19722222222222222}
Step 720: {'loss': 0.9756, 'grad_norm': 0.8082090020179749, 'learning_rate': 0.0009334259259259259, 'epoch': 0.2}
Step 730: {'loss': 0.948, 'grad_norm': 1.1809803247451782, 'learning_rate': 0.0009325000000000001, 'epoch': 0.20277777777777778}
Step 740: {'loss': 0.9161, 'grad_norm': 0.805355429649353, 'learning_rate': 0.0009315740740740741, 'epoch': 0.20555555555555555}
Step 750: {'loss': 0.9082, 'grad_norm': 0.8277738094329834, 'learning_rate': 0.0009306481481481481, 'epoch': 0.20833333333333334}
Step 760: {'loss': 0.902, 'grad_norm': 1.122376561164856, 'learning_rate': 0.0009297222222222223, 'epoch': 0.2111111111111111}
Step 770: {'loss': 0.9002, 'grad_norm': 1.1709699630737305, 'learning_rate': 0.0009287962962962964, 'epoch': 0.21388888888888888}
Step 780: {'loss': 0.8005, 'grad_norm': 0.7337714433670044, 'learning_rate': 0.0009278703703703704, 'epoch': 0.21666666666666667}
Step 790: {'loss': 0.9742, 'grad_norm': 1.13367760181427, 'learning_rate': 0.0009269444444444444, 'epoch': 0.21944444444444444}
Step 800: {'loss': 0.9036, 'grad_norm': 0.8697754144668579, 'learning_rate': 0.0009260185185185185, 'epoch': 0.2222222222222222}
Step 810: {'loss': 0.771, 'grad_norm': 0.7115522623062134, 'learning_rate': 0.0009250925925925926, 'epoch': 0.225}
Step 820: {'loss': 0.8856, 'grad_norm': 0.5346600413322449, 'learning_rate': 0.0009241666666666667, 'epoch': 0.22777777777777777}
Step 830: {'loss': 0.8813, 'grad_norm': 0.8121599555015564, 'learning_rate': 0.0009232407407407407, 'epoch': 0.23055555555555557}
Step 840: {'loss': 0.8993, 'grad_norm': 0.5946686267852783, 'learning_rate': 0.0009223148148148148, 'epoch': 0.23333333333333334}
Step 850: {'loss': 0.9759, 'grad_norm': 0.7924848198890686, 'learning_rate': 0.0009213888888888889, 'epoch': 0.2361111111111111}
Step 860: {'loss': 0.8179, 'grad_norm': 0.7311468124389648, 'learning_rate': 0.000920462962962963, 'epoch': 0.2388888888888889}
Step 870: {'loss': 1.0628, 'grad_norm': 0.6712280511856079, 'learning_rate': 0.0009195370370370371, 'epoch': 0.24166666666666667}
Step 880: {'loss': 0.8802, 'grad_norm': 1.1011717319488525, 'learning_rate': 0.0009186111111111111, 'epoch': 0.24444444444444444}
Step 890: {'loss': 0.8052, 'grad_norm': 0.7976879477500916, 'learning_rate': 0.0009176851851851852, 'epoch': 0.24722222222222223}
Step 900: {'loss': 0.8892, 'grad_norm': 0.869545578956604, 'learning_rate': 0.0009167592592592593, 'epoch': 0.25}
Step 910: {'loss': 0.92, 'grad_norm': 0.9573774933815002, 'learning_rate': 0.0009158333333333334, 'epoch': 0.25277777777777777}
Step 920: {'loss': 0.7413, 'grad_norm': 0.6285907626152039, 'learning_rate': 0.0009149074074074074, 'epoch': 0.25555555555555554}
Step 930: {'loss': 0.8756, 'grad_norm': 1.0790337324142456, 'learning_rate': 0.0009139814814814815, 'epoch': 0.25833333333333336}
Step 940: {'loss': 0.9484, 'grad_norm': 0.9049908518791199, 'learning_rate': 0.0009130555555555555, 'epoch': 0.2611111111111111}
Step 950: {'loss': 0.8846, 'grad_norm': 0.8416352272033691, 'learning_rate': 0.0009121296296296296, 'epoch': 0.2638888888888889}
Step 960: {'loss': 0.8916, 'grad_norm': 0.7555605173110962, 'learning_rate': 0.0009112037037037038, 'epoch': 0.26666666666666666}
Step 970: {'loss': 0.9739, 'grad_norm': 0.7072222828865051, 'learning_rate': 0.0009102777777777778, 'epoch': 0.26944444444444443}
Step 980: {'loss': 0.8816, 'grad_norm': 1.0239073038101196, 'learning_rate': 0.0009093518518518518, 'epoch': 0.2722222222222222}
Step 990: {'loss': 0.9125, 'grad_norm': 0.9631555080413818, 'learning_rate': 0.000908425925925926, 'epoch': 0.275}
Step 1000: {'loss': 0.9504, 'grad_norm': 0.7506704330444336, 'learning_rate': 0.0009075, 'epoch': 0.2777777777777778}
Step 1010: {'loss': 0.9517, 'grad_norm': 0.9439004063606262, 'learning_rate': 0.0009065740740740741, 'epoch': 0.28055555555555556}
Step 1020: {'loss': 0.9812, 'grad_norm': 0.918115496635437, 'learning_rate': 0.0009056481481481483, 'epoch': 0.2833333333333333}
Step 1030: {'loss': 1.0004, 'grad_norm': 1.2941254377365112, 'learning_rate': 0.0009047222222222222, 'epoch': 0.2861111111111111}
Step 1040: {'loss': 0.9185, 'grad_norm': 0.9115555882453918, 'learning_rate': 0.0009037962962962963, 'epoch': 0.28888888888888886}
Step 1050: {'loss': 1.0218, 'grad_norm': 0.9791780710220337, 'learning_rate': 0.0009028703703703704, 'epoch': 0.2916666666666667}
Step 1060: {'loss': 0.9262, 'grad_norm': 0.7034928798675537, 'learning_rate': 0.0009019444444444445, 'epoch': 0.29444444444444445}
Step 1070: {'loss': 0.8845, 'grad_norm': 1.0274090766906738, 'learning_rate': 0.0009010185185185186, 'epoch': 0.2972222222222222}
Step 1080: {'loss': 0.9062, 'grad_norm': 1.307970404624939, 'learning_rate': 0.0009000925925925925, 'epoch': 0.3}
Step 1090: {'loss': 0.7902, 'grad_norm': 1.0346705913543701, 'learning_rate': 0.0008991666666666667, 'epoch': 0.30277777777777776}
Step 1100: {'loss': 0.9394, 'grad_norm': 0.8294968605041504, 'learning_rate': 0.0008982407407407408, 'epoch': 0.3055555555555556}
Step 1110: {'loss': 0.9151, 'grad_norm': 0.6603389978408813, 'learning_rate': 0.0008973148148148148, 'epoch': 0.30833333333333335}
Step 1120: {'loss': 0.884, 'grad_norm': 0.7251160144805908, 'learning_rate': 0.000896388888888889, 'epoch': 0.3111111111111111}
Step 1130: {'loss': 0.8855, 'grad_norm': 1.6638623476028442, 'learning_rate': 0.000895462962962963, 'epoch': 0.3138888888888889}
Step 1140: {'loss': 0.9253, 'grad_norm': 0.9316718578338623, 'learning_rate': 0.000894537037037037, 'epoch': 0.31666666666666665}
Step 1150: {'loss': 0.9283, 'grad_norm': 1.0424119234085083, 'learning_rate': 0.0008936111111111112, 'epoch': 0.3194444444444444}
Step 1160: {'loss': 0.7968, 'grad_norm': 0.9579433798789978, 'learning_rate': 0.0008926851851851852, 'epoch': 0.32222222222222224}
Step 1170: {'loss': 0.9489, 'grad_norm': 1.3323718309402466, 'learning_rate': 0.0008917592592592592, 'epoch': 0.325}
Step 1180: {'loss': 0.8834, 'grad_norm': 0.8259553909301758, 'learning_rate': 0.0008908333333333334, 'epoch': 0.3277777777777778}
Step 1190: {'loss': 0.9251, 'grad_norm': 0.9681959748268127, 'learning_rate': 0.0008899074074074074, 'epoch': 0.33055555555555555}
Step 1200: {'loss': 0.8417, 'grad_norm': 0.7998581528663635, 'learning_rate': 0.0008889814814814815, 'epoch': 0.3333333333333333}
Step 1210: {'loss': 0.9401, 'grad_norm': 0.8657861948013306, 'learning_rate': 0.0008880555555555557, 'epoch': 0.33611111111111114}
Step 1220: {'loss': 0.8145, 'grad_norm': 0.9146474003791809, 'learning_rate': 0.0008871296296296296, 'epoch': 0.3388888888888889}
Step 1230: {'loss': 0.9208, 'grad_norm': 0.81626957654953, 'learning_rate': 0.0008862037037037037, 'epoch': 0.3416666666666667}
Step 1240: {'loss': 0.961, 'grad_norm': 0.8111298680305481, 'learning_rate': 0.0008852777777777778, 'epoch': 0.34444444444444444}
Step 1250: {'loss': 0.8141, 'grad_norm': 0.7460741996765137, 'learning_rate': 0.0008843518518518519, 'epoch': 0.3472222222222222}
Step 1260: {'loss': 0.8816, 'grad_norm': 0.9600512981414795, 'learning_rate': 0.000883425925925926, 'epoch': 0.35}
Step 1270: {'loss': 0.8457, 'grad_norm': 1.015589952468872, 'learning_rate': 0.0008824999999999999, 'epoch': 0.3527777777777778}
Step 1280: {'loss': 0.8749, 'grad_norm': 0.4867364764213562, 'learning_rate': 0.0008815740740740741, 'epoch': 0.35555555555555557}
Step 1290: {'loss': 0.8362, 'grad_norm': 0.7385071516036987, 'learning_rate': 0.0008806481481481482, 'epoch': 0.35833333333333334}
Step 1300: {'loss': 0.8755, 'grad_norm': 0.8052955269813538, 'learning_rate': 0.0008797222222222222, 'epoch': 0.3611111111111111}
Step 1310: {'loss': 0.875, 'grad_norm': 0.6183368563652039, 'learning_rate': 0.0008787962962962964, 'epoch': 0.3638888888888889}
Step 1320: {'loss': 0.896, 'grad_norm': 0.6029335856437683, 'learning_rate': 0.0008778703703703704, 'epoch': 0.36666666666666664}
Step 1330: {'loss': 0.975, 'grad_norm': 0.9222933053970337, 'learning_rate': 0.0008769444444444444, 'epoch': 0.36944444444444446}
Step 1340: {'loss': 0.9254, 'grad_norm': 0.9595530033111572, 'learning_rate': 0.0008760185185185186, 'epoch': 0.37222222222222223}
Step 1350: {'loss': 0.9524, 'grad_norm': 1.1877281665802002, 'learning_rate': 0.0008750925925925927, 'epoch': 0.375}
Step 1360: {'loss': 0.8288, 'grad_norm': 0.7312692999839783, 'learning_rate': 0.0008741666666666666, 'epoch': 0.37777777777777777}
Step 1370: {'loss': 0.9126, 'grad_norm': 0.8407719135284424, 'learning_rate': 0.0008732407407407407, 'epoch': 0.38055555555555554}
Step 1380: {'loss': 0.8786, 'grad_norm': 0.7268137335777283, 'learning_rate': 0.0008723148148148148, 'epoch': 0.38333333333333336}
Step 1390: {'loss': 0.9251, 'grad_norm': 0.8257051706314087, 'learning_rate': 0.0008713888888888889, 'epoch': 0.3861111111111111}
Step 1400: {'loss': 0.9098, 'grad_norm': 0.5855444073677063, 'learning_rate': 0.000870462962962963, 'epoch': 0.3888888888888889}
Step 1410: {'loss': 0.9098, 'grad_norm': 1.1484249830245972, 'learning_rate': 0.000869537037037037, 'epoch': 0.39166666666666666}
Step 1420: {'loss': 0.842, 'grad_norm': 0.7028470039367676, 'learning_rate': 0.0008686111111111111, 'epoch': 0.39444444444444443}
Step 1430: {'loss': 0.9453, 'grad_norm': 0.887872576713562, 'learning_rate': 0.0008676851851851852, 'epoch': 0.3972222222222222}
Step 1440: {'loss': 0.8888, 'grad_norm': 0.8600553274154663, 'learning_rate': 0.0008667592592592593, 'epoch': 0.4}
Step 1450: {'loss': 0.9433, 'grad_norm': 0.8118061423301697, 'learning_rate': 0.0008658333333333334, 'epoch': 0.4027777777777778}
Step 1460: {'loss': 0.9333, 'grad_norm': 0.7072805762290955, 'learning_rate': 0.0008649074074074074, 'epoch': 0.40555555555555556}
Step 1470: {'loss': 0.9387, 'grad_norm': 0.8651400804519653, 'learning_rate': 0.0008639814814814815, 'epoch': 0.4083333333333333}
Step 1480: {'loss': 0.9116, 'grad_norm': 0.9391905665397644, 'learning_rate': 0.0008630555555555556, 'epoch': 0.4111111111111111}
Step 1490: {'loss': 0.9475, 'grad_norm': 1.2679965496063232, 'learning_rate': 0.0008621296296296296, 'epoch': 0.41388888888888886}
Step 1500: {'loss': 0.9461, 'grad_norm': 0.8879150152206421, 'learning_rate': 0.0008612037037037038, 'epoch': 0.4166666666666667}
Step 1510: {'loss': 0.9339, 'grad_norm': 0.9148034453392029, 'learning_rate': 0.0008602777777777778, 'epoch': 0.41944444444444445}
Step 1520: {'loss': 0.9469, 'grad_norm': 0.8315826654434204, 'learning_rate': 0.0008593518518518518, 'epoch': 0.4222222222222222}
Step 1530: {'loss': 0.8922, 'grad_norm': 0.6956672668457031, 'learning_rate': 0.0008584259259259259, 'epoch': 0.425}
Step 1540: {'loss': 0.8638, 'grad_norm': 0.8459609150886536, 'learning_rate': 0.0008575000000000001, 'epoch': 0.42777777777777776}
Step 1550: {'loss': 0.9288, 'grad_norm': 0.5968698263168335, 'learning_rate': 0.000856574074074074, 'epoch': 0.4305555555555556}
Step 1560: {'loss': 0.9366, 'grad_norm': 0.5027870535850525, 'learning_rate': 0.0008556481481481481, 'epoch': 0.43333333333333335}
Step 1570: {'loss': 0.8721, 'grad_norm': 0.8507094979286194, 'learning_rate': 0.0008547222222222223, 'epoch': 0.4361111111111111}
Step 1580: {'loss': 0.9305, 'grad_norm': 0.6599770784378052, 'learning_rate': 0.0008537962962962963, 'epoch': 0.4388888888888889}
Step 1590: {'loss': 0.8473, 'grad_norm': 0.8254899978637695, 'learning_rate': 0.0008528703703703704, 'epoch': 0.44166666666666665}
Step 1600: {'loss': 0.8988, 'grad_norm': 0.7480797171592712, 'learning_rate': 0.0008519444444444445, 'epoch': 0.4444444444444444}
Step 1610: {'loss': 0.9451, 'grad_norm': 0.9076497554779053, 'learning_rate': 0.0008510185185185185, 'epoch': 0.44722222222222224}
Step 1620: {'loss': 0.9197, 'grad_norm': 0.9744833707809448, 'learning_rate': 0.0008500925925925926, 'epoch': 0.45}
Step 1630: {'loss': 0.8799, 'grad_norm': 0.9830833077430725, 'learning_rate': 0.0008491666666666667, 'epoch': 0.4527777777777778}
Step 1640: {'loss': 0.9034, 'grad_norm': 0.7572885751724243, 'learning_rate': 0.0008482407407407408, 'epoch': 0.45555555555555555}
Step 1650: {'loss': 0.8954, 'grad_norm': 0.7124255299568176, 'learning_rate': 0.0008473148148148148, 'epoch': 0.4583333333333333}
Step 1660: {'loss': 0.9522, 'grad_norm': 1.1918261051177979, 'learning_rate': 0.0008463888888888889, 'epoch': 0.46111111111111114}
Step 1670: {'loss': 0.9824, 'grad_norm': 0.8393080234527588, 'learning_rate': 0.000845462962962963, 'epoch': 0.4638888888888889}
Step 1680: {'loss': 0.8422, 'grad_norm': 0.8657923340797424, 'learning_rate': 0.0008445370370370371, 'epoch': 0.4666666666666667}
Step 1690: {'loss': 0.8874, 'grad_norm': 1.0793673992156982, 'learning_rate': 0.0008436111111111111, 'epoch': 0.46944444444444444}
Step 1700: {'loss': 0.9215, 'grad_norm': 1.0472756624221802, 'learning_rate': 0.0008426851851851852, 'epoch': 0.4722222222222222}
Step 1710: {'loss': 0.9729, 'grad_norm': 1.4369276762008667, 'learning_rate': 0.0008417592592592592, 'epoch': 0.475}
Step 1720: {'loss': 0.8814, 'grad_norm': 0.7665168642997742, 'learning_rate': 0.0008408333333333333, 'epoch': 0.4777777777777778}
Step 1730: {'loss': 0.9481, 'grad_norm': 0.7815970778465271, 'learning_rate': 0.0008399074074074075, 'epoch': 0.48055555555555557}
Step 1740: {'loss': 0.8746, 'grad_norm': 0.7787508964538574, 'learning_rate': 0.0008389814814814815, 'epoch': 0.48333333333333334}
Step 1750: {'loss': 0.8921, 'grad_norm': 0.9629390239715576, 'learning_rate': 0.0008380555555555555, 'epoch': 0.4861111111111111}
Step 1760: {'loss': 0.9243, 'grad_norm': 0.7954306602478027, 'learning_rate': 0.0008371296296296297, 'epoch': 0.4888888888888889}
Step 1770: {'loss': 0.9686, 'grad_norm': 0.8621756434440613, 'learning_rate': 0.0008362037037037037, 'epoch': 0.49166666666666664}
Step 1780: {'loss': 0.8532, 'grad_norm': 0.8990945816040039, 'learning_rate': 0.0008352777777777778, 'epoch': 0.49444444444444446}
Step 1790: {'loss': 0.878, 'grad_norm': 0.9995071291923523, 'learning_rate': 0.000834351851851852, 'epoch': 0.49722222222222223}
Step 1800: {'loss': 0.9095, 'grad_norm': 0.8467497229576111, 'learning_rate': 0.0008334259259259259, 'epoch': 0.5}
Step 1810: {'loss': 0.857, 'grad_norm': 0.8855928182601929, 'learning_rate': 0.0008325, 'epoch': 0.5027777777777778}
Step 1820: {'loss': 0.8594, 'grad_norm': 0.5406888723373413, 'learning_rate': 0.000831574074074074, 'epoch': 0.5055555555555555}
Step 1830: {'loss': 0.8654, 'grad_norm': 0.6820629835128784, 'learning_rate': 0.0008306481481481482, 'epoch': 0.5083333333333333}
Step 1840: {'loss': 0.875, 'grad_norm': 1.0287399291992188, 'learning_rate': 0.0008297222222222223, 'epoch': 0.5111111111111111}
Step 1850: {'loss': 0.937, 'grad_norm': 0.8345126509666443, 'learning_rate': 0.0008287962962962962, 'epoch': 0.5138888888888888}
Step 1860: {'loss': 0.9543, 'grad_norm': 0.815502941608429, 'learning_rate': 0.0008278703703703704, 'epoch': 0.5166666666666667}
Step 1870: {'loss': 0.8911, 'grad_norm': 0.6388190388679504, 'learning_rate': 0.0008269444444444445, 'epoch': 0.5194444444444445}
Step 1880: {'loss': 0.9199, 'grad_norm': 0.670357346534729, 'learning_rate': 0.0008260185185185185, 'epoch': 0.5222222222222223}
Step 1890: {'loss': 0.8457, 'grad_norm': 0.8980614542961121, 'learning_rate': 0.0008250925925925927, 'epoch': 0.525}
Step 1900: {'loss': 0.8082, 'grad_norm': 0.7092308402061462, 'learning_rate': 0.0008241666666666667, 'epoch': 0.5277777777777778}
Step 1910: {'loss': 0.9089, 'grad_norm': 0.8262720704078674, 'learning_rate': 0.0008232407407407407, 'epoch': 0.5305555555555556}
Step 1920: {'loss': 0.9032, 'grad_norm': 0.5072513818740845, 'learning_rate': 0.0008223148148148149, 'epoch': 0.5333333333333333}
Step 1930: {'loss': 0.7592, 'grad_norm': 0.7150878310203552, 'learning_rate': 0.0008213888888888889, 'epoch': 0.5361111111111111}
Step 1940: {'loss': 0.8757, 'grad_norm': 0.585084855556488, 'learning_rate': 0.0008204629629629629, 'epoch': 0.5388888888888889}
Step 1950: {'loss': 0.9427, 'grad_norm': 0.7054643034934998, 'learning_rate': 0.0008195370370370371, 'epoch': 0.5416666666666666}
Step 1960: {'loss': 1.009, 'grad_norm': 0.8332468867301941, 'learning_rate': 0.0008186111111111111, 'epoch': 0.5444444444444444}
Step 1970: {'loss': 1.0355, 'grad_norm': 0.8690201640129089, 'learning_rate': 0.0008176851851851852, 'epoch': 0.5472222222222223}
Step 1980: {'loss': 1.0022, 'grad_norm': 0.7281408905982971, 'learning_rate': 0.0008167592592592593, 'epoch': 0.55}
Step 1990: {'loss': 0.8314, 'grad_norm': 0.8572153449058533, 'learning_rate': 0.0008158333333333333, 'epoch': 0.5527777777777778}
Step 2000: {'loss': 0.915, 'grad_norm': 0.7859967350959778, 'learning_rate': 0.0008149074074074074, 'epoch': 0.5555555555555556}
Step 2010: {'loss': 0.8892, 'grad_norm': 0.9655928611755371, 'learning_rate': 0.0008139814814814815, 'epoch': 0.5583333333333333}
Step 2020: {'loss': 0.8733, 'grad_norm': 0.9884775280952454, 'learning_rate': 0.0008130555555555556, 'epoch': 0.5611111111111111}
Step 2030: {'loss': 0.9554, 'grad_norm': 0.7921600937843323, 'learning_rate': 0.0008121296296296297, 'epoch': 0.5638888888888889}
Step 2040: {'loss': 0.9551, 'grad_norm': 0.6096521615982056, 'learning_rate': 0.0008112037037037036, 'epoch': 0.5666666666666667}
Step 2050: {'loss': 0.89, 'grad_norm': 0.5649269223213196, 'learning_rate': 0.0008102777777777778, 'epoch': 0.5694444444444444}
Step 2060: {'loss': 0.7818, 'grad_norm': 0.6696966290473938, 'learning_rate': 0.0008093518518518519, 'epoch': 0.5722222222222222}
Step 2070: {'loss': 0.8947, 'grad_norm': 0.7692519426345825, 'learning_rate': 0.0008084259259259259, 'epoch': 0.575}
Step 2080: {'loss': 0.8385, 'grad_norm': 0.796414315700531, 'learning_rate': 0.0008075000000000001, 'epoch': 0.5777777777777777}
Step 2090: {'loss': 0.8108, 'grad_norm': 0.5634520053863525, 'learning_rate': 0.0008065740740740741, 'epoch': 0.5805555555555556}
Step 2100: {'loss': 0.8256, 'grad_norm': 0.575005054473877, 'learning_rate': 0.0008056481481481481, 'epoch': 0.5833333333333334}
Step 2110: {'loss': 0.8602, 'grad_norm': 1.0555778741836548, 'learning_rate': 0.0008047222222222223, 'epoch': 0.5861111111111111}
Step 2120: {'loss': 0.9408, 'grad_norm': 0.6836174130439758, 'learning_rate': 0.0008037962962962964, 'epoch': 0.5888888888888889}
Step 2130: {'loss': 0.9485, 'grad_norm': 0.8285902738571167, 'learning_rate': 0.0008028703703703703, 'epoch': 0.5916666666666667}
Step 2140: {'loss': 0.9146, 'grad_norm': 1.0693634748458862, 'learning_rate': 0.0008019444444444444, 'epoch': 0.5944444444444444}
Step 2150: {'loss': 0.9049, 'grad_norm': 0.8323511481285095, 'learning_rate': 0.0008010185185185185, 'epoch': 0.5972222222222222}
Step 2160: {'loss': 0.9544, 'grad_norm': 0.8417086601257324, 'learning_rate': 0.0008000925925925926, 'epoch': 0.6}
Step 2170: {'loss': 0.9173, 'grad_norm': 0.7907130122184753, 'learning_rate': 0.0007991666666666667, 'epoch': 0.6027777777777777}
Step 2180: {'loss': 0.8255, 'grad_norm': 0.6007708311080933, 'learning_rate': 0.0007982407407407407, 'epoch': 0.6055555555555555}
Step 2190: {'loss': 0.9607, 'grad_norm': 0.8336019515991211, 'learning_rate': 0.0007973148148148148, 'epoch': 0.6083333333333333}
Step 2200: {'loss': 0.8487, 'grad_norm': 1.0036733150482178, 'learning_rate': 0.0007963888888888889, 'epoch': 0.6111111111111112}
Step 2210: {'loss': 0.9176, 'grad_norm': 0.8318336009979248, 'learning_rate': 0.000795462962962963, 'epoch': 0.6138888888888889}
Step 2220: {'loss': 0.9218, 'grad_norm': 0.9117042422294617, 'learning_rate': 0.0007945370370370371, 'epoch': 0.6166666666666667}
Step 2230: {'loss': 0.859, 'grad_norm': 1.0140608549118042, 'learning_rate': 0.0007936111111111111, 'epoch': 0.6194444444444445}
Step 2240: {'loss': 0.9259, 'grad_norm': 1.2808221578598022, 'learning_rate': 0.0007926851851851852, 'epoch': 0.6222222222222222}
Step 2250: {'loss': 0.9037, 'grad_norm': 0.8658770322799683, 'learning_rate': 0.0007917592592592593, 'epoch': 0.625}
Step 2260: {'loss': 0.8571, 'grad_norm': 0.926517128944397, 'learning_rate': 0.0007908333333333334, 'epoch': 0.6277777777777778}
Step 2270: {'loss': 0.8739, 'grad_norm': 0.8046382665634155, 'learning_rate': 0.0007899074074074074, 'epoch': 0.6305555555555555}
Step 2280: {'loss': 0.9318, 'grad_norm': 1.2378689050674438, 'learning_rate': 0.0007889814814814815, 'epoch': 0.6333333333333333}
Step 2290: {'loss': 0.9701, 'grad_norm': 0.7702359557151794, 'learning_rate': 0.0007880555555555555, 'epoch': 0.6361111111111111}
Step 2300: {'loss': 0.9728, 'grad_norm': 0.7181092500686646, 'learning_rate': 0.0007871296296296296, 'epoch': 0.6388888888888888}
Step 2310: {'loss': 0.9051, 'grad_norm': 0.9574471116065979, 'learning_rate': 0.0007862037037037038, 'epoch': 0.6416666666666667}
Step 2320: {'loss': 0.9735, 'grad_norm': 0.8065500855445862, 'learning_rate': 0.0007852777777777778, 'epoch': 0.6444444444444445}
Step 2330: {'loss': 0.9455, 'grad_norm': 0.7667494416236877, 'learning_rate': 0.0007843518518518518, 'epoch': 0.6472222222222223}
Step 2340: {'loss': 0.8588, 'grad_norm': 0.7545123100280762, 'learning_rate': 0.000783425925925926, 'epoch': 0.65}
Step 2350: {'loss': 0.8129, 'grad_norm': 0.7981285452842712, 'learning_rate': 0.0007825, 'epoch': 0.6527777777777778}
Step 2360: {'loss': 0.8783, 'grad_norm': 0.7321203351020813, 'learning_rate': 0.0007815740740740741, 'epoch': 0.6555555555555556}
Step 2370: {'loss': 0.8512, 'grad_norm': 0.7452141642570496, 'learning_rate': 0.0007806481481481483, 'epoch': 0.6583333333333333}
Step 2380: {'loss': 0.893, 'grad_norm': 0.8287864327430725, 'learning_rate': 0.0007797222222222222, 'epoch': 0.6611111111111111}
Step 2390: {'loss': 0.9338, 'grad_norm': 0.9394960403442383, 'learning_rate': 0.0007787962962962963, 'epoch': 0.6638888888888889}
Step 2400: {'loss': 0.8047, 'grad_norm': 1.2905220985412598, 'learning_rate': 0.0007778703703703704, 'epoch': 0.6666666666666666}
Step 2410: {'loss': 0.9589, 'grad_norm': 0.8119465708732605, 'learning_rate': 0.0007769444444444445, 'epoch': 0.6694444444444444}
Step 2420: {'loss': 0.9365, 'grad_norm': 0.9564868807792664, 'learning_rate': 0.0007760185185185186, 'epoch': 0.6722222222222223}
Step 2430: {'loss': 0.9537, 'grad_norm': 0.7321638464927673, 'learning_rate': 0.0007750925925925925, 'epoch': 0.675}
Step 2440: {'loss': 0.8952, 'grad_norm': 0.727367103099823, 'learning_rate': 0.0007741666666666667, 'epoch': 0.6777777777777778}
Step 2450: {'loss': 0.8875, 'grad_norm': 0.848581075668335, 'learning_rate': 0.0007732407407407408, 'epoch': 0.6805555555555556}
Step 2460: {'loss': 0.9851, 'grad_norm': 0.9555227756500244, 'learning_rate': 0.0007723148148148148, 'epoch': 0.6833333333333333}
Step 2470: {'loss': 0.9356, 'grad_norm': 1.0538264513015747, 'learning_rate': 0.000771388888888889, 'epoch': 0.6861111111111111}
Step 2480: {'loss': 0.9211, 'grad_norm': 0.707602858543396, 'learning_rate': 0.000770462962962963, 'epoch': 0.6888888888888889}
Step 2490: {'loss': 0.8743, 'grad_norm': 0.7841326594352722, 'learning_rate': 0.000769537037037037, 'epoch': 0.6916666666666667}
Step 2500: {'loss': 0.8164, 'grad_norm': 0.9173567891120911, 'learning_rate': 0.0007686111111111112, 'epoch': 0.6944444444444444}
Step 2510: {'loss': 0.9246, 'grad_norm': 0.8681180477142334, 'learning_rate': 0.0007676851851851852, 'epoch': 0.6972222222222222}
Step 2520: {'loss': 0.8358, 'grad_norm': 0.8611476421356201, 'learning_rate': 0.0007667592592592592, 'epoch': 0.7}
Step 2530: {'loss': 0.88, 'grad_norm': 0.8702024817466736, 'learning_rate': 0.0007658333333333334, 'epoch': 0.7027777777777777}
Step 2540: {'loss': 0.8993, 'grad_norm': 0.8544368147850037, 'learning_rate': 0.0007649074074074074, 'epoch': 0.7055555555555556}
Step 2550: {'loss': 0.8648, 'grad_norm': 1.234779715538025, 'learning_rate': 0.0007639814814814815, 'epoch': 0.7083333333333334}
Step 2560: {'loss': 0.8735, 'grad_norm': 0.8069517016410828, 'learning_rate': 0.0007630555555555557, 'epoch': 0.7111111111111111}
Step 2570: {'loss': 0.834, 'grad_norm': 0.678710401058197, 'learning_rate': 0.0007621296296296296, 'epoch': 0.7138888888888889}
Step 2580: {'loss': 0.8738, 'grad_norm': 0.7162540555000305, 'learning_rate': 0.0007612037037037037, 'epoch': 0.7166666666666667}
Step 2590: {'loss': 0.82, 'grad_norm': 0.7951240539550781, 'learning_rate': 0.0007602777777777778, 'epoch': 0.7194444444444444}
Step 2600: {'loss': 0.8783, 'grad_norm': 0.5667961239814758, 'learning_rate': 0.0007593518518518519, 'epoch': 0.7222222222222222}
Step 2610: {'loss': 0.7896, 'grad_norm': 1.0375512838363647, 'learning_rate': 0.000758425925925926, 'epoch': 0.725}
Step 2620: {'loss': 0.8766, 'grad_norm': 0.8835698366165161, 'learning_rate': 0.0007574999999999999, 'epoch': 0.7277777777777777}
Step 2630: {'loss': 0.8036, 'grad_norm': 0.6822391152381897, 'learning_rate': 0.0007565740740740741, 'epoch': 0.7305555555555555}
Step 2640: {'loss': 0.7697, 'grad_norm': 0.8545679450035095, 'learning_rate': 0.0007556481481481482, 'epoch': 0.7333333333333333}
Step 2650: {'loss': 0.8724, 'grad_norm': 0.4383903443813324, 'learning_rate': 0.0007547222222222222, 'epoch': 0.7361111111111112}
Step 2660: {'loss': 0.8928, 'grad_norm': 0.873773992061615, 'learning_rate': 0.0007537962962962964, 'epoch': 0.7388888888888889}
Step 2670: {'loss': 0.8421, 'grad_norm': 0.6558694839477539, 'learning_rate': 0.0007528703703703704, 'epoch': 0.7416666666666667}
Step 2680: {'loss': 0.9042, 'grad_norm': 0.8788363933563232, 'learning_rate': 0.0007519444444444444, 'epoch': 0.7444444444444445}
Step 2690: {'loss': 0.9631, 'grad_norm': 0.9905015826225281, 'learning_rate': 0.0007510185185185186, 'epoch': 0.7472222222222222}
Step 2700: {'loss': 0.9405, 'grad_norm': 0.6972206234931946, 'learning_rate': 0.0007500925925925927, 'epoch': 0.75}
Step 2710: {'loss': 0.8955, 'grad_norm': 0.8270220160484314, 'learning_rate': 0.0007491666666666666, 'epoch': 0.7527777777777778}
Step 2720: {'loss': 0.8219, 'grad_norm': 0.6148395538330078, 'learning_rate': 0.0007482407407407407, 'epoch': 0.7555555555555555}
Step 2730: {'loss': 0.8739, 'grad_norm': 0.8338251113891602, 'learning_rate': 0.0007473148148148148, 'epoch': 0.7583333333333333}
Step 2740: {'loss': 0.8469, 'grad_norm': 0.6517735719680786, 'learning_rate': 0.0007463888888888889, 'epoch': 0.7611111111111111}
Step 2750: {'loss': 0.8215, 'grad_norm': 0.5901798009872437, 'learning_rate': 0.000745462962962963, 'epoch': 0.7638888888888888}
Step 2760: {'loss': 0.7985, 'grad_norm': 0.5550568103790283, 'learning_rate': 0.000744537037037037, 'epoch': 0.7666666666666667}
Step 2770: {'loss': 0.8681, 'grad_norm': 0.771115779876709, 'learning_rate': 0.0007436111111111111, 'epoch': 0.7694444444444445}
Step 2780: {'loss': 0.84, 'grad_norm': 0.6232898235321045, 'learning_rate': 0.0007426851851851852, 'epoch': 0.7722222222222223}
Step 2790: {'loss': 0.9162, 'grad_norm': 0.48226526379585266, 'learning_rate': 0.0007417592592592593, 'epoch': 0.775}
Step 2800: {'loss': 0.8174, 'grad_norm': 0.8450101613998413, 'learning_rate': 0.0007408333333333334, 'epoch': 0.7777777777777778}
Step 2810: {'loss': 0.9679, 'grad_norm': 1.1092408895492554, 'learning_rate': 0.0007399074074074074, 'epoch': 0.7805555555555556}
Step 2820: {'loss': 0.8653, 'grad_norm': 0.5928083062171936, 'learning_rate': 0.0007389814814814815, 'epoch': 0.7833333333333333}
Step 2830: {'loss': 0.8852, 'grad_norm': 0.6309219002723694, 'learning_rate': 0.0007380555555555556, 'epoch': 0.7861111111111111}
Step 2840: {'loss': 0.941, 'grad_norm': 0.6497180461883545, 'learning_rate': 0.0007371296296296296, 'epoch': 0.7888888888888889}
Step 2850: {'loss': 0.8342, 'grad_norm': 0.9927670955657959, 'learning_rate': 0.0007362037037037038, 'epoch': 0.7916666666666666}
Step 2860: {'loss': 0.8404, 'grad_norm': 0.6559424996376038, 'learning_rate': 0.0007352777777777778, 'epoch': 0.7944444444444444}
Step 2870: {'loss': 0.9353, 'grad_norm': 0.6050427556037903, 'learning_rate': 0.0007343518518518518, 'epoch': 0.7972222222222223}
Step 2880: {'loss': 0.891, 'grad_norm': 0.5019800066947937, 'learning_rate': 0.0007334259259259259, 'epoch': 0.8}
Step 2890: {'loss': 0.8928, 'grad_norm': 0.6143177151679993, 'learning_rate': 0.0007325000000000001, 'epoch': 0.8027777777777778}
Step 2900: {'loss': 0.8523, 'grad_norm': 1.0625594854354858, 'learning_rate': 0.000731574074074074, 'epoch': 0.8055555555555556}
Step 2910: {'loss': 0.9536, 'grad_norm': 0.7502787113189697, 'learning_rate': 0.0007306481481481481, 'epoch': 0.8083333333333333}
Step 2920: {'loss': 0.8893, 'grad_norm': 0.7110663056373596, 'learning_rate': 0.0007297222222222223, 'epoch': 0.8111111111111111}
Step 2930: {'loss': 0.8198, 'grad_norm': 0.586668848991394, 'learning_rate': 0.0007287962962962963, 'epoch': 0.8138888888888889}
Step 2940: {'loss': 0.8967, 'grad_norm': 0.8220056891441345, 'learning_rate': 0.0007278703703703704, 'epoch': 0.8166666666666667}
Step 2950: {'loss': 0.8687, 'grad_norm': 0.9798691868782043, 'learning_rate': 0.0007269444444444444, 'epoch': 0.8194444444444444}
Step 2960: {'loss': 0.9013, 'grad_norm': 0.6790851354598999, 'learning_rate': 0.0007260185185185185, 'epoch': 0.8222222222222222}
Step 2970: {'loss': 0.8249, 'grad_norm': 0.6971582770347595, 'learning_rate': 0.0007250925925925926, 'epoch': 0.825}
Step 2980: {'loss': 0.9788, 'grad_norm': 0.7594987750053406, 'learning_rate': 0.0007241666666666667, 'epoch': 0.8277777777777777}
Step 2990: {'loss': 0.9674, 'grad_norm': 0.8947362303733826, 'learning_rate': 0.0007232407407407408, 'epoch': 0.8305555555555556}
Step 3000: {'loss': 0.8828, 'grad_norm': 0.9502387046813965, 'learning_rate': 0.0007223148148148148, 'epoch': 0.8333333333333334}
Step 3010: {'loss': 0.9416, 'grad_norm': 1.0325016975402832, 'learning_rate': 0.0007213888888888889, 'epoch': 0.8361111111111111}
Step 3020: {'loss': 0.8954, 'grad_norm': 0.6470682621002197, 'learning_rate': 0.000720462962962963, 'epoch': 0.8388888888888889}
Step 3030: {'loss': 0.7958, 'grad_norm': 0.7779525518417358, 'learning_rate': 0.0007195370370370371, 'epoch': 0.8416666666666667}
Step 3040: {'loss': 0.9437, 'grad_norm': 0.6767956614494324, 'learning_rate': 0.0007186111111111111, 'epoch': 0.8444444444444444}
Step 3050: {'loss': 0.9247, 'grad_norm': 0.7990759015083313, 'learning_rate': 0.0007176851851851852, 'epoch': 0.8472222222222222}
Step 3060: {'loss': 0.7968, 'grad_norm': 0.7209581136703491, 'learning_rate': 0.0007167592592592592, 'epoch': 0.85}
Step 3070: {'loss': 0.8716, 'grad_norm': 0.7688855528831482, 'learning_rate': 0.0007158333333333333, 'epoch': 0.8527777777777777}
Step 3080: {'loss': 0.8143, 'grad_norm': 0.9539096355438232, 'learning_rate': 0.0007149074074074075, 'epoch': 0.8555555555555555}
Step 3090: {'loss': 0.9092, 'grad_norm': 0.6777171492576599, 'learning_rate': 0.0007139814814814815, 'epoch': 0.8583333333333333}
Step 3100: {'loss': 0.9583, 'grad_norm': 0.6875616908073425, 'learning_rate': 0.0007130555555555555, 'epoch': 0.8611111111111112}
Step 3110: {'loss': 0.9588, 'grad_norm': 0.8159445524215698, 'learning_rate': 0.0007121296296296297, 'epoch': 0.8638888888888889}
Step 3120: {'loss': 0.885, 'grad_norm': 1.0000535249710083, 'learning_rate': 0.0007112037037037037, 'epoch': 0.8666666666666667}
Step 3130: {'loss': 0.7892, 'grad_norm': 0.6546843647956848, 'learning_rate': 0.0007102777777777778, 'epoch': 0.8694444444444445}
Step 3140: {'loss': 0.9109, 'grad_norm': 0.7191452980041504, 'learning_rate': 0.000709351851851852, 'epoch': 0.8722222222222222}
Step 3150: {'loss': 0.9008, 'grad_norm': 0.8834969401359558, 'learning_rate': 0.0007084259259259259, 'epoch': 0.875}
Step 3160: {'loss': 0.8643, 'grad_norm': 0.7644669413566589, 'learning_rate': 0.0007075, 'epoch': 0.8777777777777778}
Step 3170: {'loss': 0.8176, 'grad_norm': 0.8881138563156128, 'learning_rate': 0.000706574074074074, 'epoch': 0.8805555555555555}
Step 3180: {'loss': 0.8007, 'grad_norm': 0.7918575406074524, 'learning_rate': 0.0007056481481481482, 'epoch': 0.8833333333333333}
Step 3190: {'loss': 0.8281, 'grad_norm': 0.6125019788742065, 'learning_rate': 0.0007047222222222223, 'epoch': 0.8861111111111111}
Step 3200: {'loss': 0.8205, 'grad_norm': 0.6921345591545105, 'learning_rate': 0.0007037962962962962, 'epoch': 0.8888888888888888}
Step 3210: {'loss': 0.8053, 'grad_norm': 0.7874687910079956, 'learning_rate': 0.0007028703703703704, 'epoch': 0.8916666666666667}
Step 3220: {'loss': 0.9079, 'grad_norm': 0.7270506024360657, 'learning_rate': 0.0007019444444444445, 'epoch': 0.8944444444444445}
Step 3230: {'loss': 0.9796, 'grad_norm': 1.1018534898757935, 'learning_rate': 0.0007010185185185185, 'epoch': 0.8972222222222223}
Step 3240: {'loss': 0.9541, 'grad_norm': 0.8015625476837158, 'learning_rate': 0.0007000925925925926, 'epoch': 0.9}
Step 3250: {'loss': 0.85, 'grad_norm': 1.0053025484085083, 'learning_rate': 0.0006991666666666667, 'epoch': 0.9027777777777778}
Step 3260: {'loss': 0.872, 'grad_norm': 0.6966462731361389, 'learning_rate': 0.0006982407407407407, 'epoch': 0.9055555555555556}
Step 3270: {'loss': 0.9654, 'grad_norm': 0.75359046459198, 'learning_rate': 0.0006973148148148149, 'epoch': 0.9083333333333333}
Step 3280: {'loss': 0.8071, 'grad_norm': 0.700758159160614, 'learning_rate': 0.0006963888888888889, 'epoch': 0.9111111111111111}
Step 3290: {'loss': 0.9165, 'grad_norm': 1.0770602226257324, 'learning_rate': 0.0006954629629629629, 'epoch': 0.9138888888888889}
Step 3300: {'loss': 0.839, 'grad_norm': 0.8829271793365479, 'learning_rate': 0.0006945370370370371, 'epoch': 0.9166666666666666}
Step 3310: {'loss': 0.9606, 'grad_norm': 0.7347406148910522, 'learning_rate': 0.0006936111111111111, 'epoch': 0.9194444444444444}
Step 3320: {'loss': 0.8282, 'grad_norm': 0.8172693848609924, 'learning_rate': 0.0006926851851851852, 'epoch': 0.9222222222222223}
Step 3330: {'loss': 0.9306, 'grad_norm': 0.7336495518684387, 'learning_rate': 0.0006917592592592593, 'epoch': 0.925}
Step 3340: {'loss': 0.8611, 'grad_norm': 0.612994372844696, 'learning_rate': 0.0006908333333333333, 'epoch': 0.9277777777777778}
Step 3350: {'loss': 0.8532, 'grad_norm': 0.6929400563240051, 'learning_rate': 0.0006899074074074074, 'epoch': 0.9305555555555556}
Step 3360: {'loss': 0.8542, 'grad_norm': 0.6603939533233643, 'learning_rate': 0.0006889814814814815, 'epoch': 0.9333333333333333}
Step 3370: {'loss': 0.8313, 'grad_norm': 0.8303788304328918, 'learning_rate': 0.0006880555555555556, 'epoch': 0.9361111111111111}
Step 3380: {'loss': 0.9453, 'grad_norm': 1.1121766567230225, 'learning_rate': 0.0006871296296296297, 'epoch': 0.9388888888888889}
Step 3390: {'loss': 0.8293, 'grad_norm': 0.7105264663696289, 'learning_rate': 0.0006862037037037036, 'epoch': 0.9416666666666667}
Step 3400: {'loss': 0.9404, 'grad_norm': 0.5013710260391235, 'learning_rate': 0.0006852777777777778, 'epoch': 0.9444444444444444}
Step 3410: {'loss': 0.9049, 'grad_norm': 0.7165019512176514, 'learning_rate': 0.0006843518518518519, 'epoch': 0.9472222222222222}
Step 3420: {'loss': 0.9743, 'grad_norm': 0.6339848637580872, 'learning_rate': 0.0006834259259259259, 'epoch': 0.95}
Step 3430: {'loss': 0.88, 'grad_norm': 0.8617343902587891, 'learning_rate': 0.0006825000000000001, 'epoch': 0.9527777777777777}
Step 3440: {'loss': 0.9516, 'grad_norm': 0.799807071685791, 'learning_rate': 0.0006815740740740741, 'epoch': 0.9555555555555556}
Step 3450: {'loss': 0.941, 'grad_norm': 1.1388896703720093, 'learning_rate': 0.0006806481481481481, 'epoch': 0.9583333333333334}
Step 3460: {'loss': 0.8815, 'grad_norm': 0.7392197251319885, 'learning_rate': 0.0006797222222222223, 'epoch': 0.9611111111111111}
Step 3470: {'loss': 0.8972, 'grad_norm': 0.6525501608848572, 'learning_rate': 0.0006787962962962964, 'epoch': 0.9638888888888889}
Step 3480: {'loss': 0.8527, 'grad_norm': 0.6052042245864868, 'learning_rate': 0.0006778703703703703, 'epoch': 0.9666666666666667}
Step 3490: {'loss': 0.9324, 'grad_norm': 0.7754153609275818, 'learning_rate': 0.0006769444444444444, 'epoch': 0.9694444444444444}
Step 3500: {'loss': 0.8252, 'grad_norm': 0.650619626045227, 'learning_rate': 0.0006760185185185185, 'epoch': 0.9722222222222222}
Step 3510: {'loss': 0.9332, 'grad_norm': 1.1818541288375854, 'learning_rate': 0.0006750925925925926, 'epoch': 0.975}
Step 3520: {'loss': 0.995, 'grad_norm': 0.8183779716491699, 'learning_rate': 0.0006741666666666667, 'epoch': 0.9777777777777777}
Step 3530: {'loss': 0.8932, 'grad_norm': 0.5128337740898132, 'learning_rate': 0.0006732407407407407, 'epoch': 0.9805555555555555}
Step 3540: {'loss': 0.8668, 'grad_norm': 0.9514197707176208, 'learning_rate': 0.0006723148148148148, 'epoch': 0.9833333333333333}
Step 3550: {'loss': 0.9586, 'grad_norm': 0.5823650360107422, 'learning_rate': 0.0006713888888888889, 'epoch': 0.9861111111111112}
Step 3560: {'loss': 0.8825, 'grad_norm': 0.8701735734939575, 'learning_rate': 0.000670462962962963, 'epoch': 0.9888888888888889}
Step 3570: {'loss': 0.8674, 'grad_norm': 0.8015859127044678, 'learning_rate': 0.0006695370370370371, 'epoch': 0.9916666666666667}
Step 3580: {'loss': 0.907, 'grad_norm': 0.577251672744751, 'learning_rate': 0.0006686111111111111, 'epoch': 0.9944444444444445}
Step 3590: {'loss': 0.8366, 'grad_norm': 0.6867671012878418, 'learning_rate': 0.0006676851851851852, 'epoch': 0.9972222222222222}
Step 3600: {'loss': 0.884, 'grad_norm': 0.7684687972068787, 'learning_rate': 0.0006667592592592593, 'epoch': 1.0}
Step 3610: {'loss': 0.8438, 'grad_norm': 0.7651779055595398, 'learning_rate': 0.0006658333333333334, 'epoch': 1.0027777777777778}
Step 3620: {'loss': 0.8647, 'grad_norm': 0.6555781364440918, 'learning_rate': 0.0006649074074074074, 'epoch': 1.0055555555555555}
Step 3630: {'loss': 0.892, 'grad_norm': 0.5931333303451538, 'learning_rate': 0.0006639814814814815, 'epoch': 1.0083333333333333}
Step 3640: {'loss': 0.7813, 'grad_norm': 0.704795777797699, 'learning_rate': 0.0006630555555555555, 'epoch': 1.011111111111111}
Step 3650: {'loss': 0.8842, 'grad_norm': 0.6239269971847534, 'learning_rate': 0.0006621296296296296, 'epoch': 1.0138888888888888}
Step 3660: {'loss': 0.8278, 'grad_norm': 0.7198467254638672, 'learning_rate': 0.0006612037037037038, 'epoch': 1.0166666666666666}
Step 3670: {'loss': 0.8133, 'grad_norm': 0.6907873749732971, 'learning_rate': 0.0006602777777777778, 'epoch': 1.0194444444444444}
Step 3680: {'loss': 0.8263, 'grad_norm': 0.7259255051612854, 'learning_rate': 0.0006593518518518518, 'epoch': 1.0222222222222221}
Step 3690: {'loss': 0.8627, 'grad_norm': 0.6919883489608765, 'learning_rate': 0.000658425925925926, 'epoch': 1.025}
Step 3700: {'loss': 0.7858, 'grad_norm': 0.4335179924964905, 'learning_rate': 0.0006575, 'epoch': 1.0277777777777777}
Step 3710: {'loss': 0.9031, 'grad_norm': 0.6765605211257935, 'learning_rate': 0.0006565740740740741, 'epoch': 1.0305555555555554}
Step 3720: {'loss': 0.869, 'grad_norm': 0.8953834772109985, 'learning_rate': 0.0006556481481481483, 'epoch': 1.0333333333333334}
Step 3730: {'loss': 0.9441, 'grad_norm': 0.8637723326683044, 'learning_rate': 0.0006547222222222222, 'epoch': 1.0361111111111112}
Step 3740: {'loss': 0.9313, 'grad_norm': 0.5812072157859802, 'learning_rate': 0.0006537962962962963, 'epoch': 1.038888888888889}
Step 3750: {'loss': 0.7494, 'grad_norm': 0.7555192112922668, 'learning_rate': 0.0006528703703703704, 'epoch': 1.0416666666666667}
Step 3760: {'loss': 0.8797, 'grad_norm': 0.8128803968429565, 'learning_rate': 0.0006519444444444445, 'epoch': 1.0444444444444445}
Step 3770: {'loss': 0.8961, 'grad_norm': 0.812157928943634, 'learning_rate': 0.0006510185185185185, 'epoch': 1.0472222222222223}
Step 3780: {'loss': 0.8765, 'grad_norm': 0.8569526076316833, 'learning_rate': 0.0006500925925925925, 'epoch': 1.05}
Step 3790: {'loss': 0.8161, 'grad_norm': 0.9055256843566895, 'learning_rate': 0.0006491666666666667, 'epoch': 1.0527777777777778}
Step 3800: {'loss': 0.9257, 'grad_norm': 0.8475835919380188, 'learning_rate': 0.0006482407407407408, 'epoch': 1.0555555555555556}
Step 3810: {'loss': 0.8996, 'grad_norm': 0.7980344295501709, 'learning_rate': 0.0006473148148148148, 'epoch': 1.0583333333333333}
Step 3820: {'loss': 0.9051, 'grad_norm': 0.6355156302452087, 'learning_rate': 0.000646388888888889, 'epoch': 1.0611111111111111}
Step 3830: {'loss': 0.8669, 'grad_norm': 0.7822411060333252, 'learning_rate': 0.000645462962962963, 'epoch': 1.0638888888888889}
Step 3840: {'loss': 0.8688, 'grad_norm': 1.059332013130188, 'learning_rate': 0.000644537037037037, 'epoch': 1.0666666666666667}
Step 3850: {'loss': 0.7809, 'grad_norm': 1.0807156562805176, 'learning_rate': 0.0006436111111111112, 'epoch': 1.0694444444444444}
Step 3860: {'loss': 0.8843, 'grad_norm': 1.0044200420379639, 'learning_rate': 0.0006426851851851852, 'epoch': 1.0722222222222222}
Step 3870: {'loss': 0.8067, 'grad_norm': 0.8097086548805237, 'learning_rate': 0.0006417592592592592, 'epoch': 1.075}
Step 3880: {'loss': 0.9583, 'grad_norm': 0.45444363355636597, 'learning_rate': 0.0006408333333333334, 'epoch': 1.0777777777777777}
Step 3890: {'loss': 0.9109, 'grad_norm': 1.1093233823776245, 'learning_rate': 0.0006399074074074074, 'epoch': 1.0805555555555555}
Step 3900: {'loss': 0.7932, 'grad_norm': 0.5878662467002869, 'learning_rate': 0.0006389814814814815, 'epoch': 1.0833333333333333}
Step 3910: {'loss': 0.9439, 'grad_norm': 0.8748218417167664, 'learning_rate': 0.0006380555555555557, 'epoch': 1.086111111111111}
Step 3920: {'loss': 0.8535, 'grad_norm': 0.934192955493927, 'learning_rate': 0.0006371296296296296, 'epoch': 1.0888888888888888}
Step 3930: {'loss': 0.8808, 'grad_norm': 0.9037065505981445, 'learning_rate': 0.0006362037037037037, 'epoch': 1.0916666666666666}
Step 3940: {'loss': 0.7497, 'grad_norm': 0.7377314567565918, 'learning_rate': 0.0006352777777777778, 'epoch': 1.0944444444444446}
Step 3950: {'loss': 0.8755, 'grad_norm': 1.0318841934204102, 'learning_rate': 0.0006343518518518519, 'epoch': 1.0972222222222223}
Step 3960: {'loss': 0.849, 'grad_norm': 1.1094419956207275, 'learning_rate': 0.000633425925925926, 'epoch': 1.1}
Step 3970: {'loss': 0.9534, 'grad_norm': 0.5675033330917358, 'learning_rate': 0.0006324999999999999, 'epoch': 1.1027777777777779}
Step 3980: {'loss': 0.9113, 'grad_norm': 0.7107585072517395, 'learning_rate': 0.0006315740740740741, 'epoch': 1.1055555555555556}
Step 3990: {'loss': 0.8631, 'grad_norm': 0.8130514621734619, 'learning_rate': 0.0006306481481481482, 'epoch': 1.1083333333333334}
Step 4000: {'loss': 0.9515, 'grad_norm': 0.6352517008781433, 'learning_rate': 0.0006297222222222222, 'epoch': 1.1111111111111112}
Step 4010: {'loss': 0.7978, 'grad_norm': 0.6701564192771912, 'learning_rate': 0.0006287962962962964, 'epoch': 1.113888888888889}
Step 4020: {'loss': 0.8809, 'grad_norm': 0.7272418737411499, 'learning_rate': 0.0006278703703703704, 'epoch': 1.1166666666666667}
Step 4030: {'loss': 0.9075, 'grad_norm': 0.8579072952270508, 'learning_rate': 0.0006269444444444444, 'epoch': 1.1194444444444445}
Step 4040: {'loss': 0.8473, 'grad_norm': 0.5838996767997742, 'learning_rate': 0.0006260185185185186, 'epoch': 1.1222222222222222}
Step 4050: {'loss': 0.7903, 'grad_norm': 0.6394103169441223, 'learning_rate': 0.0006250925925925927, 'epoch': 1.125}
Step 4060: {'loss': 0.9407, 'grad_norm': 0.9632203578948975, 'learning_rate': 0.0006241666666666666, 'epoch': 1.1277777777777778}
Step 4070: {'loss': 0.8898, 'grad_norm': 0.756191074848175, 'learning_rate': 0.0006232407407407407, 'epoch': 1.1305555555555555}
Step 4080: {'loss': 0.8479, 'grad_norm': 0.82506263256073, 'learning_rate': 0.0006223148148148148, 'epoch': 1.1333333333333333}
Step 4090: {'loss': 0.8951, 'grad_norm': 0.5970667004585266, 'learning_rate': 0.0006213888888888889, 'epoch': 1.136111111111111}
Step 4100: {'loss': 0.8595, 'grad_norm': 0.7223955988883972, 'learning_rate': 0.000620462962962963, 'epoch': 1.1388888888888888}
Step 4110: {'loss': 0.9451, 'grad_norm': 0.7281304001808167, 'learning_rate': 0.000619537037037037, 'epoch': 1.1416666666666666}
Step 4120: {'loss': 0.7719, 'grad_norm': 0.5957801342010498, 'learning_rate': 0.0006186111111111111, 'epoch': 1.1444444444444444}
Step 4130: {'loss': 0.9379, 'grad_norm': 1.2671897411346436, 'learning_rate': 0.0006176851851851852, 'epoch': 1.1472222222222221}
Step 4140: {'loss': 0.8841, 'grad_norm': 1.2151544094085693, 'learning_rate': 0.0006167592592592593, 'epoch': 1.15}
Step 4150: {'loss': 0.812, 'grad_norm': 0.7794806957244873, 'learning_rate': 0.0006158333333333334, 'epoch': 1.1527777777777777}
Step 4160: {'loss': 0.7821, 'grad_norm': 0.6677489876747131, 'learning_rate': 0.0006149074074074074, 'epoch': 1.1555555555555554}
Step 4170: {'loss': 0.8583, 'grad_norm': 0.8212933540344238, 'learning_rate': 0.0006139814814814815, 'epoch': 1.1583333333333332}
Step 4180: {'loss': 0.8319, 'grad_norm': 0.4912950098514557, 'learning_rate': 0.0006130555555555556, 'epoch': 1.1611111111111112}
Step 4190: {'loss': 0.7954, 'grad_norm': 0.807166337966919, 'learning_rate': 0.0006121296296296296, 'epoch': 1.163888888888889}
Step 4200: {'loss': 0.7913, 'grad_norm': 0.6417068839073181, 'learning_rate': 0.0006112037037037038, 'epoch': 1.1666666666666667}
Step 4210: {'loss': 0.9178, 'grad_norm': 0.651193380355835, 'learning_rate': 0.0006102777777777778, 'epoch': 1.1694444444444445}
Step 4220: {'loss': 0.9123, 'grad_norm': 0.6159990429878235, 'learning_rate': 0.0006093518518518518, 'epoch': 1.1722222222222223}
Step 4230: {'loss': 0.8986, 'grad_norm': 0.8681641817092896, 'learning_rate': 0.0006084259259259259, 'epoch': 1.175}
Step 4240: {'loss': 0.9, 'grad_norm': 0.8407369256019592, 'learning_rate': 0.0006075000000000001, 'epoch': 1.1777777777777778}
Step 4250: {'loss': 0.8907, 'grad_norm': 0.810198187828064, 'learning_rate': 0.000606574074074074, 'epoch': 1.1805555555555556}
Step 4260: {'loss': 0.8403, 'grad_norm': 0.5390080809593201, 'learning_rate': 0.0006056481481481481, 'epoch': 1.1833333333333333}
Step 4270: {'loss': 0.8334, 'grad_norm': 0.6595948934555054, 'learning_rate': 0.0006047222222222223, 'epoch': 1.1861111111111111}
Step 4280: {'loss': 0.7941, 'grad_norm': 0.7665917873382568, 'learning_rate': 0.0006037962962962963, 'epoch': 1.1888888888888889}
Step 4290: {'loss': 0.8648, 'grad_norm': 1.0828663110733032, 'learning_rate': 0.0006028703703703704, 'epoch': 1.1916666666666667}
Step 4300: {'loss': 0.8469, 'grad_norm': 0.918316662311554, 'learning_rate': 0.0006019444444444444, 'epoch': 1.1944444444444444}
Step 4310: {'loss': 0.7823, 'grad_norm': 0.9037151336669922, 'learning_rate': 0.0006010185185185185, 'epoch': 1.1972222222222222}
Step 4320: {'loss': 0.8494, 'grad_norm': 0.6314526796340942, 'learning_rate': 0.0006000925925925926, 'epoch': 1.2}
Step 4330: {'loss': 0.8021, 'grad_norm': 0.6036672592163086, 'learning_rate': 0.0005991666666666667, 'epoch': 1.2027777777777777}
Step 4340: {'loss': 0.8221, 'grad_norm': 1.0937665700912476, 'learning_rate': 0.0005982407407407408, 'epoch': 1.2055555555555555}
Step 4350: {'loss': 0.9186, 'grad_norm': 0.6491442322731018, 'learning_rate': 0.0005973148148148148, 'epoch': 1.2083333333333333}
Step 4360: {'loss': 1.0143, 'grad_norm': 0.938106119632721, 'learning_rate': 0.0005963888888888889, 'epoch': 1.211111111111111}
Step 4370: {'loss': 0.8311, 'grad_norm': 1.051365613937378, 'learning_rate': 0.000595462962962963, 'epoch': 1.2138888888888888}
Step 4380: {'loss': 0.9672, 'grad_norm': 0.8843607306480408, 'learning_rate': 0.0005945370370370371, 'epoch': 1.2166666666666668}
Step 4390: {'loss': 0.8196, 'grad_norm': 0.8828956484794617, 'learning_rate': 0.000593611111111111, 'epoch': 1.2194444444444446}
Step 4400: {'loss': 0.9067, 'grad_norm': 0.909206748008728, 'learning_rate': 0.0005926851851851852, 'epoch': 1.2222222222222223}
Step 4410: {'loss': 0.8919, 'grad_norm': 0.7023281455039978, 'learning_rate': 0.0005917592592592592, 'epoch': 1.225}
Step 4420: {'loss': 0.8444, 'grad_norm': 0.6010417342185974, 'learning_rate': 0.0005908333333333333, 'epoch': 1.2277777777777779}
Step 4430: {'loss': 0.9074, 'grad_norm': 0.6631101965904236, 'learning_rate': 0.0005899074074074075, 'epoch': 1.2305555555555556}
Step 4440: {'loss': 0.9143, 'grad_norm': 1.144407868385315, 'learning_rate': 0.0005889814814814815, 'epoch': 1.2333333333333334}
Step 4450: {'loss': 0.8718, 'grad_norm': 0.7302140593528748, 'learning_rate': 0.0005880555555555555, 'epoch': 1.2361111111111112}
Step 4460: {'loss': 0.8554, 'grad_norm': 0.6595177054405212, 'learning_rate': 0.0005871296296296297, 'epoch': 1.238888888888889}
Step 4470: {'loss': 0.7774, 'grad_norm': 0.4263032078742981, 'learning_rate': 0.0005862037037037037, 'epoch': 1.2416666666666667}
Step 4480: {'loss': 0.8505, 'grad_norm': 0.8870371580123901, 'learning_rate': 0.0005852777777777778, 'epoch': 1.2444444444444445}
Step 4490: {'loss': 0.9335, 'grad_norm': 0.7556072473526001, 'learning_rate': 0.000584351851851852, 'epoch': 1.2472222222222222}
Step 4500: {'loss': 0.9059, 'grad_norm': 0.8457773327827454, 'learning_rate': 0.0005834259259259259, 'epoch': 1.25}
Step 4510: {'loss': 0.8689, 'grad_norm': 0.6378945112228394, 'learning_rate': 0.0005825, 'epoch': 1.2527777777777778}
Step 4520: {'loss': 0.8098, 'grad_norm': 0.9400389194488525, 'learning_rate': 0.000581574074074074, 'epoch': 1.2555555555555555}
Step 4530: {'loss': 0.7999, 'grad_norm': 0.7988568544387817, 'learning_rate': 0.0005806481481481482, 'epoch': 1.2583333333333333}
Step 4540: {'loss': 0.8748, 'grad_norm': 0.7805109620094299, 'learning_rate': 0.0005797222222222222, 'epoch': 1.261111111111111}
Step 4550: {'loss': 0.9458, 'grad_norm': 0.7285276651382446, 'learning_rate': 0.0005787962962962962, 'epoch': 1.2638888888888888}
Step 4560: {'loss': 0.8079, 'grad_norm': 0.6436029672622681, 'learning_rate': 0.0005778703703703704, 'epoch': 1.2666666666666666}
Step 4570: {'loss': 0.8358, 'grad_norm': 1.2342709302902222, 'learning_rate': 0.0005769444444444445, 'epoch': 1.2694444444444444}
Step 4580: {'loss': 0.7672, 'grad_norm': 0.5153860449790955, 'learning_rate': 0.0005760185185185185, 'epoch': 1.2722222222222221}
Step 4590: {'loss': 0.8364, 'grad_norm': 0.8228080868721008, 'learning_rate': 0.0005750925925925926, 'epoch': 1.275}
Step 4600: {'loss': 0.7874, 'grad_norm': 0.6707859635353088, 'learning_rate': 0.0005741666666666667, 'epoch': 1.2777777777777777}
Step 4610: {'loss': 0.9461, 'grad_norm': 0.8298931121826172, 'learning_rate': 0.0005732407407407407, 'epoch': 1.2805555555555554}
Step 4620: {'loss': 0.8077, 'grad_norm': 0.7610036134719849, 'learning_rate': 0.0005723148148148149, 'epoch': 1.2833333333333332}
Step 4630: {'loss': 0.8564, 'grad_norm': 0.8562368750572205, 'learning_rate': 0.0005713888888888889, 'epoch': 1.286111111111111}
Step 4640: {'loss': 0.997, 'grad_norm': 0.6867830753326416, 'learning_rate': 0.0005704629629629629, 'epoch': 1.2888888888888888}
Step 4650: {'loss': 0.8057, 'grad_norm': 0.7305058836936951, 'learning_rate': 0.0005695370370370371, 'epoch': 1.2916666666666667}
Step 4660: {'loss': 0.9031, 'grad_norm': 0.7596023678779602, 'learning_rate': 0.0005686111111111111, 'epoch': 1.2944444444444445}
Step 4670: {'loss': 0.8766, 'grad_norm': 0.5527750253677368, 'learning_rate': 0.0005676851851851852, 'epoch': 1.2972222222222223}
Step 4680: {'loss': 0.903, 'grad_norm': 0.6706482172012329, 'learning_rate': 0.0005667592592592593, 'epoch': 1.3}
Step 4690: {'loss': 0.9325, 'grad_norm': 0.8577831983566284, 'learning_rate': 0.0005658333333333333, 'epoch': 1.3027777777777778}
Step 4700: {'loss': 0.8701, 'grad_norm': 0.5566287636756897, 'learning_rate': 0.0005649074074074074, 'epoch': 1.3055555555555556}
Step 4710: {'loss': 0.9197, 'grad_norm': 0.8411867022514343, 'learning_rate': 0.0005639814814814815, 'epoch': 1.3083333333333333}
Step 4720: {'loss': 0.8462, 'grad_norm': 1.0057625770568848, 'learning_rate': 0.0005630555555555556, 'epoch': 1.3111111111111111}
Step 4730: {'loss': 0.9123, 'grad_norm': 0.9760130643844604, 'learning_rate': 0.0005621296296296297, 'epoch': 1.3138888888888889}
Step 4740: {'loss': 0.87, 'grad_norm': 0.8034324645996094, 'learning_rate': 0.0005612037037037036, 'epoch': 1.3166666666666667}
Step 4750: {'loss': 0.8979, 'grad_norm': 1.1333738565444946, 'learning_rate': 0.0005602777777777778, 'epoch': 1.3194444444444444}
Step 4760: {'loss': 0.8993, 'grad_norm': 0.7456865310668945, 'learning_rate': 0.0005593518518518519, 'epoch': 1.3222222222222222}
Step 4770: {'loss': 0.793, 'grad_norm': 0.8224411606788635, 'learning_rate': 0.0005584259259259259, 'epoch': 1.325}
Step 4780: {'loss': 0.8761, 'grad_norm': 0.8162899613380432, 'learning_rate': 0.0005575, 'epoch': 1.3277777777777777}
Step 4790: {'loss': 0.89, 'grad_norm': 0.7736180424690247, 'learning_rate': 0.0005565740740740741, 'epoch': 1.3305555555555555}
Step 4800: {'loss': 0.7983, 'grad_norm': 0.7495245337486267, 'learning_rate': 0.0005556481481481481, 'epoch': 1.3333333333333333}
Step 4810: {'loss': 0.9609, 'grad_norm': 0.9312698841094971, 'learning_rate': 0.0005547222222222223, 'epoch': 1.3361111111111112}
Step 4820: {'loss': 0.7928, 'grad_norm': 0.7453866600990295, 'learning_rate': 0.0005537962962962964, 'epoch': 1.338888888888889}
Step 4830: {'loss': 0.9259, 'grad_norm': 1.0484391450881958, 'learning_rate': 0.0005528703703703703, 'epoch': 1.3416666666666668}
Step 4840: {'loss': 0.924, 'grad_norm': 0.7385136485099792, 'learning_rate': 0.0005519444444444444, 'epoch': 1.3444444444444446}
Step 4850: {'loss': 0.8863, 'grad_norm': 0.9790635704994202, 'learning_rate': 0.0005510185185185185, 'epoch': 1.3472222222222223}
Step 4860: {'loss': 0.8191, 'grad_norm': 0.6917766332626343, 'learning_rate': 0.0005500925925925926, 'epoch': 1.35}
Step 4870: {'loss': 0.8761, 'grad_norm': 0.9165292978286743, 'learning_rate': 0.0005491666666666667, 'epoch': 1.3527777777777779}
Step 4880: {'loss': 0.8959, 'grad_norm': 0.9114980101585388, 'learning_rate': 0.0005482407407407407, 'epoch': 1.3555555555555556}
Step 4890: {'loss': 0.7945, 'grad_norm': 0.6603957414627075, 'learning_rate': 0.0005473148148148148, 'epoch': 1.3583333333333334}
Step 4900: {'loss': 0.8429, 'grad_norm': 0.7276068329811096, 'learning_rate': 0.0005463888888888889, 'epoch': 1.3611111111111112}
Step 4910: {'loss': 0.8981, 'grad_norm': 0.8723286986351013, 'learning_rate': 0.000545462962962963, 'epoch': 1.363888888888889}
Step 4920: {'loss': 0.861, 'grad_norm': 0.9250594973564148, 'learning_rate': 0.0005445370370370371, 'epoch': 1.3666666666666667}
Step 4930: {'loss': 0.7397, 'grad_norm': 0.4447287917137146, 'learning_rate': 0.0005436111111111111, 'epoch': 1.3694444444444445}
Step 4940: {'loss': 0.8103, 'grad_norm': 0.6360188126564026, 'learning_rate': 0.0005426851851851852, 'epoch': 1.3722222222222222}
Step 4950: {'loss': 0.8507, 'grad_norm': 1.0669898986816406, 'learning_rate': 0.0005417592592592593, 'epoch': 1.375}
Step 4960: {'loss': 0.9218, 'grad_norm': 0.8587113618850708, 'learning_rate': 0.0005408333333333334, 'epoch': 1.3777777777777778}
Step 4970: {'loss': 0.919, 'grad_norm': 0.9767545461654663, 'learning_rate': 0.0005399074074074073, 'epoch': 1.3805555555555555}
Step 4980: {'loss': 0.9442, 'grad_norm': 0.8318716287612915, 'learning_rate': 0.0005389814814814815, 'epoch': 1.3833333333333333}
Step 4990: {'loss': 0.8955, 'grad_norm': 0.9427343606948853, 'learning_rate': 0.0005380555555555555, 'epoch': 1.386111111111111}
Step 5000: {'loss': 0.944, 'grad_norm': 0.8230947256088257, 'learning_rate': 0.0005371296296296296, 'epoch': 1.3888888888888888}
Step 5010: {'loss': 0.8694, 'grad_norm': 0.7165055871009827, 'learning_rate': 0.0005362037037037038, 'epoch': 1.3916666666666666}
Step 5020: {'loss': 0.8636, 'grad_norm': 0.6419250965118408, 'learning_rate': 0.0005352777777777777, 'epoch': 1.3944444444444444}
Step 5030: {'loss': 0.8709, 'grad_norm': 0.6076949834823608, 'learning_rate': 0.0005343518518518518, 'epoch': 1.3972222222222221}
Step 5040: {'loss': 0.8962, 'grad_norm': 0.6599763631820679, 'learning_rate': 0.000533425925925926, 'epoch': 1.4}
Step 5050: {'loss': 0.8305, 'grad_norm': 0.5975726246833801, 'learning_rate': 0.0005325, 'epoch': 1.4027777777777777}
Step 5060: {'loss': 0.838, 'grad_norm': 0.6345475912094116, 'learning_rate': 0.0005315740740740741, 'epoch': 1.4055555555555554}
Step 5070: {'loss': 0.8154, 'grad_norm': 0.7219383120536804, 'learning_rate': 0.0005306481481481483, 'epoch': 1.4083333333333332}
Step 5080: {'loss': 0.8521, 'grad_norm': 0.6435115933418274, 'learning_rate': 0.0005297222222222222, 'epoch': 1.411111111111111}
Step 5090: {'loss': 0.8473, 'grad_norm': 0.7439046502113342, 'learning_rate': 0.0005287962962962963, 'epoch': 1.4138888888888888}
Step 5100: {'loss': 0.8369, 'grad_norm': 0.6674309968948364, 'learning_rate': 0.0005278703703703704, 'epoch': 1.4166666666666667}
Step 5110: {'loss': 0.7796, 'grad_norm': 0.5860967040061951, 'learning_rate': 0.0005269444444444445, 'epoch': 1.4194444444444445}
Step 5120: {'loss': 0.8759, 'grad_norm': 0.7648124098777771, 'learning_rate': 0.0005260185185185185, 'epoch': 1.4222222222222223}
Step 5130: {'loss': 0.8494, 'grad_norm': 0.6484713554382324, 'learning_rate': 0.0005250925925925925, 'epoch': 1.425}
Step 5140: {'loss': 0.9381, 'grad_norm': 0.9772655963897705, 'learning_rate': 0.0005241666666666667, 'epoch': 1.4277777777777778}
Step 5150: {'loss': 0.8285, 'grad_norm': 0.831162691116333, 'learning_rate': 0.0005232407407407408, 'epoch': 1.4305555555555556}
Step 5160: {'loss': 0.8539, 'grad_norm': 1.0415741205215454, 'learning_rate': 0.0005223148148148148, 'epoch': 1.4333333333333333}
Step 5170: {'loss': 0.8493, 'grad_norm': 0.8648681640625, 'learning_rate': 0.0005213888888888889, 'epoch': 1.4361111111111111}
Step 5180: {'loss': 0.8561, 'grad_norm': 0.923865795135498, 'learning_rate': 0.000520462962962963, 'epoch': 1.4388888888888889}
Step 5190: {'loss': 0.9194, 'grad_norm': 0.7297170758247375, 'learning_rate': 0.000519537037037037, 'epoch': 1.4416666666666667}
Step 5200: {'loss': 0.978, 'grad_norm': 0.6402774453163147, 'learning_rate': 0.0005186111111111112, 'epoch': 1.4444444444444444}
Step 5210: {'loss': 0.9408, 'grad_norm': 0.44406914710998535, 'learning_rate': 0.0005176851851851852, 'epoch': 1.4472222222222222}
Step 5220: {'loss': 0.9299, 'grad_norm': 0.6382425427436829, 'learning_rate': 0.0005167592592592592, 'epoch': 1.45}
Step 5230: {'loss': 0.8578, 'grad_norm': 0.6983175873756409, 'learning_rate': 0.0005158333333333334, 'epoch': 1.4527777777777777}
Step 5240: {'loss': 0.8843, 'grad_norm': 0.6792157888412476, 'learning_rate': 0.0005149074074074074, 'epoch': 1.4555555555555555}
Step 5250: {'loss': 0.8961, 'grad_norm': 1.0093313455581665, 'learning_rate': 0.0005139814814814815, 'epoch': 1.4583333333333333}
Step 5260: {'loss': 0.8835, 'grad_norm': 0.5285505652427673, 'learning_rate': 0.0005130555555555557, 'epoch': 1.4611111111111112}
Step 5270: {'loss': 0.8581, 'grad_norm': 0.7451896071434021, 'learning_rate': 0.0005121296296296296, 'epoch': 1.463888888888889}
Step 5280: {'loss': 0.8972, 'grad_norm': 0.6249801516532898, 'learning_rate': 0.0005112037037037037, 'epoch': 1.4666666666666668}
Step 5290: {'loss': 0.9136, 'grad_norm': 0.4504997432231903, 'learning_rate': 0.0005102777777777778, 'epoch': 1.4694444444444446}
Step 5300: {'loss': 0.8402, 'grad_norm': 0.5871302485466003, 'learning_rate': 0.0005093518518518519, 'epoch': 1.4722222222222223}
Step 5310: {'loss': 0.8188, 'grad_norm': 0.8686734437942505, 'learning_rate': 0.000508425925925926, 'epoch': 1.475}
Step 5320: {'loss': 0.8861, 'grad_norm': 0.744178056716919, 'learning_rate': 0.0005074999999999999, 'epoch': 1.4777777777777779}
Step 5330: {'loss': 0.884, 'grad_norm': 0.9287219047546387, 'learning_rate': 0.0005065740740740741, 'epoch': 1.4805555555555556}
Step 5340: {'loss': 0.828, 'grad_norm': 0.6487817168235779, 'learning_rate': 0.0005056481481481482, 'epoch': 1.4833333333333334}
Step 5350: {'loss': 0.8564, 'grad_norm': 0.784971296787262, 'learning_rate': 0.0005047222222222222, 'epoch': 1.4861111111111112}
Step 5360: {'loss': 0.7714, 'grad_norm': 0.7480757832527161, 'learning_rate': 0.0005037962962962963, 'epoch': 1.488888888888889}
Step 5370: {'loss': 0.862, 'grad_norm': 0.6008979678153992, 'learning_rate': 0.0005028703703703704, 'epoch': 1.4916666666666667}
Step 5380: {'loss': 0.8578, 'grad_norm': 0.7920265197753906, 'learning_rate': 0.0005019444444444444, 'epoch': 1.4944444444444445}
Step 5390: {'loss': 0.83, 'grad_norm': 0.6592157483100891, 'learning_rate': 0.0005010185185185186, 'epoch': 1.4972222222222222}
Step 5400: {'loss': 0.8289, 'grad_norm': 0.41848310828208923, 'learning_rate': 0.0005000925925925927, 'epoch': 1.5}
Step 5410: {'loss': 0.8658, 'grad_norm': 0.9230576157569885, 'learning_rate': 0.0004991666666666666, 'epoch': 1.5027777777777778}
Step 5420: {'loss': 0.8605, 'grad_norm': 0.5928706526756287, 'learning_rate': 0.0004982407407407407, 'epoch': 1.5055555555555555}
Step 5430: {'loss': 0.8392, 'grad_norm': 0.8747515082359314, 'learning_rate': 0.0004973148148148148, 'epoch': 1.5083333333333333}
Step 5440: {'loss': 0.9523, 'grad_norm': 0.6913290619850159, 'learning_rate': 0.0004963888888888889, 'epoch': 1.511111111111111}
Step 5450: {'loss': 0.7755, 'grad_norm': 0.8226590752601624, 'learning_rate': 0.000495462962962963, 'epoch': 1.5138888888888888}
Step 5460: {'loss': 0.8803, 'grad_norm': 0.796339213848114, 'learning_rate': 0.000494537037037037, 'epoch': 1.5166666666666666}
Step 5470: {'loss': 0.9781, 'grad_norm': 0.8762000799179077, 'learning_rate': 0.0004936111111111111, 'epoch': 1.5194444444444444}
Step 5480: {'loss': 0.8116, 'grad_norm': 0.645983874797821, 'learning_rate': 0.0004926851851851852, 'epoch': 1.5222222222222221}
Step 5490: {'loss': 0.8342, 'grad_norm': 0.6853465437889099, 'learning_rate': 0.0004917592592592593, 'epoch': 1.525}
Step 5500: {'loss': 0.8332, 'grad_norm': 0.7225511074066162, 'learning_rate': 0.0004908333333333334, 'epoch': 1.5277777777777777}
Step 5510: {'loss': 0.8494, 'grad_norm': 0.4298620820045471, 'learning_rate': 0.0004899074074074074, 'epoch': 1.5305555555555554}
Step 5520: {'loss': 0.8834, 'grad_norm': 0.7197058796882629, 'learning_rate': 0.0004889814814814815, 'epoch': 1.5333333333333332}
Step 5530: {'loss': 0.8434, 'grad_norm': 0.8705687522888184, 'learning_rate': 0.0004880555555555556, 'epoch': 1.536111111111111}
Step 5540: {'loss': 0.8786, 'grad_norm': 0.794704258441925, 'learning_rate': 0.0004871296296296296, 'epoch': 1.5388888888888888}
Step 5550: {'loss': 0.9029, 'grad_norm': 0.7437292337417603, 'learning_rate': 0.0004862037037037037, 'epoch': 1.5416666666666665}
Step 5560: {'loss': 0.9136, 'grad_norm': 0.5836085081100464, 'learning_rate': 0.0004852777777777778, 'epoch': 1.5444444444444443}
Step 5570: {'loss': 0.8279, 'grad_norm': 0.7631860375404358, 'learning_rate': 0.00048435185185185186, 'epoch': 1.5472222222222223}
Step 5580: {'loss': 0.8676, 'grad_norm': 0.9124664664268494, 'learning_rate': 0.00048342592592592594, 'epoch': 1.55}
Step 5590: {'loss': 0.9508, 'grad_norm': 0.8505997061729431, 'learning_rate': 0.0004825, 'epoch': 1.5527777777777778}
Step 5600: {'loss': 0.8637, 'grad_norm': 0.6636788249015808, 'learning_rate': 0.0004815740740740741, 'epoch': 1.5555555555555556}
Step 5610: {'loss': 0.8678, 'grad_norm': 0.4978676736354828, 'learning_rate': 0.0004806481481481482, 'epoch': 1.5583333333333333}
Step 5620: {'loss': 0.8702, 'grad_norm': 0.6593096852302551, 'learning_rate': 0.0004797222222222222, 'epoch': 1.5611111111111111}
Step 5630: {'loss': 0.861, 'grad_norm': 0.9945763349533081, 'learning_rate': 0.0004787962962962963, 'epoch': 1.5638888888888889}
Step 5640: {'loss': 0.885, 'grad_norm': 0.8799884915351868, 'learning_rate': 0.0004778703703703704, 'epoch': 1.5666666666666667}
Step 5650: {'loss': 0.765, 'grad_norm': 0.9554637670516968, 'learning_rate': 0.00047694444444444444, 'epoch': 1.5694444444444444}
Step 5660: {'loss': 0.7978, 'grad_norm': 0.8921399712562561, 'learning_rate': 0.0004760185185185185, 'epoch': 1.5722222222222222}
Step 5670: {'loss': 0.9618, 'grad_norm': 0.7124951481819153, 'learning_rate': 0.0004750925925925926, 'epoch': 1.575}
Step 5680: {'loss': 0.8407, 'grad_norm': 0.8713764548301697, 'learning_rate': 0.0004741666666666667, 'epoch': 1.5777777777777777}
Step 5690: {'loss': 0.8435, 'grad_norm': 0.5371073484420776, 'learning_rate': 0.00047324074074074076, 'epoch': 1.5805555555555557}
Step 5700: {'loss': 0.8805, 'grad_norm': 0.860736072063446, 'learning_rate': 0.0004723148148148148, 'epoch': 1.5833333333333335}
Step 5710: {'loss': 0.8988, 'grad_norm': 0.6448770761489868, 'learning_rate': 0.0004713888888888889, 'epoch': 1.5861111111111112}
Step 5720: {'loss': 0.9307, 'grad_norm': 0.7715898752212524, 'learning_rate': 0.000470462962962963, 'epoch': 1.588888888888889}
Step 5730: {'loss': 0.9341, 'grad_norm': 1.1180049180984497, 'learning_rate': 0.000469537037037037, 'epoch': 1.5916666666666668}
Step 5740: {'loss': 0.9431, 'grad_norm': 0.49244382977485657, 'learning_rate': 0.0004686111111111111, 'epoch': 1.5944444444444446}
Step 5750: {'loss': 0.7957, 'grad_norm': 0.682732343673706, 'learning_rate': 0.00046768518518518524, 'epoch': 1.5972222222222223}
Step 5760: {'loss': 0.8314, 'grad_norm': 0.6468321681022644, 'learning_rate': 0.00046675925925925926, 'epoch': 1.6}
Step 5770: {'loss': 0.8628, 'grad_norm': 0.8480203747749329, 'learning_rate': 0.00046583333333333334, 'epoch': 1.6027777777777779}
Step 5780: {'loss': 0.7801, 'grad_norm': 0.7483790516853333, 'learning_rate': 0.00046490740740740737, 'epoch': 1.6055555555555556}
Step 5790: {'loss': 0.8675, 'grad_norm': 0.6404224634170532, 'learning_rate': 0.0004639814814814815, 'epoch': 1.6083333333333334}
Step 5800: {'loss': 0.7633, 'grad_norm': 0.5454354882240295, 'learning_rate': 0.0004630555555555556, 'epoch': 1.6111111111111112}
Step 5810: {'loss': 0.9388, 'grad_norm': 0.6914198994636536, 'learning_rate': 0.0004621296296296296, 'epoch': 1.613888888888889}
Step 5820: {'loss': 0.9561, 'grad_norm': 0.8835839629173279, 'learning_rate': 0.00046120370370370374, 'epoch': 1.6166666666666667}
Step 5830: {'loss': 0.8437, 'grad_norm': 0.7745448350906372, 'learning_rate': 0.00046027777777777777, 'epoch': 1.6194444444444445}
Step 5840: {'loss': 0.818, 'grad_norm': 0.6517853736877441, 'learning_rate': 0.00045935185185185185, 'epoch': 1.6222222222222222}
Step 5850: {'loss': 0.7661, 'grad_norm': 0.6298342943191528, 'learning_rate': 0.00045842592592592593, 'epoch': 1.625}
Step 5860: {'loss': 0.886, 'grad_norm': 0.6593674421310425, 'learning_rate': 0.0004575, 'epoch': 1.6277777777777778}
Step 5870: {'loss': 0.8688, 'grad_norm': 0.8458936810493469, 'learning_rate': 0.0004565740740740741, 'epoch': 1.6305555555555555}
Step 5880: {'loss': 0.9684, 'grad_norm': 0.621940016746521, 'learning_rate': 0.00045564814814814817, 'epoch': 1.6333333333333333}
Step 5890: {'loss': 0.7958, 'grad_norm': 0.6348020434379578, 'learning_rate': 0.00045472222222222225, 'epoch': 1.636111111111111}
Step 5900: {'loss': 0.9471, 'grad_norm': 1.0037833452224731, 'learning_rate': 0.0004537962962962963, 'epoch': 1.6388888888888888}
Step 5910: {'loss': 0.9674, 'grad_norm': 0.7325927019119263, 'learning_rate': 0.00045287037037037035, 'epoch': 1.6416666666666666}
Step 5920: {'loss': 0.9313, 'grad_norm': 0.7068500518798828, 'learning_rate': 0.00045194444444444443, 'epoch': 1.6444444444444444}
Step 5930: {'loss': 0.8403, 'grad_norm': 0.9391385912895203, 'learning_rate': 0.00045101851851851857, 'epoch': 1.6472222222222221}
Step 5940: {'loss': 0.8242, 'grad_norm': 0.675969123840332, 'learning_rate': 0.0004500925925925926, 'epoch': 1.65}
Step 5950: {'loss': 0.896, 'grad_norm': 0.8046275973320007, 'learning_rate': 0.00044916666666666667, 'epoch': 1.6527777777777777}
Step 5960: {'loss': 0.8873, 'grad_norm': 0.5938048958778381, 'learning_rate': 0.00044824074074074075, 'epoch': 1.6555555555555554}
Step 5970: {'loss': 0.9116, 'grad_norm': 1.3726156949996948, 'learning_rate': 0.00044731481481481483, 'epoch': 1.6583333333333332}
Step 5980: {'loss': 0.8354, 'grad_norm': 0.6628569960594177, 'learning_rate': 0.0004463888888888889, 'epoch': 1.661111111111111}
Step 5990: {'loss': 0.899, 'grad_norm': 0.5815669298171997, 'learning_rate': 0.00044546296296296293, 'epoch': 1.6638888888888888}
Step 6000: {'loss': 0.8813, 'grad_norm': 1.0797618627548218, 'learning_rate': 0.00044453703703703707, 'epoch': 1.6666666666666665}
Step 6010: {'loss': 0.8623, 'grad_norm': 0.8343645930290222, 'learning_rate': 0.00044361111111111115, 'epoch': 1.6694444444444443}
Step 6020: {'loss': 0.819, 'grad_norm': 0.9190864562988281, 'learning_rate': 0.0004426851851851852, 'epoch': 1.6722222222222223}
Step 6030: {'loss': 0.8572, 'grad_norm': 0.8480396866798401, 'learning_rate': 0.00044175925925925925, 'epoch': 1.675}
Step 6040: {'loss': 0.9111, 'grad_norm': 0.9112904667854309, 'learning_rate': 0.0004408333333333334, 'epoch': 1.6777777777777778}
Step 6050: {'loss': 0.7955, 'grad_norm': 0.7795993685722351, 'learning_rate': 0.0004399074074074074, 'epoch': 1.6805555555555556}
Step 6060: {'loss': 0.834, 'grad_norm': 0.8942795991897583, 'learning_rate': 0.0004389814814814815, 'epoch': 1.6833333333333333}
Step 6070: {'loss': 0.8287, 'grad_norm': 0.906332790851593, 'learning_rate': 0.0004380555555555555, 'epoch': 1.6861111111111111}
Step 6080: {'loss': 0.9464, 'grad_norm': 0.790165364742279, 'learning_rate': 0.00043712962962962965, 'epoch': 1.6888888888888889}
Step 6090: {'loss': 0.8684, 'grad_norm': 0.8906919956207275, 'learning_rate': 0.00043620370370370373, 'epoch': 1.6916666666666667}
Step 6100: {'loss': 0.8998, 'grad_norm': 0.8550463318824768, 'learning_rate': 0.00043527777777777776, 'epoch': 1.6944444444444444}
Step 6110: {'loss': 0.8522, 'grad_norm': 0.731076717376709, 'learning_rate': 0.0004343518518518519, 'epoch': 1.6972222222222222}
Step 6120: {'loss': 0.912, 'grad_norm': 1.1317808628082275, 'learning_rate': 0.00043342592592592597, 'epoch': 1.7}
Step 6130: {'loss': 0.8186, 'grad_norm': 0.5150376558303833, 'learning_rate': 0.0004325, 'epoch': 1.7027777777777777}
Step 6140: {'loss': 0.9393, 'grad_norm': 0.87539142370224, 'learning_rate': 0.0004315740740740741, 'epoch': 1.7055555555555557}
Step 6150: {'loss': 0.9151, 'grad_norm': 0.7097416520118713, 'learning_rate': 0.00043064814814814816, 'epoch': 1.7083333333333335}
Step 6160: {'loss': 0.9408, 'grad_norm': 0.7722483277320862, 'learning_rate': 0.00042972222222222224, 'epoch': 1.7111111111111112}
Step 6170: {'loss': 0.886, 'grad_norm': 1.1341357231140137, 'learning_rate': 0.0004287962962962963, 'epoch': 1.713888888888889}
Step 6180: {'loss': 0.8937, 'grad_norm': 0.9813038110733032, 'learning_rate': 0.00042787037037037034, 'epoch': 1.7166666666666668}
Step 6190: {'loss': 0.8673, 'grad_norm': 1.1732078790664673, 'learning_rate': 0.0004269444444444445, 'epoch': 1.7194444444444446}
Step 6200: {'loss': 0.7922, 'grad_norm': 0.48329052329063416, 'learning_rate': 0.00042601851851851855, 'epoch': 1.7222222222222223}
Step 6210: {'loss': 0.87, 'grad_norm': 0.7616730332374573, 'learning_rate': 0.0004250925925925926, 'epoch': 1.725}
Step 6220: {'loss': 0.9022, 'grad_norm': 1.3701163530349731, 'learning_rate': 0.0004241666666666667, 'epoch': 1.7277777777777779}
Step 6230: {'loss': 0.9941, 'grad_norm': 0.6664976477622986, 'learning_rate': 0.00042324074074074074, 'epoch': 1.7305555555555556}
Step 6240: {'loss': 0.9635, 'grad_norm': 0.7451308369636536, 'learning_rate': 0.0004223148148148148, 'epoch': 1.7333333333333334}
Step 6250: {'loss': 0.8244, 'grad_norm': 1.806810975074768, 'learning_rate': 0.0004213888888888889, 'epoch': 1.7361111111111112}
Step 6260: {'loss': 0.8795, 'grad_norm': 0.9326852560043335, 'learning_rate': 0.000420462962962963, 'epoch': 1.738888888888889}
Step 6270: {'loss': 0.8901, 'grad_norm': 1.4891808032989502, 'learning_rate': 0.00041953703703703706, 'epoch': 1.7416666666666667}
Step 6280: {'loss': 0.8794, 'grad_norm': 0.9205251336097717, 'learning_rate': 0.0004186111111111111, 'epoch': 1.7444444444444445}
Step 6290: {'loss': 0.9051, 'grad_norm': 0.7703883051872253, 'learning_rate': 0.00041768518518518516, 'epoch': 1.7472222222222222}
Step 6300: {'loss': 0.8583, 'grad_norm': 0.5650807023048401, 'learning_rate': 0.0004167592592592593, 'epoch': 1.75}
Step 6310: {'loss': 0.8776, 'grad_norm': 0.9339886903762817, 'learning_rate': 0.0004158333333333333, 'epoch': 1.7527777777777778}
Step 6320: {'loss': 0.8888, 'grad_norm': 0.6020159125328064, 'learning_rate': 0.0004149074074074074, 'epoch': 1.7555555555555555}
Step 6330: {'loss': 0.9186, 'grad_norm': 1.0063297748565674, 'learning_rate': 0.00041398148148148154, 'epoch': 1.7583333333333333}
Step 6340: {'loss': 0.9243, 'grad_norm': 1.039055585861206, 'learning_rate': 0.00041305555555555556, 'epoch': 1.761111111111111}
Step 6350: {'loss': 0.851, 'grad_norm': 0.7805650234222412, 'learning_rate': 0.00041212962962962964, 'epoch': 1.7638888888888888}
Step 6360: {'loss': 0.8546, 'grad_norm': 0.8714732527732849, 'learning_rate': 0.00041120370370370367, 'epoch': 1.7666666666666666}
Step 6370: {'loss': 0.8617, 'grad_norm': 0.8028878569602966, 'learning_rate': 0.0004102777777777778, 'epoch': 1.7694444444444444}
Step 6380: {'loss': 0.8947, 'grad_norm': 1.3427116870880127, 'learning_rate': 0.0004093518518518519, 'epoch': 1.7722222222222221}
Step 6390: {'loss': 0.8888, 'grad_norm': 0.8130441308021545, 'learning_rate': 0.0004084259259259259, 'epoch': 1.775}
Step 6400: {'loss': 0.8819, 'grad_norm': 0.7224228382110596, 'learning_rate': 0.0004075, 'epoch': 1.7777777777777777}
Step 6410: {'loss': 0.9443, 'grad_norm': 0.8699733018875122, 'learning_rate': 0.0004065740740740741, 'epoch': 1.7805555555555554}
Step 6420: {'loss': 0.9422, 'grad_norm': 0.812188446521759, 'learning_rate': 0.00040564814814814814, 'epoch': 1.7833333333333332}
Step 6430: {'loss': 0.8653, 'grad_norm': 0.8002756834030151, 'learning_rate': 0.0004047222222222222, 'epoch': 1.786111111111111}
Step 6440: {'loss': 0.823, 'grad_norm': 0.796353280544281, 'learning_rate': 0.0004037962962962963, 'epoch': 1.7888888888888888}
Step 6450: {'loss': 0.8643, 'grad_norm': 0.5560906529426575, 'learning_rate': 0.0004028703703703704, 'epoch': 1.7916666666666665}
Step 6460: {'loss': 0.8561, 'grad_norm': 0.8322193026542664, 'learning_rate': 0.00040194444444444446, 'epoch': 1.7944444444444443}
Step 6470: {'loss': 0.8662, 'grad_norm': 0.7820379137992859, 'learning_rate': 0.0004010185185185185, 'epoch': 1.7972222222222223}
Step 6480: {'loss': 0.8899, 'grad_norm': 1.1619621515274048, 'learning_rate': 0.0004000925925925926, 'epoch': 1.8}
Step 6490: {'loss': 0.9754, 'grad_norm': 0.6373778581619263, 'learning_rate': 0.0003991666666666667, 'epoch': 1.8027777777777778}
Step 6500: {'loss': 0.8744, 'grad_norm': 0.7239754796028137, 'learning_rate': 0.00039824074074074073, 'epoch': 1.8055555555555556}
Step 6510: {'loss': 0.8173, 'grad_norm': 0.7804369926452637, 'learning_rate': 0.0003973148148148148, 'epoch': 1.8083333333333333}
Step 6520: {'loss': 0.9521, 'grad_norm': 0.4808172285556793, 'learning_rate': 0.0003963888888888889, 'epoch': 1.8111111111111111}
Step 6530: {'loss': 0.8082, 'grad_norm': 0.7672305107116699, 'learning_rate': 0.00039546296296296297, 'epoch': 1.8138888888888889}
Step 6540: {'loss': 0.8866, 'grad_norm': 0.7030565142631531, 'learning_rate': 0.00039453703703703705, 'epoch': 1.8166666666666667}
Step 6550: {'loss': 0.9295, 'grad_norm': 0.8829994201660156, 'learning_rate': 0.0003936111111111111, 'epoch': 1.8194444444444444}
Step 6560: {'loss': 0.8538, 'grad_norm': 0.9070037007331848, 'learning_rate': 0.0003926851851851852, 'epoch': 1.8222222222222222}
Step 6570: {'loss': 0.922, 'grad_norm': 0.8200277090072632, 'learning_rate': 0.0003917592592592593, 'epoch': 1.825}
Step 6580: {'loss': 0.8603, 'grad_norm': 0.6404203176498413, 'learning_rate': 0.0003908333333333333, 'epoch': 1.8277777777777777}
Step 6590: {'loss': 0.917, 'grad_norm': 0.8698555827140808, 'learning_rate': 0.00038990740740740744, 'epoch': 1.8305555555555557}
Step 6600: {'loss': 0.8735, 'grad_norm': 0.9016497135162354, 'learning_rate': 0.00038898148148148147, 'epoch': 1.8333333333333335}
Step 6610: {'loss': 0.9129, 'grad_norm': 0.9006120562553406, 'learning_rate': 0.00038805555555555555, 'epoch': 1.8361111111111112}
Step 6620: {'loss': 0.8607, 'grad_norm': 0.8167561292648315, 'learning_rate': 0.0003871296296296297, 'epoch': 1.838888888888889}
Step 6630: {'loss': 0.8968, 'grad_norm': 0.6920655369758606, 'learning_rate': 0.0003862037037037037, 'epoch': 1.8416666666666668}
Step 6640: {'loss': 0.8822, 'grad_norm': 0.8261793255805969, 'learning_rate': 0.0003852777777777778, 'epoch': 1.8444444444444446}
Step 6650: {'loss': 0.7648, 'grad_norm': 0.7076237201690674, 'learning_rate': 0.00038435185185185187, 'epoch': 1.8472222222222223}
Step 6660: {'loss': 0.8885, 'grad_norm': 1.23923659324646, 'learning_rate': 0.00038342592592592595, 'epoch': 1.85}
Step 6670: {'loss': 0.867, 'grad_norm': 0.7107272148132324, 'learning_rate': 0.00038250000000000003, 'epoch': 1.8527777777777779}
Step 6680: {'loss': 0.7924, 'grad_norm': 0.7867075204849243, 'learning_rate': 0.00038157407407407405, 'epoch': 1.8555555555555556}
Step 6690: {'loss': 0.8792, 'grad_norm': 0.6386269927024841, 'learning_rate': 0.00038064814814814813, 'epoch': 1.8583333333333334}
Step 6700: {'loss': 0.8385, 'grad_norm': 0.6440865993499756, 'learning_rate': 0.00037972222222222227, 'epoch': 1.8611111111111112}
Step 6710: {'loss': 0.8076, 'grad_norm': 0.6309072971343994, 'learning_rate': 0.0003787962962962963, 'epoch': 1.863888888888889}
Step 6720: {'loss': 0.7867, 'grad_norm': 0.5029035210609436, 'learning_rate': 0.00037787037037037037, 'epoch': 1.8666666666666667}
Step 6730: {'loss': 0.8898, 'grad_norm': 0.8664852380752563, 'learning_rate': 0.0003769444444444445, 'epoch': 1.8694444444444445}
Step 6740: {'loss': 0.8393, 'grad_norm': 0.7206098437309265, 'learning_rate': 0.00037601851851851853, 'epoch': 1.8722222222222222}
Step 6750: {'loss': 0.8675, 'grad_norm': 0.7408755421638489, 'learning_rate': 0.0003750925925925926, 'epoch': 1.875}
Step 6760: {'loss': 0.9064, 'grad_norm': 0.7606884241104126, 'learning_rate': 0.00037416666666666664, 'epoch': 1.8777777777777778}
Step 6770: {'loss': 0.8522, 'grad_norm': 0.5850382447242737, 'learning_rate': 0.00037324074074074077, 'epoch': 1.8805555555555555}
Step 6780: {'loss': 0.9059, 'grad_norm': 0.8848945498466492, 'learning_rate': 0.00037231481481481485, 'epoch': 1.8833333333333333}
Step 6790: {'loss': 0.9129, 'grad_norm': 0.8027242422103882, 'learning_rate': 0.0003713888888888889, 'epoch': 1.886111111111111}
Step 6800: {'loss': 0.8562, 'grad_norm': 1.1761106252670288, 'learning_rate': 0.00037046296296296295, 'epoch': 1.8888888888888888}
Step 6810: {'loss': 0.7919, 'grad_norm': 0.7296221852302551, 'learning_rate': 0.00036953703703703703, 'epoch': 1.8916666666666666}
Step 6820: {'loss': 0.8361, 'grad_norm': 0.6736497282981873, 'learning_rate': 0.0003686111111111111, 'epoch': 1.8944444444444444}
Step 6830: {'loss': 0.8492, 'grad_norm': 0.6898335814476013, 'learning_rate': 0.0003676851851851852, 'epoch': 1.8972222222222221}
Step 6840: {'loss': 0.8678, 'grad_norm': 0.5678697824478149, 'learning_rate': 0.0003667592592592593, 'epoch': 1.9}
Step 6850: {'loss': 0.8674, 'grad_norm': 0.5633697509765625, 'learning_rate': 0.00036583333333333335, 'epoch': 1.9027777777777777}
Step 6860: {'loss': 0.9194, 'grad_norm': 0.7547320127487183, 'learning_rate': 0.00036490740740740743, 'epoch': 1.9055555555555554}
Step 6870: {'loss': 0.8956, 'grad_norm': 0.9275326132774353, 'learning_rate': 0.00036398148148148146, 'epoch': 1.9083333333333332}
Step 6880: {'loss': 0.9622, 'grad_norm': 0.6246313452720642, 'learning_rate': 0.0003630555555555556, 'epoch': 1.911111111111111}
Step 6890: {'loss': 0.8743, 'grad_norm': 0.6779184937477112, 'learning_rate': 0.0003621296296296296, 'epoch': 1.9138888888888888}
Step 6900: {'loss': 0.9831, 'grad_norm': 1.0951836109161377, 'learning_rate': 0.0003612037037037037, 'epoch': 1.9166666666666665}
Step 6910: {'loss': 0.9174, 'grad_norm': 0.607761800289154, 'learning_rate': 0.0003602777777777778, 'epoch': 1.9194444444444443}
Step 6920: {'loss': 0.903, 'grad_norm': 0.7399582266807556, 'learning_rate': 0.00035935185185185186, 'epoch': 1.9222222222222223}
Step 6930: {'loss': 0.9162, 'grad_norm': 0.7206533551216125, 'learning_rate': 0.00035842592592592594, 'epoch': 1.925}
Step 6940: {'loss': 0.8208, 'grad_norm': 0.9131532907485962, 'learning_rate': 0.0003575, 'epoch': 1.9277777777777778}
Step 6950: {'loss': 0.8903, 'grad_norm': 1.3405712842941284, 'learning_rate': 0.0003565740740740741, 'epoch': 1.9305555555555556}
Step 6960: {'loss': 0.7642, 'grad_norm': 0.503040611743927, 'learning_rate': 0.0003556481481481482, 'epoch': 1.9333333333333333}
Step 6970: {'loss': 0.9061, 'grad_norm': 0.8082334399223328, 'learning_rate': 0.0003547222222222222, 'epoch': 1.9361111111111111}
Step 6980: {'loss': 0.8225, 'grad_norm': 1.0793092250823975, 'learning_rate': 0.0003537962962962963, 'epoch': 1.9388888888888889}
Step 6990: {'loss': 0.8594, 'grad_norm': 1.1016865968704224, 'learning_rate': 0.0003528703703703704, 'epoch': 1.9416666666666667}
Step 7000: {'loss': 0.896, 'grad_norm': 0.6814110279083252, 'learning_rate': 0.00035194444444444444, 'epoch': 1.9444444444444444}
Step 7010: {'loss': 0.8999, 'grad_norm': 1.1723664999008179, 'learning_rate': 0.0003510185185185185, 'epoch': 1.9472222222222222}
Step 7020: {'loss': 0.9577, 'grad_norm': 0.6718223690986633, 'learning_rate': 0.0003500925925925926, 'epoch': 1.95}
Step 7030: {'loss': 0.8306, 'grad_norm': 0.7672560811042786, 'learning_rate': 0.0003491666666666667, 'epoch': 1.9527777777777777}
Step 7040: {'loss': 0.907, 'grad_norm': 0.5926339626312256, 'learning_rate': 0.00034824074074074076, 'epoch': 1.9555555555555557}
Step 7050: {'loss': 0.9052, 'grad_norm': 0.7439677119255066, 'learning_rate': 0.0003473148148148148, 'epoch': 1.9583333333333335}
Step 7060: {'loss': 0.8257, 'grad_norm': 0.5302775502204895, 'learning_rate': 0.0003463888888888889, 'epoch': 1.9611111111111112}
Step 7070: {'loss': 0.8438, 'grad_norm': 0.8068097233772278, 'learning_rate': 0.000345462962962963, 'epoch': 1.963888888888889}
Step 7080: {'loss': 0.866, 'grad_norm': 0.7607426643371582, 'learning_rate': 0.000344537037037037, 'epoch': 1.9666666666666668}
Step 7090: {'loss': 0.8702, 'grad_norm': 1.2265524864196777, 'learning_rate': 0.0003436111111111111, 'epoch': 1.9694444444444446}
Step 7100: {'loss': 0.8115, 'grad_norm': 0.8192732334136963, 'learning_rate': 0.00034268518518518524, 'epoch': 1.9722222222222223}
Step 7110: {'loss': 0.8228, 'grad_norm': 0.6339056491851807, 'learning_rate': 0.00034175925925925926, 'epoch': 1.975}
Step 7120: {'loss': 0.884, 'grad_norm': 0.9570708870887756, 'learning_rate': 0.00034083333333333334, 'epoch': 1.9777777777777779}
Step 7130: {'loss': 0.8318, 'grad_norm': 0.5089449882507324, 'learning_rate': 0.00033990740740740737, 'epoch': 1.9805555555555556}
Step 7140: {'loss': 0.8798, 'grad_norm': 0.6902474761009216, 'learning_rate': 0.0003389814814814815, 'epoch': 1.9833333333333334}
Step 7150: {'loss': 0.8965, 'grad_norm': 1.0526031255722046, 'learning_rate': 0.0003380555555555556, 'epoch': 1.9861111111111112}
Step 7160: {'loss': 0.8947, 'grad_norm': 0.5454483032226562, 'learning_rate': 0.0003371296296296296, 'epoch': 1.988888888888889}
Step 7170: {'loss': 0.8408, 'grad_norm': 0.9431477785110474, 'learning_rate': 0.00033620370370370374, 'epoch': 1.9916666666666667}
Step 7180: {'loss': 0.9154, 'grad_norm': 0.5889949202537537, 'learning_rate': 0.00033527777777777777, 'epoch': 1.9944444444444445}
Step 7190: {'loss': 0.8646, 'grad_norm': 1.0661550760269165, 'learning_rate': 0.00033435185185185185, 'epoch': 1.9972222222222222}
Step 7200: {'loss': 0.9091, 'grad_norm': 0.6648573279380798, 'learning_rate': 0.0003334259259259259, 'epoch': 2.0}
Step 7210: {'loss': 0.8953, 'grad_norm': 1.097898244857788, 'learning_rate': 0.0003325, 'epoch': 2.0027777777777778}
Step 7220: {'loss': 0.8211, 'grad_norm': 0.771932065486908, 'learning_rate': 0.0003315740740740741, 'epoch': 2.0055555555555555}
Step 7230: {'loss': 0.8581, 'grad_norm': 0.9098225831985474, 'learning_rate': 0.00033064814814814816, 'epoch': 2.0083333333333333}
Step 7240: {'loss': 0.8233, 'grad_norm': 0.7103932499885559, 'learning_rate': 0.00032972222222222224, 'epoch': 2.011111111111111}
Step 7250: {'loss': 0.8266, 'grad_norm': 0.6794562339782715, 'learning_rate': 0.0003287962962962963, 'epoch': 2.013888888888889}
Step 7260: {'loss': 0.8008, 'grad_norm': 0.7858784198760986, 'learning_rate': 0.00032787037037037035, 'epoch': 2.0166666666666666}
Step 7270: {'loss': 0.7829, 'grad_norm': 0.6648767590522766, 'learning_rate': 0.00032694444444444443, 'epoch': 2.0194444444444444}
Step 7280: {'loss': 0.8693, 'grad_norm': 0.8015326857566833, 'learning_rate': 0.00032601851851851856, 'epoch': 2.022222222222222}
Step 7290: {'loss': 0.8592, 'grad_norm': 0.5169349312782288, 'learning_rate': 0.0003250925925925926, 'epoch': 2.025}
Step 7300: {'loss': 0.8061, 'grad_norm': 1.5967146158218384, 'learning_rate': 0.00032416666666666667, 'epoch': 2.0277777777777777}
Step 7310: {'loss': 0.9022, 'grad_norm': 0.7813467979431152, 'learning_rate': 0.00032324074074074075, 'epoch': 2.0305555555555554}
Step 7320: {'loss': 0.8827, 'grad_norm': 0.7569156885147095, 'learning_rate': 0.0003223148148148148, 'epoch': 2.033333333333333}
Step 7330: {'loss': 0.8877, 'grad_norm': 0.6737854480743408, 'learning_rate': 0.0003213888888888889, 'epoch': 2.036111111111111}
Step 7340: {'loss': 0.7732, 'grad_norm': 0.727853000164032, 'learning_rate': 0.00032046296296296293, 'epoch': 2.0388888888888888}
Step 7350: {'loss': 0.7524, 'grad_norm': 0.9176580309867859, 'learning_rate': 0.00031953703703703707, 'epoch': 2.0416666666666665}
Step 7360: {'loss': 0.9135, 'grad_norm': 0.8048110604286194, 'learning_rate': 0.00031861111111111115, 'epoch': 2.0444444444444443}
Step 7370: {'loss': 0.8894, 'grad_norm': 0.978055477142334, 'learning_rate': 0.00031768518518518517, 'epoch': 2.047222222222222}
Step 7380: {'loss': 0.8858, 'grad_norm': 0.7449820041656494, 'learning_rate': 0.00031675925925925925, 'epoch': 2.05}
Step 7390: {'loss': 0.8204, 'grad_norm': 0.6350396871566772, 'learning_rate': 0.0003158333333333334, 'epoch': 2.0527777777777776}
Step 7400: {'loss': 0.8456, 'grad_norm': 0.6689234972000122, 'learning_rate': 0.0003149074074074074, 'epoch': 2.0555555555555554}
Step 7410: {'loss': 0.8439, 'grad_norm': 0.7733891606330872, 'learning_rate': 0.0003139814814814815, 'epoch': 2.058333333333333}
Step 7420: {'loss': 0.9533, 'grad_norm': 0.8037779927253723, 'learning_rate': 0.0003130555555555555, 'epoch': 2.061111111111111}
Step 7430: {'loss': 0.9242, 'grad_norm': 1.9570846557617188, 'learning_rate': 0.00031212962962962965, 'epoch': 2.063888888888889}
Step 7440: {'loss': 0.9255, 'grad_norm': 0.6513264775276184, 'learning_rate': 0.00031120370370370373, 'epoch': 2.066666666666667}
Step 7450: {'loss': 0.8383, 'grad_norm': 0.9748866558074951, 'learning_rate': 0.00031027777777777775, 'epoch': 2.0694444444444446}
Step 7460: {'loss': 0.8947, 'grad_norm': 0.6232558488845825, 'learning_rate': 0.0003093518518518519, 'epoch': 2.0722222222222224}
Step 7470: {'loss': 0.8273, 'grad_norm': 0.7970281839370728, 'learning_rate': 0.00030842592592592597, 'epoch': 2.075}
Step 7480: {'loss': 0.8587, 'grad_norm': 0.9756371378898621, 'learning_rate': 0.0003075, 'epoch': 2.077777777777778}
Step 7490: {'loss': 0.8854, 'grad_norm': 0.8795368671417236, 'learning_rate': 0.0003065740740740741, 'epoch': 2.0805555555555557}
Step 7500: {'loss': 0.8038, 'grad_norm': 0.688251793384552, 'learning_rate': 0.00030564814814814815, 'epoch': 2.0833333333333335}
Step 7510: {'loss': 0.8933, 'grad_norm': 0.5011821985244751, 'learning_rate': 0.00030472222222222223, 'epoch': 2.0861111111111112}
Step 7520: {'loss': 0.826, 'grad_norm': 0.8879317045211792, 'learning_rate': 0.0003037962962962963, 'epoch': 2.088888888888889}
Step 7530: {'loss': 0.9123, 'grad_norm': 0.6336624026298523, 'learning_rate': 0.00030287037037037034, 'epoch': 2.091666666666667}
Step 7540: {'loss': 0.8191, 'grad_norm': 0.8412784337997437, 'learning_rate': 0.00030194444444444447, 'epoch': 2.0944444444444446}
Step 7550: {'loss': 0.9241, 'grad_norm': 1.184234619140625, 'learning_rate': 0.00030101851851851855, 'epoch': 2.0972222222222223}
Step 7560: {'loss': 0.8464, 'grad_norm': 0.7765938639640808, 'learning_rate': 0.0003000925925925926, 'epoch': 2.1}
Step 7570: {'loss': 0.8485, 'grad_norm': 0.5926777124404907, 'learning_rate': 0.0002991666666666667, 'epoch': 2.102777777777778}
Step 7580: {'loss': 0.804, 'grad_norm': 0.6253593564033508, 'learning_rate': 0.00029824074074074074, 'epoch': 2.1055555555555556}
Step 7590: {'loss': 0.8596, 'grad_norm': 0.7979711890220642, 'learning_rate': 0.0002973148148148148, 'epoch': 2.1083333333333334}
Step 7600: {'loss': 0.8656, 'grad_norm': 1.4385734796524048, 'learning_rate': 0.0002963888888888889, 'epoch': 2.111111111111111}
Step 7610: {'loss': 0.9228, 'grad_norm': 1.2896745204925537, 'learning_rate': 0.000295462962962963, 'epoch': 2.113888888888889}
Step 7620: {'loss': 0.9407, 'grad_norm': 0.6125546097755432, 'learning_rate': 0.00029453703703703705, 'epoch': 2.1166666666666667}
Step 7630: {'loss': 0.8486, 'grad_norm': 0.8900067806243896, 'learning_rate': 0.0002936111111111111, 'epoch': 2.1194444444444445}
Step 7640: {'loss': 0.8046, 'grad_norm': 0.8116474747657776, 'learning_rate': 0.00029268518518518516, 'epoch': 2.1222222222222222}
Step 7650: {'loss': 0.8815, 'grad_norm': 0.5766079425811768, 'learning_rate': 0.0002917592592592593, 'epoch': 2.125}
Step 7660: {'loss': 0.9647, 'grad_norm': 0.8770203590393066, 'learning_rate': 0.0002908333333333333, 'epoch': 2.1277777777777778}
Step 7670: {'loss': 0.8969, 'grad_norm': 0.8347851037979126, 'learning_rate': 0.0002899074074074074, 'epoch': 2.1305555555555555}
Step 7680: {'loss': 0.8268, 'grad_norm': 0.7655237317085266, 'learning_rate': 0.00028898148148148153, 'epoch': 2.1333333333333333}
Step 7690: {'loss': 0.8424, 'grad_norm': 0.8013955950737, 'learning_rate': 0.00028805555555555556, 'epoch': 2.136111111111111}
Step 7700: {'loss': 0.8779, 'grad_norm': 0.8852135539054871, 'learning_rate': 0.00028712962962962964, 'epoch': 2.138888888888889}
Step 7710: {'loss': 0.8747, 'grad_norm': 0.9348828196525574, 'learning_rate': 0.00028620370370370366, 'epoch': 2.1416666666666666}
Step 7720: {'loss': 0.8814, 'grad_norm': 0.8977593779563904, 'learning_rate': 0.0002852777777777778, 'epoch': 2.1444444444444444}
Step 7730: {'loss': 0.8648, 'grad_norm': 0.3844805657863617, 'learning_rate': 0.0002843518518518519, 'epoch': 2.147222222222222}
Step 7740: {'loss': 0.8487, 'grad_norm': 0.7770932912826538, 'learning_rate': 0.0002834259259259259, 'epoch': 2.15}
Step 7750: {'loss': 0.894, 'grad_norm': 0.8143208026885986, 'learning_rate': 0.0002825, 'epoch': 2.1527777777777777}
Step 7760: {'loss': 0.828, 'grad_norm': 0.6156036853790283, 'learning_rate': 0.0002815740740740741, 'epoch': 2.1555555555555554}
Step 7770: {'loss': 0.8652, 'grad_norm': 0.6513522267341614, 'learning_rate': 0.00028064814814814814, 'epoch': 2.158333333333333}
Step 7780: {'loss': 0.78, 'grad_norm': 0.7064192295074463, 'learning_rate': 0.0002797222222222222, 'epoch': 2.161111111111111}
Step 7790: {'loss': 0.9361, 'grad_norm': 1.0853151082992554, 'learning_rate': 0.0002787962962962963, 'epoch': 2.1638888888888888}
Step 7800: {'loss': 0.8538, 'grad_norm': 0.7693229913711548, 'learning_rate': 0.0002778703703703704, 'epoch': 2.1666666666666665}
Step 7810: {'loss': 0.8644, 'grad_norm': 0.6117501854896545, 'learning_rate': 0.00027694444444444446, 'epoch': 2.1694444444444443}
Step 7820: {'loss': 0.8826, 'grad_norm': 0.8211246728897095, 'learning_rate': 0.0002760185185185185, 'epoch': 2.172222222222222}
Step 7830: {'loss': 0.8766, 'grad_norm': 0.9028733372688293, 'learning_rate': 0.0002750925925925926, 'epoch': 2.175}
Step 7840: {'loss': 0.8286, 'grad_norm': 0.8313004374504089, 'learning_rate': 0.0002741666666666667, 'epoch': 2.1777777777777776}
Step 7850: {'loss': 0.8585, 'grad_norm': 1.4493927955627441, 'learning_rate': 0.0002732407407407407, 'epoch': 2.1805555555555554}
Step 7860: {'loss': 0.8482, 'grad_norm': 0.9320391416549683, 'learning_rate': 0.0002723148148148148, 'epoch': 2.183333333333333}
Step 7870: {'loss': 0.9502, 'grad_norm': 0.7842094302177429, 'learning_rate': 0.0002713888888888889, 'epoch': 2.186111111111111}
Step 7880: {'loss': 0.7994, 'grad_norm': 0.8756236433982849, 'learning_rate': 0.00027046296296296296, 'epoch': 2.188888888888889}
Step 7890: {'loss': 0.8278, 'grad_norm': 0.7021762132644653, 'learning_rate': 0.00026953703703703704, 'epoch': 2.191666666666667}
Step 7900: {'loss': 0.8897, 'grad_norm': 0.7254907488822937, 'learning_rate': 0.0002686111111111111, 'epoch': 2.1944444444444446}
Step 7910: {'loss': 0.8244, 'grad_norm': 0.6589266657829285, 'learning_rate': 0.0002676851851851852, 'epoch': 2.1972222222222224}
Step 7920: {'loss': 0.892, 'grad_norm': 0.6266266703605652, 'learning_rate': 0.0002667592592592593, 'epoch': 2.2}
Step 7930: {'loss': 0.7964, 'grad_norm': 0.7978881597518921, 'learning_rate': 0.0002658333333333333, 'epoch': 2.202777777777778}
Step 7940: {'loss': 0.8476, 'grad_norm': 0.8593440055847168, 'learning_rate': 0.00026490740740740744, 'epoch': 2.2055555555555557}
Step 7950: {'loss': 0.8786, 'grad_norm': 0.7258307933807373, 'learning_rate': 0.00026398148148148147, 'epoch': 2.2083333333333335}
Step 7960: {'loss': 0.8959, 'grad_norm': 0.8343604207038879, 'learning_rate': 0.00026305555555555555, 'epoch': 2.2111111111111112}
Step 7970: {'loss': 0.9135, 'grad_norm': 0.8772467374801636, 'learning_rate': 0.0002621296296296297, 'epoch': 2.213888888888889}
Step 7980: {'loss': 1.0592, 'grad_norm': 1.0069071054458618, 'learning_rate': 0.0002612037037037037, 'epoch': 2.216666666666667}
Step 7990: {'loss': 0.8777, 'grad_norm': 1.0379027128219604, 'learning_rate': 0.0002602777777777778, 'epoch': 2.2194444444444446}
Step 8000: {'loss': 0.8027, 'grad_norm': 0.8489915728569031, 'learning_rate': 0.00025935185185185187, 'epoch': 2.2222222222222223}
Step 8010: {'loss': 0.9874, 'grad_norm': 1.1226190328598022, 'learning_rate': 0.00025842592592592595, 'epoch': 2.225}
Step 8020: {'loss': 0.9042, 'grad_norm': 1.3985663652420044, 'learning_rate': 0.0002575, 'epoch': 2.227777777777778}
Step 8030: {'loss': 0.9345, 'grad_norm': 0.9896146655082703, 'learning_rate': 0.00025657407407407405, 'epoch': 2.2305555555555556}
Step 8040: {'loss': 0.9014, 'grad_norm': 1.2413933277130127, 'learning_rate': 0.00025564814814814813, 'epoch': 2.2333333333333334}
Step 8050: {'loss': 0.8243, 'grad_norm': 0.6792013049125671, 'learning_rate': 0.00025472222222222226, 'epoch': 2.236111111111111}
Step 8060: {'loss': 0.8742, 'grad_norm': 0.872444212436676, 'learning_rate': 0.0002537962962962963, 'epoch': 2.238888888888889}
Step 8070: {'loss': 0.8736, 'grad_norm': 1.0930079221725464, 'learning_rate': 0.00025287037037037037, 'epoch': 2.2416666666666667}
Step 8080: {'loss': 0.8278, 'grad_norm': 1.0184929370880127, 'learning_rate': 0.0002519444444444445, 'epoch': 2.2444444444444445}
Step 8090: {'loss': 0.8861, 'grad_norm': 1.2574787139892578, 'learning_rate': 0.00025101851851851853, 'epoch': 2.2472222222222222}
Step 8100: {'loss': 0.874, 'grad_norm': 0.9188020825386047, 'learning_rate': 0.0002500925925925926, 'epoch': 2.25}
Step 8110: {'loss': 0.8176, 'grad_norm': 0.9551732540130615, 'learning_rate': 0.0002491666666666667, 'epoch': 2.2527777777777778}
Step 8120: {'loss': 0.8981, 'grad_norm': 0.6257482171058655, 'learning_rate': 0.0002482407407407407, 'epoch': 2.2555555555555555}
Step 8130: {'loss': 0.9446, 'grad_norm': 0.9037706851959229, 'learning_rate': 0.00024731481481481485, 'epoch': 2.2583333333333333}
Step 8140: {'loss': 0.8478, 'grad_norm': 0.6504560708999634, 'learning_rate': 0.00024638888888888887, 'epoch': 2.261111111111111}
Step 8150: {'loss': 0.878, 'grad_norm': 0.9055333137512207, 'learning_rate': 0.00024546296296296295, 'epoch': 2.263888888888889}
Step 8160: {'loss': 0.9346, 'grad_norm': 1.332686424255371, 'learning_rate': 0.00024453703703703703, 'epoch': 2.2666666666666666}
Step 8170: {'loss': 0.8989, 'grad_norm': 0.992301881313324, 'learning_rate': 0.0002436111111111111, 'epoch': 2.2694444444444444}
Step 8180: {'loss': 0.8669, 'grad_norm': 0.8414458632469177, 'learning_rate': 0.0002426851851851852, 'epoch': 2.272222222222222}
Step 8190: {'loss': 0.8683, 'grad_norm': 0.9711388349533081, 'learning_rate': 0.00024175925925925927, 'epoch': 2.275}
Step 8200: {'loss': 0.953, 'grad_norm': 0.8087661266326904, 'learning_rate': 0.00024083333333333335, 'epoch': 2.2777777777777777}
Step 8210: {'loss': 0.8024, 'grad_norm': 0.839221179485321, 'learning_rate': 0.0002399074074074074, 'epoch': 2.2805555555555554}
Step 8220: {'loss': 0.8085, 'grad_norm': 0.6834788918495178, 'learning_rate': 0.00023898148148148148, 'epoch': 2.283333333333333}
Step 8230: {'loss': 0.8364, 'grad_norm': 0.9798825979232788, 'learning_rate': 0.00023805555555555556, 'epoch': 2.286111111111111}
Step 8240: {'loss': 0.905, 'grad_norm': 0.9111716747283936, 'learning_rate': 0.00023712962962962964, 'epoch': 2.2888888888888888}
Step 8250: {'loss': 0.9303, 'grad_norm': 1.156026005744934, 'learning_rate': 0.0002362037037037037, 'epoch': 2.2916666666666665}
Step 8260: {'loss': 0.8801, 'grad_norm': 0.798388659954071, 'learning_rate': 0.00023527777777777777, 'epoch': 2.2944444444444443}
Step 8270: {'loss': 0.8595, 'grad_norm': 0.6736897826194763, 'learning_rate': 0.00023435185185185185, 'epoch': 2.297222222222222}
Step 8280: {'loss': 0.9153, 'grad_norm': 0.9140744805335999, 'learning_rate': 0.00023342592592592593, 'epoch': 2.3}
Step 8290: {'loss': 0.8527, 'grad_norm': 1.3615589141845703, 'learning_rate': 0.0002325, 'epoch': 2.3027777777777776}
Step 8300: {'loss': 0.838, 'grad_norm': 0.7156800627708435, 'learning_rate': 0.00023157407407407407, 'epoch': 2.3055555555555554}
Step 8310: {'loss': 0.9174, 'grad_norm': 1.6629492044448853, 'learning_rate': 0.00023064814814814817, 'epoch': 2.3083333333333336}
Step 8320: {'loss': 0.8902, 'grad_norm': 0.6878166198730469, 'learning_rate': 0.00022972222222222223, 'epoch': 2.311111111111111}
Step 8330: {'loss': 0.8982, 'grad_norm': 0.9387688040733337, 'learning_rate': 0.0002287962962962963, 'epoch': 2.313888888888889}
Step 8340: {'loss': 0.828, 'grad_norm': 0.8204130530357361, 'learning_rate': 0.00022787037037037036, 'epoch': 2.3166666666666664}
Step 8350: {'loss': 0.9346, 'grad_norm': 0.6728325486183167, 'learning_rate': 0.00022694444444444446, 'epoch': 2.3194444444444446}
Step 8360: {'loss': 0.9414, 'grad_norm': 0.758491575717926, 'learning_rate': 0.00022601851851851852, 'epoch': 2.3222222222222224}
Step 8370: {'loss': 0.8774, 'grad_norm': 0.4709393382072449, 'learning_rate': 0.0002250925925925926, 'epoch': 2.325}
Step 8380: {'loss': 0.867, 'grad_norm': 0.677037239074707, 'learning_rate': 0.00022416666666666665, 'epoch': 2.327777777777778}
Step 8390: {'loss': 0.8977, 'grad_norm': 1.036445140838623, 'learning_rate': 0.00022324074074074076, 'epoch': 2.3305555555555557}
Step 8400: {'loss': 0.9175, 'grad_norm': 1.0649726390838623, 'learning_rate': 0.00022231481481481484, 'epoch': 2.3333333333333335}
Step 8410: {'loss': 0.9052, 'grad_norm': 0.9448211789131165, 'learning_rate': 0.0002213888888888889, 'epoch': 2.3361111111111112}
Step 8420: {'loss': 0.8462, 'grad_norm': 0.8325251340866089, 'learning_rate': 0.00022046296296296297, 'epoch': 2.338888888888889}
Step 8430: {'loss': 0.9271, 'grad_norm': 0.6861889362335205, 'learning_rate': 0.00021953703703703705, 'epoch': 2.341666666666667}
Step 8440: {'loss': 0.9105, 'grad_norm': 0.7194905281066895, 'learning_rate': 0.00021861111111111113, 'epoch': 2.3444444444444446}
Step 8450: {'loss': 0.7952, 'grad_norm': 0.8138361573219299, 'learning_rate': 0.00021768518518518518, 'epoch': 2.3472222222222223}
Step 8460: {'loss': 0.8664, 'grad_norm': 0.5972643494606018, 'learning_rate': 0.00021675925925925926, 'epoch': 2.35}
Step 8470: {'loss': 0.8486, 'grad_norm': 0.6590229272842407, 'learning_rate': 0.00021583333333333334, 'epoch': 2.352777777777778}
Step 8480: {'loss': 0.8765, 'grad_norm': 0.8461708426475525, 'learning_rate': 0.00021490740740740742, 'epoch': 2.3555555555555556}
Step 8490: {'loss': 0.8164, 'grad_norm': 0.62876957654953, 'learning_rate': 0.0002139814814814815, 'epoch': 2.3583333333333334}
Step 8500: {'loss': 0.8529, 'grad_norm': 0.9220019578933716, 'learning_rate': 0.00021305555555555555, 'epoch': 2.361111111111111}
Step 8510: {'loss': 0.8747, 'grad_norm': 0.7403767108917236, 'learning_rate': 0.00021212962962962966, 'epoch': 2.363888888888889}
Step 8520: {'loss': 0.8799, 'grad_norm': 0.8518732786178589, 'learning_rate': 0.0002112037037037037, 'epoch': 2.3666666666666667}
Step 8530: {'loss': 0.9037, 'grad_norm': 1.0668658018112183, 'learning_rate': 0.0002102777777777778, 'epoch': 2.3694444444444445}
Step 8540: {'loss': 0.9272, 'grad_norm': 1.1483583450317383, 'learning_rate': 0.00020935185185185184, 'epoch': 2.3722222222222222}
Step 8550: {'loss': 0.8403, 'grad_norm': 0.7038275599479675, 'learning_rate': 0.00020842592592592592, 'epoch': 2.375}
Step 8560: {'loss': 0.8522, 'grad_norm': 0.5948812365531921, 'learning_rate': 0.0002075, 'epoch': 2.3777777777777778}
Step 8570: {'loss': 0.8003, 'grad_norm': 1.2138636112213135, 'learning_rate': 0.00020657407407407408, 'epoch': 2.3805555555555555}
Step 8580: {'loss': 0.8572, 'grad_norm': 0.7641993165016174, 'learning_rate': 0.00020564814814814813, 'epoch': 2.3833333333333333}
Step 8590: {'loss': 0.9239, 'grad_norm': 0.999241828918457, 'learning_rate': 0.00020472222222222221, 'epoch': 2.386111111111111}
Step 8600: {'loss': 0.8635, 'grad_norm': 0.7981249690055847, 'learning_rate': 0.00020379629629629632, 'epoch': 2.388888888888889}
Step 8610: {'loss': 0.8363, 'grad_norm': 0.8462543487548828, 'learning_rate': 0.00020287037037037037, 'epoch': 2.3916666666666666}
Step 8620: {'loss': 0.8586, 'grad_norm': 1.0414812564849854, 'learning_rate': 0.00020194444444444445, 'epoch': 2.3944444444444444}
Step 8630: {'loss': 0.8754, 'grad_norm': 0.4883544147014618, 'learning_rate': 0.0002010185185185185, 'epoch': 2.397222222222222}
Step 8640: {'loss': 0.9592, 'grad_norm': 0.6825284361839294, 'learning_rate': 0.0002000925925925926, 'epoch': 2.4}
Step 8650: {'loss': 0.973, 'grad_norm': 1.561182975769043, 'learning_rate': 0.00019916666666666667, 'epoch': 2.4027777777777777}
Step 8660: {'loss': 0.9128, 'grad_norm': 0.9098424315452576, 'learning_rate': 0.00019824074074074074, 'epoch': 2.4055555555555554}
Step 8670: {'loss': 0.8523, 'grad_norm': 0.8880612254142761, 'learning_rate': 0.0001973148148148148, 'epoch': 2.408333333333333}
Step 8680: {'loss': 0.8971, 'grad_norm': 0.8857514262199402, 'learning_rate': 0.0001963888888888889, 'epoch': 2.411111111111111}
Step 8690: {'loss': 0.9043, 'grad_norm': 1.1812314987182617, 'learning_rate': 0.00019546296296296296, 'epoch': 2.4138888888888888}
Step 8700: {'loss': 0.89, 'grad_norm': 1.00623619556427, 'learning_rate': 0.00019453703703703704, 'epoch': 2.4166666666666665}
Step 8710: {'loss': 0.8538, 'grad_norm': 0.7887763977050781, 'learning_rate': 0.00019361111111111112, 'epoch': 2.4194444444444443}
Step 8720: {'loss': 0.8026, 'grad_norm': 0.7581122517585754, 'learning_rate': 0.0001926851851851852, 'epoch': 2.422222222222222}
Step 8730: {'loss': 0.8543, 'grad_norm': 0.7931956052780151, 'learning_rate': 0.00019175925925925928, 'epoch': 2.425}
Step 8740: {'loss': 0.9053, 'grad_norm': 2.2361624240875244, 'learning_rate': 0.00019083333333333333, 'epoch': 2.4277777777777776}
Step 8750: {'loss': 0.9325, 'grad_norm': 0.8365707993507385, 'learning_rate': 0.0001899074074074074, 'epoch': 2.4305555555555554}
Step 8760: {'loss': 0.8619, 'grad_norm': 0.7061547636985779, 'learning_rate': 0.0001889814814814815, 'epoch': 2.4333333333333336}
Step 8770: {'loss': 0.9214, 'grad_norm': 1.0262627601623535, 'learning_rate': 0.00018805555555555557, 'epoch': 2.436111111111111}
Step 8780: {'loss': 0.8705, 'grad_norm': 1.0499595403671265, 'learning_rate': 0.00018712962962962962, 'epoch': 2.438888888888889}
Step 8790: {'loss': 0.9903, 'grad_norm': 1.2299646139144897, 'learning_rate': 0.0001862037037037037, 'epoch': 2.4416666666666664}
Step 8800: {'loss': 0.9515, 'grad_norm': 1.0095785856246948, 'learning_rate': 0.0001852777777777778, 'epoch': 2.4444444444444446}
Step 8810: {'loss': 0.8449, 'grad_norm': 1.0187870264053345, 'learning_rate': 0.00018435185185185186, 'epoch': 2.4472222222222224}
Step 8820: {'loss': 0.8784, 'grad_norm': 3.4381096363067627, 'learning_rate': 0.00018342592592592594, 'epoch': 2.45}
Step 8830: {'loss': 0.8653, 'grad_norm': 0.6302571296691895, 'learning_rate': 0.0001825, 'epoch': 2.452777777777778}
Step 8840: {'loss': 1.0054, 'grad_norm': 1.4543174505233765, 'learning_rate': 0.0001815740740740741, 'epoch': 2.4555555555555557}
Step 8850: {'loss': 0.9142, 'grad_norm': 0.8923544883728027, 'learning_rate': 0.00018064814814814815, 'epoch': 2.4583333333333335}
Step 8860: {'loss': 0.8792, 'grad_norm': 0.7488365173339844, 'learning_rate': 0.00017972222222222223, 'epoch': 2.4611111111111112}
Step 8870: {'loss': 0.8406, 'grad_norm': 0.6995002031326294, 'learning_rate': 0.00017879629629629628, 'epoch': 2.463888888888889}
Step 8880: {'loss': 0.9399, 'grad_norm': 1.1979646682739258, 'learning_rate': 0.0001778703703703704, 'epoch': 2.466666666666667}
Step 8890: {'loss': 0.8976, 'grad_norm': 0.9952096343040466, 'learning_rate': 0.00017694444444444444, 'epoch': 2.4694444444444446}
Step 8900: {'loss': 1.0035, 'grad_norm': 1.0767215490341187, 'learning_rate': 0.00017601851851851852, 'epoch': 2.4722222222222223}
Step 8910: {'loss': 0.7993, 'grad_norm': 0.6001728177070618, 'learning_rate': 0.0001750925925925926, 'epoch': 2.475}
Step 8920: {'loss': 1.0444, 'grad_norm': 1.0734012126922607, 'learning_rate': 0.00017416666666666668, 'epoch': 2.477777777777778}
Step 8930: {'loss': 0.7602, 'grad_norm': 0.6213898658752441, 'learning_rate': 0.00017324074074074076, 'epoch': 2.4805555555555556}
Step 8940: {'loss': 0.8841, 'grad_norm': 1.0061784982681274, 'learning_rate': 0.0001723148148148148, 'epoch': 2.4833333333333334}
Step 8950: {'loss': 0.8972, 'grad_norm': 1.4117244482040405, 'learning_rate': 0.0001713888888888889, 'epoch': 2.486111111111111}
Step 8960: {'loss': 0.8986, 'grad_norm': 1.3528538942337036, 'learning_rate': 0.00017046296296296295, 'epoch': 2.488888888888889}
Step 8970: {'loss': 0.8431, 'grad_norm': 0.9713848829269409, 'learning_rate': 0.00016953703703703705, 'epoch': 2.4916666666666667}
Step 8980: {'loss': 0.9394, 'grad_norm': 0.7622260451316833, 'learning_rate': 0.0001686111111111111, 'epoch': 2.4944444444444445}
Step 8990: {'loss': 0.9312, 'grad_norm': 0.731487512588501, 'learning_rate': 0.00016768518518518518, 'epoch': 2.4972222222222222}
Step 9000: {'loss': 0.8756, 'grad_norm': 0.7294965386390686, 'learning_rate': 0.00016675925925925924, 'epoch': 2.5}
Step 9010: {'loss': 0.8993, 'grad_norm': 0.7818551063537598, 'learning_rate': 0.00016583333333333334, 'epoch': 2.5027777777777778}
Step 9020: {'loss': 0.938, 'grad_norm': 1.0692298412322998, 'learning_rate': 0.00016490740740740742, 'epoch': 2.5055555555555555}
Step 9030: {'loss': 0.9225, 'grad_norm': 1.1280070543289185, 'learning_rate': 0.00016398148148148148, 'epoch': 2.5083333333333333}
Step 9040: {'loss': 0.8888, 'grad_norm': 2.9122612476348877, 'learning_rate': 0.00016305555555555556, 'epoch': 2.511111111111111}
Step 9050: {'loss': 0.9531, 'grad_norm': 1.1444876194000244, 'learning_rate': 0.00016212962962962964, 'epoch': 2.513888888888889}
Step 9060: {'loss': 0.9205, 'grad_norm': 0.6550197005271912, 'learning_rate': 0.00016120370370370371, 'epoch': 2.5166666666666666}
Step 9070: {'loss': 0.8414, 'grad_norm': 0.7741256356239319, 'learning_rate': 0.00016027777777777777, 'epoch': 2.5194444444444444}
Step 9080: {'loss': 0.8979, 'grad_norm': 0.7384295463562012, 'learning_rate': 0.00015935185185185185, 'epoch': 2.522222222222222}
Step 9090: {'loss': 0.9516, 'grad_norm': 0.9447488784790039, 'learning_rate': 0.00015842592592592593, 'epoch': 2.525}
Step 9100: {'loss': 0.9184, 'grad_norm': 1.6650909185409546, 'learning_rate': 0.0001575, 'epoch': 2.5277777777777777}
Step 9110: {'loss': 0.8397, 'grad_norm': 0.7831274271011353, 'learning_rate': 0.00015657407407407409, 'epoch': 2.5305555555555554}
Step 9120: {'loss': 0.9944, 'grad_norm': 1.601859211921692, 'learning_rate': 0.00015564814814814814, 'epoch': 2.533333333333333}
Step 9130: {'loss': 0.9041, 'grad_norm': 0.712550163269043, 'learning_rate': 0.00015472222222222225, 'epoch': 2.536111111111111}
Step 9140: {'loss': 0.8689, 'grad_norm': 1.2432913780212402, 'learning_rate': 0.0001537962962962963, 'epoch': 2.5388888888888888}
Step 9150: {'loss': 0.8435, 'grad_norm': 0.9683739542961121, 'learning_rate': 0.00015287037037037038, 'epoch': 2.5416666666666665}
Step 9160: {'loss': 0.9832, 'grad_norm': 0.8935695886611938, 'learning_rate': 0.00015194444444444443, 'epoch': 2.5444444444444443}
Step 9170: {'loss': 0.8853, 'grad_norm': 1.092076301574707, 'learning_rate': 0.00015101851851851854, 'epoch': 2.5472222222222225}
Step 9180: {'loss': 0.9103, 'grad_norm': 0.87964928150177, 'learning_rate': 0.0001500925925925926, 'epoch': 2.55}
Step 9190: {'loss': 0.7958, 'grad_norm': 0.9239219427108765, 'learning_rate': 0.00014916666666666667, 'epoch': 2.552777777777778}
Step 9200: {'loss': 0.8906, 'grad_norm': 0.9828038811683655, 'learning_rate': 0.00014824074074074072, 'epoch': 2.5555555555555554}
Step 9210: {'loss': 0.8484, 'grad_norm': 0.7076512575149536, 'learning_rate': 0.00014731481481481483, 'epoch': 2.5583333333333336}
Step 9220: {'loss': 0.9204, 'grad_norm': 0.6293855309486389, 'learning_rate': 0.0001463888888888889, 'epoch': 2.561111111111111}
Step 9230: {'loss': 0.9306, 'grad_norm': 1.2430531978607178, 'learning_rate': 0.00014546296296296296, 'epoch': 2.563888888888889}
Step 9240: {'loss': 0.9634, 'grad_norm': 0.8695520162582397, 'learning_rate': 0.00014453703703703704, 'epoch': 2.5666666666666664}
Step 9250: {'loss': 0.9518, 'grad_norm': 1.6868724822998047, 'learning_rate': 0.00014361111111111112, 'epoch': 2.5694444444444446}
Step 9260: {'loss': 0.922, 'grad_norm': 1.160115122795105, 'learning_rate': 0.0001426851851851852, 'epoch': 2.572222222222222}
Step 9270: {'loss': 0.9864, 'grad_norm': 0.7298718094825745, 'learning_rate': 0.00014175925925925925, 'epoch': 2.575}
Step 9280: {'loss': 0.9491, 'grad_norm': 2.4324162006378174, 'learning_rate': 0.00014083333333333333, 'epoch': 2.5777777777777775}
Step 9290: {'loss': 0.8518, 'grad_norm': 1.0349857807159424, 'learning_rate': 0.0001399074074074074, 'epoch': 2.5805555555555557}
Step 9300: {'loss': 0.9399, 'grad_norm': 1.2508938312530518, 'learning_rate': 0.0001389814814814815, 'epoch': 2.5833333333333335}
Step 9310: {'loss': 0.8505, 'grad_norm': 1.2280831336975098, 'learning_rate': 0.00013805555555555554, 'epoch': 2.5861111111111112}
Step 9320: {'loss': 0.881, 'grad_norm': 0.8626484274864197, 'learning_rate': 0.00013712962962962962, 'epoch': 2.588888888888889}
Step 9330: {'loss': 0.9253, 'grad_norm': 0.8389489054679871, 'learning_rate': 0.00013620370370370373, 'epoch': 2.591666666666667}
Step 9340: {'loss': 0.8996, 'grad_norm': 1.1604748964309692, 'learning_rate': 0.00013527777777777778, 'epoch': 2.5944444444444446}
Step 9350: {'loss': 0.9293, 'grad_norm': 1.3004159927368164, 'learning_rate': 0.00013435185185185186, 'epoch': 2.5972222222222223}
Step 9360: {'loss': 0.8885, 'grad_norm': 0.6022473573684692, 'learning_rate': 0.00013342592592592592, 'epoch': 2.6}
Step 9370: {'loss': 0.9133, 'grad_norm': 1.027193307876587, 'learning_rate': 0.00013250000000000002, 'epoch': 2.602777777777778}
Step 9380: {'loss': 0.8724, 'grad_norm': 0.8754473924636841, 'learning_rate': 0.00013157407407407407, 'epoch': 2.6055555555555556}
Step 9390: {'loss': 0.8615, 'grad_norm': 0.6885112524032593, 'learning_rate': 0.00013064814814814815, 'epoch': 2.6083333333333334}
Step 9400: {'loss': 0.8277, 'grad_norm': 0.8035663962364197, 'learning_rate': 0.0001297222222222222, 'epoch': 2.611111111111111}
Step 9410: {'loss': 0.8181, 'grad_norm': 0.9847796559333801, 'learning_rate': 0.0001287962962962963, 'epoch': 2.613888888888889}
Step 9420: {'loss': 0.8451, 'grad_norm': 0.6852736473083496, 'learning_rate': 0.0001278703703703704, 'epoch': 2.6166666666666667}
Step 9430: {'loss': 0.848, 'grad_norm': 1.0261175632476807, 'learning_rate': 0.00012694444444444445, 'epoch': 2.6194444444444445}
Step 9440: {'loss': 0.8121, 'grad_norm': 0.7730354070663452, 'learning_rate': 0.00012601851851851853, 'epoch': 2.6222222222222222}
Step 9450: {'loss': 0.9146, 'grad_norm': 1.000440239906311, 'learning_rate': 0.00012509259259259258, 'epoch': 2.625}
Step 9460: {'loss': 0.9874, 'grad_norm': 0.8509252667427063, 'learning_rate': 0.00012416666666666666, 'epoch': 2.6277777777777778}
Step 9470: {'loss': 0.8053, 'grad_norm': 0.7825955748558044, 'learning_rate': 0.00012324074074074074, 'epoch': 2.6305555555555555}
Step 9480: {'loss': 0.8284, 'grad_norm': 0.9251062870025635, 'learning_rate': 0.00012231481481481482, 'epoch': 2.6333333333333333}
Step 9490: {'loss': 0.9266, 'grad_norm': 0.7500024437904358, 'learning_rate': 0.0001213888888888889, 'epoch': 2.636111111111111}
Step 9500: {'loss': 0.9139, 'grad_norm': 0.7782852053642273, 'learning_rate': 0.00012046296296296296, 'epoch': 2.638888888888889}
Step 9510: {'loss': 0.8342, 'grad_norm': 0.578412652015686, 'learning_rate': 0.00011953703703703704, 'epoch': 2.6416666666666666}
Step 9520: {'loss': 0.9445, 'grad_norm': 1.6724233627319336, 'learning_rate': 0.00011861111111111111, 'epoch': 2.6444444444444444}
Step 9530: {'loss': 0.8142, 'grad_norm': 1.0032846927642822, 'learning_rate': 0.00011768518518518519, 'epoch': 2.647222222222222}
Step 9540: {'loss': 0.8935, 'grad_norm': 0.8355703353881836, 'learning_rate': 0.00011675925925925925, 'epoch': 2.65}
Step 9550: {'loss': 0.9575, 'grad_norm': 1.1837472915649414, 'learning_rate': 0.00011583333333333333, 'epoch': 2.6527777777777777}
Step 9560: {'loss': 0.8809, 'grad_norm': 0.9920163154602051, 'learning_rate': 0.0001149074074074074, 'epoch': 2.6555555555555554}
Step 9570: {'loss': 0.8748, 'grad_norm': 0.6695985794067383, 'learning_rate': 0.00011398148148148148, 'epoch': 2.658333333333333}
Step 9580: {'loss': 0.9458, 'grad_norm': 1.02177095413208, 'learning_rate': 0.00011305555555555556, 'epoch': 2.661111111111111}
Step 9590: {'loss': 0.85, 'grad_norm': 0.5829834342002869, 'learning_rate': 0.00011212962962962964, 'epoch': 2.6638888888888888}
Step 9600: {'loss': 0.8077, 'grad_norm': 0.7433428168296814, 'learning_rate': 0.0001112037037037037, 'epoch': 2.6666666666666665}
Step 9610: {'loss': 0.9769, 'grad_norm': 0.6313667893409729, 'learning_rate': 0.00011027777777777779, 'epoch': 2.6694444444444443}
Step 9620: {'loss': 0.8658, 'grad_norm': 0.7129629254341125, 'learning_rate': 0.00010935185185185185, 'epoch': 2.6722222222222225}
Step 9630: {'loss': 0.8666, 'grad_norm': 1.0296577215194702, 'learning_rate': 0.00010842592592592593, 'epoch': 2.675}
Step 9640: {'loss': 0.8618, 'grad_norm': 0.8701522946357727, 'learning_rate': 0.0001075, 'epoch': 2.677777777777778}
Step 9650: {'loss': 1.028, 'grad_norm': 1.2355881929397583, 'learning_rate': 0.00010657407407407408, 'epoch': 2.6805555555555554}
Step 9660: {'loss': 0.9763, 'grad_norm': 0.9524393677711487, 'learning_rate': 0.00010564814814814814, 'epoch': 2.6833333333333336}
Step 9670: {'loss': 0.902, 'grad_norm': 0.9124229550361633, 'learning_rate': 0.00010472222222222222, 'epoch': 2.686111111111111}
Step 9680: {'loss': 0.8882, 'grad_norm': 0.7961395978927612, 'learning_rate': 0.0001037962962962963, 'epoch': 2.688888888888889}
Step 9690: {'loss': 0.9052, 'grad_norm': 0.8322187662124634, 'learning_rate': 0.00010287037037037038, 'epoch': 2.6916666666666664}
Step 9700: {'loss': 0.8771, 'grad_norm': 0.8776712417602539, 'learning_rate': 0.00010194444444444445, 'epoch': 2.6944444444444446}
Step 9710: {'loss': 0.8915, 'grad_norm': 0.6533296704292297, 'learning_rate': 0.00010101851851851853, 'epoch': 2.697222222222222}
Step 9720: {'loss': 0.8684, 'grad_norm': 2.3205931186676025, 'learning_rate': 0.0001000925925925926, 'epoch': 2.7}
Step 9730: {'loss': 0.8646, 'grad_norm': 0.914840579032898, 'learning_rate': 9.916666666666667e-05, 'epoch': 2.7027777777777775}
Step 9740: {'loss': 0.8782, 'grad_norm': 0.8746634125709534, 'learning_rate': 9.824074074074074e-05, 'epoch': 2.7055555555555557}
Step 9750: {'loss': 0.8796, 'grad_norm': 1.0300406217575073, 'learning_rate': 9.731481481481482e-05, 'epoch': 2.7083333333333335}
Step 9760: {'loss': 0.8707, 'grad_norm': 0.698618471622467, 'learning_rate': 9.638888888888889e-05, 'epoch': 2.7111111111111112}
Step 9770: {'loss': 0.8813, 'grad_norm': 0.801787793636322, 'learning_rate': 9.546296296296297e-05, 'epoch': 2.713888888888889}
Step 9780: {'loss': 0.8149, 'grad_norm': 0.7772268056869507, 'learning_rate': 9.453703703703703e-05, 'epoch': 2.716666666666667}
Step 9790: {'loss': 0.8491, 'grad_norm': 1.068751573562622, 'learning_rate': 9.361111111111112e-05, 'epoch': 2.7194444444444446}
Step 9800: {'loss': 0.8317, 'grad_norm': 1.259854793548584, 'learning_rate': 9.268518518518519e-05, 'epoch': 2.7222222222222223}
Step 9810: {'loss': 0.8757, 'grad_norm': 0.8708485960960388, 'learning_rate': 9.175925925925927e-05, 'epoch': 2.725}
Step 9820: {'loss': 0.8553, 'grad_norm': 0.9544588923454285, 'learning_rate': 9.083333333333334e-05, 'epoch': 2.727777777777778}
Step 9830: {'loss': 0.9116, 'grad_norm': 0.612133264541626, 'learning_rate': 8.990740740740742e-05, 'epoch': 2.7305555555555556}
Step 9840: {'loss': 0.873, 'grad_norm': 1.2618260383605957, 'learning_rate': 8.898148148148148e-05, 'epoch': 2.7333333333333334}
Step 9850: {'loss': 0.9294, 'grad_norm': 1.1834874153137207, 'learning_rate': 8.805555555555556e-05, 'epoch': 2.736111111111111}
Step 9860: {'loss': 0.8927, 'grad_norm': 1.1876038312911987, 'learning_rate': 8.712962962962963e-05, 'epoch': 2.738888888888889}
Step 9870: {'loss': 0.9577, 'grad_norm': 1.2493984699249268, 'learning_rate': 8.62037037037037e-05, 'epoch': 2.7416666666666667}
Step 9880: {'loss': 0.8763, 'grad_norm': 1.0713199377059937, 'learning_rate': 8.527777777777777e-05, 'epoch': 2.7444444444444445}
Step 9890: {'loss': 0.8422, 'grad_norm': 2.3041274547576904, 'learning_rate': 8.435185185185185e-05, 'epoch': 2.7472222222222222}
Step 9900: {'loss': 0.8118, 'grad_norm': 0.8345575928688049, 'learning_rate': 8.342592592592593e-05, 'epoch': 2.75}
Step 9910: {'loss': 0.889, 'grad_norm': 0.8159314393997192, 'learning_rate': 8.25e-05, 'epoch': 2.7527777777777778}
Step 9920: {'loss': 0.8531, 'grad_norm': 0.9716750979423523, 'learning_rate': 8.157407407407408e-05, 'epoch': 2.7555555555555555}
Step 9930: {'loss': 0.9311, 'grad_norm': 1.0827122926712036, 'learning_rate': 8.064814814814815e-05, 'epoch': 2.7583333333333333}
Step 9940: {'loss': 0.9328, 'grad_norm': 0.7347819209098816, 'learning_rate': 7.972222222222223e-05, 'epoch': 2.761111111111111}
Step 9950: {'loss': 0.7838, 'grad_norm': 0.6436377167701721, 'learning_rate': 7.879629629629629e-05, 'epoch': 2.763888888888889}
Step 9960: {'loss': 0.9966, 'grad_norm': 0.8640357255935669, 'learning_rate': 7.787037037037037e-05, 'epoch': 2.7666666666666666}
Step 9970: {'loss': 0.8643, 'grad_norm': 0.9342508316040039, 'learning_rate': 7.694444444444444e-05, 'epoch': 2.7694444444444444}
Step 9980: {'loss': 0.9814, 'grad_norm': 0.8070090413093567, 'learning_rate': 7.601851851851852e-05, 'epoch': 2.772222222222222}
Step 9990: {'loss': 0.8686, 'grad_norm': 0.7258210778236389, 'learning_rate': 7.509259259259258e-05, 'epoch': 2.775}
Step 10000: {'loss': 0.9003, 'grad_norm': 0.8718454837799072, 'learning_rate': 7.416666666666668e-05, 'epoch': 2.7777777777777777}
Step 10010: {'loss': 0.8458, 'grad_norm': 1.5534882545471191, 'learning_rate': 7.324074074074074e-05, 'epoch': 2.7805555555555554}
Step 10020: {'loss': 0.8878, 'grad_norm': 0.953246533870697, 'learning_rate': 7.231481481481482e-05, 'epoch': 2.783333333333333}
Step 10030: {'loss': 0.8817, 'grad_norm': 0.4951074421405792, 'learning_rate': 7.138888888888889e-05, 'epoch': 2.786111111111111}
Step 10040: {'loss': 0.9659, 'grad_norm': 0.9098125696182251, 'learning_rate': 7.046296296296297e-05, 'epoch': 2.7888888888888888}
Step 10050: {'loss': 0.8799, 'grad_norm': 0.8039811849594116, 'learning_rate': 6.953703703703703e-05, 'epoch': 2.7916666666666665}
Step 10060: {'loss': 0.9276, 'grad_norm': 0.7186313271522522, 'learning_rate': 6.861111111111111e-05, 'epoch': 2.7944444444444443}
Step 10070: {'loss': 0.8826, 'grad_norm': 0.6057617664337158, 'learning_rate': 6.768518518518518e-05, 'epoch': 2.7972222222222225}
Step 10080: {'loss': 0.9442, 'grad_norm': 1.5837281942367554, 'learning_rate': 6.675925925925926e-05, 'epoch': 2.8}
Step 10090: {'loss': 0.9524, 'grad_norm': 0.8079549074172974, 'learning_rate': 6.583333333333333e-05, 'epoch': 2.802777777777778}
Step 10100: {'loss': 0.9415, 'grad_norm': 1.1137179136276245, 'learning_rate': 6.490740740740742e-05, 'epoch': 2.8055555555555554}
Step 10110: {'loss': 0.8491, 'grad_norm': 1.926722764968872, 'learning_rate': 6.398148148148148e-05, 'epoch': 2.8083333333333336}
Step 10120: {'loss': 1.0034, 'grad_norm': 1.697487473487854, 'learning_rate': 6.305555555555556e-05, 'epoch': 2.811111111111111}
Step 10130: {'loss': 0.841, 'grad_norm': 0.5839310884475708, 'learning_rate': 6.212962962962963e-05, 'epoch': 2.813888888888889}
Step 10140: {'loss': 0.8648, 'grad_norm': 1.6005444526672363, 'learning_rate': 6.120370370370371e-05, 'epoch': 2.8166666666666664}
Step 10150: {'loss': 0.861, 'grad_norm': 0.8362939953804016, 'learning_rate': 6.0277777777777776e-05, 'epoch': 2.8194444444444446}
Step 10160: {'loss': 0.9138, 'grad_norm': 0.8355082273483276, 'learning_rate': 5.935185185185185e-05, 'epoch': 2.822222222222222}
Step 10170: {'loss': 0.8396, 'grad_norm': 0.7882423400878906, 'learning_rate': 5.842592592592592e-05, 'epoch': 2.825}
Step 10180: {'loss': 0.9145, 'grad_norm': 0.9463289976119995, 'learning_rate': 5.75e-05, 'epoch': 2.8277777777777775}
Step 10190: {'loss': 0.9314, 'grad_norm': 0.5951390266418457, 'learning_rate': 5.6574074074074075e-05, 'epoch': 2.8305555555555557}
Step 10200: {'loss': 0.9311, 'grad_norm': 0.9648452997207642, 'learning_rate': 5.564814814814815e-05, 'epoch': 2.8333333333333335}
Step 10210: {'loss': 0.9071, 'grad_norm': 1.194543480873108, 'learning_rate': 5.472222222222222e-05, 'epoch': 2.8361111111111112}
Step 10220: {'loss': 0.9588, 'grad_norm': 0.8835086822509766, 'learning_rate': 5.379629629629629e-05, 'epoch': 2.838888888888889}
Step 10230: {'loss': 0.8851, 'grad_norm': 0.6362999081611633, 'learning_rate': 5.287037037037037e-05, 'epoch': 2.841666666666667}
Step 10240: {'loss': 0.9796, 'grad_norm': 1.9000250101089478, 'learning_rate': 5.1944444444444446e-05, 'epoch': 2.8444444444444446}
Step 10250: {'loss': 0.8615, 'grad_norm': 1.2778314352035522, 'learning_rate': 5.101851851851852e-05, 'epoch': 2.8472222222222223}
Step 10260: {'loss': 0.897, 'grad_norm': 0.8382249474525452, 'learning_rate': 5.009259259259259e-05, 'epoch': 2.85}
Step 10270: {'loss': 0.8388, 'grad_norm': 0.7968583703041077, 'learning_rate': 4.9166666666666665e-05, 'epoch': 2.852777777777778}
Step 10280: {'loss': 0.9324, 'grad_norm': 0.8208023905754089, 'learning_rate': 4.8240740740740744e-05, 'epoch': 2.8555555555555556}
Step 10290: {'loss': 0.8169, 'grad_norm': 4.011972904205322, 'learning_rate': 4.731481481481482e-05, 'epoch': 2.8583333333333334}
Step 10300: {'loss': 0.8578, 'grad_norm': 1.0961941480636597, 'learning_rate': 4.638888888888889e-05, 'epoch': 2.861111111111111}
Step 10310: {'loss': 0.8958, 'grad_norm': 0.8487177491188049, 'learning_rate': 4.546296296296296e-05, 'epoch': 2.863888888888889}
Step 10320: {'loss': 0.8822, 'grad_norm': 1.2422643899917603, 'learning_rate': 4.4537037037037036e-05, 'epoch': 2.8666666666666667}
Step 10330: {'loss': 0.9578, 'grad_norm': 0.5609279274940491, 'learning_rate': 4.3611111111111116e-05, 'epoch': 2.8694444444444445}
Step 10340: {'loss': 0.8607, 'grad_norm': 0.7661018967628479, 'learning_rate': 4.268518518518519e-05, 'epoch': 2.8722222222222222}
Step 10350: {'loss': 0.9398, 'grad_norm': 1.7806299924850464, 'learning_rate': 4.175925925925926e-05, 'epoch': 2.875}
Step 10360: {'loss': 0.7632, 'grad_norm': 0.9686797857284546, 'learning_rate': 4.0833333333333334e-05, 'epoch': 2.8777777777777778}
Step 10370: {'loss': 0.9359, 'grad_norm': 1.3306806087493896, 'learning_rate': 3.990740740740741e-05, 'epoch': 2.8805555555555555}
Step 10380: {'loss': 0.8477, 'grad_norm': 1.0058139562606812, 'learning_rate': 3.898148148148148e-05, 'epoch': 2.8833333333333333}
Step 10390: {'loss': 0.9311, 'grad_norm': 1.203602910041809, 'learning_rate': 3.805555555555556e-05, 'epoch': 2.886111111111111}
Step 10400: {'loss': 0.9444, 'grad_norm': 1.0151972770690918, 'learning_rate': 3.712962962962963e-05, 'epoch': 2.888888888888889}
Step 10410: {'loss': 0.9739, 'grad_norm': 0.8907392024993896, 'learning_rate': 3.6203703703703706e-05, 'epoch': 2.8916666666666666}
Step 10420: {'loss': 0.8592, 'grad_norm': 0.8808788061141968, 'learning_rate': 3.527777777777778e-05, 'epoch': 2.8944444444444444}
Step 10430: {'loss': 0.8153, 'grad_norm': 0.7878726124763489, 'learning_rate': 3.435185185185185e-05, 'epoch': 2.897222222222222}
Step 10440: {'loss': 0.913, 'grad_norm': 0.9587770104408264, 'learning_rate': 3.342592592592593e-05, 'epoch': 2.9}
Step 10450: {'loss': 0.9272, 'grad_norm': 0.6025335788726807, 'learning_rate': 3.2500000000000004e-05, 'epoch': 2.9027777777777777}
Step 10460: {'loss': 0.9079, 'grad_norm': 0.5726686120033264, 'learning_rate': 3.157407407407408e-05, 'epoch': 2.9055555555555554}
Step 10470: {'loss': 0.8587, 'grad_norm': 0.5453223586082458, 'learning_rate': 3.064814814814815e-05, 'epoch': 2.908333333333333}
Step 10480: {'loss': 0.8674, 'grad_norm': 0.8352953195571899, 'learning_rate': 2.9722222222222223e-05, 'epoch': 2.911111111111111}
Step 10490: {'loss': 0.8829, 'grad_norm': 1.0565801858901978, 'learning_rate': 2.8796296296296296e-05, 'epoch': 2.9138888888888888}
Step 10500: {'loss': 0.9281, 'grad_norm': 0.9163722991943359, 'learning_rate': 2.7870370370370372e-05, 'epoch': 2.9166666666666665}
Step 10510: {'loss': 0.8952, 'grad_norm': 0.9356446862220764, 'learning_rate': 2.6944444444444445e-05, 'epoch': 2.9194444444444443}
Step 10520: {'loss': 0.823, 'grad_norm': 0.944726288318634, 'learning_rate': 2.6018518518518518e-05, 'epoch': 2.9222222222222225}
Step 10530: {'loss': 0.8603, 'grad_norm': 0.9470548033714294, 'learning_rate': 2.5092592592592594e-05, 'epoch': 2.925}
Step 10540: {'loss': 0.8115, 'grad_norm': 0.9435086250305176, 'learning_rate': 2.4166666666666667e-05, 'epoch': 2.927777777777778}
Step 10550: {'loss': 0.9023, 'grad_norm': 1.018127679824829, 'learning_rate': 2.3240740740740743e-05, 'epoch': 2.9305555555555554}
Step 10560: {'loss': 0.9192, 'grad_norm': 0.917977511882782, 'learning_rate': 2.2314814814814816e-05, 'epoch': 2.9333333333333336}
Step 10570: {'loss': 0.8002, 'grad_norm': 1.0015937089920044, 'learning_rate': 2.138888888888889e-05, 'epoch': 2.936111111111111}
Step 10580: {'loss': 0.8677, 'grad_norm': 0.9411964416503906, 'learning_rate': 2.0462962962962965e-05, 'epoch': 2.938888888888889}
Step 10590: {'loss': 0.9286, 'grad_norm': 0.9714545607566833, 'learning_rate': 1.9537037037037038e-05, 'epoch': 2.9416666666666664}
Step 10600: {'loss': 0.8774, 'grad_norm': 0.6564654111862183, 'learning_rate': 1.861111111111111e-05, 'epoch': 2.9444444444444446}
Step 10610: {'loss': 0.8286, 'grad_norm': 1.065765380859375, 'learning_rate': 1.7685185185185187e-05, 'epoch': 2.947222222222222}
Step 10620: {'loss': 0.8378, 'grad_norm': 1.1651437282562256, 'learning_rate': 1.675925925925926e-05, 'epoch': 2.95}
Step 10630: {'loss': 0.9449, 'grad_norm': 0.8351268768310547, 'learning_rate': 1.5833333333333336e-05, 'epoch': 2.9527777777777775}
Step 10640: {'loss': 0.9004, 'grad_norm': 0.9629126787185669, 'learning_rate': 1.4907407407407408e-05, 'epoch': 2.9555555555555557}
Step 10650: {'loss': 0.8023, 'grad_norm': 0.7237406969070435, 'learning_rate': 1.3981481481481482e-05, 'epoch': 2.9583333333333335}
Step 10660: {'loss': 0.8721, 'grad_norm': 0.9837508797645569, 'learning_rate': 1.3055555555555557e-05, 'epoch': 2.9611111111111112}
Step 10670: {'loss': 0.8535, 'grad_norm': 1.1840848922729492, 'learning_rate': 1.212962962962963e-05, 'epoch': 2.963888888888889}
Step 10680: {'loss': 0.8625, 'grad_norm': 0.7043336629867554, 'learning_rate': 1.1203703703703704e-05, 'epoch': 2.966666666666667}
Step 10690: {'loss': 0.8526, 'grad_norm': 0.6954672336578369, 'learning_rate': 1.0277777777777779e-05, 'epoch': 2.9694444444444446}
Step 10700: {'loss': 0.8783, 'grad_norm': 0.634680986404419, 'learning_rate': 9.351851851851854e-06, 'epoch': 2.9722222222222223}
Step 10710: {'loss': 0.8654, 'grad_norm': 0.6537808775901794, 'learning_rate': 8.425925925925925e-06, 'epoch': 2.975}
Step 10720: {'loss': 0.9013, 'grad_norm': 0.8806823492050171, 'learning_rate': 7.5e-06, 'epoch': 2.977777777777778}
Step 10730: {'loss': 0.7973, 'grad_norm': 1.113106369972229, 'learning_rate': 6.574074074074074e-06, 'epoch': 2.9805555555555556}
Step 10740: {'loss': 0.8185, 'grad_norm': 0.677975594997406, 'learning_rate': 5.648148148148148e-06, 'epoch': 2.9833333333333334}
Step 10750: {'loss': 0.821, 'grad_norm': 1.08885657787323, 'learning_rate': 4.722222222222222e-06, 'epoch': 2.986111111111111}
Step 10760: {'loss': 0.8743, 'grad_norm': 1.628869891166687, 'learning_rate': 3.7962962962962964e-06, 'epoch': 2.988888888888889}
Step 10770: {'loss': 0.9076, 'grad_norm': 1.2836014032363892, 'learning_rate': 2.8703703703703706e-06, 'epoch': 2.9916666666666667}
Step 10780: {'loss': 0.9802, 'grad_norm': 0.9643023014068604, 'learning_rate': 1.9444444444444444e-06, 'epoch': 2.9944444444444445}
Step 10790: {'loss': 0.9247, 'grad_norm': 0.7260230183601379, 'learning_rate': 1.0185185185185185e-06, 'epoch': 2.9972222222222222}
Step 10800: {'loss': 0.9034, 'grad_norm': 1.0457242727279663, 'learning_rate': 9.259259259259259e-08, 'epoch': 3.0}
Step 10800: {'train_runtime': 1478.625, 'train_samples_per_second': 14.608, 'train_steps_per_second': 7.304, 'total_flos': 2.313862424520192e+17, 'train_loss': 0.8873865281211005, 'epoch': 3.0}
