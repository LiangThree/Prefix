Step 10: {'loss': 1.7802, 'grad_norm': 0.5415132641792297, 'learning_rate': 0.0004995833333333333, 'epoch': 0.002777777777777778}
Step 20: {'loss': 1.4852, 'grad_norm': 1.1231003999710083, 'learning_rate': 0.0004991203703703704, 'epoch': 0.005555555555555556}
Step 30: {'loss': 1.3398, 'grad_norm': 2.1442008018493652, 'learning_rate': 0.0004986574074074074, 'epoch': 0.008333333333333333}
Step 40: {'loss': 1.3054, 'grad_norm': 1.3634874820709229, 'learning_rate': 0.0004981944444444444, 'epoch': 0.011111111111111112}
Step 50: {'loss': 1.1542, 'grad_norm': 1.5484777688980103, 'learning_rate': 0.0004977314814814815, 'epoch': 0.013888888888888888}
Step 60: {'loss': 1.0429, 'grad_norm': 1.2153621912002563, 'learning_rate': 0.0004972685185185185, 'epoch': 0.016666666666666666}
Step 70: {'loss': 0.9281, 'grad_norm': 1.1214637756347656, 'learning_rate': 0.0004968055555555556, 'epoch': 0.019444444444444445}
Step 80: {'loss': 1.1378, 'grad_norm': 2.2185983657836914, 'learning_rate': 0.0004963425925925926, 'epoch': 0.022222222222222223}
Step 90: {'loss': 1.0385, 'grad_norm': 2.1870877742767334, 'learning_rate': 0.0004958796296296296, 'epoch': 0.025}
Step 100: {'loss': 1.0272, 'grad_norm': 1.2924035787582397, 'learning_rate': 0.0004954166666666667, 'epoch': 0.027777777777777776}
Step 110: {'loss': 1.1015, 'grad_norm': 1.4948315620422363, 'learning_rate': 0.0004949537037037037, 'epoch': 0.030555555555555555}
Step 120: {'loss': 0.9431, 'grad_norm': 1.189732313156128, 'learning_rate': 0.0004944907407407408, 'epoch': 0.03333333333333333}
Step 130: {'loss': 1.0342, 'grad_norm': 1.9049773216247559, 'learning_rate': 0.0004940277777777778, 'epoch': 0.03611111111111111}
Step 140: {'loss': 1.0166, 'grad_norm': 1.1960526704788208, 'learning_rate': 0.0004935648148148148, 'epoch': 0.03888888888888889}
Step 150: {'loss': 1.0123, 'grad_norm': 2.265439748764038, 'learning_rate': 0.0004931018518518519, 'epoch': 0.041666666666666664}
Step 160: {'loss': 1.0761, 'grad_norm': 1.3109241724014282, 'learning_rate': 0.0004926388888888889, 'epoch': 0.044444444444444446}
Step 170: {'loss': 1.1634, 'grad_norm': 1.834290623664856, 'learning_rate': 0.000492175925925926, 'epoch': 0.04722222222222222}
Step 180: {'loss': 1.0462, 'grad_norm': 1.5588880777359009, 'learning_rate': 0.000491712962962963, 'epoch': 0.05}
Step 190: {'loss': 1.112, 'grad_norm': 1.7409865856170654, 'learning_rate': 0.00049125, 'epoch': 0.05277777777777778}
Step 200: {'loss': 1.1073, 'grad_norm': 1.2862030267715454, 'learning_rate': 0.0004907870370370371, 'epoch': 0.05555555555555555}
Step 210: {'loss': 1.0208, 'grad_norm': 1.4972022771835327, 'learning_rate': 0.0004903240740740741, 'epoch': 0.058333333333333334}
Step 220: {'loss': 0.8986, 'grad_norm': 1.3553742170333862, 'learning_rate': 0.0004898611111111112, 'epoch': 0.06111111111111111}
Step 230: {'loss': 0.9641, 'grad_norm': 1.1785223484039307, 'learning_rate': 0.0004893981481481482, 'epoch': 0.06388888888888888}
Step 240: {'loss': 0.9205, 'grad_norm': 1.1666438579559326, 'learning_rate': 0.0004889351851851852, 'epoch': 0.06666666666666667}
Step 250: {'loss': 0.9783, 'grad_norm': 0.9372712969779968, 'learning_rate': 0.0004884722222222222, 'epoch': 0.06944444444444445}
Step 260: {'loss': 0.9725, 'grad_norm': 1.2005733251571655, 'learning_rate': 0.00048800925925925927, 'epoch': 0.07222222222222222}
Step 270: {'loss': 0.9957, 'grad_norm': 1.0504088401794434, 'learning_rate': 0.0004875462962962963, 'epoch': 0.075}
Step 280: {'loss': 1.0524, 'grad_norm': 1.6283483505249023, 'learning_rate': 0.00048708333333333335, 'epoch': 0.07777777777777778}
Step 290: {'loss': 1.0506, 'grad_norm': 1.715014934539795, 'learning_rate': 0.0004866203703703704, 'epoch': 0.08055555555555556}
Step 300: {'loss': 1.095, 'grad_norm': 1.5430935621261597, 'learning_rate': 0.0004861574074074074, 'epoch': 0.08333333333333333}
Step 310: {'loss': 0.9393, 'grad_norm': 1.0340830087661743, 'learning_rate': 0.00048569444444444447, 'epoch': 0.08611111111111111}
Step 320: {'loss': 0.9688, 'grad_norm': 1.474832534790039, 'learning_rate': 0.0004852314814814815, 'epoch': 0.08888888888888889}
Step 330: {'loss': 0.9245, 'grad_norm': 1.1158815622329712, 'learning_rate': 0.00048476851851851855, 'epoch': 0.09166666666666666}
Step 340: {'loss': 0.9319, 'grad_norm': 1.8366488218307495, 'learning_rate': 0.00048430555555555553, 'epoch': 0.09444444444444444}
Step 350: {'loss': 1.0538, 'grad_norm': 1.3385679721832275, 'learning_rate': 0.0004838425925925926, 'epoch': 0.09722222222222222}
Step 360: {'loss': 0.9497, 'grad_norm': 2.4615085124969482, 'learning_rate': 0.0004833796296296296, 'epoch': 0.1}
Step 370: {'loss': 1.0064, 'grad_norm': 1.4118977785110474, 'learning_rate': 0.00048291666666666665, 'epoch': 0.10277777777777777}
Step 380: {'loss': 0.9127, 'grad_norm': 1.0973554849624634, 'learning_rate': 0.00048245370370370374, 'epoch': 0.10555555555555556}
Step 390: {'loss': 0.8352, 'grad_norm': 1.0085755586624146, 'learning_rate': 0.00048199074074074073, 'epoch': 0.10833333333333334}
Step 400: {'loss': 1.0194, 'grad_norm': 1.4634149074554443, 'learning_rate': 0.00048152777777777777, 'epoch': 0.1111111111111111}
Step 410: {'loss': 0.9583, 'grad_norm': 1.2779648303985596, 'learning_rate': 0.00048106481481481486, 'epoch': 0.11388888888888889}
Step 420: {'loss': 0.9002, 'grad_norm': 1.008146047592163, 'learning_rate': 0.00048060185185185185, 'epoch': 0.11666666666666667}
Step 430: {'loss': 0.8931, 'grad_norm': 1.2991728782653809, 'learning_rate': 0.0004801388888888889, 'epoch': 0.11944444444444445}
Step 440: {'loss': 0.9893, 'grad_norm': 1.6664727926254272, 'learning_rate': 0.000479675925925926, 'epoch': 0.12222222222222222}
Step 450: {'loss': 1.0408, 'grad_norm': 1.2003971338272095, 'learning_rate': 0.00047921296296296297, 'epoch': 0.125}
Step 460: {'loss': 1.0016, 'grad_norm': 1.7956372499465942, 'learning_rate': 0.00047875, 'epoch': 0.12777777777777777}
Step 470: {'loss': 1.0084, 'grad_norm': 1.9430593252182007, 'learning_rate': 0.000478287037037037, 'epoch': 0.13055555555555556}
Step 480: {'loss': 0.9909, 'grad_norm': 1.1222329139709473, 'learning_rate': 0.0004778240740740741, 'epoch': 0.13333333333333333}
Step 490: {'loss': 0.9446, 'grad_norm': 1.496993899345398, 'learning_rate': 0.00047736111111111113, 'epoch': 0.1361111111111111}
Step 500: {'loss': 0.9419, 'grad_norm': 1.6385689973831177, 'learning_rate': 0.0004768981481481481, 'epoch': 0.1388888888888889}
Step 510: {'loss': 1.0034, 'grad_norm': 1.4087355136871338, 'learning_rate': 0.0004764351851851852, 'epoch': 0.14166666666666666}
Step 520: {'loss': 1.037, 'grad_norm': 2.183936357498169, 'learning_rate': 0.00047597222222222225, 'epoch': 0.14444444444444443}
Step 530: {'loss': 0.9865, 'grad_norm': 1.149964451789856, 'learning_rate': 0.00047550925925925923, 'epoch': 0.14722222222222223}
Step 540: {'loss': 0.8515, 'grad_norm': 0.99824458360672, 'learning_rate': 0.00047504629629629633, 'epoch': 0.15}
Step 550: {'loss': 0.953, 'grad_norm': 1.2980808019638062, 'learning_rate': 0.00047458333333333337, 'epoch': 0.1527777777777778}
Step 560: {'loss': 0.9552, 'grad_norm': 1.3153945207595825, 'learning_rate': 0.00047412037037037035, 'epoch': 0.15555555555555556}
Step 570: {'loss': 0.9131, 'grad_norm': 1.3497637510299683, 'learning_rate': 0.00047365740740740745, 'epoch': 0.15833333333333333}
Step 580: {'loss': 0.9279, 'grad_norm': 1.0649908781051636, 'learning_rate': 0.00047319444444444443, 'epoch': 0.16111111111111112}
Step 590: {'loss': 0.9863, 'grad_norm': 1.4734729528427124, 'learning_rate': 0.00047273148148148147, 'epoch': 0.1638888888888889}
Step 600: {'loss': 0.952, 'grad_norm': 1.3924129009246826, 'learning_rate': 0.00047226851851851857, 'epoch': 0.16666666666666666}
Step 610: {'loss': 0.96, 'grad_norm': 1.4364033937454224, 'learning_rate': 0.00047180555555555555, 'epoch': 0.16944444444444445}
Step 620: {'loss': 0.934, 'grad_norm': 0.8516467809677124, 'learning_rate': 0.0004713425925925926, 'epoch': 0.17222222222222222}
Step 630: {'loss': 0.938, 'grad_norm': 1.8020738363265991, 'learning_rate': 0.00047087962962962963, 'epoch': 0.175}
Step 640: {'loss': 0.9514, 'grad_norm': 1.4050402641296387, 'learning_rate': 0.00047041666666666667, 'epoch': 0.17777777777777778}
Step 650: {'loss': 0.9555, 'grad_norm': 1.3274825811386108, 'learning_rate': 0.0004699537037037037, 'epoch': 0.18055555555555555}
Step 660: {'loss': 0.9389, 'grad_norm': 1.5229893922805786, 'learning_rate': 0.00046949074074074075, 'epoch': 0.18333333333333332}
Step 670: {'loss': 1.0376, 'grad_norm': 1.8043239116668701, 'learning_rate': 0.0004690277777777778, 'epoch': 0.18611111111111112}
Step 680: {'loss': 0.906, 'grad_norm': 0.966619074344635, 'learning_rate': 0.00046856481481481483, 'epoch': 0.18888888888888888}
Step 690: {'loss': 0.8581, 'grad_norm': 1.1976277828216553, 'learning_rate': 0.0004681018518518518, 'epoch': 0.19166666666666668}
Step 700: {'loss': 0.8906, 'grad_norm': 1.467948079109192, 'learning_rate': 0.0004676388888888889, 'epoch': 0.19444444444444445}
Step 710: {'loss': 0.8859, 'grad_norm': 0.9309801459312439, 'learning_rate': 0.00046717592592592595, 'epoch': 0.19722222222222222}
Step 720: {'loss': 0.9942, 'grad_norm': 1.2122881412506104, 'learning_rate': 0.00046671296296296294, 'epoch': 0.2}
Step 730: {'loss': 0.8739, 'grad_norm': 1.1989690065383911, 'learning_rate': 0.00046625000000000003, 'epoch': 0.20277777777777778}
Step 740: {'loss': 0.8919, 'grad_norm': 1.2552539110183716, 'learning_rate': 0.00046578703703703707, 'epoch': 0.20555555555555555}
Step 750: {'loss': 0.8806, 'grad_norm': 1.225922703742981, 'learning_rate': 0.00046532407407407406, 'epoch': 0.20833333333333334}
Step 760: {'loss': 0.9346, 'grad_norm': 1.465922236442566, 'learning_rate': 0.00046486111111111115, 'epoch': 0.2111111111111111}
Step 770: {'loss': 0.8951, 'grad_norm': 1.068989634513855, 'learning_rate': 0.0004643981481481482, 'epoch': 0.21388888888888888}
Step 780: {'loss': 0.9199, 'grad_norm': 1.9723312854766846, 'learning_rate': 0.0004639351851851852, 'epoch': 0.21666666666666667}
Step 790: {'loss': 0.8858, 'grad_norm': 1.207298994064331, 'learning_rate': 0.0004634722222222222, 'epoch': 0.21944444444444444}
Step 800: {'loss': 0.8824, 'grad_norm': 1.5852313041687012, 'learning_rate': 0.00046300925925925925, 'epoch': 0.2222222222222222}
Step 810: {'loss': 0.8275, 'grad_norm': 1.2684662342071533, 'learning_rate': 0.0004625462962962963, 'epoch': 0.225}
Step 820: {'loss': 0.9309, 'grad_norm': 1.2964484691619873, 'learning_rate': 0.00046208333333333333, 'epoch': 0.22777777777777777}
Step 830: {'loss': 0.9483, 'grad_norm': 1.2718864679336548, 'learning_rate': 0.0004616203703703704, 'epoch': 0.23055555555555557}
Step 840: {'loss': 0.8703, 'grad_norm': 1.1748474836349487, 'learning_rate': 0.0004611574074074074, 'epoch': 0.23333333333333334}
Step 850: {'loss': 0.9087, 'grad_norm': 1.259770393371582, 'learning_rate': 0.00046069444444444445, 'epoch': 0.2361111111111111}
Step 860: {'loss': 0.9081, 'grad_norm': 1.2291154861450195, 'learning_rate': 0.0004602314814814815, 'epoch': 0.2388888888888889}
Step 870: {'loss': 0.9527, 'grad_norm': 1.1376546621322632, 'learning_rate': 0.00045976851851851853, 'epoch': 0.24166666666666667}
Step 880: {'loss': 0.9672, 'grad_norm': 1.14698326587677, 'learning_rate': 0.0004593055555555556, 'epoch': 0.24444444444444444}
Step 890: {'loss': 0.8371, 'grad_norm': 1.2002207040786743, 'learning_rate': 0.0004588425925925926, 'epoch': 0.24722222222222223}
Step 900: {'loss': 0.8546, 'grad_norm': 0.858273983001709, 'learning_rate': 0.00045837962962962965, 'epoch': 0.25}
Step 910: {'loss': 0.9175, 'grad_norm': 1.3510125875473022, 'learning_rate': 0.0004579166666666667, 'epoch': 0.25277777777777777}
Step 920: {'loss': 0.9137, 'grad_norm': 1.5797569751739502, 'learning_rate': 0.0004574537037037037, 'epoch': 0.25555555555555554}
Step 930: {'loss': 0.8868, 'grad_norm': 1.4038734436035156, 'learning_rate': 0.00045699074074074077, 'epoch': 0.25833333333333336}
Step 940: {'loss': 0.9774, 'grad_norm': 1.2919621467590332, 'learning_rate': 0.00045652777777777776, 'epoch': 0.2611111111111111}
Step 950: {'loss': 0.8645, 'grad_norm': 1.1606420278549194, 'learning_rate': 0.0004560648148148148, 'epoch': 0.2638888888888889}
Step 960: {'loss': 0.8974, 'grad_norm': 1.1724287271499634, 'learning_rate': 0.0004556018518518519, 'epoch': 0.26666666666666666}
Step 970: {'loss': 0.9813, 'grad_norm': 1.6783772706985474, 'learning_rate': 0.0004551388888888889, 'epoch': 0.26944444444444443}
Step 980: {'loss': 0.846, 'grad_norm': 1.3861593008041382, 'learning_rate': 0.0004546759259259259, 'epoch': 0.2722222222222222}
Step 990: {'loss': 0.887, 'grad_norm': 1.315226674079895, 'learning_rate': 0.000454212962962963, 'epoch': 0.275}
Step 1000: {'loss': 0.951, 'grad_norm': 1.918197512626648, 'learning_rate': 0.00045375, 'epoch': 0.2777777777777778}
Step 1010: {'loss': 0.9109, 'grad_norm': 0.8402800559997559, 'learning_rate': 0.00045328703703703704, 'epoch': 0.28055555555555556}
Step 1020: {'loss': 0.9913, 'grad_norm': 2.0053915977478027, 'learning_rate': 0.00045282407407407413, 'epoch': 0.2833333333333333}
Step 1030: {'loss': 0.9093, 'grad_norm': 0.9567340016365051, 'learning_rate': 0.0004523611111111111, 'epoch': 0.2861111111111111}
Step 1040: {'loss': 0.9059, 'grad_norm': 1.0819662809371948, 'learning_rate': 0.00045189814814814816, 'epoch': 0.28888888888888886}
Step 1050: {'loss': 1.0198, 'grad_norm': 1.357256293296814, 'learning_rate': 0.0004514351851851852, 'epoch': 0.2916666666666667}
Step 1060: {'loss': 0.9543, 'grad_norm': 1.4716579914093018, 'learning_rate': 0.00045097222222222224, 'epoch': 0.29444444444444445}
Step 1070: {'loss': 0.9003, 'grad_norm': 1.3275113105773926, 'learning_rate': 0.0004505092592592593, 'epoch': 0.2972222222222222}
Step 1080: {'loss': 0.8283, 'grad_norm': 1.3003867864608765, 'learning_rate': 0.00045004629629629626, 'epoch': 0.3}
Step 1090: {'loss': 1.0468, 'grad_norm': 1.5770149230957031, 'learning_rate': 0.00044958333333333336, 'epoch': 0.30277777777777776}
Step 1100: {'loss': 0.9421, 'grad_norm': 1.4716590642929077, 'learning_rate': 0.0004491203703703704, 'epoch': 0.3055555555555556}
Step 1110: {'loss': 0.8578, 'grad_norm': 1.5536880493164062, 'learning_rate': 0.0004486574074074074, 'epoch': 0.30833333333333335}
Step 1120: {'loss': 0.954, 'grad_norm': 1.172020435333252, 'learning_rate': 0.0004481944444444445, 'epoch': 0.3111111111111111}
Step 1130: {'loss': 0.8509, 'grad_norm': 1.23866605758667, 'learning_rate': 0.0004477314814814815, 'epoch': 0.3138888888888889}
Step 1140: {'loss': 0.9825, 'grad_norm': 1.4587384462356567, 'learning_rate': 0.0004472685185185185, 'epoch': 0.31666666666666665}
Step 1150: {'loss': 0.9507, 'grad_norm': 2.466651678085327, 'learning_rate': 0.0004468055555555556, 'epoch': 0.3194444444444444}
Step 1160: {'loss': 0.9119, 'grad_norm': 1.1492692232131958, 'learning_rate': 0.0004463425925925926, 'epoch': 0.32222222222222224}
Step 1170: {'loss': 0.9648, 'grad_norm': 1.4099230766296387, 'learning_rate': 0.0004458796296296296, 'epoch': 0.325}
Step 1180: {'loss': 0.9519, 'grad_norm': 1.4747235774993896, 'learning_rate': 0.0004454166666666667, 'epoch': 0.3277777777777778}
Step 1190: {'loss': 0.9149, 'grad_norm': 1.4003239870071411, 'learning_rate': 0.0004449537037037037, 'epoch': 0.33055555555555555}
Step 1200: {'loss': 0.9649, 'grad_norm': 1.3872712850570679, 'learning_rate': 0.00044449074074074074, 'epoch': 0.3333333333333333}
Step 1210: {'loss': 1.0352, 'grad_norm': 1.180082082748413, 'learning_rate': 0.00044402777777777783, 'epoch': 0.33611111111111114}
Step 1220: {'loss': 0.9217, 'grad_norm': 1.2466222047805786, 'learning_rate': 0.0004435648148148148, 'epoch': 0.3388888888888889}
Step 1230: {'loss': 0.8667, 'grad_norm': 1.1681337356567383, 'learning_rate': 0.00044310185185185186, 'epoch': 0.3416666666666667}
Step 1240: {'loss': 0.9529, 'grad_norm': 1.5946691036224365, 'learning_rate': 0.0004426388888888889, 'epoch': 0.34444444444444444}
Step 1250: {'loss': 0.9047, 'grad_norm': 1.1419438123703003, 'learning_rate': 0.00044217592592592594, 'epoch': 0.3472222222222222}
Step 1260: {'loss': 0.9399, 'grad_norm': 1.0857490301132202, 'learning_rate': 0.000441712962962963, 'epoch': 0.35}
Step 1270: {'loss': 0.8406, 'grad_norm': 1.122656226158142, 'learning_rate': 0.00044124999999999996, 'epoch': 0.3527777777777778}
Step 1280: {'loss': 0.8457, 'grad_norm': 1.0431363582611084, 'learning_rate': 0.00044078703703703706, 'epoch': 0.35555555555555557}
Step 1290: {'loss': 0.7836, 'grad_norm': 0.9088615775108337, 'learning_rate': 0.0004403240740740741, 'epoch': 0.35833333333333334}
Step 1300: {'loss': 0.9243, 'grad_norm': 1.3562899827957153, 'learning_rate': 0.0004398611111111111, 'epoch': 0.3611111111111111}
Step 1310: {'loss': 0.8864, 'grad_norm': 0.8112387657165527, 'learning_rate': 0.0004393981481481482, 'epoch': 0.3638888888888889}
Step 1320: {'loss': 0.9143, 'grad_norm': 1.19364333152771, 'learning_rate': 0.0004389351851851852, 'epoch': 0.36666666666666664}
Step 1330: {'loss': 0.7953, 'grad_norm': 1.3702104091644287, 'learning_rate': 0.0004384722222222222, 'epoch': 0.36944444444444446}
Step 1340: {'loss': 0.9418, 'grad_norm': 1.2003364562988281, 'learning_rate': 0.0004380092592592593, 'epoch': 0.37222222222222223}
Step 1350: {'loss': 0.839, 'grad_norm': 1.037833571434021, 'learning_rate': 0.00043754629629629634, 'epoch': 0.375}
Step 1360: {'loss': 0.9658, 'grad_norm': 1.1870743036270142, 'learning_rate': 0.0004370833333333333, 'epoch': 0.37777777777777777}
Step 1370: {'loss': 0.8554, 'grad_norm': 1.052071213722229, 'learning_rate': 0.00043662037037037036, 'epoch': 0.38055555555555554}
Step 1380: {'loss': 0.9976, 'grad_norm': 2.0413131713867188, 'learning_rate': 0.0004361574074074074, 'epoch': 0.38333333333333336}
Step 1390: {'loss': 0.9475, 'grad_norm': 1.9844051599502563, 'learning_rate': 0.00043569444444444444, 'epoch': 0.3861111111111111}
Step 1400: {'loss': 0.9266, 'grad_norm': 1.0980114936828613, 'learning_rate': 0.0004352314814814815, 'epoch': 0.3888888888888889}
Step 1410: {'loss': 0.8814, 'grad_norm': 1.075271487236023, 'learning_rate': 0.0004347685185185185, 'epoch': 0.39166666666666666}
Step 1420: {'loss': 0.9268, 'grad_norm': 1.4673258066177368, 'learning_rate': 0.00043430555555555556, 'epoch': 0.39444444444444443}
Step 1430: {'loss': 0.9247, 'grad_norm': 1.2105917930603027, 'learning_rate': 0.0004338425925925926, 'epoch': 0.3972222222222222}
Step 1440: {'loss': 0.9801, 'grad_norm': 1.4072977304458618, 'learning_rate': 0.00043337962962962964, 'epoch': 0.4}
Step 1450: {'loss': 0.9208, 'grad_norm': 1.7100025415420532, 'learning_rate': 0.0004329166666666667, 'epoch': 0.4027777777777778}
Step 1460: {'loss': 0.9232, 'grad_norm': 1.3575847148895264, 'learning_rate': 0.0004324537037037037, 'epoch': 0.40555555555555556}
Step 1470: {'loss': 0.8586, 'grad_norm': 1.2660318613052368, 'learning_rate': 0.00043199074074074076, 'epoch': 0.4083333333333333}
Step 1480: {'loss': 0.9206, 'grad_norm': 1.1220009326934814, 'learning_rate': 0.0004315277777777778, 'epoch': 0.4111111111111111}
Step 1490: {'loss': 0.9567, 'grad_norm': 2.738865613937378, 'learning_rate': 0.0004310648148148148, 'epoch': 0.41388888888888886}
Step 1500: {'loss': 0.8897, 'grad_norm': 1.398033857345581, 'learning_rate': 0.0004306018518518519, 'epoch': 0.4166666666666667}
Step 1510: {'loss': 0.8929, 'grad_norm': 1.1717995405197144, 'learning_rate': 0.0004301388888888889, 'epoch': 0.41944444444444445}
Step 1520: {'loss': 0.9124, 'grad_norm': 1.981594443321228, 'learning_rate': 0.0004296759259259259, 'epoch': 0.4222222222222222}
Step 1530: {'loss': 0.9291, 'grad_norm': 1.1374571323394775, 'learning_rate': 0.00042921296296296295, 'epoch': 0.425}
Step 1540: {'loss': 0.9003, 'grad_norm': 1.585708498954773, 'learning_rate': 0.00042875000000000004, 'epoch': 0.42777777777777776}
Step 1550: {'loss': 0.8528, 'grad_norm': 1.1342116594314575, 'learning_rate': 0.000428287037037037, 'epoch': 0.4305555555555556}
Step 1560: {'loss': 0.9853, 'grad_norm': 1.7251238822937012, 'learning_rate': 0.00042782407407407407, 'epoch': 0.43333333333333335}
Step 1570: {'loss': 0.9369, 'grad_norm': 1.765994668006897, 'learning_rate': 0.00042736111111111116, 'epoch': 0.4361111111111111}
Step 1580: {'loss': 0.9215, 'grad_norm': 1.6346263885498047, 'learning_rate': 0.00042689814814814815, 'epoch': 0.4388888888888889}
Step 1590: {'loss': 0.8719, 'grad_norm': 1.0330134630203247, 'learning_rate': 0.0004264351851851852, 'epoch': 0.44166666666666665}
Step 1600: {'loss': 0.9506, 'grad_norm': 1.182052731513977, 'learning_rate': 0.0004259722222222222, 'epoch': 0.4444444444444444}
Step 1610: {'loss': 0.9009, 'grad_norm': 1.1014529466629028, 'learning_rate': 0.00042550925925925927, 'epoch': 0.44722222222222224}
Step 1620: {'loss': 0.929, 'grad_norm': 1.2583634853363037, 'learning_rate': 0.0004250462962962963, 'epoch': 0.45}
Step 1630: {'loss': 0.9231, 'grad_norm': 1.533836007118225, 'learning_rate': 0.00042458333333333334, 'epoch': 0.4527777777777778}
Step 1640: {'loss': 0.9591, 'grad_norm': 1.0509477853775024, 'learning_rate': 0.0004241203703703704, 'epoch': 0.45555555555555555}
Step 1650: {'loss': 0.849, 'grad_norm': 1.5148029327392578, 'learning_rate': 0.0004236574074074074, 'epoch': 0.4583333333333333}
Step 1660: {'loss': 1.0184, 'grad_norm': 1.5260058641433716, 'learning_rate': 0.00042319444444444446, 'epoch': 0.46111111111111114}
Step 1670: {'loss': 0.8779, 'grad_norm': 1.6831938028335571, 'learning_rate': 0.0004227314814814815, 'epoch': 0.4638888888888889}
Step 1680: {'loss': 0.9211, 'grad_norm': 0.9052222371101379, 'learning_rate': 0.00042226851851851854, 'epoch': 0.4666666666666667}
Step 1690: {'loss': 0.9504, 'grad_norm': 1.55514395236969, 'learning_rate': 0.00042180555555555553, 'epoch': 0.46944444444444444}
Step 1700: {'loss': 0.8822, 'grad_norm': 1.4412517547607422, 'learning_rate': 0.0004213425925925926, 'epoch': 0.4722222222222222}
Step 1710: {'loss': 0.8763, 'grad_norm': 0.9746518135070801, 'learning_rate': 0.0004208796296296296, 'epoch': 0.475}
Step 1720: {'loss': 0.7941, 'grad_norm': 1.3236833810806274, 'learning_rate': 0.00042041666666666665, 'epoch': 0.4777777777777778}
Step 1730: {'loss': 0.9172, 'grad_norm': 1.1028834581375122, 'learning_rate': 0.00041995370370370374, 'epoch': 0.48055555555555557}
Step 1740: {'loss': 0.9185, 'grad_norm': 0.9818318486213684, 'learning_rate': 0.00041949074074074073, 'epoch': 0.48333333333333334}
Step 1750: {'loss': 0.992, 'grad_norm': 1.0776636600494385, 'learning_rate': 0.00041902777777777777, 'epoch': 0.4861111111111111}
Step 1760: {'loss': 0.8959, 'grad_norm': 1.3305788040161133, 'learning_rate': 0.00041856481481481486, 'epoch': 0.4888888888888889}
Step 1770: {'loss': 0.8997, 'grad_norm': 0.9491644501686096, 'learning_rate': 0.00041810185185185185, 'epoch': 0.49166666666666664}
Step 1780: {'loss': 0.8761, 'grad_norm': 1.3107575178146362, 'learning_rate': 0.0004176388888888889, 'epoch': 0.49444444444444446}
Step 1790: {'loss': 0.9116, 'grad_norm': 1.6950206756591797, 'learning_rate': 0.000417175925925926, 'epoch': 0.49722222222222223}
Step 1800: {'loss': 0.8568, 'grad_norm': 1.0794109106063843, 'learning_rate': 0.00041671296296296297, 'epoch': 0.5}
Step 1810: {'loss': 1.014, 'grad_norm': 1.5723203420639038, 'learning_rate': 0.00041625, 'epoch': 0.5027777777777778}
Step 1820: {'loss': 0.8943, 'grad_norm': 1.1039490699768066, 'learning_rate': 0.000415787037037037, 'epoch': 0.5055555555555555}
Step 1830: {'loss': 0.9078, 'grad_norm': 1.1153556108474731, 'learning_rate': 0.0004153240740740741, 'epoch': 0.5083333333333333}
Step 1840: {'loss': 1.0061, 'grad_norm': 1.4117764234542847, 'learning_rate': 0.0004148611111111111, 'epoch': 0.5111111111111111}
Step 1850: {'loss': 0.932, 'grad_norm': 0.9179611206054688, 'learning_rate': 0.0004143981481481481, 'epoch': 0.5138888888888888}
Step 1860: {'loss': 0.8693, 'grad_norm': 1.2009834051132202, 'learning_rate': 0.0004139351851851852, 'epoch': 0.5166666666666667}
Step 1870: {'loss': 0.8829, 'grad_norm': 1.5510133504867554, 'learning_rate': 0.00041347222222222225, 'epoch': 0.5194444444444445}
Step 1880: {'loss': 0.9043, 'grad_norm': 0.9221265912055969, 'learning_rate': 0.00041300925925925923, 'epoch': 0.5222222222222223}
Step 1890: {'loss': 0.9398, 'grad_norm': 1.5053094625473022, 'learning_rate': 0.0004125462962962963, 'epoch': 0.525}
Step 1900: {'loss': 0.9029, 'grad_norm': 1.0479934215545654, 'learning_rate': 0.00041208333333333337, 'epoch': 0.5277777777777778}
Step 1910: {'loss': 0.8556, 'grad_norm': 1.119367241859436, 'learning_rate': 0.00041162037037037035, 'epoch': 0.5305555555555556}
Step 1920: {'loss': 0.8225, 'grad_norm': 1.0118552446365356, 'learning_rate': 0.00041115740740740745, 'epoch': 0.5333333333333333}
Step 1930: {'loss': 0.9172, 'grad_norm': 1.1623077392578125, 'learning_rate': 0.00041069444444444443, 'epoch': 0.5361111111111111}
Step 1940: {'loss': 0.9185, 'grad_norm': 1.8385717868804932, 'learning_rate': 0.00041023148148148147, 'epoch': 0.5388888888888889}
Step 1950: {'loss': 0.8442, 'grad_norm': 1.214744210243225, 'learning_rate': 0.00040976851851851857, 'epoch': 0.5416666666666666}
Step 1960: {'loss': 0.94, 'grad_norm': 1.604249358177185, 'learning_rate': 0.00040930555555555555, 'epoch': 0.5444444444444444}
Step 1970: {'loss': 0.9536, 'grad_norm': 2.203831911087036, 'learning_rate': 0.0004088425925925926, 'epoch': 0.5472222222222223}
Step 1980: {'loss': 0.9877, 'grad_norm': 2.052213191986084, 'learning_rate': 0.00040837962962962963, 'epoch': 0.55}
Step 1990: {'loss': 0.9174, 'grad_norm': 1.5322521924972534, 'learning_rate': 0.00040791666666666667, 'epoch': 0.5527777777777778}
Step 2000: {'loss': 1.0167, 'grad_norm': 1.84324049949646, 'learning_rate': 0.0004074537037037037, 'epoch': 0.5555555555555556}
Step 2010: {'loss': 0.9807, 'grad_norm': 1.4926124811172485, 'learning_rate': 0.00040699074074074075, 'epoch': 0.5583333333333333}
Step 2020: {'loss': 0.888, 'grad_norm': 1.3837333917617798, 'learning_rate': 0.0004065277777777778, 'epoch': 0.5611111111111111}
Step 2030: {'loss': 0.9081, 'grad_norm': 1.2494014501571655, 'learning_rate': 0.00040606481481481483, 'epoch': 0.5638888888888889}
Step 2040: {'loss': 0.99, 'grad_norm': 1.6085480451583862, 'learning_rate': 0.0004056018518518518, 'epoch': 0.5666666666666667}
Step 2050: {'loss': 1.0089, 'grad_norm': 2.121648073196411, 'learning_rate': 0.0004051388888888889, 'epoch': 0.5694444444444444}
Step 2060: {'loss': 0.9788, 'grad_norm': 1.391332983970642, 'learning_rate': 0.00040467592592592595, 'epoch': 0.5722222222222222}
Step 2070: {'loss': 0.939, 'grad_norm': 2.2653305530548096, 'learning_rate': 0.00040421296296296293, 'epoch': 0.575}
Step 2080: {'loss': 0.9252, 'grad_norm': 1.6871365308761597, 'learning_rate': 0.00040375000000000003, 'epoch': 0.5777777777777777}
Step 2090: {'loss': 0.9146, 'grad_norm': 1.334465503692627, 'learning_rate': 0.00040328703703703707, 'epoch': 0.5805555555555556}
Step 2100: {'loss': 0.9112, 'grad_norm': 1.2952301502227783, 'learning_rate': 0.00040282407407407405, 'epoch': 0.5833333333333334}
Step 2110: {'loss': 0.9133, 'grad_norm': 1.541038155555725, 'learning_rate': 0.00040236111111111115, 'epoch': 0.5861111111111111}
Step 2120: {'loss': 0.9707, 'grad_norm': 1.5393123626708984, 'learning_rate': 0.0004018981481481482, 'epoch': 0.5888888888888889}
Step 2130: {'loss': 0.9081, 'grad_norm': 1.2088134288787842, 'learning_rate': 0.0004014351851851852, 'epoch': 0.5916666666666667}
Step 2140: {'loss': 0.8649, 'grad_norm': 1.0310606956481934, 'learning_rate': 0.0004009722222222222, 'epoch': 0.5944444444444444}
Step 2150: {'loss': 0.921, 'grad_norm': 1.148405909538269, 'learning_rate': 0.00040050925925925925, 'epoch': 0.5972222222222222}
Step 2160: {'loss': 0.8445, 'grad_norm': 1.128427267074585, 'learning_rate': 0.0004000462962962963, 'epoch': 0.6}
Step 2170: {'loss': 0.9098, 'grad_norm': 1.487304449081421, 'learning_rate': 0.00039958333333333333, 'epoch': 0.6027777777777777}
Step 2180: {'loss': 0.8735, 'grad_norm': 1.186935544013977, 'learning_rate': 0.0003991203703703704, 'epoch': 0.6055555555555555}
Step 2190: {'loss': 1.0664, 'grad_norm': 1.0467755794525146, 'learning_rate': 0.0003986574074074074, 'epoch': 0.6083333333333333}
Step 2200: {'loss': 0.865, 'grad_norm': 1.4079935550689697, 'learning_rate': 0.00039819444444444445, 'epoch': 0.6111111111111112}
Step 2210: {'loss': 0.8135, 'grad_norm': 1.092393159866333, 'learning_rate': 0.0003977314814814815, 'epoch': 0.6138888888888889}
Step 2220: {'loss': 0.9818, 'grad_norm': 1.5156306028366089, 'learning_rate': 0.00039726851851851853, 'epoch': 0.6166666666666667}
Step 2230: {'loss': 0.9273, 'grad_norm': 4.7936272621154785, 'learning_rate': 0.00039680555555555557, 'epoch': 0.6194444444444445}
Step 2240: {'loss': 0.9172, 'grad_norm': 1.2152390480041504, 'learning_rate': 0.0003963425925925926, 'epoch': 0.6222222222222222}
Step 2250: {'loss': 1.0444, 'grad_norm': 1.347139835357666, 'learning_rate': 0.00039587962962962965, 'epoch': 0.625}
Step 2260: {'loss': 1.0026, 'grad_norm': 0.9573273062705994, 'learning_rate': 0.0003954166666666667, 'epoch': 0.6277777777777778}
Step 2270: {'loss': 1.002, 'grad_norm': 1.2037537097930908, 'learning_rate': 0.0003949537037037037, 'epoch': 0.6305555555555555}
Step 2280: {'loss': 0.9904, 'grad_norm': 1.7512478828430176, 'learning_rate': 0.00039449074074074077, 'epoch': 0.6333333333333333}
Step 2290: {'loss': 1.0852, 'grad_norm': 1.827575445175171, 'learning_rate': 0.00039402777777777776, 'epoch': 0.6361111111111111}
Step 2300: {'loss': 1.0834, 'grad_norm': 1.87510085105896, 'learning_rate': 0.0003935648148148148, 'epoch': 0.6388888888888888}
Step 2310: {'loss': 0.8876, 'grad_norm': 1.5947355031967163, 'learning_rate': 0.0003931018518518519, 'epoch': 0.6416666666666667}
Step 2320: {'loss': 0.9531, 'grad_norm': 1.7945830821990967, 'learning_rate': 0.0003926388888888889, 'epoch': 0.6444444444444445}
Step 2330: {'loss': 0.9092, 'grad_norm': 1.3963285684585571, 'learning_rate': 0.0003921759259259259, 'epoch': 0.6472222222222223}
Step 2340: {'loss': 0.9906, 'grad_norm': 1.2108958959579468, 'learning_rate': 0.000391712962962963, 'epoch': 0.65}
Step 2350: {'loss': 0.8379, 'grad_norm': 1.1617411375045776, 'learning_rate': 0.00039125, 'epoch': 0.6527777777777778}
Step 2360: {'loss': 0.9646, 'grad_norm': 1.5961079597473145, 'learning_rate': 0.00039078703703703704, 'epoch': 0.6555555555555556}
Step 2370: {'loss': 0.994, 'grad_norm': 1.7923732995986938, 'learning_rate': 0.00039032407407407413, 'epoch': 0.6583333333333333}
Step 2380: {'loss': 0.9437, 'grad_norm': 1.6662495136260986, 'learning_rate': 0.0003898611111111111, 'epoch': 0.6611111111111111}
Step 2390: {'loss': 0.9159, 'grad_norm': 1.2894171476364136, 'learning_rate': 0.00038939814814814816, 'epoch': 0.6638888888888889}
Step 2400: {'loss': 0.9693, 'grad_norm': 1.1375550031661987, 'learning_rate': 0.0003889351851851852, 'epoch': 0.6666666666666666}
Step 2410: {'loss': 0.9245, 'grad_norm': 2.0191490650177, 'learning_rate': 0.00038847222222222224, 'epoch': 0.6694444444444444}
Step 2420: {'loss': 0.8536, 'grad_norm': 1.0962167978286743, 'learning_rate': 0.0003880092592592593, 'epoch': 0.6722222222222223}
Step 2430: {'loss': 1.0516, 'grad_norm': 1.9081008434295654, 'learning_rate': 0.00038754629629629626, 'epoch': 0.675}
Step 2440: {'loss': 0.9378, 'grad_norm': 1.431825876235962, 'learning_rate': 0.00038708333333333335, 'epoch': 0.6777777777777778}
Step 2450: {'loss': 0.9408, 'grad_norm': 1.6917767524719238, 'learning_rate': 0.0003866203703703704, 'epoch': 0.6805555555555556}
Step 2460: {'loss': 0.9229, 'grad_norm': 1.3762619495391846, 'learning_rate': 0.0003861574074074074, 'epoch': 0.6833333333333333}
Step 2470: {'loss': 1.0366, 'grad_norm': 2.109375476837158, 'learning_rate': 0.0003856944444444445, 'epoch': 0.6861111111111111}
Step 2480: {'loss': 0.9848, 'grad_norm': 1.5715314149856567, 'learning_rate': 0.0003852314814814815, 'epoch': 0.6888888888888889}
Step 2490: {'loss': 0.9756, 'grad_norm': 1.378267526626587, 'learning_rate': 0.0003847685185185185, 'epoch': 0.6916666666666667}
Step 2500: {'loss': 0.958, 'grad_norm': 2.0039494037628174, 'learning_rate': 0.0003843055555555556, 'epoch': 0.6944444444444444}
Step 2510: {'loss': 0.861, 'grad_norm': 1.161647081375122, 'learning_rate': 0.0003838425925925926, 'epoch': 0.6972222222222222}
Step 2520: {'loss': 0.9047, 'grad_norm': 1.948643445968628, 'learning_rate': 0.0003833796296296296, 'epoch': 0.7}
Step 2530: {'loss': 0.97, 'grad_norm': 1.4551177024841309, 'learning_rate': 0.0003829166666666667, 'epoch': 0.7027777777777777}
Step 2540: {'loss': 0.8931, 'grad_norm': 1.0535025596618652, 'learning_rate': 0.0003824537037037037, 'epoch': 0.7055555555555556}
Step 2550: {'loss': 0.9203, 'grad_norm': 1.4316776990890503, 'learning_rate': 0.00038199074074074074, 'epoch': 0.7083333333333334}
Step 2560: {'loss': 0.9374, 'grad_norm': 1.0402216911315918, 'learning_rate': 0.00038152777777777783, 'epoch': 0.7111111111111111}
Step 2570: {'loss': 1.0137, 'grad_norm': 1.3863556385040283, 'learning_rate': 0.0003810648148148148, 'epoch': 0.7138888888888889}
Step 2580: {'loss': 1.0426, 'grad_norm': 2.582249641418457, 'learning_rate': 0.00038060185185185186, 'epoch': 0.7166666666666667}
Step 2590: {'loss': 0.9492, 'grad_norm': 1.215476393699646, 'learning_rate': 0.0003801388888888889, 'epoch': 0.7194444444444444}
Step 2600: {'loss': 0.9651, 'grad_norm': 1.4967432022094727, 'learning_rate': 0.00037967592592592594, 'epoch': 0.7222222222222222}
Step 2610: {'loss': 0.9032, 'grad_norm': 1.558264970779419, 'learning_rate': 0.000379212962962963, 'epoch': 0.725}
Step 2620: {'loss': 0.9606, 'grad_norm': 1.5519726276397705, 'learning_rate': 0.00037874999999999996, 'epoch': 0.7277777777777777}
Step 2630: {'loss': 0.908, 'grad_norm': 1.4090765714645386, 'learning_rate': 0.00037828703703703706, 'epoch': 0.7305555555555555}
Step 2640: {'loss': 0.9654, 'grad_norm': 1.893864393234253, 'learning_rate': 0.0003778240740740741, 'epoch': 0.7333333333333333}
Step 2650: {'loss': 0.9337, 'grad_norm': 1.4483981132507324, 'learning_rate': 0.0003773611111111111, 'epoch': 0.7361111111111112}
Step 2660: {'loss': 1.1205, 'grad_norm': 2.2089171409606934, 'learning_rate': 0.0003768981481481482, 'epoch': 0.7388888888888889}
Step 2670: {'loss': 0.9288, 'grad_norm': 1.0863878726959229, 'learning_rate': 0.0003764351851851852, 'epoch': 0.7416666666666667}
Step 2680: {'loss': 0.9179, 'grad_norm': 0.9988771677017212, 'learning_rate': 0.0003759722222222222, 'epoch': 0.7444444444444445}
Step 2690: {'loss': 0.9797, 'grad_norm': 2.0022807121276855, 'learning_rate': 0.0003755092592592593, 'epoch': 0.7472222222222222}
Step 2700: {'loss': 0.9499, 'grad_norm': 1.5264451503753662, 'learning_rate': 0.00037504629629629634, 'epoch': 0.75}
Step 2710: {'loss': 0.9097, 'grad_norm': 0.7755150198936462, 'learning_rate': 0.0003745833333333333, 'epoch': 0.7527777777777778}
Step 2720: {'loss': 0.974, 'grad_norm': 1.409143328666687, 'learning_rate': 0.00037412037037037036, 'epoch': 0.7555555555555555}
Step 2730: {'loss': 1.0138, 'grad_norm': 1.227077841758728, 'learning_rate': 0.0003736574074074074, 'epoch': 0.7583333333333333}
Step 2740: {'loss': 0.918, 'grad_norm': 1.159605860710144, 'learning_rate': 0.00037319444444444444, 'epoch': 0.7611111111111111}
Step 2750: {'loss': 0.8999, 'grad_norm': 1.1113250255584717, 'learning_rate': 0.0003727314814814815, 'epoch': 0.7638888888888888}
Step 2760: {'loss': 0.9617, 'grad_norm': 1.3032790422439575, 'learning_rate': 0.0003722685185185185, 'epoch': 0.7666666666666667}
Step 2770: {'loss': 0.8821, 'grad_norm': 1.2794986963272095, 'learning_rate': 0.00037180555555555556, 'epoch': 0.7694444444444445}
Step 2780: {'loss': 0.912, 'grad_norm': 1.2137209177017212, 'learning_rate': 0.0003713425925925926, 'epoch': 0.7722222222222223}
Step 2790: {'loss': 0.9887, 'grad_norm': 2.415536642074585, 'learning_rate': 0.00037087962962962964, 'epoch': 0.775}
Step 2800: {'loss': 0.8814, 'grad_norm': 1.4501575231552124, 'learning_rate': 0.0003704166666666667, 'epoch': 0.7777777777777778}
Step 2810: {'loss': 0.9417, 'grad_norm': 3.92681622505188, 'learning_rate': 0.0003699537037037037, 'epoch': 0.7805555555555556}
Step 2820: {'loss': 0.8784, 'grad_norm': 1.438948154449463, 'learning_rate': 0.00036949074074074076, 'epoch': 0.7833333333333333}
Step 2830: {'loss': 0.908, 'grad_norm': 1.635411024093628, 'learning_rate': 0.0003690277777777778, 'epoch': 0.7861111111111111}
Step 2840: {'loss': 0.9578, 'grad_norm': 1.166778564453125, 'learning_rate': 0.0003685648148148148, 'epoch': 0.7888888888888889}
Step 2850: {'loss': 1.1284, 'grad_norm': 2.0508711338043213, 'learning_rate': 0.0003681018518518519, 'epoch': 0.7916666666666666}
Step 2860: {'loss': 0.958, 'grad_norm': 1.1354678869247437, 'learning_rate': 0.0003676388888888889, 'epoch': 0.7944444444444444}
Step 2870: {'loss': 0.9078, 'grad_norm': 1.2902679443359375, 'learning_rate': 0.0003671759259259259, 'epoch': 0.7972222222222223}
Step 2880: {'loss': 0.966, 'grad_norm': 1.713193416595459, 'learning_rate': 0.00036671296296296295, 'epoch': 0.8}
Step 2890: {'loss': 0.854, 'grad_norm': 1.1317124366760254, 'learning_rate': 0.00036625000000000004, 'epoch': 0.8027777777777778}
Step 2900: {'loss': 1.0801, 'grad_norm': 1.8374457359313965, 'learning_rate': 0.000365787037037037, 'epoch': 0.8055555555555556}
Step 2910: {'loss': 1.0703, 'grad_norm': 1.7839478254318237, 'learning_rate': 0.00036532407407407406, 'epoch': 0.8083333333333333}
Step 2920: {'loss': 1.0248, 'grad_norm': 1.685563564300537, 'learning_rate': 0.00036486111111111116, 'epoch': 0.8111111111111111}
Step 2930: {'loss': 1.0768, 'grad_norm': 1.5103260278701782, 'learning_rate': 0.00036439814814814814, 'epoch': 0.8138888888888889}
Step 2940: {'loss': 0.9991, 'grad_norm': 2.039963483810425, 'learning_rate': 0.0003639351851851852, 'epoch': 0.8166666666666667}
Step 2950: {'loss': 0.9192, 'grad_norm': 1.4806311130523682, 'learning_rate': 0.0003634722222222222, 'epoch': 0.8194444444444444}
Step 2960: {'loss': 0.9844, 'grad_norm': 1.6211509704589844, 'learning_rate': 0.00036300925925925926, 'epoch': 0.8222222222222222}
Step 2970: {'loss': 0.9417, 'grad_norm': 1.2805957794189453, 'learning_rate': 0.0003625462962962963, 'epoch': 0.825}
Step 2980: {'loss': 0.9883, 'grad_norm': 1.5661587715148926, 'learning_rate': 0.00036208333333333334, 'epoch': 0.8277777777777777}
Step 2990: {'loss': 0.9894, 'grad_norm': 1.4443833827972412, 'learning_rate': 0.0003616203703703704, 'epoch': 0.8305555555555556}
Step 3000: {'loss': 1.0386, 'grad_norm': 1.1980096101760864, 'learning_rate': 0.0003611574074074074, 'epoch': 0.8333333333333334}
Step 3010: {'loss': 0.9379, 'grad_norm': 1.608750581741333, 'learning_rate': 0.00036069444444444446, 'epoch': 0.8361111111111111}
Step 3020: {'loss': 1.0007, 'grad_norm': 2.5759146213531494, 'learning_rate': 0.0003602314814814815, 'epoch': 0.8388888888888889}
Step 3030: {'loss': 0.9852, 'grad_norm': 1.1228512525558472, 'learning_rate': 0.00035976851851851854, 'epoch': 0.8416666666666667}
Step 3040: {'loss': 0.9888, 'grad_norm': 1.3492729663848877, 'learning_rate': 0.00035930555555555553, 'epoch': 0.8444444444444444}
Step 3050: {'loss': 0.8907, 'grad_norm': 0.7981839179992676, 'learning_rate': 0.0003588425925925926, 'epoch': 0.8472222222222222}
Step 3060: {'loss': 0.9024, 'grad_norm': 0.8759351372718811, 'learning_rate': 0.0003583796296296296, 'epoch': 0.85}
Step 3070: {'loss': 0.9531, 'grad_norm': 1.2792789936065674, 'learning_rate': 0.00035791666666666665, 'epoch': 0.8527777777777777}
Step 3080: {'loss': 0.9569, 'grad_norm': 1.5829200744628906, 'learning_rate': 0.00035745370370370374, 'epoch': 0.8555555555555555}
Step 3090: {'loss': 0.8971, 'grad_norm': 1.600546956062317, 'learning_rate': 0.00035699074074074073, 'epoch': 0.8583333333333333}
Step 3100: {'loss': 0.8821, 'grad_norm': 1.8910315036773682, 'learning_rate': 0.00035652777777777777, 'epoch': 0.8611111111111112}
Step 3110: {'loss': 0.9656, 'grad_norm': 1.3309696912765503, 'learning_rate': 0.00035606481481481486, 'epoch': 0.8638888888888889}
Step 3120: {'loss': 0.9563, 'grad_norm': 2.462552309036255, 'learning_rate': 0.00035560185185185185, 'epoch': 0.8666666666666667}
Step 3130: {'loss': 0.9766, 'grad_norm': 1.7356185913085938, 'learning_rate': 0.0003551388888888889, 'epoch': 0.8694444444444445}
Step 3140: {'loss': 0.9797, 'grad_norm': 1.3161052465438843, 'learning_rate': 0.000354675925925926, 'epoch': 0.8722222222222222}
Step 3150: {'loss': 1.0086, 'grad_norm': 1.5171304941177368, 'learning_rate': 0.00035421296296296297, 'epoch': 0.875}
Step 3160: {'loss': 0.9261, 'grad_norm': 1.65755033493042, 'learning_rate': 0.00035375, 'epoch': 0.8777777777777778}
Step 3170: {'loss': 0.961, 'grad_norm': 1.1877745389938354, 'learning_rate': 0.000353287037037037, 'epoch': 0.8805555555555555}
Step 3180: {'loss': 0.9273, 'grad_norm': 1.7818045616149902, 'learning_rate': 0.0003528240740740741, 'epoch': 0.8833333333333333}
Step 3190: {'loss': 0.887, 'grad_norm': 2.013075828552246, 'learning_rate': 0.0003523611111111111, 'epoch': 0.8861111111111111}
Step 3200: {'loss': 0.9522, 'grad_norm': 1.4532673358917236, 'learning_rate': 0.0003518981481481481, 'epoch': 0.8888888888888888}
Step 3210: {'loss': 0.9805, 'grad_norm': 1.7587302923202515, 'learning_rate': 0.0003514351851851852, 'epoch': 0.8916666666666667}
Step 3220: {'loss': 0.9321, 'grad_norm': 1.672318696975708, 'learning_rate': 0.00035097222222222225, 'epoch': 0.8944444444444445}
Step 3230: {'loss': 0.9893, 'grad_norm': 0.8746587038040161, 'learning_rate': 0.00035050925925925923, 'epoch': 0.8972222222222223}
Step 3240: {'loss': 1.0143, 'grad_norm': 2.5944600105285645, 'learning_rate': 0.0003500462962962963, 'epoch': 0.9}
Step 3250: {'loss': 1.0016, 'grad_norm': 1.30802321434021, 'learning_rate': 0.00034958333333333336, 'epoch': 0.9027777777777778}
Step 3260: {'loss': 0.9202, 'grad_norm': 1.1526882648468018, 'learning_rate': 0.00034912037037037035, 'epoch': 0.9055555555555556}
Step 3270: {'loss': 0.9759, 'grad_norm': 1.0446209907531738, 'learning_rate': 0.00034865740740740744, 'epoch': 0.9083333333333333}
Step 3280: {'loss': 0.9094, 'grad_norm': 1.1541095972061157, 'learning_rate': 0.00034819444444444443, 'epoch': 0.9111111111111111}
Step 3290: {'loss': 0.9478, 'grad_norm': 1.241509199142456, 'learning_rate': 0.00034773148148148147, 'epoch': 0.9138888888888889}
Step 3300: {'loss': 1.0235, 'grad_norm': 1.053043246269226, 'learning_rate': 0.00034726851851851856, 'epoch': 0.9166666666666666}
Step 3310: {'loss': 1.1284, 'grad_norm': 1.8131718635559082, 'learning_rate': 0.00034680555555555555, 'epoch': 0.9194444444444444}
Step 3320: {'loss': 1.0374, 'grad_norm': 1.8309850692749023, 'learning_rate': 0.0003463425925925926, 'epoch': 0.9222222222222223}
Step 3330: {'loss': 0.9788, 'grad_norm': 1.0991146564483643, 'learning_rate': 0.00034587962962962963, 'epoch': 0.925}
Step 3340: {'loss': 0.9755, 'grad_norm': 1.0787739753723145, 'learning_rate': 0.00034541666666666667, 'epoch': 0.9277777777777778}
Step 3350: {'loss': 1.0444, 'grad_norm': 1.640211582183838, 'learning_rate': 0.0003449537037037037, 'epoch': 0.9305555555555556}
Step 3360: {'loss': 0.9035, 'grad_norm': 1.314778447151184, 'learning_rate': 0.00034449074074074075, 'epoch': 0.9333333333333333}
Step 3370: {'loss': 0.9125, 'grad_norm': 1.1159491539001465, 'learning_rate': 0.0003440277777777778, 'epoch': 0.9361111111111111}
Step 3380: {'loss': 1.0187, 'grad_norm': 1.4778224229812622, 'learning_rate': 0.00034356481481481483, 'epoch': 0.9388888888888889}
Step 3390: {'loss': 0.9792, 'grad_norm': 0.8435239195823669, 'learning_rate': 0.0003431018518518518, 'epoch': 0.9416666666666667}
Step 3400: {'loss': 0.9351, 'grad_norm': 1.639775276184082, 'learning_rate': 0.0003426388888888889, 'epoch': 0.9444444444444444}
Step 3410: {'loss': 0.9932, 'grad_norm': 1.7732213735580444, 'learning_rate': 0.00034217592592592595, 'epoch': 0.9472222222222222}
Step 3420: {'loss': 0.9641, 'grad_norm': 0.8838902711868286, 'learning_rate': 0.00034171296296296293, 'epoch': 0.95}
Step 3430: {'loss': 0.9274, 'grad_norm': 1.0509073734283447, 'learning_rate': 0.00034125000000000003, 'epoch': 0.9527777777777777}
Step 3440: {'loss': 0.9543, 'grad_norm': 1.3919340372085571, 'learning_rate': 0.00034078703703703707, 'epoch': 0.9555555555555556}
Step 3450: {'loss': 1.0148, 'grad_norm': 1.4692615270614624, 'learning_rate': 0.00034032407407407405, 'epoch': 0.9583333333333334}
Step 3460: {'loss': 1.0227, 'grad_norm': 1.4093644618988037, 'learning_rate': 0.00033986111111111115, 'epoch': 0.9611111111111111}
Step 3470: {'loss': 0.9702, 'grad_norm': 2.03787899017334, 'learning_rate': 0.0003393981481481482, 'epoch': 0.9638888888888889}
Step 3480: {'loss': 0.9863, 'grad_norm': 1.43126380443573, 'learning_rate': 0.00033893518518518517, 'epoch': 0.9666666666666667}
Step 3490: {'loss': 0.8335, 'grad_norm': 0.9792706966400146, 'learning_rate': 0.0003384722222222222, 'epoch': 0.9694444444444444}
Step 3500: {'loss': 0.9265, 'grad_norm': 1.3284711837768555, 'learning_rate': 0.00033800925925925925, 'epoch': 0.9722222222222222}
Step 3510: {'loss': 0.9327, 'grad_norm': 1.610673189163208, 'learning_rate': 0.0003375462962962963, 'epoch': 0.975}
Step 3520: {'loss': 0.8915, 'grad_norm': 1.2268388271331787, 'learning_rate': 0.00033708333333333333, 'epoch': 0.9777777777777777}
Step 3530: {'loss': 0.9781, 'grad_norm': 1.2672263383865356, 'learning_rate': 0.00033662037037037037, 'epoch': 0.9805555555555555}
Step 3540: {'loss': 1.0435, 'grad_norm': 1.8639417886734009, 'learning_rate': 0.0003361574074074074, 'epoch': 0.9833333333333333}
Step 3550: {'loss': 0.9588, 'grad_norm': 0.9299412369728088, 'learning_rate': 0.00033569444444444445, 'epoch': 0.9861111111111112}
Step 3560: {'loss': 0.9316, 'grad_norm': 1.3046435117721558, 'learning_rate': 0.0003352314814814815, 'epoch': 0.9888888888888889}
Step 3570: {'loss': 0.9622, 'grad_norm': 1.037905216217041, 'learning_rate': 0.00033476851851851853, 'epoch': 0.9916666666666667}
Step 3580: {'loss': 0.9361, 'grad_norm': 1.6637823581695557, 'learning_rate': 0.00033430555555555557, 'epoch': 0.9944444444444445}
Step 3590: {'loss': 0.9311, 'grad_norm': 1.205613613128662, 'learning_rate': 0.0003338425925925926, 'epoch': 0.9972222222222222}
Step 3600: {'loss': 1.0333, 'grad_norm': 1.3629469871520996, 'learning_rate': 0.00033337962962962965, 'epoch': 1.0}
Step 3610: {'loss': 0.9195, 'grad_norm': 1.203555941581726, 'learning_rate': 0.0003329166666666667, 'epoch': 1.0027777777777778}
Step 3620: {'loss': 0.9168, 'grad_norm': 1.600598931312561, 'learning_rate': 0.0003324537037037037, 'epoch': 1.0055555555555555}
Step 3630: {'loss': 0.8499, 'grad_norm': 2.366262912750244, 'learning_rate': 0.00033199074074074077, 'epoch': 1.0083333333333333}
Step 3640: {'loss': 0.8725, 'grad_norm': 1.610153079032898, 'learning_rate': 0.00033152777777777776, 'epoch': 1.011111111111111}
Step 3650: {'loss': 1.0009, 'grad_norm': 1.4878076314926147, 'learning_rate': 0.0003310648148148148, 'epoch': 1.0138888888888888}
Step 3660: {'loss': 1.0066, 'grad_norm': 1.1845436096191406, 'learning_rate': 0.0003306018518518519, 'epoch': 1.0166666666666666}
Step 3670: {'loss': 0.8712, 'grad_norm': 1.8733636140823364, 'learning_rate': 0.0003301388888888889, 'epoch': 1.0194444444444444}
Step 3680: {'loss': 1.0143, 'grad_norm': 2.378018856048584, 'learning_rate': 0.0003296759259259259, 'epoch': 1.0222222222222221}
Step 3690: {'loss': 1.0239, 'grad_norm': 1.7198752164840698, 'learning_rate': 0.000329212962962963, 'epoch': 1.025}
Step 3700: {'loss': 0.8879, 'grad_norm': 1.562486171722412, 'learning_rate': 0.00032875, 'epoch': 1.0277777777777777}
Step 3710: {'loss': 0.9924, 'grad_norm': 1.61729896068573, 'learning_rate': 0.00032828703703703703, 'epoch': 1.0305555555555554}
Step 3720: {'loss': 0.8676, 'grad_norm': 1.360790491104126, 'learning_rate': 0.00032782407407407413, 'epoch': 1.0333333333333334}
Step 3730: {'loss': 0.9837, 'grad_norm': 1.2908345460891724, 'learning_rate': 0.0003273611111111111, 'epoch': 1.0361111111111112}
Step 3740: {'loss': 0.8743, 'grad_norm': 1.4541208744049072, 'learning_rate': 0.00032689814814814815, 'epoch': 1.038888888888889}
Step 3750: {'loss': 0.8946, 'grad_norm': 1.3550617694854736, 'learning_rate': 0.0003264351851851852, 'epoch': 1.0416666666666667}
Step 3760: {'loss': 0.8605, 'grad_norm': 1.544461727142334, 'learning_rate': 0.00032597222222222223, 'epoch': 1.0444444444444445}
Step 3770: {'loss': 0.9161, 'grad_norm': 2.3040268421173096, 'learning_rate': 0.0003255092592592593, 'epoch': 1.0472222222222223}
Step 3780: {'loss': 0.9596, 'grad_norm': 1.587257742881775, 'learning_rate': 0.00032504629629629626, 'epoch': 1.05}
Step 3790: {'loss': 0.8993, 'grad_norm': 1.5914071798324585, 'learning_rate': 0.00032458333333333335, 'epoch': 1.0527777777777778}
Step 3800: {'loss': 0.923, 'grad_norm': 1.3046116828918457, 'learning_rate': 0.0003241203703703704, 'epoch': 1.0555555555555556}
Step 3810: {'loss': 0.9701, 'grad_norm': 1.452170968055725, 'learning_rate': 0.0003236574074074074, 'epoch': 1.0583333333333333}
Step 3820: {'loss': 0.9617, 'grad_norm': 1.3989070653915405, 'learning_rate': 0.0003231944444444445, 'epoch': 1.0611111111111111}
Step 3830: {'loss': 0.9236, 'grad_norm': 1.3793416023254395, 'learning_rate': 0.0003227314814814815, 'epoch': 1.0638888888888889}
Step 3840: {'loss': 1.0023, 'grad_norm': 1.1202119588851929, 'learning_rate': 0.0003222685185185185, 'epoch': 1.0666666666666667}
Step 3850: {'loss': 1.0176, 'grad_norm': 1.6849702596664429, 'learning_rate': 0.0003218055555555556, 'epoch': 1.0694444444444444}
Step 3860: {'loss': 0.9515, 'grad_norm': 1.0204155445098877, 'learning_rate': 0.0003213425925925926, 'epoch': 1.0722222222222222}
Step 3870: {'loss': 1.0377, 'grad_norm': 1.576777458190918, 'learning_rate': 0.0003208796296296296, 'epoch': 1.075}
Step 3880: {'loss': 0.9482, 'grad_norm': 1.6420130729675293, 'learning_rate': 0.0003204166666666667, 'epoch': 1.0777777777777777}
Step 3890: {'loss': 0.9915, 'grad_norm': 1.3162420988082886, 'learning_rate': 0.0003199537037037037, 'epoch': 1.0805555555555555}
Step 3900: {'loss': 0.9373, 'grad_norm': 1.0137113332748413, 'learning_rate': 0.00031949074074074074, 'epoch': 1.0833333333333333}
Step 3910: {'loss': 0.9901, 'grad_norm': 1.5111714601516724, 'learning_rate': 0.00031902777777777783, 'epoch': 1.086111111111111}
Step 3920: {'loss': 0.9943, 'grad_norm': 0.8561367392539978, 'learning_rate': 0.0003185648148148148, 'epoch': 1.0888888888888888}
Step 3930: {'loss': 1.013, 'grad_norm': 1.5471587181091309, 'learning_rate': 0.00031810185185185186, 'epoch': 1.0916666666666666}
Step 3940: {'loss': 0.9563, 'grad_norm': 1.6353713274002075, 'learning_rate': 0.0003176388888888889, 'epoch': 1.0944444444444446}
Step 3950: {'loss': 0.9917, 'grad_norm': 1.1821298599243164, 'learning_rate': 0.00031717592592592594, 'epoch': 1.0972222222222223}
Step 3960: {'loss': 0.9138, 'grad_norm': 2.2056241035461426, 'learning_rate': 0.000316712962962963, 'epoch': 1.1}
Step 3970: {'loss': 0.9422, 'grad_norm': 1.0878046751022339, 'learning_rate': 0.00031624999999999996, 'epoch': 1.1027777777777779}
Step 3980: {'loss': 0.9384, 'grad_norm': 1.879827618598938, 'learning_rate': 0.00031578703703703706, 'epoch': 1.1055555555555556}
Step 3990: {'loss': 0.9436, 'grad_norm': 2.069315195083618, 'learning_rate': 0.0003153240740740741, 'epoch': 1.1083333333333334}
Step 4000: {'loss': 0.8634, 'grad_norm': 0.982427716255188, 'learning_rate': 0.0003148611111111111, 'epoch': 1.1111111111111112}
Step 4010: {'loss': 0.8673, 'grad_norm': 1.2910419702529907, 'learning_rate': 0.0003143981481481482, 'epoch': 1.113888888888889}
Step 4020: {'loss': 0.8726, 'grad_norm': 1.1548924446105957, 'learning_rate': 0.0003139351851851852, 'epoch': 1.1166666666666667}
Step 4030: {'loss': 0.9086, 'grad_norm': 1.3201498985290527, 'learning_rate': 0.0003134722222222222, 'epoch': 1.1194444444444445}
Step 4040: {'loss': 1.0068, 'grad_norm': 1.2492791414260864, 'learning_rate': 0.0003130092592592593, 'epoch': 1.1222222222222222}
Step 4050: {'loss': 0.884, 'grad_norm': 1.2298113107681274, 'learning_rate': 0.00031254629629629634, 'epoch': 1.125}
Step 4060: {'loss': 0.906, 'grad_norm': 1.2581504583358765, 'learning_rate': 0.0003120833333333333, 'epoch': 1.1277777777777778}
Step 4070: {'loss': 1.0098, 'grad_norm': 1.2786895036697388, 'learning_rate': 0.00031162037037037036, 'epoch': 1.1305555555555555}
Step 4080: {'loss': 0.8989, 'grad_norm': 6.417592525482178, 'learning_rate': 0.0003111574074074074, 'epoch': 1.1333333333333333}
Step 4090: {'loss': 0.9343, 'grad_norm': 1.3112692832946777, 'learning_rate': 0.00031069444444444444, 'epoch': 1.136111111111111}
Step 4100: {'loss': 0.9373, 'grad_norm': 1.1338082551956177, 'learning_rate': 0.0003102314814814815, 'epoch': 1.1388888888888888}
Step 4110: {'loss': 0.9604, 'grad_norm': 1.3855159282684326, 'learning_rate': 0.0003097685185185185, 'epoch': 1.1416666666666666}
Step 4120: {'loss': 0.956, 'grad_norm': 1.4904332160949707, 'learning_rate': 0.00030930555555555556, 'epoch': 1.1444444444444444}
Step 4130: {'loss': 0.9854, 'grad_norm': 1.8204056024551392, 'learning_rate': 0.0003088425925925926, 'epoch': 1.1472222222222221}
Step 4140: {'loss': 0.9408, 'grad_norm': 1.3657851219177246, 'learning_rate': 0.00030837962962962964, 'epoch': 1.15}
Step 4150: {'loss': 0.9222, 'grad_norm': 1.7391228675842285, 'learning_rate': 0.0003079166666666667, 'epoch': 1.1527777777777777}
Step 4160: {'loss': 0.9439, 'grad_norm': 1.4376755952835083, 'learning_rate': 0.0003074537037037037, 'epoch': 1.1555555555555554}
Step 4170: {'loss': 0.9987, 'grad_norm': 1.3748334646224976, 'learning_rate': 0.00030699074074074076, 'epoch': 1.1583333333333332}
Step 4180: {'loss': 0.9848, 'grad_norm': 3.106443405151367, 'learning_rate': 0.0003065277777777778, 'epoch': 1.1611111111111112}
Step 4190: {'loss': 0.9904, 'grad_norm': 1.448082447052002, 'learning_rate': 0.0003060648148148148, 'epoch': 1.163888888888889}
Step 4200: {'loss': 0.8729, 'grad_norm': 1.4089444875717163, 'learning_rate': 0.0003056018518518519, 'epoch': 1.1666666666666667}
Step 4210: {'loss': 0.9661, 'grad_norm': 1.0753211975097656, 'learning_rate': 0.0003051388888888889, 'epoch': 1.1694444444444445}
Step 4220: {'loss': 0.908, 'grad_norm': 1.6062110662460327, 'learning_rate': 0.0003046759259259259, 'epoch': 1.1722222222222223}
Step 4230: {'loss': 0.8846, 'grad_norm': 1.5382766723632812, 'learning_rate': 0.00030421296296296294, 'epoch': 1.175}
Step 4240: {'loss': 0.8385, 'grad_norm': 1.0302313566207886, 'learning_rate': 0.00030375000000000004, 'epoch': 1.1777777777777778}
Step 4250: {'loss': 0.9356, 'grad_norm': 1.2527151107788086, 'learning_rate': 0.000303287037037037, 'epoch': 1.1805555555555556}
Step 4260: {'loss': 0.9056, 'grad_norm': 1.105635643005371, 'learning_rate': 0.00030282407407407406, 'epoch': 1.1833333333333333}
Step 4270: {'loss': 0.8961, 'grad_norm': 1.6630901098251343, 'learning_rate': 0.00030236111111111116, 'epoch': 1.1861111111111111}
Step 4280: {'loss': 0.9129, 'grad_norm': 2.30450439453125, 'learning_rate': 0.00030189814814814814, 'epoch': 1.1888888888888889}
Step 4290: {'loss': 0.8421, 'grad_norm': 1.0337002277374268, 'learning_rate': 0.0003014351851851852, 'epoch': 1.1916666666666667}
Step 4300: {'loss': 0.9613, 'grad_norm': 1.985643744468689, 'learning_rate': 0.0003009722222222222, 'epoch': 1.1944444444444444}
Step 4310: {'loss': 1.0465, 'grad_norm': 1.8907504081726074, 'learning_rate': 0.00030050925925925926, 'epoch': 1.1972222222222222}
Step 4320: {'loss': 1.0031, 'grad_norm': 1.4472100734710693, 'learning_rate': 0.0003000462962962963, 'epoch': 1.2}
Step 4330: {'loss': 0.9187, 'grad_norm': 1.074934720993042, 'learning_rate': 0.00029958333333333334, 'epoch': 1.2027777777777777}
Step 4340: {'loss': 0.9552, 'grad_norm': 1.1459765434265137, 'learning_rate': 0.0002991203703703704, 'epoch': 1.2055555555555555}
Step 4350: {'loss': 0.9987, 'grad_norm': 2.1164402961730957, 'learning_rate': 0.0002986574074074074, 'epoch': 1.2083333333333333}
Step 4360: {'loss': 0.9225, 'grad_norm': 1.0240399837493896, 'learning_rate': 0.00029819444444444446, 'epoch': 1.211111111111111}
Step 4370: {'loss': 0.9044, 'grad_norm': 1.465808629989624, 'learning_rate': 0.0002977314814814815, 'epoch': 1.2138888888888888}
Step 4380: {'loss': 0.9106, 'grad_norm': 1.5746396780014038, 'learning_rate': 0.00029726851851851854, 'epoch': 1.2166666666666668}
Step 4390: {'loss': 0.9986, 'grad_norm': 1.385507345199585, 'learning_rate': 0.0002968055555555555, 'epoch': 1.2194444444444446}
Step 4400: {'loss': 0.9046, 'grad_norm': 1.0875709056854248, 'learning_rate': 0.0002963425925925926, 'epoch': 1.2222222222222223}
Step 4410: {'loss': 1.0178, 'grad_norm': 1.321047067642212, 'learning_rate': 0.0002958796296296296, 'epoch': 1.225}
Step 4420: {'loss': 0.9388, 'grad_norm': 0.9476124048233032, 'learning_rate': 0.00029541666666666665, 'epoch': 1.2277777777777779}
Step 4430: {'loss': 0.9636, 'grad_norm': 1.5306322574615479, 'learning_rate': 0.00029495370370370374, 'epoch': 1.2305555555555556}
Step 4440: {'loss': 0.9766, 'grad_norm': 1.8007301092147827, 'learning_rate': 0.0002944907407407407, 'epoch': 1.2333333333333334}
Step 4450: {'loss': 0.8939, 'grad_norm': 1.3694913387298584, 'learning_rate': 0.00029402777777777777, 'epoch': 1.2361111111111112}
Step 4460: {'loss': 0.9316, 'grad_norm': 1.1915334463119507, 'learning_rate': 0.00029356481481481486, 'epoch': 1.238888888888889}
Step 4470: {'loss': 0.8339, 'grad_norm': 1.5680445432662964, 'learning_rate': 0.00029310185185185185, 'epoch': 1.2416666666666667}
Step 4480: {'loss': 1.0339, 'grad_norm': 1.268444299697876, 'learning_rate': 0.0002926388888888889, 'epoch': 1.2444444444444445}
Step 4490: {'loss': 0.9071, 'grad_norm': 1.7337634563446045, 'learning_rate': 0.000292175925925926, 'epoch': 1.2472222222222222}
Step 4500: {'loss': 0.9772, 'grad_norm': 1.0935019254684448, 'learning_rate': 0.00029171296296296297, 'epoch': 1.25}
Step 4510: {'loss': 0.8885, 'grad_norm': 1.5936228036880493, 'learning_rate': 0.00029125, 'epoch': 1.2527777777777778}
Step 4520: {'loss': 1.0001, 'grad_norm': 1.4821398258209229, 'learning_rate': 0.000290787037037037, 'epoch': 1.2555555555555555}
Step 4530: {'loss': 0.92, 'grad_norm': 1.0332157611846924, 'learning_rate': 0.0002903240740740741, 'epoch': 1.2583333333333333}
Step 4540: {'loss': 0.9426, 'grad_norm': 1.5716139078140259, 'learning_rate': 0.0002898611111111111, 'epoch': 1.261111111111111}
Step 4550: {'loss': 0.938, 'grad_norm': 1.4635236263275146, 'learning_rate': 0.0002893981481481481, 'epoch': 1.2638888888888888}
Step 4560: {'loss': 0.9132, 'grad_norm': 4.007880687713623, 'learning_rate': 0.0002889351851851852, 'epoch': 1.2666666666666666}
Step 4570: {'loss': 0.8981, 'grad_norm': 1.2220442295074463, 'learning_rate': 0.00028847222222222224, 'epoch': 1.2694444444444444}
Step 4580: {'loss': 0.8753, 'grad_norm': 0.8145832419395447, 'learning_rate': 0.00028800925925925923, 'epoch': 1.2722222222222221}
Step 4590: {'loss': 0.8754, 'grad_norm': 2.4319934844970703, 'learning_rate': 0.0002875462962962963, 'epoch': 1.275}
Step 4600: {'loss': 0.875, 'grad_norm': 1.3695976734161377, 'learning_rate': 0.00028708333333333336, 'epoch': 1.2777777777777777}
Step 4610: {'loss': 0.9354, 'grad_norm': 1.2351405620574951, 'learning_rate': 0.00028662037037037035, 'epoch': 1.2805555555555554}
Step 4620: {'loss': 0.8699, 'grad_norm': 1.4522408246994019, 'learning_rate': 0.00028615740740740744, 'epoch': 1.2833333333333332}
Step 4630: {'loss': 0.9237, 'grad_norm': 1.4558013677597046, 'learning_rate': 0.00028569444444444443, 'epoch': 1.286111111111111}
Step 4640: {'loss': 0.9455, 'grad_norm': 1.2106947898864746, 'learning_rate': 0.00028523148148148147, 'epoch': 1.2888888888888888}
Step 4650: {'loss': 0.9662, 'grad_norm': 1.8185670375823975, 'learning_rate': 0.00028476851851851856, 'epoch': 1.2916666666666667}
Step 4660: {'loss': 0.9285, 'grad_norm': 1.1460118293762207, 'learning_rate': 0.00028430555555555555, 'epoch': 1.2944444444444445}
Step 4670: {'loss': 0.9851, 'grad_norm': 1.194729208946228, 'learning_rate': 0.0002838425925925926, 'epoch': 1.2972222222222223}
Step 4680: {'loss': 0.9427, 'grad_norm': 2.1087167263031006, 'learning_rate': 0.00028337962962962963, 'epoch': 1.3}
Step 4690: {'loss': 0.98, 'grad_norm': 1.442001223564148, 'learning_rate': 0.00028291666666666667, 'epoch': 1.3027777777777778}
Step 4700: {'loss': 0.8987, 'grad_norm': 1.0532684326171875, 'learning_rate': 0.0002824537037037037, 'epoch': 1.3055555555555556}
Step 4710: {'loss': 0.8609, 'grad_norm': 0.8291841149330139, 'learning_rate': 0.00028199074074074075, 'epoch': 1.3083333333333333}
Step 4720: {'loss': 0.9139, 'grad_norm': 2.020505666732788, 'learning_rate': 0.0002815277777777778, 'epoch': 1.3111111111111111}
Step 4730: {'loss': 0.9618, 'grad_norm': 1.7107067108154297, 'learning_rate': 0.00028106481481481483, 'epoch': 1.3138888888888889}
Step 4740: {'loss': 0.9097, 'grad_norm': 1.169777512550354, 'learning_rate': 0.0002806018518518518, 'epoch': 1.3166666666666667}
Step 4750: {'loss': 1.0022, 'grad_norm': 3.4602811336517334, 'learning_rate': 0.0002801388888888889, 'epoch': 1.3194444444444444}
Step 4760: {'loss': 0.9709, 'grad_norm': 2.096109390258789, 'learning_rate': 0.00027967592592592595, 'epoch': 1.3222222222222222}
Step 4770: {'loss': 0.9544, 'grad_norm': 1.2382616996765137, 'learning_rate': 0.00027921296296296293, 'epoch': 1.325}
Step 4780: {'loss': 0.9339, 'grad_norm': 1.0960159301757812, 'learning_rate': 0.00027875, 'epoch': 1.3277777777777777}
Step 4790: {'loss': 0.9306, 'grad_norm': 1.0024197101593018, 'learning_rate': 0.00027828703703703707, 'epoch': 1.3305555555555555}
Step 4800: {'loss': 0.9537, 'grad_norm': 1.5622247457504272, 'learning_rate': 0.00027782407407407405, 'epoch': 1.3333333333333333}
Step 4810: {'loss': 0.9943, 'grad_norm': 1.319867730140686, 'learning_rate': 0.00027736111111111115, 'epoch': 1.3361111111111112}
Step 4820: {'loss': 0.9681, 'grad_norm': 1.2890313863754272, 'learning_rate': 0.0002768981481481482, 'epoch': 1.338888888888889}
Step 4830: {'loss': 0.9488, 'grad_norm': 1.2886756658554077, 'learning_rate': 0.00027643518518518517, 'epoch': 1.3416666666666668}
Step 4840: {'loss': 0.971, 'grad_norm': 1.4494050741195679, 'learning_rate': 0.0002759722222222222, 'epoch': 1.3444444444444446}
Step 4850: {'loss': 0.9912, 'grad_norm': 1.5540721416473389, 'learning_rate': 0.00027550925925925925, 'epoch': 1.3472222222222223}
Step 4860: {'loss': 0.9173, 'grad_norm': 1.2468500137329102, 'learning_rate': 0.0002750462962962963, 'epoch': 1.35}
Step 4870: {'loss': 0.9817, 'grad_norm': 1.3072926998138428, 'learning_rate': 0.00027458333333333333, 'epoch': 1.3527777777777779}
Step 4880: {'loss': 1.0049, 'grad_norm': 1.5308045148849487, 'learning_rate': 0.00027412037037037037, 'epoch': 1.3555555555555556}
Step 4890: {'loss': 0.8174, 'grad_norm': 0.9423808455467224, 'learning_rate': 0.0002736574074074074, 'epoch': 1.3583333333333334}
Step 4900: {'loss': 0.986, 'grad_norm': 2.0694520473480225, 'learning_rate': 0.00027319444444444445, 'epoch': 1.3611111111111112}
Step 4910: {'loss': 0.8888, 'grad_norm': 0.7984654307365417, 'learning_rate': 0.0002727314814814815, 'epoch': 1.363888888888889}
Step 4920: {'loss': 0.826, 'grad_norm': 1.2726609706878662, 'learning_rate': 0.00027226851851851853, 'epoch': 1.3666666666666667}
Step 4930: {'loss': 0.9168, 'grad_norm': 1.252550721168518, 'learning_rate': 0.00027180555555555557, 'epoch': 1.3694444444444445}
Step 4940: {'loss': 0.8917, 'grad_norm': 3.140131950378418, 'learning_rate': 0.0002713425925925926, 'epoch': 1.3722222222222222}
Step 4950: {'loss': 0.893, 'grad_norm': 1.0168975591659546, 'learning_rate': 0.00027087962962962965, 'epoch': 1.375}
Step 4960: {'loss': 1.0334, 'grad_norm': 1.3481721878051758, 'learning_rate': 0.0002704166666666667, 'epoch': 1.3777777777777778}
Step 4970: {'loss': 1.026, 'grad_norm': 1.3979642391204834, 'learning_rate': 0.0002699537037037037, 'epoch': 1.3805555555555555}
Step 4980: {'loss': 0.9608, 'grad_norm': 1.434470534324646, 'learning_rate': 0.00026949074074074077, 'epoch': 1.3833333333333333}
Step 4990: {'loss': 0.8566, 'grad_norm': 1.095261573791504, 'learning_rate': 0.00026902777777777775, 'epoch': 1.386111111111111}
Step 5000: {'loss': 0.8943, 'grad_norm': 1.3042017221450806, 'learning_rate': 0.0002685648148148148, 'epoch': 1.3888888888888888}
Step 5010: {'loss': 0.9448, 'grad_norm': 1.3453149795532227, 'learning_rate': 0.0002681018518518519, 'epoch': 1.3916666666666666}
Step 5020: {'loss': 0.9286, 'grad_norm': 1.377098560333252, 'learning_rate': 0.0002676388888888889, 'epoch': 1.3944444444444444}
Step 5030: {'loss': 0.9017, 'grad_norm': 1.4078385829925537, 'learning_rate': 0.0002671759259259259, 'epoch': 1.3972222222222221}
Step 5040: {'loss': 0.9435, 'grad_norm': 0.8615010380744934, 'learning_rate': 0.000266712962962963, 'epoch': 1.4}
Step 5050: {'loss': 0.9549, 'grad_norm': 2.444314956665039, 'learning_rate': 0.00026625, 'epoch': 1.4027777777777777}
Step 5060: {'loss': 0.8974, 'grad_norm': 1.102774977684021, 'learning_rate': 0.00026578703703703703, 'epoch': 1.4055555555555554}
Step 5070: {'loss': 1.0353, 'grad_norm': 2.0028040409088135, 'learning_rate': 0.00026532407407407413, 'epoch': 1.4083333333333332}
Step 5080: {'loss': 0.8952, 'grad_norm': 1.1312028169631958, 'learning_rate': 0.0002648611111111111, 'epoch': 1.411111111111111}
Step 5090: {'loss': 0.965, 'grad_norm': 1.512522578239441, 'learning_rate': 0.00026439814814814815, 'epoch': 1.4138888888888888}
Step 5100: {'loss': 0.8728, 'grad_norm': 1.0391710996627808, 'learning_rate': 0.0002639351851851852, 'epoch': 1.4166666666666667}
Step 5110: {'loss': 0.9345, 'grad_norm': 1.3059970140457153, 'learning_rate': 0.00026347222222222223, 'epoch': 1.4194444444444445}
Step 5120: {'loss': 1.1058, 'grad_norm': 1.4478193521499634, 'learning_rate': 0.00026300925925925927, 'epoch': 1.4222222222222223}
Step 5130: {'loss': 0.8959, 'grad_norm': 1.5037364959716797, 'learning_rate': 0.00026254629629629626, 'epoch': 1.425}
Step 5140: {'loss': 0.9224, 'grad_norm': 1.53831946849823, 'learning_rate': 0.00026208333333333335, 'epoch': 1.4277777777777778}
Step 5150: {'loss': 0.9751, 'grad_norm': 1.728713870048523, 'learning_rate': 0.0002616203703703704, 'epoch': 1.4305555555555556}
Step 5160: {'loss': 0.968, 'grad_norm': 1.6001988649368286, 'learning_rate': 0.0002611574074074074, 'epoch': 1.4333333333333333}
Step 5170: {'loss': 0.9134, 'grad_norm': 1.1050094366073608, 'learning_rate': 0.00026069444444444447, 'epoch': 1.4361111111111111}
Step 5180: {'loss': 0.9546, 'grad_norm': 1.3261932134628296, 'learning_rate': 0.0002602314814814815, 'epoch': 1.4388888888888889}
Step 5190: {'loss': 1.0094, 'grad_norm': 1.377912998199463, 'learning_rate': 0.0002597685185185185, 'epoch': 1.4416666666666667}
Step 5200: {'loss': 1.04, 'grad_norm': 1.7688930034637451, 'learning_rate': 0.0002593055555555556, 'epoch': 1.4444444444444444}
Step 5210: {'loss': 0.874, 'grad_norm': 1.3167616128921509, 'learning_rate': 0.0002588425925925926, 'epoch': 1.4472222222222222}
Step 5220: {'loss': 0.8242, 'grad_norm': 1.2512322664260864, 'learning_rate': 0.0002583796296296296, 'epoch': 1.45}
Step 5230: {'loss': 0.8918, 'grad_norm': 2.597134828567505, 'learning_rate': 0.0002579166666666667, 'epoch': 1.4527777777777777}
Step 5240: {'loss': 0.8988, 'grad_norm': 1.2630707025527954, 'learning_rate': 0.0002574537037037037, 'epoch': 1.4555555555555555}
Step 5250: {'loss': 1.0044, 'grad_norm': 0.7064247727394104, 'learning_rate': 0.00025699074074074074, 'epoch': 1.4583333333333333}
Step 5260: {'loss': 1.039, 'grad_norm': 1.9688260555267334, 'learning_rate': 0.00025652777777777783, 'epoch': 1.4611111111111112}
Step 5270: {'loss': 0.988, 'grad_norm': 1.697064757347107, 'learning_rate': 0.0002560648148148148, 'epoch': 1.463888888888889}
Step 5280: {'loss': 0.9267, 'grad_norm': 1.2345881462097168, 'learning_rate': 0.00025560185185185186, 'epoch': 1.4666666666666668}
Step 5290: {'loss': 0.9229, 'grad_norm': 1.5193477869033813, 'learning_rate': 0.0002551388888888889, 'epoch': 1.4694444444444446}
Step 5300: {'loss': 0.8883, 'grad_norm': 1.348248839378357, 'learning_rate': 0.00025467592592592594, 'epoch': 1.4722222222222223}
Step 5310: {'loss': 0.9086, 'grad_norm': 1.7373894453048706, 'learning_rate': 0.000254212962962963, 'epoch': 1.475}
Step 5320: {'loss': 0.9482, 'grad_norm': 1.397039771080017, 'learning_rate': 0.00025374999999999996, 'epoch': 1.4777777777777779}
Step 5330: {'loss': 0.8861, 'grad_norm': 1.7578262090682983, 'learning_rate': 0.00025328703703703705, 'epoch': 1.4805555555555556}
Step 5340: {'loss': 0.9832, 'grad_norm': 1.246684193611145, 'learning_rate': 0.0002528240740740741, 'epoch': 1.4833333333333334}
Step 5350: {'loss': 0.96, 'grad_norm': 1.6009103059768677, 'learning_rate': 0.0002523611111111111, 'epoch': 1.4861111111111112}
Step 5360: {'loss': 0.9498, 'grad_norm': 1.2957017421722412, 'learning_rate': 0.0002518981481481482, 'epoch': 1.488888888888889}
Step 5370: {'loss': 0.8437, 'grad_norm': 1.4440072774887085, 'learning_rate': 0.0002514351851851852, 'epoch': 1.4916666666666667}
Step 5380: {'loss': 0.9273, 'grad_norm': 2.807109832763672, 'learning_rate': 0.0002509722222222222, 'epoch': 1.4944444444444445}
Step 5390: {'loss': 0.9788, 'grad_norm': 1.4105031490325928, 'learning_rate': 0.0002505092592592593, 'epoch': 1.4972222222222222}
Step 5400: {'loss': 0.8949, 'grad_norm': 1.2305724620819092, 'learning_rate': 0.00025004629629629633, 'epoch': 1.5}
Step 5410: {'loss': 1.041, 'grad_norm': 1.5798976421356201, 'learning_rate': 0.0002495833333333333, 'epoch': 1.5027777777777778}
Step 5420: {'loss': 0.8737, 'grad_norm': 1.0350074768066406, 'learning_rate': 0.00024912037037037036, 'epoch': 1.5055555555555555}
Step 5430: {'loss': 0.9778, 'grad_norm': 1.0048253536224365, 'learning_rate': 0.0002486574074074074, 'epoch': 1.5083333333333333}
Step 5440: {'loss': 0.9006, 'grad_norm': 1.723285436630249, 'learning_rate': 0.00024819444444444444, 'epoch': 1.511111111111111}
Step 5450: {'loss': 0.9194, 'grad_norm': 1.4907618761062622, 'learning_rate': 0.0002477314814814815, 'epoch': 1.5138888888888888}
Step 5460: {'loss': 0.983, 'grad_norm': 1.3178961277008057, 'learning_rate': 0.0002472685185185185, 'epoch': 1.5166666666666666}
Step 5470: {'loss': 0.9245, 'grad_norm': 1.4969336986541748, 'learning_rate': 0.00024680555555555556, 'epoch': 1.5194444444444444}
Step 5480: {'loss': 1.0485, 'grad_norm': 1.9864797592163086, 'learning_rate': 0.0002463425925925926, 'epoch': 1.5222222222222221}
Step 5490: {'loss': 0.9857, 'grad_norm': 1.2876378297805786, 'learning_rate': 0.00024587962962962964, 'epoch': 1.525}
Step 5500: {'loss': 0.9935, 'grad_norm': 0.978484570980072, 'learning_rate': 0.0002454166666666667, 'epoch': 1.5277777777777777}
Step 5510: {'loss': 1.0048, 'grad_norm': 1.263780117034912, 'learning_rate': 0.0002449537037037037, 'epoch': 1.5305555555555554}
Step 5520: {'loss': 0.9134, 'grad_norm': 1.3879808187484741, 'learning_rate': 0.00024449074074074076, 'epoch': 1.5333333333333332}
Step 5530: {'loss': 0.8826, 'grad_norm': 1.515508770942688, 'learning_rate': 0.0002440277777777778, 'epoch': 1.536111111111111}
Step 5540: {'loss': 0.8413, 'grad_norm': 1.2846643924713135, 'learning_rate': 0.0002435648148148148, 'epoch': 1.5388888888888888}
Step 5550: {'loss': 0.9512, 'grad_norm': 1.1312264204025269, 'learning_rate': 0.00024310185185185185, 'epoch': 1.5416666666666665}
Step 5560: {'loss': 0.8891, 'grad_norm': 1.3606398105621338, 'learning_rate': 0.0002426388888888889, 'epoch': 1.5444444444444443}
Step 5570: {'loss': 0.9808, 'grad_norm': 1.4077246189117432, 'learning_rate': 0.00024217592592592593, 'epoch': 1.5472222222222223}
Step 5580: {'loss': 0.9812, 'grad_norm': 1.622786283493042, 'learning_rate': 0.00024171296296296297, 'epoch': 1.55}
Step 5590: {'loss': 0.962, 'grad_norm': 1.544255018234253, 'learning_rate': 0.00024125, 'epoch': 1.5527777777777778}
Step 5600: {'loss': 0.8818, 'grad_norm': 1.256593108177185, 'learning_rate': 0.00024078703703703705, 'epoch': 1.5555555555555556}
Step 5610: {'loss': 0.8964, 'grad_norm': 1.4219839572906494, 'learning_rate': 0.0002403240740740741, 'epoch': 1.5583333333333333}
Step 5620: {'loss': 0.965, 'grad_norm': 1.487182855606079, 'learning_rate': 0.0002398611111111111, 'epoch': 1.5611111111111111}
Step 5630: {'loss': 0.9334, 'grad_norm': 4.4092583656311035, 'learning_rate': 0.00023939814814814814, 'epoch': 1.5638888888888889}
Step 5640: {'loss': 0.8853, 'grad_norm': 1.147076964378357, 'learning_rate': 0.0002389351851851852, 'epoch': 1.5666666666666667}
Step 5650: {'loss': 0.9143, 'grad_norm': 1.2266490459442139, 'learning_rate': 0.00023847222222222222, 'epoch': 1.5694444444444444}
Step 5660: {'loss': 0.9697, 'grad_norm': 1.1288560628890991, 'learning_rate': 0.00023800925925925926, 'epoch': 1.5722222222222222}
Step 5670: {'loss': 0.8945, 'grad_norm': 1.3949719667434692, 'learning_rate': 0.0002375462962962963, 'epoch': 1.575}
Step 5680: {'loss': 0.9787, 'grad_norm': 1.5597429275512695, 'learning_rate': 0.00023708333333333334, 'epoch': 1.5777777777777777}
Step 5690: {'loss': 0.9073, 'grad_norm': 1.6086645126342773, 'learning_rate': 0.00023662037037037038, 'epoch': 1.5805555555555557}
Step 5700: {'loss': 0.886, 'grad_norm': 1.4088759422302246, 'learning_rate': 0.0002361574074074074, 'epoch': 1.5833333333333335}
Step 5710: {'loss': 0.9514, 'grad_norm': 1.9636541604995728, 'learning_rate': 0.00023569444444444446, 'epoch': 1.5861111111111112}
Step 5720: {'loss': 0.8902, 'grad_norm': 2.536822557449341, 'learning_rate': 0.0002352314814814815, 'epoch': 1.588888888888889}
Step 5730: {'loss': 1.0306, 'grad_norm': 2.131769895553589, 'learning_rate': 0.0002347685185185185, 'epoch': 1.5916666666666668}
Step 5740: {'loss': 0.9188, 'grad_norm': 1.5068188905715942, 'learning_rate': 0.00023430555555555555, 'epoch': 1.5944444444444446}
Step 5750: {'loss': 0.9117, 'grad_norm': 1.3405847549438477, 'learning_rate': 0.00023384259259259262, 'epoch': 1.5972222222222223}
Step 5760: {'loss': 0.9088, 'grad_norm': 1.059118390083313, 'learning_rate': 0.00023337962962962963, 'epoch': 1.6}
Step 5770: {'loss': 0.8922, 'grad_norm': 1.4384355545043945, 'learning_rate': 0.00023291666666666667, 'epoch': 1.6027777777777779}
Step 5780: {'loss': 0.9163, 'grad_norm': 1.2254143953323364, 'learning_rate': 0.00023245370370370368, 'epoch': 1.6055555555555556}
Step 5790: {'loss': 0.8428, 'grad_norm': 0.9638590216636658, 'learning_rate': 0.00023199074074074075, 'epoch': 1.6083333333333334}
Step 5800: {'loss': 0.9444, 'grad_norm': 1.3418869972229004, 'learning_rate': 0.0002315277777777778, 'epoch': 1.6111111111111112}
Step 5810: {'loss': 0.9297, 'grad_norm': 1.7190661430358887, 'learning_rate': 0.0002310648148148148, 'epoch': 1.613888888888889}
Step 5820: {'loss': 0.989, 'grad_norm': 1.7611218690872192, 'learning_rate': 0.00023060185185185187, 'epoch': 1.6166666666666667}
Step 5830: {'loss': 0.9704, 'grad_norm': 1.4493263959884644, 'learning_rate': 0.00023013888888888888, 'epoch': 1.6194444444444445}
Step 5840: {'loss': 0.9188, 'grad_norm': 1.4728554487228394, 'learning_rate': 0.00022967592592592592, 'epoch': 1.6222222222222222}
Step 5850: {'loss': 0.7817, 'grad_norm': 1.3376121520996094, 'learning_rate': 0.00022921296296296296, 'epoch': 1.625}
Step 5860: {'loss': 1.0526, 'grad_norm': 1.1123783588409424, 'learning_rate': 0.00022875, 'epoch': 1.6277777777777778}
Step 5870: {'loss': 0.8839, 'grad_norm': 1.3542922735214233, 'learning_rate': 0.00022828703703703704, 'epoch': 1.6305555555555555}
Step 5880: {'loss': 0.9672, 'grad_norm': 1.534995675086975, 'learning_rate': 0.00022782407407407408, 'epoch': 1.6333333333333333}
Step 5890: {'loss': 0.9581, 'grad_norm': 1.1104559898376465, 'learning_rate': 0.00022736111111111112, 'epoch': 1.636111111111111}
Step 5900: {'loss': 0.9743, 'grad_norm': 2.0436251163482666, 'learning_rate': 0.00022689814814814816, 'epoch': 1.6388888888888888}
Step 5910: {'loss': 0.9457, 'grad_norm': 1.4256170988082886, 'learning_rate': 0.00022643518518518518, 'epoch': 1.6416666666666666}
Step 5920: {'loss': 0.8773, 'grad_norm': 1.079181432723999, 'learning_rate': 0.00022597222222222222, 'epoch': 1.6444444444444444}
Step 5930: {'loss': 0.9065, 'grad_norm': 1.2517427206039429, 'learning_rate': 0.00022550925925925928, 'epoch': 1.6472222222222221}
Step 5940: {'loss': 0.9671, 'grad_norm': 1.8916573524475098, 'learning_rate': 0.0002250462962962963, 'epoch': 1.65}
Step 5950: {'loss': 0.954, 'grad_norm': 0.891333818435669, 'learning_rate': 0.00022458333333333334, 'epoch': 1.6527777777777777}
Step 5960: {'loss': 0.9706, 'grad_norm': 1.3169227838516235, 'learning_rate': 0.00022412037037037037, 'epoch': 1.6555555555555554}
Step 5970: {'loss': 0.8645, 'grad_norm': 1.0794997215270996, 'learning_rate': 0.00022365740740740741, 'epoch': 1.6583333333333332}
Step 5980: {'loss': 0.9051, 'grad_norm': 1.4701178073883057, 'learning_rate': 0.00022319444444444445, 'epoch': 1.661111111111111}
Step 5990: {'loss': 0.9628, 'grad_norm': 1.9778972864151, 'learning_rate': 0.00022273148148148147, 'epoch': 1.6638888888888888}
Step 6000: {'loss': 0.9336, 'grad_norm': 1.2548335790634155, 'learning_rate': 0.00022226851851851853, 'epoch': 1.6666666666666665}
Step 6010: {'loss': 0.8965, 'grad_norm': 1.8300693035125732, 'learning_rate': 0.00022180555555555557, 'epoch': 1.6694444444444443}
Step 6020: {'loss': 0.9398, 'grad_norm': 1.6763546466827393, 'learning_rate': 0.0002213425925925926, 'epoch': 1.6722222222222223}
Step 6030: {'loss': 0.9658, 'grad_norm': 1.2353123426437378, 'learning_rate': 0.00022087962962962963, 'epoch': 1.675}
Step 6040: {'loss': 0.9988, 'grad_norm': 1.9756799936294556, 'learning_rate': 0.0002204166666666667, 'epoch': 1.6777777777777778}
Step 6050: {'loss': 0.9215, 'grad_norm': 1.181517481803894, 'learning_rate': 0.0002199537037037037, 'epoch': 1.6805555555555556}
Step 6060: {'loss': 0.9611, 'grad_norm': 0.9082151055335999, 'learning_rate': 0.00021949074074074075, 'epoch': 1.6833333333333333}
Step 6070: {'loss': 0.8543, 'grad_norm': 1.6792997121810913, 'learning_rate': 0.00021902777777777776, 'epoch': 1.6861111111111111}
Step 6080: {'loss': 0.9081, 'grad_norm': 1.1373577117919922, 'learning_rate': 0.00021856481481481483, 'epoch': 1.6888888888888889}
Step 6090: {'loss': 0.9031, 'grad_norm': 1.8097095489501953, 'learning_rate': 0.00021810185185185187, 'epoch': 1.6916666666666667}
Step 6100: {'loss': 0.9206, 'grad_norm': 1.371770977973938, 'learning_rate': 0.00021763888888888888, 'epoch': 1.6944444444444444}
Step 6110: {'loss': 1.046, 'grad_norm': 1.1581532955169678, 'learning_rate': 0.00021717592592592595, 'epoch': 1.6972222222222222}
Step 6120: {'loss': 0.8178, 'grad_norm': 1.0441398620605469, 'learning_rate': 0.00021671296296296299, 'epoch': 1.7}
Step 6130: {'loss': 0.9197, 'grad_norm': 1.4812886714935303, 'learning_rate': 0.00021625, 'epoch': 1.7027777777777777}
Step 6140: {'loss': 0.9746, 'grad_norm': 1.2000300884246826, 'learning_rate': 0.00021578703703703704, 'epoch': 1.7055555555555557}
Step 6150: {'loss': 0.9666, 'grad_norm': 3.6854536533355713, 'learning_rate': 0.00021532407407407408, 'epoch': 1.7083333333333335}
Step 6160: {'loss': 0.9349, 'grad_norm': 1.8215992450714111, 'learning_rate': 0.00021486111111111112, 'epoch': 1.7111111111111112}
Step 6170: {'loss': 0.8935, 'grad_norm': 1.8687211275100708, 'learning_rate': 0.00021439814814814816, 'epoch': 1.713888888888889}
Step 6180: {'loss': 0.8927, 'grad_norm': 0.8745041489601135, 'learning_rate': 0.00021393518518518517, 'epoch': 1.7166666666666668}
Step 6190: {'loss': 0.9941, 'grad_norm': 1.1560276746749878, 'learning_rate': 0.00021347222222222224, 'epoch': 1.7194444444444446}
Step 6200: {'loss': 0.9166, 'grad_norm': 1.0057629346847534, 'learning_rate': 0.00021300925925925928, 'epoch': 1.7222222222222223}
Step 6210: {'loss': 0.8886, 'grad_norm': 1.6559020280838013, 'learning_rate': 0.0002125462962962963, 'epoch': 1.725}
Step 6220: {'loss': 0.9778, 'grad_norm': 2.2627317905426025, 'learning_rate': 0.00021208333333333336, 'epoch': 1.7277777777777779}
Step 6230: {'loss': 0.9934, 'grad_norm': 1.5722359418869019, 'learning_rate': 0.00021162037037037037, 'epoch': 1.7305555555555556}
Step 6240: {'loss': 0.8756, 'grad_norm': 0.8160022497177124, 'learning_rate': 0.0002111574074074074, 'epoch': 1.7333333333333334}
Step 6250: {'loss': 0.9761, 'grad_norm': 1.2374953031539917, 'learning_rate': 0.00021069444444444445, 'epoch': 1.7361111111111112}
Step 6260: {'loss': 0.9734, 'grad_norm': 1.279855728149414, 'learning_rate': 0.0002102314814814815, 'epoch': 1.738888888888889}
Step 6270: {'loss': 0.8517, 'grad_norm': 1.2282047271728516, 'learning_rate': 0.00020976851851851853, 'epoch': 1.7416666666666667}
Step 6280: {'loss': 0.9808, 'grad_norm': 1.574471116065979, 'learning_rate': 0.00020930555555555554, 'epoch': 1.7444444444444445}
Step 6290: {'loss': 0.8591, 'grad_norm': 2.3490941524505615, 'learning_rate': 0.00020884259259259258, 'epoch': 1.7472222222222222}
Step 6300: {'loss': 0.9933, 'grad_norm': 2.040919303894043, 'learning_rate': 0.00020837962962962965, 'epoch': 1.75}
Step 6310: {'loss': 0.9409, 'grad_norm': 1.034289836883545, 'learning_rate': 0.00020791666666666666, 'epoch': 1.7527777777777778}
Step 6320: {'loss': 0.8941, 'grad_norm': 1.7589174509048462, 'learning_rate': 0.0002074537037037037, 'epoch': 1.7555555555555555}
Step 6330: {'loss': 0.9459, 'grad_norm': 1.8139337301254272, 'learning_rate': 0.00020699074074074077, 'epoch': 1.7583333333333333}
Step 6340: {'loss': 0.9086, 'grad_norm': 1.629348635673523, 'learning_rate': 0.00020652777777777778, 'epoch': 1.761111111111111}
Step 6350: {'loss': 0.8452, 'grad_norm': 1.8587937355041504, 'learning_rate': 0.00020606481481481482, 'epoch': 1.7638888888888888}
Step 6360: {'loss': 0.9632, 'grad_norm': 1.4398118257522583, 'learning_rate': 0.00020560185185185183, 'epoch': 1.7666666666666666}
Step 6370: {'loss': 0.9997, 'grad_norm': 1.316418170928955, 'learning_rate': 0.0002051388888888889, 'epoch': 1.7694444444444444}
Step 6380: {'loss': 0.9577, 'grad_norm': 1.3677928447723389, 'learning_rate': 0.00020467592592592594, 'epoch': 1.7722222222222221}
Step 6390: {'loss': 0.9983, 'grad_norm': 1.575919508934021, 'learning_rate': 0.00020421296296296295, 'epoch': 1.775}
Step 6400: {'loss': 0.9059, 'grad_norm': 1.7637840509414673, 'learning_rate': 0.00020375, 'epoch': 1.7777777777777777}
Step 6410: {'loss': 0.9719, 'grad_norm': 1.6261390447616577, 'learning_rate': 0.00020328703703703706, 'epoch': 1.7805555555555554}
Step 6420: {'loss': 0.8932, 'grad_norm': 1.6572819948196411, 'learning_rate': 0.00020282407407407407, 'epoch': 1.7833333333333332}
Step 6430: {'loss': 0.9113, 'grad_norm': 1.6813769340515137, 'learning_rate': 0.0002023611111111111, 'epoch': 1.786111111111111}
Step 6440: {'loss': 0.9289, 'grad_norm': 1.4245966672897339, 'learning_rate': 0.00020189814814814815, 'epoch': 1.7888888888888888}
Step 6450: {'loss': 0.9346, 'grad_norm': 1.15824556350708, 'learning_rate': 0.0002014351851851852, 'epoch': 1.7916666666666665}
Step 6460: {'loss': 0.9589, 'grad_norm': 2.236802577972412, 'learning_rate': 0.00020097222222222223, 'epoch': 1.7944444444444443}
Step 6470: {'loss': 0.898, 'grad_norm': 1.3638439178466797, 'learning_rate': 0.00020050925925925924, 'epoch': 1.7972222222222223}
Step 6480: {'loss': 0.9089, 'grad_norm': 1.4347141981124878, 'learning_rate': 0.0002000462962962963, 'epoch': 1.8}
Step 6490: {'loss': 1.0779, 'grad_norm': 1.3294563293457031, 'learning_rate': 0.00019958333333333335, 'epoch': 1.8027777777777778}
Step 6500: {'loss': 0.9788, 'grad_norm': 5.057348728179932, 'learning_rate': 0.00019912037037037036, 'epoch': 1.8055555555555556}
Step 6510: {'loss': 0.8415, 'grad_norm': 1.1680591106414795, 'learning_rate': 0.0001986574074074074, 'epoch': 1.8083333333333333}
Step 6520: {'loss': 0.9442, 'grad_norm': 1.358420491218567, 'learning_rate': 0.00019819444444444444, 'epoch': 1.8111111111111111}
Step 6530: {'loss': 0.9485, 'grad_norm': 0.99285489320755, 'learning_rate': 0.00019773148148148148, 'epoch': 1.8138888888888889}
Step 6540: {'loss': 0.9401, 'grad_norm': 1.1726890802383423, 'learning_rate': 0.00019726851851851852, 'epoch': 1.8166666666666667}
Step 6550: {'loss': 0.9494, 'grad_norm': 1.9068355560302734, 'learning_rate': 0.00019680555555555556, 'epoch': 1.8194444444444444}
Step 6560: {'loss': 0.9114, 'grad_norm': 1.820510745048523, 'learning_rate': 0.0001963425925925926, 'epoch': 1.8222222222222222}
Step 6570: {'loss': 0.9844, 'grad_norm': 2.3119640350341797, 'learning_rate': 0.00019587962962962964, 'epoch': 1.825}
Step 6580: {'loss': 0.9601, 'grad_norm': 0.9709125757217407, 'learning_rate': 0.00019541666666666666, 'epoch': 1.8277777777777777}
Step 6590: {'loss': 0.9115, 'grad_norm': 1.1395502090454102, 'learning_rate': 0.00019495370370370372, 'epoch': 1.8305555555555557}
Step 6600: {'loss': 0.9936, 'grad_norm': 1.1984353065490723, 'learning_rate': 0.00019449074074074073, 'epoch': 1.8333333333333335}
Step 6610: {'loss': 0.9205, 'grad_norm': 1.7422553300857544, 'learning_rate': 0.00019402777777777777, 'epoch': 1.8361111111111112}
Step 6620: {'loss': 0.8706, 'grad_norm': 1.2797788381576538, 'learning_rate': 0.00019356481481481484, 'epoch': 1.838888888888889}
Step 6630: {'loss': 0.9597, 'grad_norm': 1.6122286319732666, 'learning_rate': 0.00019310185185185185, 'epoch': 1.8416666666666668}
Step 6640: {'loss': 0.9429, 'grad_norm': 1.3270782232284546, 'learning_rate': 0.0001926388888888889, 'epoch': 1.8444444444444446}
Step 6650: {'loss': 0.9085, 'grad_norm': 1.6065047979354858, 'learning_rate': 0.00019217592592592593, 'epoch': 1.8472222222222223}
Step 6660: {'loss': 0.9517, 'grad_norm': 1.2782334089279175, 'learning_rate': 0.00019171296296296297, 'epoch': 1.85}
Step 6670: {'loss': 0.9758, 'grad_norm': 1.3576416969299316, 'learning_rate': 0.00019125000000000001, 'epoch': 1.8527777777777779}
Step 6680: {'loss': 0.8168, 'grad_norm': 0.8864341974258423, 'learning_rate': 0.00019078703703703703, 'epoch': 1.8555555555555556}
Step 6690: {'loss': 1.0056, 'grad_norm': 1.8151633739471436, 'learning_rate': 0.00019032407407407407, 'epoch': 1.8583333333333334}
Step 6700: {'loss': 0.9075, 'grad_norm': 1.4235275983810425, 'learning_rate': 0.00018986111111111113, 'epoch': 1.8611111111111112}
Step 6710: {'loss': 0.9467, 'grad_norm': 1.7337113618850708, 'learning_rate': 0.00018939814814814815, 'epoch': 1.863888888888889}
Step 6720: {'loss': 0.9286, 'grad_norm': 1.403464913368225, 'learning_rate': 0.00018893518518518519, 'epoch': 1.8666666666666667}
Step 6730: {'loss': 0.9526, 'grad_norm': 1.2560675144195557, 'learning_rate': 0.00018847222222222225, 'epoch': 1.8694444444444445}
Step 6740: {'loss': 0.915, 'grad_norm': 2.4781453609466553, 'learning_rate': 0.00018800925925925927, 'epoch': 1.8722222222222222}
Step 6750: {'loss': 0.9328, 'grad_norm': 1.2554826736450195, 'learning_rate': 0.0001875462962962963, 'epoch': 1.875}
Step 6760: {'loss': 0.934, 'grad_norm': 1.586047649383545, 'learning_rate': 0.00018708333333333332, 'epoch': 1.8777777777777778}
Step 6770: {'loss': 1.1161, 'grad_norm': 1.9280660152435303, 'learning_rate': 0.00018662037037037039, 'epoch': 1.8805555555555555}
Step 6780: {'loss': 0.9283, 'grad_norm': 1.093762993812561, 'learning_rate': 0.00018615740740740742, 'epoch': 1.8833333333333333}
Step 6790: {'loss': 1.0187, 'grad_norm': 1.4158403873443604, 'learning_rate': 0.00018569444444444444, 'epoch': 1.886111111111111}
Step 6800: {'loss': 0.9735, 'grad_norm': 1.715380072593689, 'learning_rate': 0.00018523148148148148, 'epoch': 1.8888888888888888}
Step 6810: {'loss': 0.9495, 'grad_norm': 1.4719117879867554, 'learning_rate': 0.00018476851851851852, 'epoch': 1.8916666666666666}
Step 6820: {'loss': 0.8945, 'grad_norm': 1.2146214246749878, 'learning_rate': 0.00018430555555555556, 'epoch': 1.8944444444444444}
Step 6830: {'loss': 0.9594, 'grad_norm': 1.516519546508789, 'learning_rate': 0.0001838425925925926, 'epoch': 1.8972222222222221}
Step 6840: {'loss': 0.8344, 'grad_norm': 1.3733536005020142, 'learning_rate': 0.00018337962962962964, 'epoch': 1.9}
Step 6850: {'loss': 0.9652, 'grad_norm': 1.5006849765777588, 'learning_rate': 0.00018291666666666668, 'epoch': 1.9027777777777777}
Step 6860: {'loss': 0.9256, 'grad_norm': 1.5700833797454834, 'learning_rate': 0.00018245370370370372, 'epoch': 1.9055555555555554}
Step 6870: {'loss': 0.9822, 'grad_norm': 2.2767159938812256, 'learning_rate': 0.00018199074074074073, 'epoch': 1.9083333333333332}
Step 6880: {'loss': 0.9215, 'grad_norm': 1.8161687850952148, 'learning_rate': 0.0001815277777777778, 'epoch': 1.911111111111111}
Step 6890: {'loss': 0.9624, 'grad_norm': 1.3605189323425293, 'learning_rate': 0.0001810648148148148, 'epoch': 1.9138888888888888}
Step 6900: {'loss': 1.0133, 'grad_norm': 1.7117815017700195, 'learning_rate': 0.00018060185185185185, 'epoch': 1.9166666666666665}
Step 6910: {'loss': 0.9781, 'grad_norm': 1.7238575220108032, 'learning_rate': 0.0001801388888888889, 'epoch': 1.9194444444444443}
Step 6920: {'loss': 0.95, 'grad_norm': 1.2633343935012817, 'learning_rate': 0.00017967592592592593, 'epoch': 1.9222222222222223}
Step 6930: {'loss': 0.9522, 'grad_norm': 1.2317317724227905, 'learning_rate': 0.00017921296296296297, 'epoch': 1.925}
Step 6940: {'loss': 0.9588, 'grad_norm': 1.1122183799743652, 'learning_rate': 0.00017875, 'epoch': 1.9277777777777778}
Step 6950: {'loss': 0.962, 'grad_norm': 1.3761215209960938, 'learning_rate': 0.00017828703703703705, 'epoch': 1.9305555555555556}
Step 6960: {'loss': 0.8972, 'grad_norm': 1.2222563028335571, 'learning_rate': 0.0001778240740740741, 'epoch': 1.9333333333333333}
Step 6970: {'loss': 0.9837, 'grad_norm': 1.3194222450256348, 'learning_rate': 0.0001773611111111111, 'epoch': 1.9361111111111111}
Step 6980: {'loss': 0.9461, 'grad_norm': 1.1624988317489624, 'learning_rate': 0.00017689814814814814, 'epoch': 1.9388888888888889}
Step 6990: {'loss': 0.951, 'grad_norm': 1.853593111038208, 'learning_rate': 0.0001764351851851852, 'epoch': 1.9416666666666667}
Step 7000: {'loss': 0.9394, 'grad_norm': 2.4312195777893066, 'learning_rate': 0.00017597222222222222, 'epoch': 1.9444444444444444}
Step 7010: {'loss': 1.0109, 'grad_norm': 1.9934722185134888, 'learning_rate': 0.00017550925925925926, 'epoch': 1.9472222222222222}
Step 7020: {'loss': 0.937, 'grad_norm': 1.1242152452468872, 'learning_rate': 0.0001750462962962963, 'epoch': 1.95}
Step 7030: {'loss': 0.9635, 'grad_norm': 1.685095191001892, 'learning_rate': 0.00017458333333333334, 'epoch': 1.9527777777777777}
Step 7040: {'loss': 0.9589, 'grad_norm': 1.5207562446594238, 'learning_rate': 0.00017412037037037038, 'epoch': 1.9555555555555557}
Step 7050: {'loss': 0.906, 'grad_norm': 1.1054476499557495, 'learning_rate': 0.0001736574074074074, 'epoch': 1.9583333333333335}
Step 7060: {'loss': 0.9312, 'grad_norm': 1.108997106552124, 'learning_rate': 0.00017319444444444446, 'epoch': 1.9611111111111112}
Step 7070: {'loss': 0.8684, 'grad_norm': 1.1059558391571045, 'learning_rate': 0.0001727314814814815, 'epoch': 1.963888888888889}
Step 7080: {'loss': 0.9168, 'grad_norm': 1.187538743019104, 'learning_rate': 0.0001722685185185185, 'epoch': 1.9666666666666668}
Step 7090: {'loss': 0.9922, 'grad_norm': 1.3543130159378052, 'learning_rate': 0.00017180555555555555, 'epoch': 1.9694444444444446}
Step 7100: {'loss': 0.9836, 'grad_norm': 1.7785357236862183, 'learning_rate': 0.00017134259259259262, 'epoch': 1.9722222222222223}
Step 7110: {'loss': 0.906, 'grad_norm': 1.218937635421753, 'learning_rate': 0.00017087962962962963, 'epoch': 1.975}
Step 7120: {'loss': 1.0226, 'grad_norm': 2.2780752182006836, 'learning_rate': 0.00017041666666666667, 'epoch': 1.9777777777777779}
Step 7130: {'loss': 1.0026, 'grad_norm': 1.708168864250183, 'learning_rate': 0.00016995370370370368, 'epoch': 1.9805555555555556}
Step 7140: {'loss': 0.8632, 'grad_norm': 1.2138097286224365, 'learning_rate': 0.00016949074074074075, 'epoch': 1.9833333333333334}
Step 7150: {'loss': 1.0769, 'grad_norm': 1.6897112131118774, 'learning_rate': 0.0001690277777777778, 'epoch': 1.9861111111111112}
Step 7160: {'loss': 0.8864, 'grad_norm': 1.169217824935913, 'learning_rate': 0.0001685648148148148, 'epoch': 1.988888888888889}
Step 7170: {'loss': 0.8997, 'grad_norm': 1.4421831369400024, 'learning_rate': 0.00016810185185185187, 'epoch': 1.9916666666666667}
Step 7180: {'loss': 1.011, 'grad_norm': 2.6442372798919678, 'learning_rate': 0.00016763888888888888, 'epoch': 1.9944444444444445}
Step 7190: {'loss': 0.9657, 'grad_norm': 1.7957165241241455, 'learning_rate': 0.00016717592592592592, 'epoch': 1.9972222222222222}
Step 7200: {'loss': 0.9598, 'grad_norm': 2.324308156967163, 'learning_rate': 0.00016671296296296296, 'epoch': 2.0}
Step 7210: {'loss': 0.9291, 'grad_norm': 1.1909788846969604, 'learning_rate': 0.00016625, 'epoch': 2.0027777777777778}
Step 7220: {'loss': 0.9812, 'grad_norm': 0.949760913848877, 'learning_rate': 0.00016578703703703704, 'epoch': 2.0055555555555555}
Step 7230: {'loss': 0.8388, 'grad_norm': 1.1197692155838013, 'learning_rate': 0.00016532407407407408, 'epoch': 2.0083333333333333}
Step 7240: {'loss': 0.9515, 'grad_norm': 1.5945738554000854, 'learning_rate': 0.00016486111111111112, 'epoch': 2.011111111111111}
Step 7250: {'loss': 0.9024, 'grad_norm': 2.1710352897644043, 'learning_rate': 0.00016439814814814816, 'epoch': 2.013888888888889}
Step 7260: {'loss': 0.9872, 'grad_norm': 1.7795319557189941, 'learning_rate': 0.00016393518518518517, 'epoch': 2.0166666666666666}
Step 7270: {'loss': 0.9666, 'grad_norm': 1.2567710876464844, 'learning_rate': 0.00016347222222222221, 'epoch': 2.0194444444444444}
Step 7280: {'loss': 0.8713, 'grad_norm': 2.1864850521087646, 'learning_rate': 0.00016300925925925928, 'epoch': 2.022222222222222}
Step 7290: {'loss': 0.9755, 'grad_norm': 1.7348273992538452, 'learning_rate': 0.0001625462962962963, 'epoch': 2.025}
Step 7300: {'loss': 0.8414, 'grad_norm': 1.3216451406478882, 'learning_rate': 0.00016208333333333333, 'epoch': 2.0277777777777777}
Step 7310: {'loss': 1.0215, 'grad_norm': 1.3069146871566772, 'learning_rate': 0.00016162037037037037, 'epoch': 2.0305555555555554}
Step 7320: {'loss': 0.8322, 'grad_norm': 1.9729983806610107, 'learning_rate': 0.0001611574074074074, 'epoch': 2.033333333333333}
Step 7330: {'loss': 0.9146, 'grad_norm': 1.385109782218933, 'learning_rate': 0.00016069444444444445, 'epoch': 2.036111111111111}
Step 7340: {'loss': 0.9103, 'grad_norm': 1.1941933631896973, 'learning_rate': 0.00016023148148148147, 'epoch': 2.0388888888888888}
Step 7350: {'loss': 0.8709, 'grad_norm': 1.8060933351516724, 'learning_rate': 0.00015976851851851853, 'epoch': 2.0416666666666665}
Step 7360: {'loss': 0.9323, 'grad_norm': 1.2453042268753052, 'learning_rate': 0.00015930555555555557, 'epoch': 2.0444444444444443}
Step 7370: {'loss': 0.9165, 'grad_norm': 1.2247713804244995, 'learning_rate': 0.00015884259259259259, 'epoch': 2.047222222222222}
Step 7380: {'loss': 0.9943, 'grad_norm': 1.3692983388900757, 'learning_rate': 0.00015837962962962963, 'epoch': 2.05}
Step 7390: {'loss': 0.943, 'grad_norm': 1.91129732131958, 'learning_rate': 0.0001579166666666667, 'epoch': 2.0527777777777776}
Step 7400: {'loss': 0.9218, 'grad_norm': 1.1260806322097778, 'learning_rate': 0.0001574537037037037, 'epoch': 2.0555555555555554}
Step 7410: {'loss': 0.9646, 'grad_norm': 1.2632062435150146, 'learning_rate': 0.00015699074074074074, 'epoch': 2.058333333333333}
Step 7420: {'loss': 0.8207, 'grad_norm': 1.2101714611053467, 'learning_rate': 0.00015652777777777776, 'epoch': 2.061111111111111}
Step 7430: {'loss': 0.8759, 'grad_norm': 2.164452075958252, 'learning_rate': 0.00015606481481481482, 'epoch': 2.063888888888889}
Step 7440: {'loss': 0.9887, 'grad_norm': 1.27642822265625, 'learning_rate': 0.00015560185185185186, 'epoch': 2.066666666666667}
Step 7450: {'loss': 0.9959, 'grad_norm': 1.807470679283142, 'learning_rate': 0.00015513888888888888, 'epoch': 2.0694444444444446}
Step 7460: {'loss': 0.9327, 'grad_norm': 1.5691121816635132, 'learning_rate': 0.00015467592592592594, 'epoch': 2.0722222222222224}
Step 7470: {'loss': 0.9911, 'grad_norm': 1.0859582424163818, 'learning_rate': 0.00015421296296296298, 'epoch': 2.075}
Step 7480: {'loss': 0.9572, 'grad_norm': 1.6754738092422485, 'learning_rate': 0.00015375, 'epoch': 2.077777777777778}
Step 7490: {'loss': 0.9734, 'grad_norm': 1.8494902849197388, 'learning_rate': 0.00015328703703703704, 'epoch': 2.0805555555555557}
Step 7500: {'loss': 0.9978, 'grad_norm': 1.5237809419631958, 'learning_rate': 0.00015282407407407408, 'epoch': 2.0833333333333335}
Step 7510: {'loss': 0.957, 'grad_norm': 1.7465823888778687, 'learning_rate': 0.00015236111111111112, 'epoch': 2.0861111111111112}
Step 7520: {'loss': 0.9837, 'grad_norm': 1.6900144815444946, 'learning_rate': 0.00015189814814814816, 'epoch': 2.088888888888889}
Step 7530: {'loss': 0.9676, 'grad_norm': 1.662278413772583, 'learning_rate': 0.00015143518518518517, 'epoch': 2.091666666666667}
Step 7540: {'loss': 0.8317, 'grad_norm': 0.9819433093070984, 'learning_rate': 0.00015097222222222224, 'epoch': 2.0944444444444446}
Step 7550: {'loss': 0.941, 'grad_norm': 0.9628384709358215, 'learning_rate': 0.00015050925925925928, 'epoch': 2.0972222222222223}
Step 7560: {'loss': 0.9534, 'grad_norm': 1.2128236293792725, 'learning_rate': 0.0001500462962962963, 'epoch': 2.1}
Step 7570: {'loss': 1.0221, 'grad_norm': 1.5679210424423218, 'learning_rate': 0.00014958333333333336, 'epoch': 2.102777777777778}
Step 7580: {'loss': 0.9419, 'grad_norm': 1.1530274152755737, 'learning_rate': 0.00014912037037037037, 'epoch': 2.1055555555555556}
Step 7590: {'loss': 1.0184, 'grad_norm': 2.052863836288452, 'learning_rate': 0.0001486574074074074, 'epoch': 2.1083333333333334}
Step 7600: {'loss': 0.8999, 'grad_norm': 1.8879469633102417, 'learning_rate': 0.00014819444444444445, 'epoch': 2.111111111111111}
Step 7610: {'loss': 0.9085, 'grad_norm': 1.6962218284606934, 'learning_rate': 0.0001477314814814815, 'epoch': 2.113888888888889}
Step 7620: {'loss': 0.9691, 'grad_norm': 1.4075024127960205, 'learning_rate': 0.00014726851851851853, 'epoch': 2.1166666666666667}
Step 7630: {'loss': 0.9418, 'grad_norm': 2.0006370544433594, 'learning_rate': 0.00014680555555555554, 'epoch': 2.1194444444444445}
Step 7640: {'loss': 0.9612, 'grad_norm': 1.4902058839797974, 'learning_rate': 0.00014634259259259258, 'epoch': 2.1222222222222222}
Step 7650: {'loss': 0.9814, 'grad_norm': 2.104703187942505, 'learning_rate': 0.00014587962962962965, 'epoch': 2.125}
Step 7660: {'loss': 0.9716, 'grad_norm': 1.697593331336975, 'learning_rate': 0.00014541666666666666, 'epoch': 2.1277777777777778}
Step 7670: {'loss': 0.8289, 'grad_norm': 1.2376673221588135, 'learning_rate': 0.0001449537037037037, 'epoch': 2.1305555555555555}
Step 7680: {'loss': 0.9818, 'grad_norm': 1.371238350868225, 'learning_rate': 0.00014449074074074077, 'epoch': 2.1333333333333333}
Step 7690: {'loss': 0.9799, 'grad_norm': 0.9535771608352661, 'learning_rate': 0.00014402777777777778, 'epoch': 2.136111111111111}
Step 7700: {'loss': 0.9976, 'grad_norm': 1.3216558694839478, 'learning_rate': 0.00014356481481481482, 'epoch': 2.138888888888889}
Step 7710: {'loss': 0.8464, 'grad_norm': 1.449402093887329, 'learning_rate': 0.00014310185185185183, 'epoch': 2.1416666666666666}
Step 7720: {'loss': 1.0321, 'grad_norm': 2.4958994388580322, 'learning_rate': 0.0001426388888888889, 'epoch': 2.1444444444444444}
Step 7730: {'loss': 0.8625, 'grad_norm': 1.4955910444259644, 'learning_rate': 0.00014217592592592594, 'epoch': 2.147222222222222}
Step 7740: {'loss': 0.9466, 'grad_norm': 1.2172560691833496, 'learning_rate': 0.00014171296296296295, 'epoch': 2.15}
Step 7750: {'loss': 0.9006, 'grad_norm': 1.0985116958618164, 'learning_rate': 0.00014125, 'epoch': 2.1527777777777777}
Step 7760: {'loss': 0.8278, 'grad_norm': 1.2127355337142944, 'learning_rate': 0.00014078703703703706, 'epoch': 2.1555555555555554}
Step 7770: {'loss': 0.9541, 'grad_norm': 1.1063882112503052, 'learning_rate': 0.00014032407407407407, 'epoch': 2.158333333333333}
Step 7780: {'loss': 0.8838, 'grad_norm': 1.03187894821167, 'learning_rate': 0.0001398611111111111, 'epoch': 2.161111111111111}
Step 7790: {'loss': 0.9723, 'grad_norm': 4.015143871307373, 'learning_rate': 0.00013939814814814815, 'epoch': 2.1638888888888888}
Step 7800: {'loss': 0.9964, 'grad_norm': 1.202360987663269, 'learning_rate': 0.0001389351851851852, 'epoch': 2.1666666666666665}
Step 7810: {'loss': 0.8993, 'grad_norm': 1.2368515729904175, 'learning_rate': 0.00013847222222222223, 'epoch': 2.1694444444444443}
Step 7820: {'loss': 0.7931, 'grad_norm': 1.1974371671676636, 'learning_rate': 0.00013800925925925924, 'epoch': 2.172222222222222}
Step 7830: {'loss': 0.886, 'grad_norm': 1.291899561882019, 'learning_rate': 0.0001375462962962963, 'epoch': 2.175}
Step 7840: {'loss': 0.8527, 'grad_norm': 1.3248554468154907, 'learning_rate': 0.00013708333333333335, 'epoch': 2.1777777777777776}
Step 7850: {'loss': 0.9885, 'grad_norm': 1.6508212089538574, 'learning_rate': 0.00013662037037037036, 'epoch': 2.1805555555555554}
Step 7860: {'loss': 0.9325, 'grad_norm': 2.2929489612579346, 'learning_rate': 0.0001361574074074074, 'epoch': 2.183333333333333}
Step 7870: {'loss': 0.9076, 'grad_norm': 1.1897974014282227, 'learning_rate': 0.00013569444444444444, 'epoch': 2.186111111111111}
Step 7880: {'loss': 0.9458, 'grad_norm': 2.1259067058563232, 'learning_rate': 0.00013523148148148148, 'epoch': 2.188888888888889}
Step 7890: {'loss': 0.849, 'grad_norm': 1.6147406101226807, 'learning_rate': 0.00013476851851851852, 'epoch': 2.191666666666667}
Step 7900: {'loss': 0.9943, 'grad_norm': 1.7357791662216187, 'learning_rate': 0.00013430555555555556, 'epoch': 2.1944444444444446}
Step 7910: {'loss': 0.9054, 'grad_norm': 1.1775684356689453, 'learning_rate': 0.0001338425925925926, 'epoch': 2.1972222222222224}
Step 7920: {'loss': 0.9429, 'grad_norm': 1.6360434293746948, 'learning_rate': 0.00013337962962962964, 'epoch': 2.2}
Step 7930: {'loss': 0.8854, 'grad_norm': 1.1337906122207642, 'learning_rate': 0.00013291666666666665, 'epoch': 2.202777777777778}
Step 7940: {'loss': 0.9201, 'grad_norm': 1.8513528108596802, 'learning_rate': 0.00013245370370370372, 'epoch': 2.2055555555555557}
Step 7950: {'loss': 0.7773, 'grad_norm': 1.3622767925262451, 'learning_rate': 0.00013199074074074073, 'epoch': 2.2083333333333335}
Step 7960: {'loss': 0.8821, 'grad_norm': 2.090229034423828, 'learning_rate': 0.00013152777777777777, 'epoch': 2.2111111111111112}
Step 7970: {'loss': 0.9519, 'grad_norm': 1.80543053150177, 'learning_rate': 0.00013106481481481484, 'epoch': 2.213888888888889}
Step 7980: {'loss': 1.0148, 'grad_norm': 1.3595492839813232, 'learning_rate': 0.00013060185185185185, 'epoch': 2.216666666666667}
Step 7990: {'loss': 0.8432, 'grad_norm': 1.5269256830215454, 'learning_rate': 0.0001301388888888889, 'epoch': 2.2194444444444446}
Step 8000: {'loss': 0.9013, 'grad_norm': 1.7821258306503296, 'learning_rate': 0.00012967592592592593, 'epoch': 2.2222222222222223}
Step 8010: {'loss': 0.9956, 'grad_norm': 1.343567132949829, 'learning_rate': 0.00012921296296296297, 'epoch': 2.225}
Step 8020: {'loss': 0.9361, 'grad_norm': 1.0746407508850098, 'learning_rate': 0.00012875, 'epoch': 2.227777777777778}
Step 8030: {'loss': 0.9963, 'grad_norm': 1.2110973596572876, 'learning_rate': 0.00012828703703703703, 'epoch': 2.2305555555555556}
Step 8040: {'loss': 0.9134, 'grad_norm': 0.9915282726287842, 'learning_rate': 0.00012782407407407407, 'epoch': 2.2333333333333334}
Step 8050: {'loss': 0.9926, 'grad_norm': 1.4776405096054077, 'learning_rate': 0.00012736111111111113, 'epoch': 2.236111111111111}
Step 8060: {'loss': 0.9874, 'grad_norm': 1.124361515045166, 'learning_rate': 0.00012689814814814814, 'epoch': 2.238888888888889}
Step 8070: {'loss': 0.9182, 'grad_norm': 1.1763914823532104, 'learning_rate': 0.00012643518518518518, 'epoch': 2.2416666666666667}
Step 8080: {'loss': 0.8833, 'grad_norm': 1.6771796941757202, 'learning_rate': 0.00012597222222222225, 'epoch': 2.2444444444444445}
Step 8090: {'loss': 0.8658, 'grad_norm': 1.083120584487915, 'learning_rate': 0.00012550925925925926, 'epoch': 2.2472222222222222}
Step 8100: {'loss': 0.9525, 'grad_norm': 0.9693548083305359, 'learning_rate': 0.0001250462962962963, 'epoch': 2.25}
Step 8110: {'loss': 0.9665, 'grad_norm': 1.4301974773406982, 'learning_rate': 0.00012458333333333334, 'epoch': 2.2527777777777778}
Step 8120: {'loss': 0.8387, 'grad_norm': 1.433249592781067, 'learning_rate': 0.00012412037037037036, 'epoch': 2.2555555555555555}
Step 8130: {'loss': 0.8995, 'grad_norm': 1.1536623239517212, 'learning_rate': 0.00012365740740740742, 'epoch': 2.2583333333333333}
Step 8140: {'loss': 0.8765, 'grad_norm': 1.2351047992706299, 'learning_rate': 0.00012319444444444444, 'epoch': 2.261111111111111}
Step 8150: {'loss': 0.9048, 'grad_norm': 0.9400542974472046, 'learning_rate': 0.00012273148148148148, 'epoch': 2.263888888888889}
Step 8160: {'loss': 0.8678, 'grad_norm': 1.4397588968276978, 'learning_rate': 0.00012226851851851852, 'epoch': 2.2666666666666666}
Step 8170: {'loss': 0.9386, 'grad_norm': 1.71975576877594, 'learning_rate': 0.00012180555555555556, 'epoch': 2.2694444444444444}
Step 8180: {'loss': 0.9331, 'grad_norm': 1.2160966396331787, 'learning_rate': 0.0001213425925925926, 'epoch': 2.272222222222222}
Step 8190: {'loss': 0.9144, 'grad_norm': 2.350905656814575, 'learning_rate': 0.00012087962962962964, 'epoch': 2.275}
Step 8200: {'loss': 0.933, 'grad_norm': 1.092485785484314, 'learning_rate': 0.00012041666666666668, 'epoch': 2.2777777777777777}
Step 8210: {'loss': 0.8879, 'grad_norm': 1.3231604099273682, 'learning_rate': 0.0001199537037037037, 'epoch': 2.2805555555555554}
Step 8220: {'loss': 0.9605, 'grad_norm': 1.0068001747131348, 'learning_rate': 0.00011949074074074074, 'epoch': 2.283333333333333}
Step 8230: {'loss': 0.8615, 'grad_norm': 1.4145854711532593, 'learning_rate': 0.00011902777777777778, 'epoch': 2.286111111111111}
Step 8240: {'loss': 0.9296, 'grad_norm': 2.576486349105835, 'learning_rate': 0.00011856481481481482, 'epoch': 2.2888888888888888}
Step 8250: {'loss': 0.9641, 'grad_norm': 1.4322850704193115, 'learning_rate': 0.00011810185185185185, 'epoch': 2.2916666666666665}
Step 8260: {'loss': 0.9797, 'grad_norm': 1.9321680068969727, 'learning_rate': 0.00011763888888888889, 'epoch': 2.2944444444444443}
Step 8270: {'loss': 0.9568, 'grad_norm': 3.8264544010162354, 'learning_rate': 0.00011717592592592593, 'epoch': 2.297222222222222}
Step 8280: {'loss': 0.9595, 'grad_norm': 1.4211586713790894, 'learning_rate': 0.00011671296296296297, 'epoch': 2.3}
Step 8290: {'loss': 0.9389, 'grad_norm': 1.0299413204193115, 'learning_rate': 0.00011625, 'epoch': 2.3027777777777776}
Step 8300: {'loss': 0.9589, 'grad_norm': 2.2317800521850586, 'learning_rate': 0.00011578703703703703, 'epoch': 2.3055555555555554}
Step 8310: {'loss': 0.9612, 'grad_norm': 1.694348931312561, 'learning_rate': 0.00011532407407407409, 'epoch': 2.3083333333333336}
Step 8320: {'loss': 0.9244, 'grad_norm': 0.6873835921287537, 'learning_rate': 0.00011486111111111111, 'epoch': 2.311111111111111}
Step 8330: {'loss': 0.9706, 'grad_norm': 1.4031318426132202, 'learning_rate': 0.00011439814814814815, 'epoch': 2.313888888888889}
Step 8340: {'loss': 0.8903, 'grad_norm': 1.980040192604065, 'learning_rate': 0.00011393518518518518, 'epoch': 2.3166666666666664}
Step 8350: {'loss': 0.9566, 'grad_norm': 2.4636435508728027, 'learning_rate': 0.00011347222222222223, 'epoch': 2.3194444444444446}
Step 8360: {'loss': 0.9766, 'grad_norm': 1.236715316772461, 'learning_rate': 0.00011300925925925926, 'epoch': 2.3222222222222224}
Step 8370: {'loss': 0.9762, 'grad_norm': 1.0002329349517822, 'learning_rate': 0.0001125462962962963, 'epoch': 2.325}
Step 8380: {'loss': 0.9004, 'grad_norm': 1.2417616844177246, 'learning_rate': 0.00011208333333333332, 'epoch': 2.327777777777778}
Step 8390: {'loss': 0.9697, 'grad_norm': 1.1861152648925781, 'learning_rate': 0.00011162037037037038, 'epoch': 2.3305555555555557}
Step 8400: {'loss': 0.9713, 'grad_norm': 1.1607614755630493, 'learning_rate': 0.00011115740740740742, 'epoch': 2.3333333333333335}
Step 8410: {'loss': 0.9058, 'grad_norm': 1.115474820137024, 'learning_rate': 0.00011069444444444444, 'epoch': 2.3361111111111112}
Step 8420: {'loss': 0.9603, 'grad_norm': 1.5065513849258423, 'learning_rate': 0.00011023148148148148, 'epoch': 2.338888888888889}
Step 8430: {'loss': 0.9262, 'grad_norm': 0.9988471865653992, 'learning_rate': 0.00010976851851851852, 'epoch': 2.341666666666667}
Step 8440: {'loss': 0.821, 'grad_norm': 1.3542544841766357, 'learning_rate': 0.00010930555555555556, 'epoch': 2.3444444444444446}
Step 8450: {'loss': 0.9068, 'grad_norm': 1.1355957984924316, 'learning_rate': 0.00010884259259259259, 'epoch': 2.3472222222222223}
Step 8460: {'loss': 0.8966, 'grad_norm': 2.6716933250427246, 'learning_rate': 0.00010837962962962963, 'epoch': 2.35}
Step 8470: {'loss': 0.8428, 'grad_norm': 1.3689218759536743, 'learning_rate': 0.00010791666666666667, 'epoch': 2.352777777777778}
Step 8480: {'loss': 1.0427, 'grad_norm': 1.3319694995880127, 'learning_rate': 0.00010745370370370371, 'epoch': 2.3555555555555556}
Step 8490: {'loss': 0.9123, 'grad_norm': 1.4146091938018799, 'learning_rate': 0.00010699074074074075, 'epoch': 2.3583333333333334}
Step 8500: {'loss': 0.8057, 'grad_norm': 0.5315817594528198, 'learning_rate': 0.00010652777777777778, 'epoch': 2.361111111111111}
Step 8510: {'loss': 0.9446, 'grad_norm': 1.6087932586669922, 'learning_rate': 0.00010606481481481483, 'epoch': 2.363888888888889}
Step 8520: {'loss': 0.9256, 'grad_norm': 1.9821605682373047, 'learning_rate': 0.00010560185185185186, 'epoch': 2.3666666666666667}
Step 8530: {'loss': 0.9546, 'grad_norm': 1.6039605140686035, 'learning_rate': 0.0001051388888888889, 'epoch': 2.3694444444444445}
Step 8540: {'loss': 1.007, 'grad_norm': 2.477123975753784, 'learning_rate': 0.00010467592592592592, 'epoch': 2.3722222222222222}
Step 8550: {'loss': 0.9274, 'grad_norm': 1.719655990600586, 'learning_rate': 0.00010421296296296296, 'epoch': 2.375}
Step 8560: {'loss': 0.9392, 'grad_norm': 1.576402187347412, 'learning_rate': 0.00010375, 'epoch': 2.3777777777777778}
Step 8570: {'loss': 0.9118, 'grad_norm': 1.5468165874481201, 'learning_rate': 0.00010328703703703704, 'epoch': 2.3805555555555555}
Step 8580: {'loss': 1.0078, 'grad_norm': 1.876474142074585, 'learning_rate': 0.00010282407407407407, 'epoch': 2.3833333333333333}
Step 8590: {'loss': 0.9701, 'grad_norm': 1.2736098766326904, 'learning_rate': 0.00010236111111111111, 'epoch': 2.386111111111111}
Step 8600: {'loss': 0.9115, 'grad_norm': 1.6892189979553223, 'learning_rate': 0.00010189814814814816, 'epoch': 2.388888888888889}
Step 8610: {'loss': 0.9861, 'grad_norm': 1.94313645362854, 'learning_rate': 0.00010143518518518519, 'epoch': 2.3916666666666666}
Step 8620: {'loss': 0.8925, 'grad_norm': 1.5208313465118408, 'learning_rate': 0.00010097222222222223, 'epoch': 2.3944444444444444}
Step 8630: {'loss': 0.9656, 'grad_norm': 1.5022324323654175, 'learning_rate': 0.00010050925925925925, 'epoch': 2.397222222222222}
Step 8640: {'loss': 0.962, 'grad_norm': 1.868601679801941, 'learning_rate': 0.0001000462962962963, 'epoch': 2.4}
Step 8650: {'loss': 1.0001, 'grad_norm': 3.4524006843566895, 'learning_rate': 9.958333333333333e-05, 'epoch': 2.4027777777777777}
Step 8660: {'loss': 1.0168, 'grad_norm': 1.695278286933899, 'learning_rate': 9.912037037037037e-05, 'epoch': 2.4055555555555554}
Step 8670: {'loss': 0.8633, 'grad_norm': 0.8523101210594177, 'learning_rate': 9.86574074074074e-05, 'epoch': 2.408333333333333}
Step 8680: {'loss': 0.7954, 'grad_norm': 1.172692894935608, 'learning_rate': 9.819444444444445e-05, 'epoch': 2.411111111111111}
Step 8690: {'loss': 0.9665, 'grad_norm': 1.793499231338501, 'learning_rate': 9.773148148148148e-05, 'epoch': 2.4138888888888888}
Step 8700: {'loss': 0.9498, 'grad_norm': 1.2358719110488892, 'learning_rate': 9.726851851851852e-05, 'epoch': 2.4166666666666665}
Step 8710: {'loss': 0.9091, 'grad_norm': 6.37314510345459, 'learning_rate': 9.680555555555556e-05, 'epoch': 2.4194444444444443}
Step 8720: {'loss': 0.8649, 'grad_norm': 3.3742423057556152, 'learning_rate': 9.63425925925926e-05, 'epoch': 2.422222222222222}
Step 8730: {'loss': 0.9172, 'grad_norm': 1.1545740365982056, 'learning_rate': 9.587962962962964e-05, 'epoch': 2.425}
Step 8740: {'loss': 1.0269, 'grad_norm': 1.8239346742630005, 'learning_rate': 9.541666666666666e-05, 'epoch': 2.4277777777777776}
Step 8750: {'loss': 0.9677, 'grad_norm': 2.1701784133911133, 'learning_rate': 9.49537037037037e-05, 'epoch': 2.4305555555555554}
Step 8760: {'loss': 0.8959, 'grad_norm': 1.0846487283706665, 'learning_rate': 9.449074074074074e-05, 'epoch': 2.4333333333333336}
Step 8770: {'loss': 0.8962, 'grad_norm': 1.108816146850586, 'learning_rate': 9.402777777777778e-05, 'epoch': 2.436111111111111}
Step 8780: {'loss': 0.8653, 'grad_norm': 1.2379045486450195, 'learning_rate': 9.356481481481481e-05, 'epoch': 2.438888888888889}
Step 8790: {'loss': 0.8786, 'grad_norm': 1.5447112321853638, 'learning_rate': 9.310185185185185e-05, 'epoch': 2.4416666666666664}
Step 8800: {'loss': 0.9205, 'grad_norm': 1.0519344806671143, 'learning_rate': 9.26388888888889e-05, 'epoch': 2.4444444444444446}
Step 8810: {'loss': 0.9641, 'grad_norm': 1.6212291717529297, 'learning_rate': 9.217592592592593e-05, 'epoch': 2.4472222222222224}
Step 8820: {'loss': 0.8877, 'grad_norm': 1.3472092151641846, 'learning_rate': 9.171296296296297e-05, 'epoch': 2.45}
Step 8830: {'loss': 0.8744, 'grad_norm': 1.132436752319336, 'learning_rate': 9.125e-05, 'epoch': 2.452777777777778}
Step 8840: {'loss': 0.9785, 'grad_norm': 1.200108528137207, 'learning_rate': 9.078703703703705e-05, 'epoch': 2.4555555555555557}
Step 8850: {'loss': 0.9296, 'grad_norm': 1.3507699966430664, 'learning_rate': 9.032407407407408e-05, 'epoch': 2.4583333333333335}
Step 8860: {'loss': 0.9083, 'grad_norm': 1.6305339336395264, 'learning_rate': 8.986111111111111e-05, 'epoch': 2.4611111111111112}
Step 8870: {'loss': 0.9191, 'grad_norm': 2.570150136947632, 'learning_rate': 8.939814814814814e-05, 'epoch': 2.463888888888889}
Step 8880: {'loss': 0.9086, 'grad_norm': 1.3290683031082153, 'learning_rate': 8.89351851851852e-05, 'epoch': 2.466666666666667}
Step 8890: {'loss': 0.888, 'grad_norm': 1.4693118333816528, 'learning_rate': 8.847222222222222e-05, 'epoch': 2.4694444444444446}
Step 8900: {'loss': 0.9523, 'grad_norm': 1.7005140781402588, 'learning_rate': 8.800925925925926e-05, 'epoch': 2.4722222222222223}
Step 8910: {'loss': 0.8979, 'grad_norm': 1.6548199653625488, 'learning_rate': 8.75462962962963e-05, 'epoch': 2.475}
Step 8920: {'loss': 0.9332, 'grad_norm': 1.8880146741867065, 'learning_rate': 8.708333333333334e-05, 'epoch': 2.477777777777778}
Step 8930: {'loss': 0.8666, 'grad_norm': 1.1833726167678833, 'learning_rate': 8.662037037037038e-05, 'epoch': 2.4805555555555556}
Step 8940: {'loss': 0.9966, 'grad_norm': 1.5799005031585693, 'learning_rate': 8.61574074074074e-05, 'epoch': 2.4833333333333334}
Step 8950: {'loss': 0.8654, 'grad_norm': 0.8956646919250488, 'learning_rate': 8.569444444444445e-05, 'epoch': 2.486111111111111}
Step 8960: {'loss': 0.8409, 'grad_norm': 1.2223286628723145, 'learning_rate': 8.523148148148147e-05, 'epoch': 2.488888888888889}
Step 8970: {'loss': 0.9774, 'grad_norm': 1.1469769477844238, 'learning_rate': 8.476851851851853e-05, 'epoch': 2.4916666666666667}
Step 8980: {'loss': 0.9899, 'grad_norm': 2.6894845962524414, 'learning_rate': 8.430555555555555e-05, 'epoch': 2.4944444444444445}
Step 8990: {'loss': 0.849, 'grad_norm': 1.77978515625, 'learning_rate': 8.384259259259259e-05, 'epoch': 2.4972222222222222}
Step 9000: {'loss': 0.924, 'grad_norm': 1.3135380744934082, 'learning_rate': 8.337962962962962e-05, 'epoch': 2.5}
Step 9010: {'loss': 0.876, 'grad_norm': 2.1646697521209717, 'learning_rate': 8.291666666666667e-05, 'epoch': 2.5027777777777778}
Step 9020: {'loss': 0.8108, 'grad_norm': 1.2744059562683105, 'learning_rate': 8.245370370370371e-05, 'epoch': 2.5055555555555555}
Step 9030: {'loss': 0.9814, 'grad_norm': 1.3040077686309814, 'learning_rate': 8.199074074074074e-05, 'epoch': 2.5083333333333333}
Step 9040: {'loss': 0.8414, 'grad_norm': 0.773323655128479, 'learning_rate': 8.152777777777778e-05, 'epoch': 2.511111111111111}
Step 9050: {'loss': 0.944, 'grad_norm': 1.762400507926941, 'learning_rate': 8.106481481481482e-05, 'epoch': 2.513888888888889}
Step 9060: {'loss': 0.8812, 'grad_norm': 1.6388741731643677, 'learning_rate': 8.060185185185186e-05, 'epoch': 2.5166666666666666}
Step 9070: {'loss': 0.9448, 'grad_norm': 1.4051928520202637, 'learning_rate': 8.013888888888888e-05, 'epoch': 2.5194444444444444}
Step 9080: {'loss': 0.9794, 'grad_norm': 1.4451625347137451, 'learning_rate': 7.967592592592592e-05, 'epoch': 2.522222222222222}
Step 9090: {'loss': 0.9316, 'grad_norm': 1.5048186779022217, 'learning_rate': 7.921296296296296e-05, 'epoch': 2.525}
Step 9100: {'loss': 0.9369, 'grad_norm': 1.1206656694412231, 'learning_rate': 7.875e-05, 'epoch': 2.5277777777777777}
Step 9110: {'loss': 0.841, 'grad_norm': 1.4379676580429077, 'learning_rate': 7.828703703703704e-05, 'epoch': 2.5305555555555554}
Step 9120: {'loss': 1.0029, 'grad_norm': 1.3144900798797607, 'learning_rate': 7.782407407407407e-05, 'epoch': 2.533333333333333}
Step 9130: {'loss': 1.0098, 'grad_norm': 1.425931692123413, 'learning_rate': 7.736111111111112e-05, 'epoch': 2.536111111111111}
Step 9140: {'loss': 0.9617, 'grad_norm': 1.2532291412353516, 'learning_rate': 7.689814814814815e-05, 'epoch': 2.5388888888888888}
Step 9150: {'loss': 0.8678, 'grad_norm': 1.3137729167938232, 'learning_rate': 7.643518518518519e-05, 'epoch': 2.5416666666666665}
Step 9160: {'loss': 0.9193, 'grad_norm': 1.4326832294464111, 'learning_rate': 7.597222222222222e-05, 'epoch': 2.5444444444444443}
Step 9170: {'loss': 0.9508, 'grad_norm': 1.7968088388442993, 'learning_rate': 7.550925925925927e-05, 'epoch': 2.5472222222222225}
Step 9180: {'loss': 0.8666, 'grad_norm': 1.167512059211731, 'learning_rate': 7.50462962962963e-05, 'epoch': 2.55}
Step 9190: {'loss': 0.9245, 'grad_norm': 1.0492351055145264, 'learning_rate': 7.458333333333333e-05, 'epoch': 2.552777777777778}
Step 9200: {'loss': 0.9013, 'grad_norm': 1.128529667854309, 'learning_rate': 7.412037037037036e-05, 'epoch': 2.5555555555555554}
Step 9210: {'loss': 0.8637, 'grad_norm': 1.3641802072525024, 'learning_rate': 7.365740740740741e-05, 'epoch': 2.5583333333333336}
Step 9220: {'loss': 0.9265, 'grad_norm': 1.3881977796554565, 'learning_rate': 7.319444444444445e-05, 'epoch': 2.561111111111111}
Step 9230: {'loss': 0.9268, 'grad_norm': 1.0820947885513306, 'learning_rate': 7.273148148148148e-05, 'epoch': 2.563888888888889}
Step 9240: {'loss': 0.9195, 'grad_norm': 0.8964859247207642, 'learning_rate': 7.226851851851852e-05, 'epoch': 2.5666666666666664}
Step 9250: {'loss': 0.9466, 'grad_norm': 1.1477283239364624, 'learning_rate': 7.180555555555556e-05, 'epoch': 2.5694444444444446}
Step 9260: {'loss': 1.0415, 'grad_norm': 1.4162830114364624, 'learning_rate': 7.13425925925926e-05, 'epoch': 2.572222222222222}
Step 9270: {'loss': 0.9797, 'grad_norm': 0.9307308197021484, 'learning_rate': 7.087962962962963e-05, 'epoch': 2.575}
Step 9280: {'loss': 0.8791, 'grad_norm': 1.5157009363174438, 'learning_rate': 7.041666666666667e-05, 'epoch': 2.5777777777777775}
Step 9290: {'loss': 1.0437, 'grad_norm': 1.712752103805542, 'learning_rate': 6.99537037037037e-05, 'epoch': 2.5805555555555557}
Step 9300: {'loss': 0.9683, 'grad_norm': 2.059433937072754, 'learning_rate': 6.949074074074075e-05, 'epoch': 2.5833333333333335}
Step 9310: {'loss': 0.9711, 'grad_norm': 1.064527988433838, 'learning_rate': 6.902777777777777e-05, 'epoch': 2.5861111111111112}
Step 9320: {'loss': 0.8679, 'grad_norm': 1.3357658386230469, 'learning_rate': 6.856481481481481e-05, 'epoch': 2.588888888888889}
Step 9330: {'loss': 0.8674, 'grad_norm': 1.393730878829956, 'learning_rate': 6.810185185185187e-05, 'epoch': 2.591666666666667}
Step 9340: {'loss': 0.8847, 'grad_norm': 1.4904530048370361, 'learning_rate': 6.763888888888889e-05, 'epoch': 2.5944444444444446}
Step 9350: {'loss': 0.8681, 'grad_norm': 1.2622120380401611, 'learning_rate': 6.717592592592593e-05, 'epoch': 2.5972222222222223}
Step 9360: {'loss': 0.9068, 'grad_norm': 2.1265242099761963, 'learning_rate': 6.671296296296296e-05, 'epoch': 2.6}
Step 9370: {'loss': 0.961, 'grad_norm': 0.924288809299469, 'learning_rate': 6.625000000000001e-05, 'epoch': 2.602777777777778}
Step 9380: {'loss': 1.0464, 'grad_norm': 1.462645411491394, 'learning_rate': 6.578703703703704e-05, 'epoch': 2.6055555555555556}
Step 9390: {'loss': 1.0124, 'grad_norm': 1.427559733390808, 'learning_rate': 6.532407407407408e-05, 'epoch': 2.6083333333333334}
Step 9400: {'loss': 0.9203, 'grad_norm': 1.1651359796524048, 'learning_rate': 6.48611111111111e-05, 'epoch': 2.611111111111111}
Step 9410: {'loss': 0.9401, 'grad_norm': 1.2208259105682373, 'learning_rate': 6.439814814814814e-05, 'epoch': 2.613888888888889}
Step 9420: {'loss': 0.8884, 'grad_norm': 1.5156407356262207, 'learning_rate': 6.39351851851852e-05, 'epoch': 2.6166666666666667}
Step 9430: {'loss': 0.8481, 'grad_norm': 1.199906826019287, 'learning_rate': 6.347222222222222e-05, 'epoch': 2.6194444444444445}
Step 9440: {'loss': 0.9648, 'grad_norm': 1.6140440702438354, 'learning_rate': 6.300925925925926e-05, 'epoch': 2.6222222222222222}
Step 9450: {'loss': 0.8961, 'grad_norm': 1.4736636877059937, 'learning_rate': 6.254629629629629e-05, 'epoch': 2.625}
Step 9460: {'loss': 1.075, 'grad_norm': 2.110703229904175, 'learning_rate': 6.208333333333333e-05, 'epoch': 2.6277777777777778}
Step 9470: {'loss': 0.9054, 'grad_norm': 1.5735676288604736, 'learning_rate': 6.162037037037037e-05, 'epoch': 2.6305555555555555}
Step 9480: {'loss': 0.9368, 'grad_norm': 1.4139735698699951, 'learning_rate': 6.115740740740741e-05, 'epoch': 2.6333333333333333}
Step 9490: {'loss': 0.9843, 'grad_norm': 0.9155236482620239, 'learning_rate': 6.069444444444445e-05, 'epoch': 2.636111111111111}
Step 9500: {'loss': 0.9331, 'grad_norm': 1.1651378870010376, 'learning_rate': 6.023148148148148e-05, 'epoch': 2.638888888888889}
Step 9510: {'loss': 0.9997, 'grad_norm': 1.780482292175293, 'learning_rate': 5.976851851851852e-05, 'epoch': 2.6416666666666666}
Step 9520: {'loss': 0.8921, 'grad_norm': 1.4046086072921753, 'learning_rate': 5.9305555555555555e-05, 'epoch': 2.6444444444444444}
Step 9530: {'loss': 0.8434, 'grad_norm': 1.2434080839157104, 'learning_rate': 5.8842592592592594e-05, 'epoch': 2.647222222222222}
Step 9540: {'loss': 0.9692, 'grad_norm': 1.5326201915740967, 'learning_rate': 5.837962962962963e-05, 'epoch': 2.65}
Step 9550: {'loss': 0.8917, 'grad_norm': 1.6961365938186646, 'learning_rate': 5.791666666666667e-05, 'epoch': 2.6527777777777777}
Step 9560: {'loss': 0.8674, 'grad_norm': 1.2361161708831787, 'learning_rate': 5.74537037037037e-05, 'epoch': 2.6555555555555554}
Step 9570: {'loss': 0.867, 'grad_norm': 2.1753406524658203, 'learning_rate': 5.699074074074074e-05, 'epoch': 2.658333333333333}
Step 9580: {'loss': 0.8952, 'grad_norm': 1.6577746868133545, 'learning_rate': 5.652777777777778e-05, 'epoch': 2.661111111111111}
Step 9590: {'loss': 0.8722, 'grad_norm': 1.744515299797058, 'learning_rate': 5.606481481481482e-05, 'epoch': 2.6638888888888888}
Step 9600: {'loss': 1.0409, 'grad_norm': 2.502521514892578, 'learning_rate': 5.560185185185185e-05, 'epoch': 2.6666666666666665}
Step 9610: {'loss': 0.9107, 'grad_norm': 0.9235899448394775, 'learning_rate': 5.513888888888889e-05, 'epoch': 2.6694444444444443}
Step 9620: {'loss': 0.8721, 'grad_norm': 2.5527331829071045, 'learning_rate': 5.4675925925925926e-05, 'epoch': 2.6722222222222225}
Step 9630: {'loss': 0.9575, 'grad_norm': 1.2669224739074707, 'learning_rate': 5.4212962962962966e-05, 'epoch': 2.675}
Step 9640: {'loss': 0.8922, 'grad_norm': 1.45762038230896, 'learning_rate': 5.375e-05, 'epoch': 2.677777777777778}
Step 9650: {'loss': 0.9602, 'grad_norm': 1.3173668384552002, 'learning_rate': 5.328703703703704e-05, 'epoch': 2.6805555555555554}
Step 9660: {'loss': 0.9644, 'grad_norm': 1.6308428049087524, 'learning_rate': 5.282407407407407e-05, 'epoch': 2.6833333333333336}
Step 9670: {'loss': 0.8802, 'grad_norm': 1.6060969829559326, 'learning_rate': 5.236111111111111e-05, 'epoch': 2.686111111111111}
Step 9680: {'loss': 0.938, 'grad_norm': 1.4465879201889038, 'learning_rate': 5.189814814814815e-05, 'epoch': 2.688888888888889}
Step 9690: {'loss': 0.9151, 'grad_norm': 1.5761078596115112, 'learning_rate': 5.143518518518519e-05, 'epoch': 2.6916666666666664}
Step 9700: {'loss': 0.9695, 'grad_norm': 1.4391120672225952, 'learning_rate': 5.0972222222222224e-05, 'epoch': 2.6944444444444446}
Step 9710: {'loss': 1.0536, 'grad_norm': 1.831111192703247, 'learning_rate': 5.0509259259259264e-05, 'epoch': 2.697222222222222}
Step 9720: {'loss': 0.9226, 'grad_norm': 2.37362003326416, 'learning_rate': 5.00462962962963e-05, 'epoch': 2.7}
Step 9730: {'loss': 0.8744, 'grad_norm': 1.425923466682434, 'learning_rate': 4.958333333333334e-05, 'epoch': 2.7027777777777775}
Step 9740: {'loss': 0.9443, 'grad_norm': 1.0070279836654663, 'learning_rate': 4.912037037037037e-05, 'epoch': 2.7055555555555557}
Step 9750: {'loss': 0.8749, 'grad_norm': 1.6937263011932373, 'learning_rate': 4.865740740740741e-05, 'epoch': 2.7083333333333335}
Step 9760: {'loss': 0.9275, 'grad_norm': 1.2494316101074219, 'learning_rate': 4.819444444444444e-05, 'epoch': 2.7111111111111112}
Step 9770: {'loss': 0.9379, 'grad_norm': 1.218604326248169, 'learning_rate': 4.773148148148148e-05, 'epoch': 2.713888888888889}
Step 9780: {'loss': 0.8856, 'grad_norm': 1.5147172212600708, 'learning_rate': 4.7268518518518516e-05, 'epoch': 2.716666666666667}
Step 9790: {'loss': 0.9781, 'grad_norm': 1.7230719327926636, 'learning_rate': 4.680555555555556e-05, 'epoch': 2.7194444444444446}
Step 9800: {'loss': 0.9326, 'grad_norm': 1.1820608377456665, 'learning_rate': 4.6342592592592595e-05, 'epoch': 2.7222222222222223}
Step 9810: {'loss': 0.876, 'grad_norm': 1.6785866022109985, 'learning_rate': 4.5879629629629635e-05, 'epoch': 2.725}
Step 9820: {'loss': 0.9246, 'grad_norm': 0.9392068982124329, 'learning_rate': 4.541666666666667e-05, 'epoch': 2.727777777777778}
Step 9830: {'loss': 0.9733, 'grad_norm': 1.5715913772583008, 'learning_rate': 4.495370370370371e-05, 'epoch': 2.7305555555555556}
Step 9840: {'loss': 0.9035, 'grad_norm': 1.3336490392684937, 'learning_rate': 4.449074074074074e-05, 'epoch': 2.7333333333333334}
Step 9850: {'loss': 0.9969, 'grad_norm': 1.7152907848358154, 'learning_rate': 4.402777777777778e-05, 'epoch': 2.736111111111111}
Step 9860: {'loss': 0.9504, 'grad_norm': 1.481990098953247, 'learning_rate': 4.3564814814814814e-05, 'epoch': 2.738888888888889}
Step 9870: {'loss': 0.9505, 'grad_norm': 1.4220285415649414, 'learning_rate': 4.310185185185185e-05, 'epoch': 2.7416666666666667}
Step 9880: {'loss': 0.9242, 'grad_norm': 1.241881012916565, 'learning_rate': 4.263888888888889e-05, 'epoch': 2.7444444444444445}
Step 9890: {'loss': 0.8656, 'grad_norm': 1.3535228967666626, 'learning_rate': 4.217592592592593e-05, 'epoch': 2.7472222222222222}
Step 9900: {'loss': 0.8948, 'grad_norm': 1.895330786705017, 'learning_rate': 4.171296296296297e-05, 'epoch': 2.75}
Step 9910: {'loss': 0.8571, 'grad_norm': 0.933299720287323, 'learning_rate': 4.125e-05, 'epoch': 2.7527777777777778}
Step 9920: {'loss': 0.9585, 'grad_norm': 1.2204878330230713, 'learning_rate': 4.078703703703704e-05, 'epoch': 2.7555555555555555}
Step 9930: {'loss': 0.8974, 'grad_norm': 1.0865353345870972, 'learning_rate': 4.032407407407407e-05, 'epoch': 2.7583333333333333}
Step 9940: {'loss': 0.9688, 'grad_norm': 1.5620735883712769, 'learning_rate': 3.986111111111111e-05, 'epoch': 2.761111111111111}
Step 9950: {'loss': 0.9036, 'grad_norm': 1.0338244438171387, 'learning_rate': 3.9398148148148146e-05, 'epoch': 2.763888888888889}
Step 9960: {'loss': 0.957, 'grad_norm': 1.796406865119934, 'learning_rate': 3.8935185185185185e-05, 'epoch': 2.7666666666666666}
Step 9970: {'loss': 0.8625, 'grad_norm': 1.1785647869110107, 'learning_rate': 3.847222222222222e-05, 'epoch': 2.7694444444444444}
Step 9980: {'loss': 0.9619, 'grad_norm': 1.2269233465194702, 'learning_rate': 3.800925925925926e-05, 'epoch': 2.772222222222222}
Step 9990: {'loss': 0.9197, 'grad_norm': 1.634590983390808, 'learning_rate': 3.754629629629629e-05, 'epoch': 2.775}
Step 10000: {'loss': 0.938, 'grad_norm': 1.9209414720535278, 'learning_rate': 3.708333333333334e-05, 'epoch': 2.7777777777777777}
Step 10010: {'loss': 0.9851, 'grad_norm': 2.103663682937622, 'learning_rate': 3.662037037037037e-05, 'epoch': 2.7805555555555554}
Step 10020: {'loss': 0.8151, 'grad_norm': 1.591740369796753, 'learning_rate': 3.615740740740741e-05, 'epoch': 2.783333333333333}
Step 10030: {'loss': 0.9147, 'grad_norm': 2.287766695022583, 'learning_rate': 3.5694444444444444e-05, 'epoch': 2.786111111111111}
Step 10040: {'loss': 0.8353, 'grad_norm': 2.24454927444458, 'learning_rate': 3.5231481481481484e-05, 'epoch': 2.7888888888888888}
Step 10050: {'loss': 0.8674, 'grad_norm': 1.1862518787384033, 'learning_rate': 3.476851851851852e-05, 'epoch': 2.7916666666666665}
Step 10060: {'loss': 0.9734, 'grad_norm': 1.8384618759155273, 'learning_rate': 3.430555555555556e-05, 'epoch': 2.7944444444444443}
Step 10070: {'loss': 0.7976, 'grad_norm': 1.4895330667495728, 'learning_rate': 3.384259259259259e-05, 'epoch': 2.7972222222222225}
Step 10080: {'loss': 0.8742, 'grad_norm': 1.0825754404067993, 'learning_rate': 3.337962962962963e-05, 'epoch': 2.8}
Step 10090: {'loss': 1.0332, 'grad_norm': 1.326170563697815, 'learning_rate': 3.291666666666666e-05, 'epoch': 2.802777777777778}
Step 10100: {'loss': 0.968, 'grad_norm': 1.2170531749725342, 'learning_rate': 3.245370370370371e-05, 'epoch': 2.8055555555555554}
Step 10110: {'loss': 0.9217, 'grad_norm': 1.7256801128387451, 'learning_rate': 3.199074074074074e-05, 'epoch': 2.8083333333333336}
Step 10120: {'loss': 0.9739, 'grad_norm': 1.3644222021102905, 'learning_rate': 3.152777777777778e-05, 'epoch': 2.811111111111111}
Step 10130: {'loss': 0.9441, 'grad_norm': 1.2912875413894653, 'learning_rate': 3.1064814814814815e-05, 'epoch': 2.813888888888889}
Step 10140: {'loss': 0.9905, 'grad_norm': 1.3817622661590576, 'learning_rate': 3.0601851851851855e-05, 'epoch': 2.8166666666666664}
Step 10150: {'loss': 0.9283, 'grad_norm': 1.635204553604126, 'learning_rate': 3.0138888888888888e-05, 'epoch': 2.8194444444444446}
Step 10160: {'loss': 0.9063, 'grad_norm': 1.7081421613693237, 'learning_rate': 2.9675925925925925e-05, 'epoch': 2.822222222222222}
Step 10170: {'loss': 0.924, 'grad_norm': 1.7527166604995728, 'learning_rate': 2.921296296296296e-05, 'epoch': 2.825}
Step 10180: {'loss': 0.9049, 'grad_norm': 2.102631092071533, 'learning_rate': 2.875e-05, 'epoch': 2.8277777777777775}
Step 10190: {'loss': 0.9627, 'grad_norm': 1.3899118900299072, 'learning_rate': 2.8287037037037037e-05, 'epoch': 2.8305555555555557}
Step 10200: {'loss': 0.7869, 'grad_norm': 0.47658005356788635, 'learning_rate': 2.7824074074074074e-05, 'epoch': 2.8333333333333335}
Step 10210: {'loss': 0.9721, 'grad_norm': 2.1858370304107666, 'learning_rate': 2.736111111111111e-05, 'epoch': 2.8361111111111112}
Step 10220: {'loss': 0.9401, 'grad_norm': 1.4311647415161133, 'learning_rate': 2.6898148148148147e-05, 'epoch': 2.838888888888889}
Step 10230: {'loss': 1.0237, 'grad_norm': 3.0960140228271484, 'learning_rate': 2.6435185185185187e-05, 'epoch': 2.841666666666667}
Step 10240: {'loss': 1.0499, 'grad_norm': 1.2855896949768066, 'learning_rate': 2.5972222222222223e-05, 'epoch': 2.8444444444444446}
Step 10250: {'loss': 0.8725, 'grad_norm': 1.9431201219558716, 'learning_rate': 2.550925925925926e-05, 'epoch': 2.8472222222222223}
Step 10260: {'loss': 0.9716, 'grad_norm': 1.5386831760406494, 'learning_rate': 2.5046296296296296e-05, 'epoch': 2.85}
Step 10270: {'loss': 0.8676, 'grad_norm': 1.3648687601089478, 'learning_rate': 2.4583333333333332e-05, 'epoch': 2.852777777777778}
Step 10280: {'loss': 0.9719, 'grad_norm': 2.153379201889038, 'learning_rate': 2.4120370370370372e-05, 'epoch': 2.8555555555555556}
Step 10290: {'loss': 0.9173, 'grad_norm': 0.8905815482139587, 'learning_rate': 2.365740740740741e-05, 'epoch': 2.8583333333333334}
Step 10300: {'loss': 0.929, 'grad_norm': 2.3505513668060303, 'learning_rate': 2.3194444444444445e-05, 'epoch': 2.861111111111111}
Step 10310: {'loss': 0.9205, 'grad_norm': 1.3883740901947021, 'learning_rate': 2.273148148148148e-05, 'epoch': 2.863888888888889}
Step 10320: {'loss': 1.0154, 'grad_norm': 1.5958020687103271, 'learning_rate': 2.2268518518518518e-05, 'epoch': 2.8666666666666667}
Step 10330: {'loss': 0.962, 'grad_norm': 0.8463643193244934, 'learning_rate': 2.1805555555555558e-05, 'epoch': 2.8694444444444445}
Step 10340: {'loss': 0.9798, 'grad_norm': 1.192266583442688, 'learning_rate': 2.1342592592592594e-05, 'epoch': 2.8722222222222222}
Step 10350: {'loss': 0.8843, 'grad_norm': 2.3997368812561035, 'learning_rate': 2.087962962962963e-05, 'epoch': 2.875}
Step 10360: {'loss': 0.9202, 'grad_norm': 2.5328915119171143, 'learning_rate': 2.0416666666666667e-05, 'epoch': 2.8777777777777778}
Step 10370: {'loss': 0.8984, 'grad_norm': 1.292099118232727, 'learning_rate': 1.9953703703703704e-05, 'epoch': 2.8805555555555555}
Step 10380: {'loss': 0.9755, 'grad_norm': 1.163648247718811, 'learning_rate': 1.949074074074074e-05, 'epoch': 2.8833333333333333}
Step 10390: {'loss': 0.9174, 'grad_norm': 2.4284472465515137, 'learning_rate': 1.902777777777778e-05, 'epoch': 2.886111111111111}
Step 10400: {'loss': 1.0127, 'grad_norm': 1.515344262123108, 'learning_rate': 1.8564814814814816e-05, 'epoch': 2.888888888888889}
Step 10410: {'loss': 0.8583, 'grad_norm': 1.2529383897781372, 'learning_rate': 1.8101851851851853e-05, 'epoch': 2.8916666666666666}
Step 10420: {'loss': 0.9064, 'grad_norm': 2.047262191772461, 'learning_rate': 1.763888888888889e-05, 'epoch': 2.8944444444444444}
Step 10430: {'loss': 0.9133, 'grad_norm': 1.4073237180709839, 'learning_rate': 1.7175925925925926e-05, 'epoch': 2.897222222222222}
Step 10440: {'loss': 0.8721, 'grad_norm': 1.6386717557907104, 'learning_rate': 1.6712962962962966e-05, 'epoch': 2.9}
Step 10450: {'loss': 0.9749, 'grad_norm': 1.8051129579544067, 'learning_rate': 1.6250000000000002e-05, 'epoch': 2.9027777777777777}
Step 10460: {'loss': 0.9544, 'grad_norm': 1.304119348526001, 'learning_rate': 1.578703703703704e-05, 'epoch': 2.9055555555555554}
Step 10470: {'loss': 0.9516, 'grad_norm': 1.3802109956741333, 'learning_rate': 1.5324074074074075e-05, 'epoch': 2.908333333333333}
Step 10480: {'loss': 0.9449, 'grad_norm': 1.5916486978530884, 'learning_rate': 1.4861111111111111e-05, 'epoch': 2.911111111111111}
Step 10490: {'loss': 0.9605, 'grad_norm': 1.7147706747055054, 'learning_rate': 1.4398148148148148e-05, 'epoch': 2.9138888888888888}
Step 10500: {'loss': 0.879, 'grad_norm': 1.2397050857543945, 'learning_rate': 1.3935185185185186e-05, 'epoch': 2.9166666666666665}
Step 10510: {'loss': 0.9562, 'grad_norm': 1.0678895711898804, 'learning_rate': 1.3472222222222222e-05, 'epoch': 2.9194444444444443}
Step 10520: {'loss': 0.9075, 'grad_norm': 1.6341384649276733, 'learning_rate': 1.3009259259259259e-05, 'epoch': 2.9222222222222225}
Step 10530: {'loss': 0.9486, 'grad_norm': 1.8417150974273682, 'learning_rate': 1.2546296296296297e-05, 'epoch': 2.925}
Step 10540: {'loss': 0.9588, 'grad_norm': 1.7981948852539062, 'learning_rate': 1.2083333333333333e-05, 'epoch': 2.927777777777778}
Step 10550: {'loss': 0.8819, 'grad_norm': 1.5501132011413574, 'learning_rate': 1.1620370370370372e-05, 'epoch': 2.9305555555555554}
Step 10560: {'loss': 0.9337, 'grad_norm': 1.6241061687469482, 'learning_rate': 1.1157407407407408e-05, 'epoch': 2.9333333333333336}
Step 10570: {'loss': 0.9463, 'grad_norm': 1.408655047416687, 'learning_rate': 1.0694444444444444e-05, 'epoch': 2.936111111111111}
Step 10580: {'loss': 0.9134, 'grad_norm': 2.1507489681243896, 'learning_rate': 1.0231481481481483e-05, 'epoch': 2.938888888888889}
Step 10590: {'loss': 0.891, 'grad_norm': 1.3263636827468872, 'learning_rate': 9.768518518518519e-06, 'epoch': 2.9416666666666664}
Step 10600: {'loss': 0.8523, 'grad_norm': 1.1127095222473145, 'learning_rate': 9.305555555555555e-06, 'epoch': 2.9444444444444446}
Step 10610: {'loss': 0.958, 'grad_norm': 1.9061768054962158, 'learning_rate': 8.842592592592594e-06, 'epoch': 2.947222222222222}
Step 10620: {'loss': 0.839, 'grad_norm': 1.1071135997772217, 'learning_rate': 8.37962962962963e-06, 'epoch': 2.95}
Step 10630: {'loss': 0.8993, 'grad_norm': 1.3503620624542236, 'learning_rate': 7.916666666666668e-06, 'epoch': 2.9527777777777775}
Step 10640: {'loss': 0.8312, 'grad_norm': 0.9905372858047485, 'learning_rate': 7.453703703703704e-06, 'epoch': 2.9555555555555557}
Step 10650: {'loss': 0.8785, 'grad_norm': 1.0987417697906494, 'learning_rate': 6.990740740740741e-06, 'epoch': 2.9583333333333335}
Step 10660: {'loss': 0.8939, 'grad_norm': 1.4275587797164917, 'learning_rate': 6.5277777777777784e-06, 'epoch': 2.9611111111111112}
Step 10670: {'loss': 0.954, 'grad_norm': 1.7033580541610718, 'learning_rate': 6.064814814814815e-06, 'epoch': 2.963888888888889}
Step 10680: {'loss': 0.9539, 'grad_norm': 1.6836810111999512, 'learning_rate': 5.601851851851852e-06, 'epoch': 2.966666666666667}
Step 10690: {'loss': 0.8329, 'grad_norm': 1.3696774244308472, 'learning_rate': 5.1388888888888895e-06, 'epoch': 2.9694444444444446}
Step 10700: {'loss': 0.8971, 'grad_norm': 1.4494928121566772, 'learning_rate': 4.675925925925927e-06, 'epoch': 2.9722222222222223}
Step 10710: {'loss': 0.8609, 'grad_norm': 1.8841906785964966, 'learning_rate': 4.212962962962962e-06, 'epoch': 2.975}
Step 10720: {'loss': 0.9544, 'grad_norm': 2.049356460571289, 'learning_rate': 3.75e-06, 'epoch': 2.977777777777778}
Step 10730: {'loss': 0.9407, 'grad_norm': 1.1334933042526245, 'learning_rate': 3.287037037037037e-06, 'epoch': 2.9805555555555556}
Step 10740: {'loss': 0.8931, 'grad_norm': 1.1025208234786987, 'learning_rate': 2.824074074074074e-06, 'epoch': 2.9833333333333334}
Step 10750: {'loss': 0.8993, 'grad_norm': 1.2961689233779907, 'learning_rate': 2.361111111111111e-06, 'epoch': 2.986111111111111}
Step 10760: {'loss': 1.0194, 'grad_norm': 1.3298301696777344, 'learning_rate': 1.8981481481481482e-06, 'epoch': 2.988888888888889}
Step 10770: {'loss': 0.8624, 'grad_norm': 1.3971725702285767, 'learning_rate': 1.4351851851851853e-06, 'epoch': 2.9916666666666667}
Step 10780: {'loss': 0.8626, 'grad_norm': 1.4916104078292847, 'learning_rate': 9.722222222222222e-07, 'epoch': 2.9944444444444445}
Step 10790: {'loss': 0.9524, 'grad_norm': 1.3108340501785278, 'learning_rate': 5.092592592592593e-07, 'epoch': 2.9972222222222222}
Step 10800: {'loss': 0.9157, 'grad_norm': 1.2352701425552368, 'learning_rate': 4.6296296296296295e-08, 'epoch': 3.0}
Step 10800: {'train_runtime': 1578.5667, 'train_samples_per_second': 13.683, 'train_steps_per_second': 6.842, 'total_flos': 2.304528320935895e+17, 'train_loss': 0.9396199055954262, 'epoch': 3.0}
