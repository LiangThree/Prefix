Step 10: {'loss': 1.4158, 'grad_norm': 1.1269850730895996, 'learning_rate': 0.004995833333333333, 'epoch': 0.002777777777777778}
Step 20: {'loss': 1.1296, 'grad_norm': 0.878030002117157, 'learning_rate': 0.004991203703703704, 'epoch': 0.005555555555555556}
Step 30: {'loss': 1.0986, 'grad_norm': 0.6804242134094238, 'learning_rate': 0.004986574074074074, 'epoch': 0.008333333333333333}
Step 40: {'loss': 1.0363, 'grad_norm': 0.643130898475647, 'learning_rate': 0.004981944444444444, 'epoch': 0.011111111111111112}
Step 50: {'loss': 0.9282, 'grad_norm': 0.5384539365768433, 'learning_rate': 0.0049773148148148155, 'epoch': 0.013888888888888888}
Step 60: {'loss': 1.1567, 'grad_norm': 0.8043615221977234, 'learning_rate': 0.004972685185185185, 'epoch': 0.016666666666666666}
Step 70: {'loss': 1.0216, 'grad_norm': 0.5433093905448914, 'learning_rate': 0.004968055555555555, 'epoch': 0.019444444444444445}
Step 80: {'loss': 0.9586, 'grad_norm': 0.578259289264679, 'learning_rate': 0.004963425925925926, 'epoch': 0.022222222222222223}
Step 90: {'loss': 0.9974, 'grad_norm': 0.6064121127128601, 'learning_rate': 0.004958796296296296, 'epoch': 0.025}
Step 100: {'loss': 0.966, 'grad_norm': 0.5604612827301025, 'learning_rate': 0.004954166666666667, 'epoch': 0.027777777777777776}
Step 110: {'loss': 0.9796, 'grad_norm': 0.8032829761505127, 'learning_rate': 0.004949537037037037, 'epoch': 0.030555555555555555}
Step 120: {'loss': 0.9957, 'grad_norm': 0.4847157597541809, 'learning_rate': 0.004944907407407407, 'epoch': 0.03333333333333333}
Step 130: {'loss': 0.9833, 'grad_norm': 0.9057942628860474, 'learning_rate': 0.004940277777777778, 'epoch': 0.03611111111111111}
Step 140: {'loss': 1.0743, 'grad_norm': 0.6379407048225403, 'learning_rate': 0.004935648148148148, 'epoch': 0.03888888888888889}
Step 150: {'loss': 1.0412, 'grad_norm': 0.6803471446037292, 'learning_rate': 0.004931018518518519, 'epoch': 0.041666666666666664}
Step 160: {'loss': 1.1122, 'grad_norm': 0.625744104385376, 'learning_rate': 0.004926388888888889, 'epoch': 0.044444444444444446}
Step 170: {'loss': 1.0251, 'grad_norm': 0.560485303401947, 'learning_rate': 0.004921759259259259, 'epoch': 0.04722222222222222}
Step 180: {'loss': 1.0586, 'grad_norm': 0.7657594084739685, 'learning_rate': 0.00491712962962963, 'epoch': 0.05}
Step 190: {'loss': 0.9179, 'grad_norm': 0.6272537708282471, 'learning_rate': 0.0049125, 'epoch': 0.05277777777777778}
Step 200: {'loss': 0.982, 'grad_norm': 0.46627846360206604, 'learning_rate': 0.004907870370370371, 'epoch': 0.05555555555555555}
Step 210: {'loss': 0.9824, 'grad_norm': 0.5475212335586548, 'learning_rate': 0.004903240740740741, 'epoch': 0.058333333333333334}
Step 220: {'loss': 1.028, 'grad_norm': 0.5669561624526978, 'learning_rate': 0.004898611111111111, 'epoch': 0.06111111111111111}
Step 230: {'loss': 1.0368, 'grad_norm': 0.7656169533729553, 'learning_rate': 0.004893981481481482, 'epoch': 0.06388888888888888}
Step 240: {'loss': 0.9995, 'grad_norm': 0.4476396441459656, 'learning_rate': 0.004889351851851852, 'epoch': 0.06666666666666667}
Step 250: {'loss': 0.9862, 'grad_norm': 0.7722144722938538, 'learning_rate': 0.004884722222222222, 'epoch': 0.06944444444444445}
Step 260: {'loss': 0.9798, 'grad_norm': 0.7332770824432373, 'learning_rate': 0.004880092592592593, 'epoch': 0.07222222222222222}
Step 270: {'loss': 0.9517, 'grad_norm': 0.8462305068969727, 'learning_rate': 0.004875462962962963, 'epoch': 0.075}
Step 280: {'loss': 0.9716, 'grad_norm': 0.6550981998443604, 'learning_rate': 0.004870833333333333, 'epoch': 0.07777777777777778}
Step 290: {'loss': 0.953, 'grad_norm': 0.43412935733795166, 'learning_rate': 0.004866203703703704, 'epoch': 0.08055555555555556}
Step 300: {'loss': 1.0119, 'grad_norm': 0.3833317458629608, 'learning_rate': 0.004861574074074075, 'epoch': 0.08333333333333333}
Step 310: {'loss': 0.9494, 'grad_norm': 0.5333985090255737, 'learning_rate': 0.004856944444444444, 'epoch': 0.08611111111111111}
Step 320: {'loss': 0.9071, 'grad_norm': 0.4335028827190399, 'learning_rate': 0.004852314814814815, 'epoch': 0.08888888888888889}
Step 330: {'loss': 0.9709, 'grad_norm': 0.6619937419891357, 'learning_rate': 0.004847685185185186, 'epoch': 0.09166666666666666}
Step 340: {'loss': 0.9377, 'grad_norm': 0.6629326939582825, 'learning_rate': 0.004843055555555555, 'epoch': 0.09444444444444444}
Step 350: {'loss': 1.0081, 'grad_norm': 0.8062015771865845, 'learning_rate': 0.004838425925925926, 'epoch': 0.09722222222222222}
Step 360: {'loss': 0.9023, 'grad_norm': 0.498205304145813, 'learning_rate': 0.004833796296296296, 'epoch': 0.1}
Step 370: {'loss': 0.971, 'grad_norm': 0.7333583831787109, 'learning_rate': 0.0048291666666666665, 'epoch': 0.10277777777777777}
Step 380: {'loss': 1.0143, 'grad_norm': 0.7384545803070068, 'learning_rate': 0.004824537037037037, 'epoch': 0.10555555555555556}
Step 390: {'loss': 0.9534, 'grad_norm': 0.6361697316169739, 'learning_rate': 0.004819907407407407, 'epoch': 0.10833333333333334}
Step 400: {'loss': 1.0085, 'grad_norm': 0.854253351688385, 'learning_rate': 0.004815277777777778, 'epoch': 0.1111111111111111}
Step 410: {'loss': 1.0186, 'grad_norm': 0.5366758704185486, 'learning_rate': 0.004810648148148148, 'epoch': 0.11388888888888889}
Step 420: {'loss': 0.96, 'grad_norm': 0.530992329120636, 'learning_rate': 0.0048060185185185185, 'epoch': 0.11666666666666667}
Step 430: {'loss': 0.9745, 'grad_norm': 0.6405065655708313, 'learning_rate': 0.004801388888888889, 'epoch': 0.11944444444444445}
Step 440: {'loss': 0.9256, 'grad_norm': 0.6201745867729187, 'learning_rate': 0.004796759259259259, 'epoch': 0.12222222222222222}
Step 450: {'loss': 0.9349, 'grad_norm': 0.8297809958457947, 'learning_rate': 0.00479212962962963, 'epoch': 0.125}
Step 460: {'loss': 0.9741, 'grad_norm': 0.4289446175098419, 'learning_rate': 0.0047875, 'epoch': 0.12777777777777777}
Step 470: {'loss': 0.9222, 'grad_norm': 0.633669376373291, 'learning_rate': 0.0047828703703703705, 'epoch': 0.13055555555555556}
Step 480: {'loss': 0.9617, 'grad_norm': 0.9116618037223816, 'learning_rate': 0.004778240740740741, 'epoch': 0.13333333333333333}
Step 490: {'loss': 0.9704, 'grad_norm': 0.5910740494728088, 'learning_rate': 0.004773611111111111, 'epoch': 0.1361111111111111}
Step 500: {'loss': 0.9062, 'grad_norm': 0.40948015451431274, 'learning_rate': 0.004768981481481482, 'epoch': 0.1388888888888889}
Step 510: {'loss': 1.0623, 'grad_norm': 0.7724609971046448, 'learning_rate': 0.004764351851851852, 'epoch': 0.14166666666666666}
Step 520: {'loss': 0.9438, 'grad_norm': 0.6548120975494385, 'learning_rate': 0.0047597222222222225, 'epoch': 0.14444444444444443}
Step 530: {'loss': 1.015, 'grad_norm': 0.742460310459137, 'learning_rate': 0.004755092592592593, 'epoch': 0.14722222222222223}
Step 540: {'loss': 0.9606, 'grad_norm': 0.4867318570613861, 'learning_rate': 0.004750462962962963, 'epoch': 0.15}
Step 550: {'loss': 0.9261, 'grad_norm': 0.5782471895217896, 'learning_rate': 0.004745833333333334, 'epoch': 0.1527777777777778}
Step 560: {'loss': 0.9178, 'grad_norm': 0.5606014728546143, 'learning_rate': 0.004741203703703704, 'epoch': 0.15555555555555556}
Step 570: {'loss': 0.9012, 'grad_norm': 0.6987161040306091, 'learning_rate': 0.0047365740740740745, 'epoch': 0.15833333333333333}
Step 580: {'loss': 0.9505, 'grad_norm': 0.5032804012298584, 'learning_rate': 0.004731944444444444, 'epoch': 0.16111111111111112}
Step 590: {'loss': 0.9015, 'grad_norm': 0.5104968547821045, 'learning_rate': 0.004727314814814815, 'epoch': 0.1638888888888889}
Step 600: {'loss': 0.9668, 'grad_norm': 0.8348782062530518, 'learning_rate': 0.004722685185185186, 'epoch': 0.16666666666666666}
Step 610: {'loss': 1.0368, 'grad_norm': 0.6384134292602539, 'learning_rate': 0.004718055555555555, 'epoch': 0.16944444444444445}
Step 620: {'loss': 0.9173, 'grad_norm': 0.7228387594223022, 'learning_rate': 0.004713425925925926, 'epoch': 0.17222222222222222}
Step 630: {'loss': 0.9172, 'grad_norm': 0.6761106848716736, 'learning_rate': 0.004708796296296297, 'epoch': 0.175}
Step 640: {'loss': 1.149, 'grad_norm': 0.5550603270530701, 'learning_rate': 0.004704166666666666, 'epoch': 0.17777777777777778}
Step 650: {'loss': 0.9555, 'grad_norm': 0.5069757103919983, 'learning_rate': 0.004699537037037037, 'epoch': 0.18055555555555555}
Step 660: {'loss': 1.0093, 'grad_norm': 0.5685664415359497, 'learning_rate': 0.004694907407407408, 'epoch': 0.18333333333333332}
Step 670: {'loss': 1.0017, 'grad_norm': 0.7865511775016785, 'learning_rate': 0.004690277777777778, 'epoch': 0.18611111111111112}
Step 680: {'loss': 0.9221, 'grad_norm': 0.7058616876602173, 'learning_rate': 0.004685648148148148, 'epoch': 0.18888888888888888}
Step 690: {'loss': 0.9632, 'grad_norm': 0.38512009382247925, 'learning_rate': 0.004681018518518518, 'epoch': 0.19166666666666668}
Step 700: {'loss': 1.0934, 'grad_norm': 0.6904234290122986, 'learning_rate': 0.004676388888888889, 'epoch': 0.19444444444444445}
Step 710: {'loss': 1.0299, 'grad_norm': 0.6884428858757019, 'learning_rate': 0.004671759259259259, 'epoch': 0.19722222222222222}
Step 720: {'loss': 0.9809, 'grad_norm': 0.5947049260139465, 'learning_rate': 0.00466712962962963, 'epoch': 0.2}
Step 730: {'loss': 0.9733, 'grad_norm': 0.5382149815559387, 'learning_rate': 0.0046625, 'epoch': 0.20277777777777778}
Step 740: {'loss': 1.0734, 'grad_norm': 0.8821700215339661, 'learning_rate': 0.00465787037037037, 'epoch': 0.20555555555555555}
Step 750: {'loss': 1.0024, 'grad_norm': 0.5628542900085449, 'learning_rate': 0.004653240740740741, 'epoch': 0.20833333333333334}
Step 760: {'loss': 0.9592, 'grad_norm': 0.46344470977783203, 'learning_rate': 0.004648611111111111, 'epoch': 0.2111111111111111}
Step 770: {'loss': 0.9262, 'grad_norm': 0.6656364798545837, 'learning_rate': 0.004643981481481482, 'epoch': 0.21388888888888888}
Step 780: {'loss': 0.9092, 'grad_norm': 0.6876776218414307, 'learning_rate': 0.004639351851851852, 'epoch': 0.21666666666666667}
Step 790: {'loss': 1.0946, 'grad_norm': 0.6812503337860107, 'learning_rate': 0.004634722222222222, 'epoch': 0.21944444444444444}
Step 800: {'loss': 0.93, 'grad_norm': 0.5574154257774353, 'learning_rate': 0.004630092592592593, 'epoch': 0.2222222222222222}
Step 810: {'loss': 1.0147, 'grad_norm': 0.6418565511703491, 'learning_rate': 0.004625462962962963, 'epoch': 0.225}
Step 820: {'loss': 1.033, 'grad_norm': 0.6434011459350586, 'learning_rate': 0.0046208333333333336, 'epoch': 0.22777777777777777}
Step 830: {'loss': 0.91, 'grad_norm': 0.8970142006874084, 'learning_rate': 0.004616203703703704, 'epoch': 0.23055555555555557}
Step 840: {'loss': 0.9741, 'grad_norm': 0.5114428997039795, 'learning_rate': 0.004611574074074074, 'epoch': 0.23333333333333334}
Step 850: {'loss': 0.9204, 'grad_norm': 0.5110620260238647, 'learning_rate': 0.004606944444444445, 'epoch': 0.2361111111111111}
Step 860: {'loss': 0.835, 'grad_norm': 0.5450771450996399, 'learning_rate': 0.004602314814814815, 'epoch': 0.2388888888888889}
Step 870: {'loss': 0.9768, 'grad_norm': 0.5985272526741028, 'learning_rate': 0.0045976851851851856, 'epoch': 0.24166666666666667}
Step 880: {'loss': 0.8864, 'grad_norm': 0.7476564049720764, 'learning_rate': 0.004593055555555556, 'epoch': 0.24444444444444444}
Step 890: {'loss': 1.0067, 'grad_norm': 0.8572697043418884, 'learning_rate': 0.0045884259259259255, 'epoch': 0.24722222222222223}
Step 900: {'loss': 0.9884, 'grad_norm': 0.4750540852546692, 'learning_rate': 0.004583796296296297, 'epoch': 0.25}
Step 910: {'loss': 0.9736, 'grad_norm': 0.5489722490310669, 'learning_rate': 0.004579166666666667, 'epoch': 0.25277777777777777}
Step 920: {'loss': 1.0187, 'grad_norm': 1.3458139896392822, 'learning_rate': 0.004574537037037037, 'epoch': 0.25555555555555554}
Step 930: {'loss': 0.9992, 'grad_norm': 0.4919204115867615, 'learning_rate': 0.004569907407407408, 'epoch': 0.25833333333333336}
Step 940: {'loss': 0.957, 'grad_norm': 0.6420310139656067, 'learning_rate': 0.0045652777777777775, 'epoch': 0.2611111111111111}
Step 950: {'loss': 1.0637, 'grad_norm': 0.7150123119354248, 'learning_rate': 0.004560648148148148, 'epoch': 0.2638888888888889}
Step 960: {'loss': 0.9264, 'grad_norm': 0.7066187262535095, 'learning_rate': 0.004556018518518519, 'epoch': 0.26666666666666666}
Step 970: {'loss': 0.9297, 'grad_norm': 0.6105939745903015, 'learning_rate': 0.004551388888888889, 'epoch': 0.26944444444444443}
Step 980: {'loss': 0.933, 'grad_norm': 0.5941967964172363, 'learning_rate': 0.004546759259259259, 'epoch': 0.2722222222222222}
Step 990: {'loss': 0.9591, 'grad_norm': 0.4076458215713501, 'learning_rate': 0.00454212962962963, 'epoch': 0.275}
Step 1000: {'loss': 0.9559, 'grad_norm': 0.4945108890533447, 'learning_rate': 0.0045375, 'epoch': 0.2777777777777778}
Step 1010: {'loss': 0.9845, 'grad_norm': 0.7885345220565796, 'learning_rate': 0.00453287037037037, 'epoch': 0.28055555555555556}
Step 1020: {'loss': 0.9285, 'grad_norm': 0.45543432235717773, 'learning_rate': 0.0045282407407407415, 'epoch': 0.2833333333333333}
Step 1030: {'loss': 1.0293, 'grad_norm': 0.9429647922515869, 'learning_rate': 0.004523611111111111, 'epoch': 0.2861111111111111}
Step 1040: {'loss': 1.0968, 'grad_norm': 0.5026959180831909, 'learning_rate': 0.0045189814814814815, 'epoch': 0.28888888888888886}
Step 1050: {'loss': 0.9576, 'grad_norm': 0.6712488532066345, 'learning_rate': 0.004514351851851852, 'epoch': 0.2916666666666667}
Step 1060: {'loss': 1.0378, 'grad_norm': 0.9554492235183716, 'learning_rate': 0.004509722222222222, 'epoch': 0.29444444444444445}
Step 1070: {'loss': 1.0472, 'grad_norm': 0.5333801507949829, 'learning_rate': 0.004505092592592593, 'epoch': 0.2972222222222222}
Step 1080: {'loss': 1.005, 'grad_norm': 0.5533141493797302, 'learning_rate': 0.004500462962962963, 'epoch': 0.3}
Step 1090: {'loss': 0.9639, 'grad_norm': 0.6952240467071533, 'learning_rate': 0.0044958333333333335, 'epoch': 0.30277777777777776}
Step 1100: {'loss': 1.0315, 'grad_norm': 0.7400302290916443, 'learning_rate': 0.004491203703703704, 'epoch': 0.3055555555555556}
Step 1110: {'loss': 0.9821, 'grad_norm': 0.9933661818504333, 'learning_rate': 0.004486574074074074, 'epoch': 0.30833333333333335}
Step 1120: {'loss': 0.992, 'grad_norm': 1.4590559005737305, 'learning_rate': 0.004481944444444445, 'epoch': 0.3111111111111111}
Step 1130: {'loss': 0.9629, 'grad_norm': 0.5546985268592834, 'learning_rate': 0.004477314814814815, 'epoch': 0.3138888888888889}
Step 1140: {'loss': 0.9413, 'grad_norm': 0.7312831282615662, 'learning_rate': 0.0044726851851851854, 'epoch': 0.31666666666666665}
Step 1150: {'loss': 1.1382, 'grad_norm': 0.6928946375846863, 'learning_rate': 0.004468055555555556, 'epoch': 0.3194444444444444}
Step 1160: {'loss': 0.9919, 'grad_norm': 0.6875424385070801, 'learning_rate': 0.004463425925925925, 'epoch': 0.32222222222222224}
Step 1170: {'loss': 1.0601, 'grad_norm': 0.5755794048309326, 'learning_rate': 0.004458796296296297, 'epoch': 0.325}
Step 1180: {'loss': 0.9872, 'grad_norm': 0.8401677012443542, 'learning_rate': 0.004454166666666667, 'epoch': 0.3277777777777778}
Step 1190: {'loss': 1.0103, 'grad_norm': 0.5153188705444336, 'learning_rate': 0.004449537037037037, 'epoch': 0.33055555555555555}
Step 1200: {'loss': 1.0111, 'grad_norm': 0.687743067741394, 'learning_rate': 0.004444907407407408, 'epoch': 0.3333333333333333}
Step 1210: {'loss': 0.9823, 'grad_norm': 0.6712241172790527, 'learning_rate': 0.004440277777777778, 'epoch': 0.33611111111111114}
Step 1220: {'loss': 0.9323, 'grad_norm': 0.7104376554489136, 'learning_rate': 0.004435648148148148, 'epoch': 0.3388888888888889}
Step 1230: {'loss': 0.9492, 'grad_norm': 0.9300121068954468, 'learning_rate': 0.004431018518518519, 'epoch': 0.3416666666666667}
Step 1240: {'loss': 0.9959, 'grad_norm': 0.6151374578475952, 'learning_rate': 0.004426388888888889, 'epoch': 0.34444444444444444}
Step 1250: {'loss': 1.0071, 'grad_norm': 0.6786796450614929, 'learning_rate': 0.004421759259259259, 'epoch': 0.3472222222222222}
Step 1260: {'loss': 1.0612, 'grad_norm': 0.7008880376815796, 'learning_rate': 0.00441712962962963, 'epoch': 0.35}
Step 1270: {'loss': 0.993, 'grad_norm': 0.6984597444534302, 'learning_rate': 0.0044125, 'epoch': 0.3527777777777778}
Step 1280: {'loss': 0.9766, 'grad_norm': 0.7197062969207764, 'learning_rate': 0.00440787037037037, 'epoch': 0.35555555555555557}
Step 1290: {'loss': 1.0143, 'grad_norm': 0.6321749687194824, 'learning_rate': 0.004403240740740741, 'epoch': 0.35833333333333334}
Step 1300: {'loss': 1.0938, 'grad_norm': 0.5535100102424622, 'learning_rate': 0.004398611111111111, 'epoch': 0.3611111111111111}
Step 1310: {'loss': 1.0341, 'grad_norm': 0.6861311197280884, 'learning_rate': 0.004393981481481481, 'epoch': 0.3638888888888889}
Step 1320: {'loss': 1.006, 'grad_norm': 0.4747249186038971, 'learning_rate': 0.004389351851851852, 'epoch': 0.36666666666666664}
Step 1330: {'loss': 0.9981, 'grad_norm': 0.6444417238235474, 'learning_rate': 0.004384722222222222, 'epoch': 0.36944444444444446}
Step 1340: {'loss': 1.0618, 'grad_norm': 0.8124707937240601, 'learning_rate': 0.0043800925925925925, 'epoch': 0.37222222222222223}
Step 1350: {'loss': 0.9937, 'grad_norm': 0.6861783862113953, 'learning_rate': 0.004375462962962963, 'epoch': 0.375}
Step 1360: {'loss': 0.8998, 'grad_norm': 0.719432532787323, 'learning_rate': 0.004370833333333333, 'epoch': 0.37777777777777777}
Step 1370: {'loss': 0.9735, 'grad_norm': 0.6971895098686218, 'learning_rate': 0.004366203703703704, 'epoch': 0.38055555555555554}
Step 1380: {'loss': 1.0836, 'grad_norm': 0.5743926763534546, 'learning_rate': 0.004361574074074074, 'epoch': 0.38333333333333336}
Step 1390: {'loss': 1.0212, 'grad_norm': 0.6459131240844727, 'learning_rate': 0.0043569444444444445, 'epoch': 0.3861111111111111}
Step 1400: {'loss': 1.003, 'grad_norm': 0.6916782855987549, 'learning_rate': 0.004352314814814815, 'epoch': 0.3888888888888889}
Step 1410: {'loss': 1.0754, 'grad_norm': 0.9982240796089172, 'learning_rate': 0.004347685185185185, 'epoch': 0.39166666666666666}
Step 1420: {'loss': 1.0156, 'grad_norm': 0.5832237005233765, 'learning_rate': 0.004343055555555556, 'epoch': 0.39444444444444443}
Step 1430: {'loss': 0.9295, 'grad_norm': 0.4763408601284027, 'learning_rate': 0.004338425925925926, 'epoch': 0.3972222222222222}
Step 1440: {'loss': 1.0081, 'grad_norm': 0.8675951361656189, 'learning_rate': 0.0043337962962962965, 'epoch': 0.4}
Step 1450: {'loss': 1.0254, 'grad_norm': 0.5175222158432007, 'learning_rate': 0.004329166666666667, 'epoch': 0.4027777777777778}
Step 1460: {'loss': 0.9378, 'grad_norm': 0.7540171146392822, 'learning_rate': 0.004324537037037037, 'epoch': 0.40555555555555556}
Step 1470: {'loss': 1.0245, 'grad_norm': 0.7710272669792175, 'learning_rate': 0.004319907407407408, 'epoch': 0.4083333333333333}
Step 1480: {'loss': 1.1082, 'grad_norm': 1.4354337453842163, 'learning_rate': 0.004315277777777778, 'epoch': 0.4111111111111111}
Step 1490: {'loss': 1.0161, 'grad_norm': 1.0037386417388916, 'learning_rate': 0.004310648148148148, 'epoch': 0.41388888888888886}
Step 1500: {'loss': 0.9885, 'grad_norm': 0.6396504044532776, 'learning_rate': 0.004306018518518519, 'epoch': 0.4166666666666667}
Step 1510: {'loss': 0.9501, 'grad_norm': 0.6431679129600525, 'learning_rate': 0.004301388888888889, 'epoch': 0.41944444444444445}
Step 1520: {'loss': 1.0443, 'grad_norm': 1.039528489112854, 'learning_rate': 0.004296759259259259, 'epoch': 0.4222222222222222}
Step 1530: {'loss': 0.9743, 'grad_norm': 0.4716019630432129, 'learning_rate': 0.00429212962962963, 'epoch': 0.425}
Step 1540: {'loss': 0.9083, 'grad_norm': 0.6229981184005737, 'learning_rate': 0.0042875000000000005, 'epoch': 0.42777777777777776}
Step 1550: {'loss': 0.878, 'grad_norm': 0.8940414190292358, 'learning_rate': 0.00428287037037037, 'epoch': 0.4305555555555556}
Step 1560: {'loss': 1.0431, 'grad_norm': 0.6940493583679199, 'learning_rate': 0.004278240740740741, 'epoch': 0.43333333333333335}
Step 1570: {'loss': 0.9835, 'grad_norm': 0.8622116446495056, 'learning_rate': 0.004273611111111112, 'epoch': 0.4361111111111111}
Step 1580: {'loss': 1.1051, 'grad_norm': 0.47837191820144653, 'learning_rate': 0.004268981481481481, 'epoch': 0.4388888888888889}
Step 1590: {'loss': 0.962, 'grad_norm': 0.8040961623191833, 'learning_rate': 0.004264351851851852, 'epoch': 0.44166666666666665}
Step 1600: {'loss': 1.0783, 'grad_norm': 0.8230637907981873, 'learning_rate': 0.004259722222222222, 'epoch': 0.4444444444444444}
Step 1610: {'loss': 1.0538, 'grad_norm': 0.6783211827278137, 'learning_rate': 0.004255092592592592, 'epoch': 0.44722222222222224}
Step 1620: {'loss': 0.9338, 'grad_norm': 0.551116406917572, 'learning_rate': 0.004250462962962963, 'epoch': 0.45}
Step 1630: {'loss': 1.0661, 'grad_norm': 0.9337629675865173, 'learning_rate': 0.004245833333333333, 'epoch': 0.4527777777777778}
Step 1640: {'loss': 1.0678, 'grad_norm': 1.1303783655166626, 'learning_rate': 0.004241203703703704, 'epoch': 0.45555555555555555}
Step 1650: {'loss': 0.9469, 'grad_norm': 0.7161524891853333, 'learning_rate': 0.004236574074074074, 'epoch': 0.4583333333333333}
Step 1660: {'loss': 0.9629, 'grad_norm': 0.4931817352771759, 'learning_rate': 0.004231944444444444, 'epoch': 0.46111111111111114}
Step 1670: {'loss': 1.0258, 'grad_norm': 0.7699881792068481, 'learning_rate': 0.004227314814814815, 'epoch': 0.4638888888888889}
Step 1680: {'loss': 0.9552, 'grad_norm': 0.5141074657440186, 'learning_rate': 0.004222685185185185, 'epoch': 0.4666666666666667}
Step 1690: {'loss': 1.0787, 'grad_norm': 0.7678179740905762, 'learning_rate': 0.004218055555555556, 'epoch': 0.46944444444444444}
Step 1700: {'loss': 0.9915, 'grad_norm': 0.4715314507484436, 'learning_rate': 0.004213425925925926, 'epoch': 0.4722222222222222}
Step 1710: {'loss': 0.8627, 'grad_norm': 1.0298486948013306, 'learning_rate': 0.004208796296296296, 'epoch': 0.475}
Step 1720: {'loss': 0.9097, 'grad_norm': 0.5647039413452148, 'learning_rate': 0.004204166666666667, 'epoch': 0.4777777777777778}
Step 1730: {'loss': 0.9818, 'grad_norm': 0.7117286324501038, 'learning_rate': 0.004199537037037037, 'epoch': 0.48055555555555557}
Step 1740: {'loss': 0.93, 'grad_norm': 0.41994041204452515, 'learning_rate': 0.004194907407407408, 'epoch': 0.48333333333333334}
Step 1750: {'loss': 1.0178, 'grad_norm': 0.35840851068496704, 'learning_rate': 0.004190277777777778, 'epoch': 0.4861111111111111}
Step 1760: {'loss': 1.0123, 'grad_norm': 0.6284063458442688, 'learning_rate': 0.004185648148148148, 'epoch': 0.4888888888888889}
Step 1770: {'loss': 0.9891, 'grad_norm': 0.7780128717422485, 'learning_rate': 0.004181018518518519, 'epoch': 0.49166666666666664}
Step 1780: {'loss': 1.092, 'grad_norm': 0.6726415157318115, 'learning_rate': 0.004176388888888889, 'epoch': 0.49444444444444446}
Step 1790: {'loss': 0.87, 'grad_norm': 0.569731593132019, 'learning_rate': 0.00417175925925926, 'epoch': 0.49722222222222223}
Step 1800: {'loss': 0.9226, 'grad_norm': 0.4053480923175812, 'learning_rate': 0.00416712962962963, 'epoch': 0.5}
Step 1810: {'loss': 1.0383, 'grad_norm': 0.7124475240707397, 'learning_rate': 0.0041625, 'epoch': 0.5027777777777778}
Step 1820: {'loss': 1.0032, 'grad_norm': 2.5860097408294678, 'learning_rate': 0.00415787037037037, 'epoch': 0.5055555555555555}
Step 1830: {'loss': 1.0026, 'grad_norm': 0.9244422912597656, 'learning_rate': 0.00415324074074074, 'epoch': 0.5083333333333333}
Step 1840: {'loss': 1.0836, 'grad_norm': 1.38266921043396, 'learning_rate': 0.004148611111111112, 'epoch': 0.5111111111111111}
Step 1850: {'loss': 0.9633, 'grad_norm': 0.5205812454223633, 'learning_rate': 0.004143981481481481, 'epoch': 0.5138888888888888}
Step 1860: {'loss': 1.0079, 'grad_norm': 1.0523717403411865, 'learning_rate': 0.0041393518518518515, 'epoch': 0.5166666666666667}
Step 1870: {'loss': 0.9728, 'grad_norm': 0.7993826866149902, 'learning_rate': 0.004134722222222223, 'epoch': 0.5194444444444445}
Step 1880: {'loss': 1.0699, 'grad_norm': 0.7059488892555237, 'learning_rate': 0.004130092592592592, 'epoch': 0.5222222222222223}
Step 1890: {'loss': 1.0682, 'grad_norm': 0.6054837107658386, 'learning_rate': 0.004125462962962963, 'epoch': 0.525}
Step 1900: {'loss': 1.0128, 'grad_norm': 0.37519899010658264, 'learning_rate': 0.004120833333333334, 'epoch': 0.5277777777777778}
Step 1910: {'loss': 1.0258, 'grad_norm': 0.5164667963981628, 'learning_rate': 0.0041162037037037035, 'epoch': 0.5305555555555556}
Step 1920: {'loss': 1.0249, 'grad_norm': 0.6921266317367554, 'learning_rate': 0.004111574074074074, 'epoch': 0.5333333333333333}
Step 1930: {'loss': 0.9655, 'grad_norm': 0.9139309525489807, 'learning_rate': 0.004106944444444444, 'epoch': 0.5361111111111111}
Step 1940: {'loss': 1.0083, 'grad_norm': 0.5275819301605225, 'learning_rate': 0.004102314814814815, 'epoch': 0.5388888888888889}
Step 1950: {'loss': 1.0587, 'grad_norm': 0.7848653793334961, 'learning_rate': 0.004097685185185185, 'epoch': 0.5416666666666666}
Step 1960: {'loss': 0.9427, 'grad_norm': 0.5645484328269958, 'learning_rate': 0.0040930555555555555, 'epoch': 0.5444444444444444}
Step 1970: {'loss': 1.0048, 'grad_norm': 0.7512447834014893, 'learning_rate': 0.004088425925925926, 'epoch': 0.5472222222222223}
Step 1980: {'loss': 0.9397, 'grad_norm': 0.6449562311172485, 'learning_rate': 0.004083796296296296, 'epoch': 0.55}
Step 1990: {'loss': 0.9646, 'grad_norm': 1.2024930715560913, 'learning_rate': 0.004079166666666667, 'epoch': 0.5527777777777778}
Step 2000: {'loss': 0.9479, 'grad_norm': 0.5773962736129761, 'learning_rate': 0.004074537037037037, 'epoch': 0.5555555555555556}
Step 2010: {'loss': 1.0777, 'grad_norm': 0.8503831624984741, 'learning_rate': 0.0040699074074074075, 'epoch': 0.5583333333333333}
Step 2020: {'loss': 0.9199, 'grad_norm': 0.5589181184768677, 'learning_rate': 0.004065277777777778, 'epoch': 0.5611111111111111}
Step 2030: {'loss': 1.0907, 'grad_norm': 0.7647795677185059, 'learning_rate': 0.004060648148148148, 'epoch': 0.5638888888888889}
Step 2040: {'loss': 0.9835, 'grad_norm': 0.48101159930229187, 'learning_rate': 0.004056018518518519, 'epoch': 0.5666666666666667}
Step 2050: {'loss': 1.0415, 'grad_norm': 2.4999613761901855, 'learning_rate': 0.004051388888888889, 'epoch': 0.5694444444444444}
Step 2060: {'loss': 1.0003, 'grad_norm': 0.7829573750495911, 'learning_rate': 0.0040467592592592595, 'epoch': 0.5722222222222222}
Step 2070: {'loss': 0.91, 'grad_norm': 0.585033118724823, 'learning_rate': 0.00404212962962963, 'epoch': 0.575}
Step 2080: {'loss': 1.0093, 'grad_norm': 0.8870834112167358, 'learning_rate': 0.0040375, 'epoch': 0.5777777777777777}
Step 2090: {'loss': 0.9985, 'grad_norm': 0.984714150428772, 'learning_rate': 0.004032870370370371, 'epoch': 0.5805555555555556}
Step 2100: {'loss': 0.9613, 'grad_norm': 0.6238752007484436, 'learning_rate': 0.00402824074074074, 'epoch': 0.5833333333333334}
Step 2110: {'loss': 0.9986, 'grad_norm': 0.5063631534576416, 'learning_rate': 0.0040236111111111115, 'epoch': 0.5861111111111111}
Step 2120: {'loss': 1.0263, 'grad_norm': 0.9010687470436096, 'learning_rate': 0.004018981481481482, 'epoch': 0.5888888888888889}
Step 2130: {'loss': 1.0603, 'grad_norm': 0.8155955076217651, 'learning_rate': 0.004014351851851851, 'epoch': 0.5916666666666667}
Step 2140: {'loss': 0.9656, 'grad_norm': 0.6024032235145569, 'learning_rate': 0.004009722222222223, 'epoch': 0.5944444444444444}
Step 2150: {'loss': 1.0591, 'grad_norm': 0.580586850643158, 'learning_rate': 0.004005092592592592, 'epoch': 0.5972222222222222}
Step 2160: {'loss': 1.0035, 'grad_norm': 0.732961893081665, 'learning_rate': 0.004000462962962963, 'epoch': 0.6}
Step 2170: {'loss': 1.0027, 'grad_norm': 0.8950125575065613, 'learning_rate': 0.003995833333333334, 'epoch': 0.6027777777777777}
Step 2180: {'loss': 0.9019, 'grad_norm': 0.6199916005134583, 'learning_rate': 0.003991203703703703, 'epoch': 0.6055555555555555}
Step 2190: {'loss': 0.8976, 'grad_norm': 0.48660603165626526, 'learning_rate': 0.003986574074074074, 'epoch': 0.6083333333333333}
Step 2200: {'loss': 0.9621, 'grad_norm': 1.2832763195037842, 'learning_rate': 0.003981944444444445, 'epoch': 0.6111111111111112}
Step 2210: {'loss': 1.0217, 'grad_norm': 0.8621729016304016, 'learning_rate': 0.003977314814814815, 'epoch': 0.6138888888888889}
Step 2220: {'loss': 0.9095, 'grad_norm': 0.504427433013916, 'learning_rate': 0.003972685185185185, 'epoch': 0.6166666666666667}
Step 2230: {'loss': 0.9778, 'grad_norm': 0.6042575240135193, 'learning_rate': 0.003968055555555556, 'epoch': 0.6194444444444445}
Step 2240: {'loss': 0.9725, 'grad_norm': 0.7308717370033264, 'learning_rate': 0.003963425925925926, 'epoch': 0.6222222222222222}
Step 2250: {'loss': 1.0072, 'grad_norm': 1.0177236795425415, 'learning_rate': 0.003958796296296296, 'epoch': 0.625}
Step 2260: {'loss': 0.9946, 'grad_norm': 0.747555136680603, 'learning_rate': 0.003954166666666667, 'epoch': 0.6277777777777778}
Step 2270: {'loss': 1.0244, 'grad_norm': 0.8903943300247192, 'learning_rate': 0.003949537037037037, 'epoch': 0.6305555555555555}
Step 2280: {'loss': 1.0009, 'grad_norm': 0.5566120147705078, 'learning_rate': 0.003944907407407407, 'epoch': 0.6333333333333333}
Step 2290: {'loss': 1.0874, 'grad_norm': 0.7050886154174805, 'learning_rate': 0.003940277777777778, 'epoch': 0.6361111111111111}
Step 2300: {'loss': 1.0245, 'grad_norm': 0.570802628993988, 'learning_rate': 0.003935648148148148, 'epoch': 0.6388888888888888}
Step 2310: {'loss': 1.101, 'grad_norm': 0.4703541398048401, 'learning_rate': 0.003931018518518519, 'epoch': 0.6416666666666667}
Step 2320: {'loss': 0.9651, 'grad_norm': 0.7257375121116638, 'learning_rate': 0.003926388888888889, 'epoch': 0.6444444444444445}
Step 2330: {'loss': 0.9869, 'grad_norm': 0.6836537718772888, 'learning_rate': 0.003921759259259259, 'epoch': 0.6472222222222223}
Step 2340: {'loss': 1.1043, 'grad_norm': 0.6851959228515625, 'learning_rate': 0.00391712962962963, 'epoch': 0.65}
Step 2350: {'loss': 0.9708, 'grad_norm': 0.626146137714386, 'learning_rate': 0.0039125, 'epoch': 0.6527777777777778}
Step 2360: {'loss': 1.0514, 'grad_norm': 0.43564772605895996, 'learning_rate': 0.003907870370370371, 'epoch': 0.6555555555555556}
Step 2370: {'loss': 0.9848, 'grad_norm': 1.1264252662658691, 'learning_rate': 0.003903240740740741, 'epoch': 0.6583333333333333}
Step 2380: {'loss': 1.0332, 'grad_norm': 0.7352638840675354, 'learning_rate': 0.003898611111111111, 'epoch': 0.6611111111111111}
Step 2390: {'loss': 0.9876, 'grad_norm': 0.5818620920181274, 'learning_rate': 0.0038939814814814818, 'epoch': 0.6638888888888889}
Step 2400: {'loss': 1.0417, 'grad_norm': 0.5883997678756714, 'learning_rate': 0.0038893518518518517, 'epoch': 0.6666666666666666}
Step 2410: {'loss': 0.9447, 'grad_norm': 0.8121737241744995, 'learning_rate': 0.003884722222222222, 'epoch': 0.6694444444444444}
Step 2420: {'loss': 0.9356, 'grad_norm': 0.6739466190338135, 'learning_rate': 0.003880092592592593, 'epoch': 0.6722222222222223}
Step 2430: {'loss': 1.0338, 'grad_norm': 0.9505485892295837, 'learning_rate': 0.003875462962962963, 'epoch': 0.675}
Step 2440: {'loss': 0.9854, 'grad_norm': 0.925451397895813, 'learning_rate': 0.0038708333333333333, 'epoch': 0.6777777777777778}
Step 2450: {'loss': 1.1346, 'grad_norm': 1.8631863594055176, 'learning_rate': 0.003866203703703704, 'epoch': 0.6805555555555556}
Step 2460: {'loss': 1.0191, 'grad_norm': 0.8854275345802307, 'learning_rate': 0.003861574074074074, 'epoch': 0.6833333333333333}
Step 2470: {'loss': 0.864, 'grad_norm': 0.6058257818222046, 'learning_rate': 0.0038569444444444445, 'epoch': 0.6861111111111111}
Step 2480: {'loss': 1.083, 'grad_norm': 0.5875774025917053, 'learning_rate': 0.0038523148148148154, 'epoch': 0.6888888888888889}
Step 2490: {'loss': 0.8877, 'grad_norm': 0.4697844088077545, 'learning_rate': 0.0038476851851851853, 'epoch': 0.6916666666666667}
Step 2500: {'loss': 0.9707, 'grad_norm': 0.7738073468208313, 'learning_rate': 0.0038430555555555557, 'epoch': 0.6944444444444444}
Step 2510: {'loss': 0.9977, 'grad_norm': 0.6813169717788696, 'learning_rate': 0.0038384259259259257, 'epoch': 0.6972222222222222}
Step 2520: {'loss': 0.9623, 'grad_norm': 0.8379675149917603, 'learning_rate': 0.0038337962962962965, 'epoch': 0.7}
Step 2530: {'loss': 0.9953, 'grad_norm': 1.1098233461380005, 'learning_rate': 0.003829166666666667, 'epoch': 0.7027777777777777}
Step 2540: {'loss': 1.0429, 'grad_norm': 0.8513820767402649, 'learning_rate': 0.003824537037037037, 'epoch': 0.7055555555555556}
Step 2550: {'loss': 1.0579, 'grad_norm': 0.5813382267951965, 'learning_rate': 0.0038199074074074077, 'epoch': 0.7083333333333334}
Step 2560: {'loss': 1.0606, 'grad_norm': 0.7552135586738586, 'learning_rate': 0.003815277777777778, 'epoch': 0.7111111111111111}
Step 2570: {'loss': 1.0251, 'grad_norm': 0.6668994426727295, 'learning_rate': 0.003810648148148148, 'epoch': 0.7138888888888889}
Step 2580: {'loss': 0.9118, 'grad_norm': 0.608885645866394, 'learning_rate': 0.0038060185185185185, 'epoch': 0.7166666666666667}
Step 2590: {'loss': 0.9866, 'grad_norm': 1.2824640274047852, 'learning_rate': 0.0038013888888888893, 'epoch': 0.7194444444444444}
Step 2600: {'loss': 0.9591, 'grad_norm': 0.7594204545021057, 'learning_rate': 0.0037967592592592593, 'epoch': 0.7222222222222222}
Step 2610: {'loss': 0.9962, 'grad_norm': 0.60715651512146, 'learning_rate': 0.0037921296296296297, 'epoch': 0.725}
Step 2620: {'loss': 1.0071, 'grad_norm': 0.8596702218055725, 'learning_rate': 0.0037874999999999996, 'epoch': 0.7277777777777777}
Step 2630: {'loss': 0.9091, 'grad_norm': 0.5772264003753662, 'learning_rate': 0.0037828703703703705, 'epoch': 0.7305555555555555}
Step 2640: {'loss': 1.0345, 'grad_norm': 1.0079177618026733, 'learning_rate': 0.003778240740740741, 'epoch': 0.7333333333333333}
Step 2650: {'loss': 0.8763, 'grad_norm': 1.4028102159500122, 'learning_rate': 0.003773611111111111, 'epoch': 0.7361111111111112}
Step 2660: {'loss': 1.0014, 'grad_norm': 0.92085862159729, 'learning_rate': 0.0037689814814814817, 'epoch': 0.7388888888888889}
Step 2670: {'loss': 1.0805, 'grad_norm': 1.051285743713379, 'learning_rate': 0.003764351851851852, 'epoch': 0.7416666666666667}
Step 2680: {'loss': 0.907, 'grad_norm': 1.0717986822128296, 'learning_rate': 0.003759722222222222, 'epoch': 0.7444444444444445}
Step 2690: {'loss': 1.0274, 'grad_norm': 0.6148674488067627, 'learning_rate': 0.003755092592592593, 'epoch': 0.7472222222222222}
Step 2700: {'loss': 0.8958, 'grad_norm': 0.8077870011329651, 'learning_rate': 0.0037504629629629633, 'epoch': 0.75}
Step 2710: {'loss': 0.9551, 'grad_norm': 0.5073457360267639, 'learning_rate': 0.0037458333333333332, 'epoch': 0.7527777777777778}
Step 2720: {'loss': 0.9911, 'grad_norm': 0.5500054359436035, 'learning_rate': 0.003741203703703704, 'epoch': 0.7555555555555555}
Step 2730: {'loss': 0.9554, 'grad_norm': 0.5323106646537781, 'learning_rate': 0.003736574074074074, 'epoch': 0.7583333333333333}
Step 2740: {'loss': 0.8807, 'grad_norm': 1.2335243225097656, 'learning_rate': 0.0037319444444444444, 'epoch': 0.7611111111111111}
Step 2750: {'loss': 0.9637, 'grad_norm': 0.9037595391273499, 'learning_rate': 0.0037273148148148152, 'epoch': 0.7638888888888888}
Step 2760: {'loss': 0.9365, 'grad_norm': 0.5774649977684021, 'learning_rate': 0.003722685185185185, 'epoch': 0.7666666666666667}
Step 2770: {'loss': 1.0411, 'grad_norm': 0.6343432068824768, 'learning_rate': 0.0037180555555555556, 'epoch': 0.7694444444444445}
Step 2780: {'loss': 1.0318, 'grad_norm': 0.7889916300773621, 'learning_rate': 0.003713425925925926, 'epoch': 0.7722222222222223}
Step 2790: {'loss': 0.9832, 'grad_norm': 0.5918552875518799, 'learning_rate': 0.0037087962962962964, 'epoch': 0.775}
Step 2800: {'loss': 0.9421, 'grad_norm': 0.8720073103904724, 'learning_rate': 0.003704166666666667, 'epoch': 0.7777777777777778}
Step 2810: {'loss': 1.0465, 'grad_norm': 0.9257757067680359, 'learning_rate': 0.003699537037037037, 'epoch': 0.7805555555555556}
Step 2820: {'loss': 1.0495, 'grad_norm': 1.4534404277801514, 'learning_rate': 0.003694907407407407, 'epoch': 0.7833333333333333}
Step 2830: {'loss': 0.9618, 'grad_norm': 0.3823768198490143, 'learning_rate': 0.003690277777777778, 'epoch': 0.7861111111111111}
Step 2840: {'loss': 1.0348, 'grad_norm': 0.7898159623146057, 'learning_rate': 0.003685648148148148, 'epoch': 0.7888888888888889}
Step 2850: {'loss': 1.0331, 'grad_norm': 0.6585178971290588, 'learning_rate': 0.0036810185185185184, 'epoch': 0.7916666666666666}
Step 2860: {'loss': 0.9611, 'grad_norm': 0.6206633448600769, 'learning_rate': 0.003676388888888889, 'epoch': 0.7944444444444444}
Step 2870: {'loss': 1.0284, 'grad_norm': 0.6091976165771484, 'learning_rate': 0.003671759259259259, 'epoch': 0.7972222222222223}
Step 2880: {'loss': 0.9542, 'grad_norm': 0.4687734544277191, 'learning_rate': 0.0036671296296296296, 'epoch': 0.8}
Step 2890: {'loss': 1.0259, 'grad_norm': 0.8825892210006714, 'learning_rate': 0.0036625000000000004, 'epoch': 0.8027777777777778}
Step 2900: {'loss': 0.9551, 'grad_norm': 0.9318912625312805, 'learning_rate': 0.0036578703703703704, 'epoch': 0.8055555555555556}
Step 2910: {'loss': 0.9338, 'grad_norm': 0.5757991671562195, 'learning_rate': 0.0036532407407407408, 'epoch': 0.8083333333333333}
Step 2920: {'loss': 0.9944, 'grad_norm': 1.757048487663269, 'learning_rate': 0.0036486111111111116, 'epoch': 0.8111111111111111}
Step 2930: {'loss': 1.0556, 'grad_norm': 0.8363745212554932, 'learning_rate': 0.0036439814814814816, 'epoch': 0.8138888888888889}
Step 2940: {'loss': 0.9327, 'grad_norm': 0.6338754296302795, 'learning_rate': 0.003639351851851852, 'epoch': 0.8166666666666667}
Step 2950: {'loss': 0.9799, 'grad_norm': 0.8198199272155762, 'learning_rate': 0.003634722222222222, 'epoch': 0.8194444444444444}
Step 2960: {'loss': 0.9283, 'grad_norm': 0.675087571144104, 'learning_rate': 0.0036300925925925927, 'epoch': 0.8222222222222222}
Step 2970: {'loss': 1.1134, 'grad_norm': 0.5065359473228455, 'learning_rate': 0.003625462962962963, 'epoch': 0.825}
Step 2980: {'loss': 0.9837, 'grad_norm': 0.904851496219635, 'learning_rate': 0.003620833333333333, 'epoch': 0.8277777777777777}
Step 2990: {'loss': 0.9317, 'grad_norm': 0.8052393794059753, 'learning_rate': 0.003616203703703704, 'epoch': 0.8305555555555556}
Step 3000: {'loss': 0.9376, 'grad_norm': 1.075209140777588, 'learning_rate': 0.0036115740740740743, 'epoch': 0.8333333333333334}
Step 3010: {'loss': 0.9551, 'grad_norm': 0.7531440258026123, 'learning_rate': 0.0036069444444444443, 'epoch': 0.8361111111111111}
Step 3020: {'loss': 0.9968, 'grad_norm': 0.573009729385376, 'learning_rate': 0.003602314814814815, 'epoch': 0.8388888888888889}
Step 3030: {'loss': 0.9813, 'grad_norm': 0.8471225500106812, 'learning_rate': 0.0035976851851851855, 'epoch': 0.8416666666666667}
Step 3040: {'loss': 1.1155, 'grad_norm': 0.9253695607185364, 'learning_rate': 0.0035930555555555555, 'epoch': 0.8444444444444444}
Step 3050: {'loss': 0.9883, 'grad_norm': 1.4642974138259888, 'learning_rate': 0.003588425925925926, 'epoch': 0.8472222222222222}
Step 3060: {'loss': 1.0318, 'grad_norm': 2.080758571624756, 'learning_rate': 0.0035837962962962963, 'epoch': 0.85}
Step 3070: {'loss': 1.0537, 'grad_norm': 0.8227841258049011, 'learning_rate': 0.0035791666666666667, 'epoch': 0.8527777777777777}
Step 3080: {'loss': 0.9093, 'grad_norm': 0.615106463432312, 'learning_rate': 0.003574537037037037, 'epoch': 0.8555555555555555}
Step 3090: {'loss': 0.8902, 'grad_norm': 0.6336429715156555, 'learning_rate': 0.003569907407407407, 'epoch': 0.8583333333333333}
Step 3100: {'loss': 0.9472, 'grad_norm': 0.7860575318336487, 'learning_rate': 0.003565277777777778, 'epoch': 0.8611111111111112}
Step 3110: {'loss': 0.9615, 'grad_norm': 0.6196541786193848, 'learning_rate': 0.0035606481481481483, 'epoch': 0.8638888888888889}
Step 3120: {'loss': 0.9432, 'grad_norm': 0.5343948006629944, 'learning_rate': 0.0035560185185185183, 'epoch': 0.8666666666666667}
Step 3130: {'loss': 1.0118, 'grad_norm': 0.7289242148399353, 'learning_rate': 0.003551388888888889, 'epoch': 0.8694444444444445}
Step 3140: {'loss': 1.0501, 'grad_norm': 0.7253367304801941, 'learning_rate': 0.0035467592592592595, 'epoch': 0.8722222222222222}
Step 3150: {'loss': 0.9662, 'grad_norm': 0.6727230548858643, 'learning_rate': 0.0035421296296296294, 'epoch': 0.875}
Step 3160: {'loss': 1.0552, 'grad_norm': 0.8073897957801819, 'learning_rate': 0.0035375000000000003, 'epoch': 0.8777777777777778}
Step 3170: {'loss': 1.0845, 'grad_norm': 0.8010309338569641, 'learning_rate': 0.0035328703703703702, 'epoch': 0.8805555555555555}
Step 3180: {'loss': 1.0623, 'grad_norm': 0.805317223072052, 'learning_rate': 0.0035282407407407406, 'epoch': 0.8833333333333333}
Step 3190: {'loss': 0.9986, 'grad_norm': 0.7382935881614685, 'learning_rate': 0.0035236111111111115, 'epoch': 0.8861111111111111}
Step 3200: {'loss': 0.9039, 'grad_norm': 0.5014585256576538, 'learning_rate': 0.0035189814814814814, 'epoch': 0.8888888888888888}
Step 3210: {'loss': 0.9313, 'grad_norm': 0.6878932118415833, 'learning_rate': 0.003514351851851852, 'epoch': 0.8916666666666667}
Step 3220: {'loss': 0.9581, 'grad_norm': 0.8705236911773682, 'learning_rate': 0.0035097222222222227, 'epoch': 0.8944444444444445}
Step 3230: {'loss': 1.0098, 'grad_norm': 0.8341553211212158, 'learning_rate': 0.0035050925925925926, 'epoch': 0.8972222222222223}
Step 3240: {'loss': 0.9794, 'grad_norm': 0.763251543045044, 'learning_rate': 0.003500462962962963, 'epoch': 0.9}
Step 3250: {'loss': 0.998, 'grad_norm': 0.8211257457733154, 'learning_rate': 0.0034958333333333334, 'epoch': 0.9027777777777778}
Step 3260: {'loss': 0.8943, 'grad_norm': 0.37264561653137207, 'learning_rate': 0.003491203703703704, 'epoch': 0.9055555555555556}
Step 3270: {'loss': 0.9547, 'grad_norm': 0.6203956007957458, 'learning_rate': 0.0034865740740740742, 'epoch': 0.9083333333333333}
Step 3280: {'loss': 0.9784, 'grad_norm': 1.3479517698287964, 'learning_rate': 0.003481944444444444, 'epoch': 0.9111111111111111}
Step 3290: {'loss': 0.9271, 'grad_norm': 0.5902318358421326, 'learning_rate': 0.003477314814814815, 'epoch': 0.9138888888888889}
Step 3300: {'loss': 1.035, 'grad_norm': 0.9185988306999207, 'learning_rate': 0.0034726851851851854, 'epoch': 0.9166666666666666}
Step 3310: {'loss': 1.0063, 'grad_norm': 0.8938297629356384, 'learning_rate': 0.0034680555555555554, 'epoch': 0.9194444444444444}
Step 3320: {'loss': 0.8674, 'grad_norm': 0.5834231972694397, 'learning_rate': 0.003463425925925926, 'epoch': 0.9222222222222223}
Step 3330: {'loss': 0.9476, 'grad_norm': 0.5740340948104858, 'learning_rate': 0.0034587962962962966, 'epoch': 0.925}
Step 3340: {'loss': 1.0063, 'grad_norm': 0.516341507434845, 'learning_rate': 0.0034541666666666666, 'epoch': 0.9277777777777778}
Step 3350: {'loss': 0.9967, 'grad_norm': 1.3975170850753784, 'learning_rate': 0.003449537037037037, 'epoch': 0.9305555555555556}
Step 3360: {'loss': 1.0255, 'grad_norm': 0.6438247561454773, 'learning_rate': 0.003444907407407408, 'epoch': 0.9333333333333333}
Step 3370: {'loss': 1.027, 'grad_norm': 0.469149112701416, 'learning_rate': 0.0034402777777777778, 'epoch': 0.9361111111111111}
Step 3380: {'loss': 1.0189, 'grad_norm': 0.6424371600151062, 'learning_rate': 0.003435648148148148, 'epoch': 0.9388888888888889}
Step 3390: {'loss': 1.0477, 'grad_norm': 0.7432215809822083, 'learning_rate': 0.003431018518518518, 'epoch': 0.9416666666666667}
Step 3400: {'loss': 0.9251, 'grad_norm': 0.5512802600860596, 'learning_rate': 0.003426388888888889, 'epoch': 0.9444444444444444}
Step 3410: {'loss': 1.0009, 'grad_norm': 0.5672736763954163, 'learning_rate': 0.0034217592592592594, 'epoch': 0.9472222222222222}
Step 3420: {'loss': 0.9241, 'grad_norm': 0.552383542060852, 'learning_rate': 0.0034171296296296293, 'epoch': 0.95}
Step 3430: {'loss': 1.0863, 'grad_norm': 0.5517839193344116, 'learning_rate': 0.0034125, 'epoch': 0.9527777777777777}
Step 3440: {'loss': 0.9775, 'grad_norm': 0.5335602760314941, 'learning_rate': 0.0034078703703703706, 'epoch': 0.9555555555555556}
Step 3450: {'loss': 0.9092, 'grad_norm': 0.7593528628349304, 'learning_rate': 0.0034032407407407405, 'epoch': 0.9583333333333334}
Step 3460: {'loss': 1.0904, 'grad_norm': 0.9979831576347351, 'learning_rate': 0.0033986111111111114, 'epoch': 0.9611111111111111}
Step 3470: {'loss': 0.9953, 'grad_norm': 0.6831222772598267, 'learning_rate': 0.0033939814814814818, 'epoch': 0.9638888888888889}
Step 3480: {'loss': 1.0869, 'grad_norm': 0.545855700969696, 'learning_rate': 0.0033893518518518517, 'epoch': 0.9666666666666667}
Step 3490: {'loss': 0.9092, 'grad_norm': 0.7495828866958618, 'learning_rate': 0.0033847222222222226, 'epoch': 0.9694444444444444}
Step 3500: {'loss': 0.9997, 'grad_norm': 2.206984758377075, 'learning_rate': 0.0033800925925925925, 'epoch': 0.9722222222222222}
Step 3510: {'loss': 0.9936, 'grad_norm': 0.6540281176567078, 'learning_rate': 0.003375462962962963, 'epoch': 0.975}
Step 3520: {'loss': 0.9854, 'grad_norm': 0.7837520837783813, 'learning_rate': 0.0033708333333333333, 'epoch': 0.9777777777777777}
Step 3530: {'loss': 0.9642, 'grad_norm': 0.5786992907524109, 'learning_rate': 0.0033662037037037037, 'epoch': 0.9805555555555555}
Step 3540: {'loss': 1.0498, 'grad_norm': 0.8136866688728333, 'learning_rate': 0.003361574074074074, 'epoch': 0.9833333333333333}
Step 3550: {'loss': 0.9823, 'grad_norm': 0.6467296481132507, 'learning_rate': 0.0033569444444444445, 'epoch': 0.9861111111111112}
Step 3560: {'loss': 0.9954, 'grad_norm': 0.5534241199493408, 'learning_rate': 0.003352314814814815, 'epoch': 0.9888888888888889}
Step 3570: {'loss': 0.9833, 'grad_norm': 0.7701665759086609, 'learning_rate': 0.0033476851851851853, 'epoch': 0.9916666666666667}
Step 3580: {'loss': 0.9398, 'grad_norm': 0.9222577214241028, 'learning_rate': 0.0033430555555555557, 'epoch': 0.9944444444444445}
Step 3590: {'loss': 1.0061, 'grad_norm': 0.4217373728752136, 'learning_rate': 0.0033384259259259257, 'epoch': 0.9972222222222222}
Step 3600: {'loss': 0.974, 'grad_norm': 0.6248281598091125, 'learning_rate': 0.0033337962962962965, 'epoch': 1.0}
Step 3610: {'loss': 0.9239, 'grad_norm': 0.4696577489376068, 'learning_rate': 0.003329166666666667, 'epoch': 1.0027777777777778}
Step 3620: {'loss': 0.9587, 'grad_norm': 0.5079407691955566, 'learning_rate': 0.003324537037037037, 'epoch': 1.0055555555555555}
Step 3630: {'loss': 0.9534, 'grad_norm': 0.6178263425827026, 'learning_rate': 0.0033199074074074077, 'epoch': 1.0083333333333333}
Step 3640: {'loss': 0.9475, 'grad_norm': 0.6063477993011475, 'learning_rate': 0.0033152777777777777, 'epoch': 1.011111111111111}
Step 3650: {'loss': 1.0718, 'grad_norm': 0.938980758190155, 'learning_rate': 0.003310648148148148, 'epoch': 1.0138888888888888}
Step 3660: {'loss': 0.9743, 'grad_norm': 0.8405179381370544, 'learning_rate': 0.003306018518518519, 'epoch': 1.0166666666666666}
Step 3670: {'loss': 0.9646, 'grad_norm': 0.819792628288269, 'learning_rate': 0.003301388888888889, 'epoch': 1.0194444444444444}
Step 3680: {'loss': 1.0146, 'grad_norm': 0.6700773239135742, 'learning_rate': 0.0032967592592592593, 'epoch': 1.0222222222222221}
Step 3690: {'loss': 0.9418, 'grad_norm': 0.7580466270446777, 'learning_rate': 0.00329212962962963, 'epoch': 1.025}
Step 3700: {'loss': 0.994, 'grad_norm': 0.815723180770874, 'learning_rate': 0.0032875, 'epoch': 1.0277777777777777}
Step 3710: {'loss': 0.9933, 'grad_norm': 0.7891783714294434, 'learning_rate': 0.0032828703703703705, 'epoch': 1.0305555555555554}
Step 3720: {'loss': 0.9984, 'grad_norm': 0.7493714094161987, 'learning_rate': 0.0032782407407407413, 'epoch': 1.0333333333333334}
Step 3730: {'loss': 0.9151, 'grad_norm': 0.47400593757629395, 'learning_rate': 0.0032736111111111113, 'epoch': 1.0361111111111112}
Step 3740: {'loss': 0.9223, 'grad_norm': 0.7914524078369141, 'learning_rate': 0.0032689814814814817, 'epoch': 1.038888888888889}
Step 3750: {'loss': 1.0143, 'grad_norm': 0.6281678676605225, 'learning_rate': 0.0032643518518518516, 'epoch': 1.0416666666666667}
Step 3760: {'loss': 0.9877, 'grad_norm': 0.4720380902290344, 'learning_rate': 0.0032597222222222224, 'epoch': 1.0444444444444445}
Step 3770: {'loss': 0.9486, 'grad_norm': 0.5359512567520142, 'learning_rate': 0.003255092592592593, 'epoch': 1.0472222222222223}
Step 3780: {'loss': 0.9345, 'grad_norm': 1.1085911989212036, 'learning_rate': 0.003250462962962963, 'epoch': 1.05}
Step 3790: {'loss': 0.9687, 'grad_norm': 0.5228312015533447, 'learning_rate': 0.003245833333333333, 'epoch': 1.0527777777777778}
Step 3800: {'loss': 0.9771, 'grad_norm': 0.8865668177604675, 'learning_rate': 0.003241203703703704, 'epoch': 1.0555555555555556}
Step 3810: {'loss': 0.9714, 'grad_norm': 0.3994119167327881, 'learning_rate': 0.003236574074074074, 'epoch': 1.0583333333333333}
Step 3820: {'loss': 1.0221, 'grad_norm': 0.80585777759552, 'learning_rate': 0.0032319444444444444, 'epoch': 1.0611111111111111}
Step 3830: {'loss': 0.8837, 'grad_norm': 1.0225507020950317, 'learning_rate': 0.0032273148148148152, 'epoch': 1.0638888888888889}
Step 3840: {'loss': 1.1085, 'grad_norm': 2.130018472671509, 'learning_rate': 0.003222685185185185, 'epoch': 1.0666666666666667}
Step 3850: {'loss': 1.1839, 'grad_norm': 1.4422683715820312, 'learning_rate': 0.0032180555555555556, 'epoch': 1.0694444444444444}
Step 3860: {'loss': 0.9564, 'grad_norm': 0.7382330298423767, 'learning_rate': 0.0032134259259259256, 'epoch': 1.0722222222222222}
Step 3870: {'loss': 0.9921, 'grad_norm': 0.5941969156265259, 'learning_rate': 0.0032087962962962964, 'epoch': 1.075}
Step 3880: {'loss': 0.9117, 'grad_norm': 0.5320618152618408, 'learning_rate': 0.003204166666666667, 'epoch': 1.0777777777777777}
Step 3890: {'loss': 0.8731, 'grad_norm': 0.8809211850166321, 'learning_rate': 0.0031995370370370368, 'epoch': 1.0805555555555555}
Step 3900: {'loss': 0.9556, 'grad_norm': 0.8532367944717407, 'learning_rate': 0.0031949074074074076, 'epoch': 1.0833333333333333}
Step 3910: {'loss': 0.9337, 'grad_norm': 0.8880555629730225, 'learning_rate': 0.003190277777777778, 'epoch': 1.086111111111111}
Step 3920: {'loss': 1.1189, 'grad_norm': 0.937388002872467, 'learning_rate': 0.003185648148148148, 'epoch': 1.0888888888888888}
Step 3930: {'loss': 0.9974, 'grad_norm': 0.7838035821914673, 'learning_rate': 0.003181018518518519, 'epoch': 1.0916666666666666}
Step 3940: {'loss': 0.9963, 'grad_norm': 0.6961789131164551, 'learning_rate': 0.003176388888888889, 'epoch': 1.0944444444444446}
Step 3950: {'loss': 0.9703, 'grad_norm': 0.7626816630363464, 'learning_rate': 0.003171759259259259, 'epoch': 1.0972222222222223}
Step 3960: {'loss': 1.008, 'grad_norm': 0.5209380984306335, 'learning_rate': 0.00316712962962963, 'epoch': 1.1}
Step 3970: {'loss': 0.992, 'grad_norm': 1.2322490215301514, 'learning_rate': 0.0031625, 'epoch': 1.1027777777777779}
Step 3980: {'loss': 1.0008, 'grad_norm': 0.8293406963348389, 'learning_rate': 0.0031578703703703703, 'epoch': 1.1055555555555556}
Step 3990: {'loss': 1.0079, 'grad_norm': 0.9521097540855408, 'learning_rate': 0.003153240740740741, 'epoch': 1.1083333333333334}
Step 4000: {'loss': 0.9613, 'grad_norm': 0.6956976056098938, 'learning_rate': 0.003148611111111111, 'epoch': 1.1111111111111112}
Step 4010: {'loss': 0.9309, 'grad_norm': 0.7754068970680237, 'learning_rate': 0.0031439814814814815, 'epoch': 1.113888888888889}
Step 4020: {'loss': 0.9794, 'grad_norm': 1.0649336576461792, 'learning_rate': 0.003139351851851852, 'epoch': 1.1166666666666667}
Step 4030: {'loss': 0.9617, 'grad_norm': 0.6611917018890381, 'learning_rate': 0.0031347222222222223, 'epoch': 1.1194444444444445}
Step 4040: {'loss': 1.0625, 'grad_norm': 0.865551769733429, 'learning_rate': 0.0031300925925925927, 'epoch': 1.1222222222222222}
Step 4050: {'loss': 1.0428, 'grad_norm': 0.702124834060669, 'learning_rate': 0.003125462962962963, 'epoch': 1.125}
Step 4060: {'loss': 1.0237, 'grad_norm': 1.2648588418960571, 'learning_rate': 0.003120833333333333, 'epoch': 1.1277777777777778}
Step 4070: {'loss': 1.0143, 'grad_norm': 0.8677571415901184, 'learning_rate': 0.003116203703703704, 'epoch': 1.1305555555555555}
Step 4080: {'loss': 0.9864, 'grad_norm': 0.5733693242073059, 'learning_rate': 0.003111574074074074, 'epoch': 1.1333333333333333}
Step 4090: {'loss': 0.9714, 'grad_norm': 0.8020169734954834, 'learning_rate': 0.0031069444444444443, 'epoch': 1.136111111111111}
Step 4100: {'loss': 0.8431, 'grad_norm': 0.46160560846328735, 'learning_rate': 0.003102314814814815, 'epoch': 1.1388888888888888}
Step 4110: {'loss': 1.0291, 'grad_norm': 0.8275063037872314, 'learning_rate': 0.003097685185185185, 'epoch': 1.1416666666666666}
Step 4120: {'loss': 1.0736, 'grad_norm': 1.0643001794815063, 'learning_rate': 0.0030930555555555555, 'epoch': 1.1444444444444444}
Step 4130: {'loss': 0.8893, 'grad_norm': 0.7275570034980774, 'learning_rate': 0.0030884259259259263, 'epoch': 1.1472222222222221}
Step 4140: {'loss': 0.9105, 'grad_norm': 0.7171430587768555, 'learning_rate': 0.0030837962962962963, 'epoch': 1.15}
Step 4150: {'loss': 0.9699, 'grad_norm': 0.4436420202255249, 'learning_rate': 0.0030791666666666667, 'epoch': 1.1527777777777777}
Step 4160: {'loss': 1.0299, 'grad_norm': 0.7875614762306213, 'learning_rate': 0.0030745370370370375, 'epoch': 1.1555555555555554}
Step 4170: {'loss': 0.9162, 'grad_norm': 1.0035932064056396, 'learning_rate': 0.0030699074074074075, 'epoch': 1.1583333333333332}
Step 4180: {'loss': 1.0326, 'grad_norm': 0.5400253534317017, 'learning_rate': 0.003065277777777778, 'epoch': 1.1611111111111112}
Step 4190: {'loss': 0.9702, 'grad_norm': 0.8084263205528259, 'learning_rate': 0.003060648148148148, 'epoch': 1.163888888888889}
Step 4200: {'loss': 0.933, 'grad_norm': 1.024268388748169, 'learning_rate': 0.0030560185185185187, 'epoch': 1.1666666666666667}
Step 4210: {'loss': 0.9548, 'grad_norm': 0.5420724749565125, 'learning_rate': 0.003051388888888889, 'epoch': 1.1694444444444445}
Step 4220: {'loss': 0.9573, 'grad_norm': 0.9173500537872314, 'learning_rate': 0.003046759259259259, 'epoch': 1.1722222222222223}
Step 4230: {'loss': 0.9908, 'grad_norm': 0.6274397969245911, 'learning_rate': 0.00304212962962963, 'epoch': 1.175}
Step 4240: {'loss': 0.9224, 'grad_norm': 0.6209598183631897, 'learning_rate': 0.0030375000000000003, 'epoch': 1.1777777777777778}
Step 4250: {'loss': 0.9747, 'grad_norm': 0.6946879625320435, 'learning_rate': 0.0030328703703703702, 'epoch': 1.1805555555555556}
Step 4260: {'loss': 0.8981, 'grad_norm': 0.46710652112960815, 'learning_rate': 0.003028240740740741, 'epoch': 1.1833333333333333}
Step 4270: {'loss': 0.9236, 'grad_norm': 2.293081760406494, 'learning_rate': 0.0030236111111111115, 'epoch': 1.1861111111111111}
Step 4280: {'loss': 0.8804, 'grad_norm': 0.4769435524940491, 'learning_rate': 0.0030189814814814814, 'epoch': 1.1888888888888889}
Step 4290: {'loss': 1.0363, 'grad_norm': 0.8974617719650269, 'learning_rate': 0.003014351851851852, 'epoch': 1.1916666666666667}
Step 4300: {'loss': 0.9855, 'grad_norm': 0.8481622338294983, 'learning_rate': 0.0030097222222222222, 'epoch': 1.1944444444444444}
Step 4310: {'loss': 0.9709, 'grad_norm': 0.7362507581710815, 'learning_rate': 0.0030050925925925926, 'epoch': 1.1972222222222222}
Step 4320: {'loss': 0.9816, 'grad_norm': 0.6271814703941345, 'learning_rate': 0.003000462962962963, 'epoch': 1.2}
Step 4330: {'loss': 0.9374, 'grad_norm': 1.2560553550720215, 'learning_rate': 0.002995833333333333, 'epoch': 1.2027777777777777}
Step 4340: {'loss': 1.0203, 'grad_norm': 0.592736005783081, 'learning_rate': 0.002991203703703704, 'epoch': 1.2055555555555555}
Step 4350: {'loss': 1.0501, 'grad_norm': 0.6047136783599854, 'learning_rate': 0.0029865740740740742, 'epoch': 1.2083333333333333}
Step 4360: {'loss': 0.9842, 'grad_norm': 0.895092248916626, 'learning_rate': 0.002981944444444444, 'epoch': 1.211111111111111}
Step 4370: {'loss': 0.982, 'grad_norm': 0.6579451560974121, 'learning_rate': 0.002977314814814815, 'epoch': 1.2138888888888888}
Step 4380: {'loss': 1.0677, 'grad_norm': 0.6958960890769958, 'learning_rate': 0.0029726851851851854, 'epoch': 1.2166666666666668}
Step 4390: {'loss': 0.9529, 'grad_norm': 0.7146307826042175, 'learning_rate': 0.0029680555555555554, 'epoch': 1.2194444444444446}
Step 4400: {'loss': 1.0521, 'grad_norm': 0.5354122519493103, 'learning_rate': 0.002963425925925926, 'epoch': 1.2222222222222223}
Step 4410: {'loss': 0.9398, 'grad_norm': 0.9154769778251648, 'learning_rate': 0.002958796296296296, 'epoch': 1.225}
Step 4420: {'loss': 0.9341, 'grad_norm': 0.4591127932071686, 'learning_rate': 0.0029541666666666666, 'epoch': 1.2277777777777779}
Step 4430: {'loss': 0.9033, 'grad_norm': 0.9257230758666992, 'learning_rate': 0.0029495370370370374, 'epoch': 1.2305555555555556}
Step 4440: {'loss': 0.9504, 'grad_norm': 0.7459108829498291, 'learning_rate': 0.0029449074074074074, 'epoch': 1.2333333333333334}
Step 4450: {'loss': 0.8894, 'grad_norm': 1.5049800872802734, 'learning_rate': 0.0029402777777777778, 'epoch': 1.2361111111111112}
Step 4460: {'loss': 0.9528, 'grad_norm': 0.3761054277420044, 'learning_rate': 0.0029356481481481486, 'epoch': 1.238888888888889}
Step 4470: {'loss': 0.9703, 'grad_norm': 0.7911985516548157, 'learning_rate': 0.0029310185185185186, 'epoch': 1.2416666666666667}
Step 4480: {'loss': 0.9658, 'grad_norm': 1.0959562063217163, 'learning_rate': 0.002926388888888889, 'epoch': 1.2444444444444445}
Step 4490: {'loss': 1.0075, 'grad_norm': 0.5225343108177185, 'learning_rate': 0.0029217592592592594, 'epoch': 1.2472222222222222}
Step 4500: {'loss': 0.9159, 'grad_norm': 0.6900057196617126, 'learning_rate': 0.0029171296296296298, 'epoch': 1.25}
Step 4510: {'loss': 0.8721, 'grad_norm': 0.6389632821083069, 'learning_rate': 0.0029125, 'epoch': 1.2527777777777778}
Step 4520: {'loss': 1.0087, 'grad_norm': 0.5799069404602051, 'learning_rate': 0.00290787037037037, 'epoch': 1.2555555555555555}
Step 4530: {'loss': 0.9456, 'grad_norm': 1.3507806062698364, 'learning_rate': 0.0029032407407407405, 'epoch': 1.2583333333333333}
Step 4540: {'loss': 0.9627, 'grad_norm': 0.8060018420219421, 'learning_rate': 0.0028986111111111114, 'epoch': 1.261111111111111}
Step 4550: {'loss': 1.0426, 'grad_norm': 0.6413387656211853, 'learning_rate': 0.0028939814814814813, 'epoch': 1.2638888888888888}
Step 4560: {'loss': 0.9472, 'grad_norm': 0.8172662854194641, 'learning_rate': 0.0028893518518518517, 'epoch': 1.2666666666666666}
Step 4570: {'loss': 0.977, 'grad_norm': 0.8260421752929688, 'learning_rate': 0.0028847222222222225, 'epoch': 1.2694444444444444}
Step 4580: {'loss': 0.9187, 'grad_norm': 1.299668788909912, 'learning_rate': 0.0028800925925925925, 'epoch': 1.2722222222222221}
Step 4590: {'loss': 0.9743, 'grad_norm': 0.7780667543411255, 'learning_rate': 0.002875462962962963, 'epoch': 1.275}
Step 4600: {'loss': 1.0242, 'grad_norm': 0.5386031866073608, 'learning_rate': 0.0028708333333333337, 'epoch': 1.2777777777777777}
Step 4610: {'loss': 1.0085, 'grad_norm': 0.6871601939201355, 'learning_rate': 0.0028662037037037037, 'epoch': 1.2805555555555554}
Step 4620: {'loss': 1.049, 'grad_norm': 0.8773804306983948, 'learning_rate': 0.002861574074074074, 'epoch': 1.2833333333333332}
Step 4630: {'loss': 0.9856, 'grad_norm': 0.5860471725463867, 'learning_rate': 0.002856944444444444, 'epoch': 1.286111111111111}
Step 4640: {'loss': 1.0156, 'grad_norm': 0.8513035774230957, 'learning_rate': 0.002852314814814815, 'epoch': 1.2888888888888888}
Step 4650: {'loss': 0.9727, 'grad_norm': 0.6333831548690796, 'learning_rate': 0.0028476851851851853, 'epoch': 1.2916666666666667}
Step 4660: {'loss': 0.907, 'grad_norm': 0.5413286089897156, 'learning_rate': 0.0028430555555555553, 'epoch': 1.2944444444444445}
Step 4670: {'loss': 1.0057, 'grad_norm': 0.8770620822906494, 'learning_rate': 0.002838425925925926, 'epoch': 1.2972222222222223}
Step 4680: {'loss': 1.0349, 'grad_norm': 0.9337611198425293, 'learning_rate': 0.0028337962962962965, 'epoch': 1.3}
Step 4690: {'loss': 0.9301, 'grad_norm': 1.9277231693267822, 'learning_rate': 0.0028291666666666665, 'epoch': 1.3027777777777778}
Step 4700: {'loss': 1.0165, 'grad_norm': 0.6785998344421387, 'learning_rate': 0.0028245370370370373, 'epoch': 1.3055555555555556}
Step 4710: {'loss': 0.9156, 'grad_norm': 1.0152783393859863, 'learning_rate': 0.0028199074074074077, 'epoch': 1.3083333333333333}
Step 4720: {'loss': 0.8912, 'grad_norm': 0.5687128305435181, 'learning_rate': 0.0028152777777777777, 'epoch': 1.3111111111111111}
Step 4730: {'loss': 1.0589, 'grad_norm': 0.6802530884742737, 'learning_rate': 0.0028106481481481485, 'epoch': 1.3138888888888889}
Step 4740: {'loss': 1.015, 'grad_norm': 0.5100449323654175, 'learning_rate': 0.0028060185185185185, 'epoch': 1.3166666666666667}
Step 4750: {'loss': 0.9896, 'grad_norm': 0.6543084383010864, 'learning_rate': 0.002801388888888889, 'epoch': 1.3194444444444444}
Step 4760: {'loss': 1.0601, 'grad_norm': 0.4339512586593628, 'learning_rate': 0.0027967592592592593, 'epoch': 1.3222222222222222}
Step 4770: {'loss': 0.991, 'grad_norm': 0.5498743653297424, 'learning_rate': 0.0027921296296296296, 'epoch': 1.325}
Step 4780: {'loss': 0.9055, 'grad_norm': 1.6212658882141113, 'learning_rate': 0.0027875, 'epoch': 1.3277777777777777}
Step 4790: {'loss': 1.0158, 'grad_norm': 0.7786479592323303, 'learning_rate': 0.0027828703703703704, 'epoch': 1.3305555555555555}
Step 4800: {'loss': 0.9282, 'grad_norm': 0.7778531908988953, 'learning_rate': 0.0027782407407407404, 'epoch': 1.3333333333333333}
Step 4810: {'loss': 1.0336, 'grad_norm': 0.43094974756240845, 'learning_rate': 0.0027736111111111112, 'epoch': 1.3361111111111112}
Step 4820: {'loss': 0.9659, 'grad_norm': 0.6641483306884766, 'learning_rate': 0.0027689814814814816, 'epoch': 1.338888888888889}
Step 4830: {'loss': 1.0419, 'grad_norm': 0.8280059695243835, 'learning_rate': 0.0027643518518518516, 'epoch': 1.3416666666666668}
Step 4840: {'loss': 0.9053, 'grad_norm': 0.736798882484436, 'learning_rate': 0.0027597222222222224, 'epoch': 1.3444444444444446}
Step 4850: {'loss': 0.9344, 'grad_norm': 0.6508973836898804, 'learning_rate': 0.0027550925925925924, 'epoch': 1.3472222222222223}
Step 4860: {'loss': 0.9311, 'grad_norm': 0.5691977739334106, 'learning_rate': 0.002750462962962963, 'epoch': 1.35}
Step 4870: {'loss': 1.0023, 'grad_norm': 0.6616650223731995, 'learning_rate': 0.0027458333333333336, 'epoch': 1.3527777777777779}
Step 4880: {'loss': 0.9673, 'grad_norm': 0.4659731984138489, 'learning_rate': 0.0027412037037037036, 'epoch': 1.3555555555555556}
Step 4890: {'loss': 0.9494, 'grad_norm': 0.6145816445350647, 'learning_rate': 0.002736574074074074, 'epoch': 1.3583333333333334}
Step 4900: {'loss': 0.9485, 'grad_norm': 1.0518772602081299, 'learning_rate': 0.002731944444444445, 'epoch': 1.3611111111111112}
Step 4910: {'loss': 0.8653, 'grad_norm': 0.45122596621513367, 'learning_rate': 0.002727314814814815, 'epoch': 1.363888888888889}
Step 4920: {'loss': 0.9857, 'grad_norm': 0.6777462959289551, 'learning_rate': 0.002722685185185185, 'epoch': 1.3666666666666667}
Step 4930: {'loss': 0.9884, 'grad_norm': 0.6639353632926941, 'learning_rate': 0.002718055555555556, 'epoch': 1.3694444444444445}
Step 4940: {'loss': 1.0005, 'grad_norm': 0.6550988554954529, 'learning_rate': 0.002713425925925926, 'epoch': 1.3722222222222222}
Step 4950: {'loss': 0.9831, 'grad_norm': 0.5610223412513733, 'learning_rate': 0.0027087962962962964, 'epoch': 1.375}
Step 4960: {'loss': 0.9866, 'grad_norm': 0.5322606563568115, 'learning_rate': 0.002704166666666667, 'epoch': 1.3777777777777778}
Step 4970: {'loss': 0.9697, 'grad_norm': 0.48319464921951294, 'learning_rate': 0.002699537037037037, 'epoch': 1.3805555555555555}
Step 4980: {'loss': 0.9432, 'grad_norm': 0.6890885233879089, 'learning_rate': 0.0026949074074074076, 'epoch': 1.3833333333333333}
Step 4990: {'loss': 0.9454, 'grad_norm': 1.1786962747573853, 'learning_rate': 0.0026902777777777775, 'epoch': 1.386111111111111}
Step 5000: {'loss': 1.0311, 'grad_norm': 0.9623560905456543, 'learning_rate': 0.0026856481481481484, 'epoch': 1.3888888888888888}
Step 5010: {'loss': 0.9218, 'grad_norm': 0.8344826698303223, 'learning_rate': 0.0026810185185185188, 'epoch': 1.3916666666666666}
Step 5020: {'loss': 0.9218, 'grad_norm': 0.5797446966171265, 'learning_rate': 0.0026763888888888887, 'epoch': 1.3944444444444444}
Step 5030: {'loss': 1.0585, 'grad_norm': 0.6962007880210876, 'learning_rate': 0.002671759259259259, 'epoch': 1.3972222222222221}
Step 5040: {'loss': 1.0138, 'grad_norm': 0.939024806022644, 'learning_rate': 0.00266712962962963, 'epoch': 1.4}
Step 5050: {'loss': 1.0308, 'grad_norm': 0.6250815987586975, 'learning_rate': 0.0026625, 'epoch': 1.4027777777777777}
Step 5060: {'loss': 1.0082, 'grad_norm': 0.6935603618621826, 'learning_rate': 0.0026578703703703703, 'epoch': 1.4055555555555554}
Step 5070: {'loss': 0.9421, 'grad_norm': 0.7493770122528076, 'learning_rate': 0.002653240740740741, 'epoch': 1.4083333333333332}
Step 5080: {'loss': 0.935, 'grad_norm': 0.5739965438842773, 'learning_rate': 0.002648611111111111, 'epoch': 1.411111111111111}
Step 5090: {'loss': 0.9229, 'grad_norm': 0.7352971434593201, 'learning_rate': 0.0026439814814814815, 'epoch': 1.4138888888888888}
Step 5100: {'loss': 0.9831, 'grad_norm': 1.051814317703247, 'learning_rate': 0.0026393518518518515, 'epoch': 1.4166666666666667}
Step 5110: {'loss': 0.9984, 'grad_norm': 0.7105496525764465, 'learning_rate': 0.0026347222222222223, 'epoch': 1.4194444444444445}
Step 5120: {'loss': 0.9129, 'grad_norm': 0.6543266177177429, 'learning_rate': 0.0026300925925925927, 'epoch': 1.4222222222222223}
Step 5130: {'loss': 0.9592, 'grad_norm': 0.7103948593139648, 'learning_rate': 0.0026254629629629627, 'epoch': 1.425}
Step 5140: {'loss': 0.9684, 'grad_norm': 1.522143840789795, 'learning_rate': 0.0026208333333333335, 'epoch': 1.4277777777777778}
Step 5150: {'loss': 1.0225, 'grad_norm': 0.7344604134559631, 'learning_rate': 0.002616203703703704, 'epoch': 1.4305555555555556}
Step 5160: {'loss': 1.0088, 'grad_norm': 0.7179572582244873, 'learning_rate': 0.002611574074074074, 'epoch': 1.4333333333333333}
Step 5170: {'loss': 0.9599, 'grad_norm': 0.6468861103057861, 'learning_rate': 0.0026069444444444447, 'epoch': 1.4361111111111111}
Step 5180: {'loss': 1.0168, 'grad_norm': 1.2023190259933472, 'learning_rate': 0.002602314814814815, 'epoch': 1.4388888888888889}
Step 5190: {'loss': 1.0032, 'grad_norm': 0.4552686810493469, 'learning_rate': 0.002597685185185185, 'epoch': 1.4416666666666667}
Step 5200: {'loss': 0.903, 'grad_norm': 0.5307928323745728, 'learning_rate': 0.002593055555555556, 'epoch': 1.4444444444444444}
Step 5210: {'loss': 0.9645, 'grad_norm': 0.38579848408699036, 'learning_rate': 0.002588425925925926, 'epoch': 1.4472222222222222}
Step 5220: {'loss': 0.975, 'grad_norm': 0.9048053622245789, 'learning_rate': 0.0025837962962962963, 'epoch': 1.45}
Step 5230: {'loss': 0.993, 'grad_norm': 1.0582374334335327, 'learning_rate': 0.0025791666666666667, 'epoch': 1.4527777777777777}
Step 5240: {'loss': 0.8891, 'grad_norm': 0.536668598651886, 'learning_rate': 0.002574537037037037, 'epoch': 1.4555555555555555}
Step 5250: {'loss': 0.9687, 'grad_norm': 0.6038035154342651, 'learning_rate': 0.0025699074074074075, 'epoch': 1.4583333333333333}
Step 5260: {'loss': 0.8685, 'grad_norm': 0.6309667229652405, 'learning_rate': 0.002565277777777778, 'epoch': 1.4611111111111112}
Step 5270: {'loss': 0.8845, 'grad_norm': 0.5838062167167664, 'learning_rate': 0.0025606481481481483, 'epoch': 1.463888888888889}
Step 5280: {'loss': 1.0368, 'grad_norm': 0.8953534960746765, 'learning_rate': 0.0025560185185185187, 'epoch': 1.4666666666666668}
Step 5290: {'loss': 0.9874, 'grad_norm': 0.6888865232467651, 'learning_rate': 0.002551388888888889, 'epoch': 1.4694444444444446}
Step 5300: {'loss': 0.9367, 'grad_norm': 0.8538880944252014, 'learning_rate': 0.002546759259259259, 'epoch': 1.4722222222222223}
Step 5310: {'loss': 0.9018, 'grad_norm': 0.6305438280105591, 'learning_rate': 0.00254212962962963, 'epoch': 1.475}
Step 5320: {'loss': 1.0412, 'grad_norm': 2.7681849002838135, 'learning_rate': 0.0025375, 'epoch': 1.4777777777777779}
Step 5330: {'loss': 1.1172, 'grad_norm': 1.5593221187591553, 'learning_rate': 0.0025328703703703702, 'epoch': 1.4805555555555556}
Step 5340: {'loss': 1.1843, 'grad_norm': 1.1776163578033447, 'learning_rate': 0.002528240740740741, 'epoch': 1.4833333333333334}
Step 5350: {'loss': 1.0701, 'grad_norm': 2.3145129680633545, 'learning_rate': 0.002523611111111111, 'epoch': 1.4861111111111112}
Step 5360: {'loss': 0.9252, 'grad_norm': 0.6142382621765137, 'learning_rate': 0.0025189814814814814, 'epoch': 1.488888888888889}
Step 5370: {'loss': 0.9899, 'grad_norm': 0.46745604276657104, 'learning_rate': 0.0025143518518518523, 'epoch': 1.4916666666666667}
Step 5380: {'loss': 1.0124, 'grad_norm': 1.1829801797866821, 'learning_rate': 0.002509722222222222, 'epoch': 1.4944444444444445}
Step 5390: {'loss': 0.9524, 'grad_norm': 0.4536547064781189, 'learning_rate': 0.0025050925925925926, 'epoch': 1.4972222222222222}
Step 5400: {'loss': 1.0304, 'grad_norm': 0.6284598112106323, 'learning_rate': 0.0025004629629629634, 'epoch': 1.5}
Step 5410: {'loss': 0.9886, 'grad_norm': 0.6658861637115479, 'learning_rate': 0.0024958333333333334, 'epoch': 1.5027777777777778}
Step 5420: {'loss': 0.9266, 'grad_norm': 0.6606156826019287, 'learning_rate': 0.002491203703703704, 'epoch': 1.5055555555555555}
Step 5430: {'loss': 0.9426, 'grad_norm': 0.665854811668396, 'learning_rate': 0.002486574074074074, 'epoch': 1.5083333333333333}
Step 5440: {'loss': 0.9878, 'grad_norm': 0.7867692112922668, 'learning_rate': 0.0024819444444444446, 'epoch': 1.511111111111111}
Step 5450: {'loss': 1.0147, 'grad_norm': 0.6408870220184326, 'learning_rate': 0.0024773148148148146, 'epoch': 1.5138888888888888}
Step 5460: {'loss': 0.9606, 'grad_norm': 0.7301777005195618, 'learning_rate': 0.0024726851851851854, 'epoch': 1.5166666666666666}
Step 5470: {'loss': 0.9942, 'grad_norm': 0.8570608496665955, 'learning_rate': 0.002468055555555556, 'epoch': 1.5194444444444444}
Step 5480: {'loss': 0.9689, 'grad_norm': 0.7756512761116028, 'learning_rate': 0.0024634259259259258, 'epoch': 1.5222222222222221}
Step 5490: {'loss': 0.9365, 'grad_norm': 0.46709007024765015, 'learning_rate': 0.0024587962962962966, 'epoch': 1.525}
Step 5500: {'loss': 0.9338, 'grad_norm': 1.621314525604248, 'learning_rate': 0.0024541666666666666, 'epoch': 1.5277777777777777}
Step 5510: {'loss': 0.8871, 'grad_norm': 0.5116807222366333, 'learning_rate': 0.002449537037037037, 'epoch': 1.5305555555555554}
Step 5520: {'loss': 0.9698, 'grad_norm': 0.6032304167747498, 'learning_rate': 0.0024449074074074074, 'epoch': 1.5333333333333332}
Step 5530: {'loss': 0.9213, 'grad_norm': 0.6190026998519897, 'learning_rate': 0.0024402777777777778, 'epoch': 1.536111111111111}
Step 5540: {'loss': 0.9822, 'grad_norm': 0.9993652105331421, 'learning_rate': 0.002435648148148148, 'epoch': 1.5388888888888888}
Step 5550: {'loss': 1.0196, 'grad_norm': 0.5734419226646423, 'learning_rate': 0.0024310185185185186, 'epoch': 1.5416666666666665}
Step 5560: {'loss': 1.043, 'grad_norm': 0.827523946762085, 'learning_rate': 0.002426388888888889, 'epoch': 1.5444444444444443}
Step 5570: {'loss': 0.974, 'grad_norm': 0.44027119874954224, 'learning_rate': 0.0024217592592592594, 'epoch': 1.5472222222222223}
Step 5580: {'loss': 0.9521, 'grad_norm': 0.5203918218612671, 'learning_rate': 0.0024171296296296297, 'epoch': 1.55}
Step 5590: {'loss': 0.9335, 'grad_norm': 0.8845337629318237, 'learning_rate': 0.0024125, 'epoch': 1.5527777777777778}
Step 5600: {'loss': 1.0513, 'grad_norm': 0.5199037194252014, 'learning_rate': 0.0024078703703703705, 'epoch': 1.5555555555555556}
Step 5610: {'loss': 1.0185, 'grad_norm': 0.7069188952445984, 'learning_rate': 0.002403240740740741, 'epoch': 1.5583333333333333}
Step 5620: {'loss': 1.0834, 'grad_norm': 0.9675118327140808, 'learning_rate': 0.0023986111111111113, 'epoch': 1.5611111111111111}
Step 5630: {'loss': 0.888, 'grad_norm': 0.9023653268814087, 'learning_rate': 0.0023939814814814813, 'epoch': 1.5638888888888889}
Step 5640: {'loss': 0.9444, 'grad_norm': 0.9138733744621277, 'learning_rate': 0.002389351851851852, 'epoch': 1.5666666666666667}
Step 5650: {'loss': 0.9568, 'grad_norm': 0.8214371204376221, 'learning_rate': 0.002384722222222222, 'epoch': 1.5694444444444444}
Step 5660: {'loss': 0.994, 'grad_norm': 0.6672786474227905, 'learning_rate': 0.0023800925925925925, 'epoch': 1.5722222222222222}
Step 5670: {'loss': 1.0376, 'grad_norm': 0.7438244223594666, 'learning_rate': 0.002375462962962963, 'epoch': 1.575}
Step 5680: {'loss': 1.0606, 'grad_norm': 0.6420551538467407, 'learning_rate': 0.0023708333333333333, 'epoch': 1.5777777777777777}
Step 5690: {'loss': 0.9485, 'grad_norm': 1.1828958988189697, 'learning_rate': 0.0023662037037037037, 'epoch': 1.5805555555555557}
Step 5700: {'loss': 1.0372, 'grad_norm': 1.0277044773101807, 'learning_rate': 0.002361574074074074, 'epoch': 1.5833333333333335}
Step 5710: {'loss': 0.9384, 'grad_norm': 0.5451705455780029, 'learning_rate': 0.0023569444444444445, 'epoch': 1.5861111111111112}
Step 5720: {'loss': 0.9404, 'grad_norm': 0.5824726819992065, 'learning_rate': 0.002352314814814815, 'epoch': 1.588888888888889}
Step 5730: {'loss': 0.9839, 'grad_norm': 0.6861312389373779, 'learning_rate': 0.0023476851851851853, 'epoch': 1.5916666666666668}
Step 5740: {'loss': 1.0037, 'grad_norm': 0.9626807570457458, 'learning_rate': 0.0023430555555555557, 'epoch': 1.5944444444444446}
Step 5750: {'loss': 1.0013, 'grad_norm': 0.6019099950790405, 'learning_rate': 0.002338425925925926, 'epoch': 1.5972222222222223}
Step 5760: {'loss': 0.9708, 'grad_norm': 1.091091275215149, 'learning_rate': 0.0023337962962962965, 'epoch': 1.6}
Step 5770: {'loss': 1.041, 'grad_norm': 0.855815052986145, 'learning_rate': 0.0023291666666666665, 'epoch': 1.6027777777777779}
Step 5780: {'loss': 0.9645, 'grad_norm': 0.9047199487686157, 'learning_rate': 0.002324537037037037, 'epoch': 1.6055555555555556}
Step 5790: {'loss': 0.8815, 'grad_norm': 0.7125455737113953, 'learning_rate': 0.0023199074074074077, 'epoch': 1.6083333333333334}
Step 5800: {'loss': 0.9553, 'grad_norm': 0.6342752575874329, 'learning_rate': 0.0023152777777777776, 'epoch': 1.6111111111111112}
Step 5810: {'loss': 1.0619, 'grad_norm': 1.1309863328933716, 'learning_rate': 0.002310648148148148, 'epoch': 1.613888888888889}
Step 5820: {'loss': 1.0105, 'grad_norm': 0.6398746967315674, 'learning_rate': 0.002306018518518519, 'epoch': 1.6166666666666667}
Step 5830: {'loss': 0.8616, 'grad_norm': 0.3955681622028351, 'learning_rate': 0.002301388888888889, 'epoch': 1.6194444444444445}
Step 5840: {'loss': 0.9542, 'grad_norm': 2.073951244354248, 'learning_rate': 0.0022967592592592592, 'epoch': 1.6222222222222222}
Step 5850: {'loss': 0.9645, 'grad_norm': 0.47598370909690857, 'learning_rate': 0.0022921296296296296, 'epoch': 1.625}
Step 5860: {'loss': 0.9969, 'grad_norm': 1.0813407897949219, 'learning_rate': 0.0022875, 'epoch': 1.6277777777777778}
Step 5870: {'loss': 1.0079, 'grad_norm': 0.5477398633956909, 'learning_rate': 0.0022828703703703704, 'epoch': 1.6305555555555555}
Step 5880: {'loss': 1.0235, 'grad_norm': 0.9070947766304016, 'learning_rate': 0.002278240740740741, 'epoch': 1.6333333333333333}
Step 5890: {'loss': 0.8985, 'grad_norm': 0.39130866527557373, 'learning_rate': 0.0022736111111111112, 'epoch': 1.636111111111111}
Step 5900: {'loss': 0.9578, 'grad_norm': 0.39328551292419434, 'learning_rate': 0.0022689814814814816, 'epoch': 1.6388888888888888}
Step 5910: {'loss': 1.0097, 'grad_norm': 0.5599215030670166, 'learning_rate': 0.002264351851851852, 'epoch': 1.6416666666666666}
Step 5920: {'loss': 0.9328, 'grad_norm': 0.5238904356956482, 'learning_rate': 0.002259722222222222, 'epoch': 1.6444444444444444}
Step 5930: {'loss': 0.9566, 'grad_norm': 0.8008165955543518, 'learning_rate': 0.002255092592592593, 'epoch': 1.6472222222222221}
Step 5940: {'loss': 0.9208, 'grad_norm': 0.8001784086227417, 'learning_rate': 0.0022504629629629632, 'epoch': 1.65}
Step 5950: {'loss': 0.9298, 'grad_norm': 0.7423326969146729, 'learning_rate': 0.002245833333333333, 'epoch': 1.6527777777777777}
Step 5960: {'loss': 1.0192, 'grad_norm': 0.5601226091384888, 'learning_rate': 0.0022412037037037036, 'epoch': 1.6555555555555554}
Step 5970: {'loss': 0.8998, 'grad_norm': 0.6729182004928589, 'learning_rate': 0.0022365740740740744, 'epoch': 1.6583333333333332}
Step 5980: {'loss': 0.9649, 'grad_norm': 0.834086537361145, 'learning_rate': 0.0022319444444444444, 'epoch': 1.661111111111111}
Step 5990: {'loss': 0.9942, 'grad_norm': 1.3401347398757935, 'learning_rate': 0.0022273148148148148, 'epoch': 1.6638888888888888}
Step 6000: {'loss': 0.9535, 'grad_norm': 0.6208024621009827, 'learning_rate': 0.002222685185185185, 'epoch': 1.6666666666666665}
Step 6010: {'loss': 0.978, 'grad_norm': 0.5015783905982971, 'learning_rate': 0.0022180555555555556, 'epoch': 1.6694444444444443}
Step 6020: {'loss': 0.9076, 'grad_norm': 0.6444588303565979, 'learning_rate': 0.002213425925925926, 'epoch': 1.6722222222222223}
Step 6030: {'loss': 0.9134, 'grad_norm': 0.7332871556282043, 'learning_rate': 0.0022087962962962964, 'epoch': 1.675}
Step 6040: {'loss': 1.0161, 'grad_norm': 1.3680827617645264, 'learning_rate': 0.0022041666666666668, 'epoch': 1.6777777777777778}
Step 6050: {'loss': 0.9942, 'grad_norm': 0.7757023572921753, 'learning_rate': 0.002199537037037037, 'epoch': 1.6805555555555556}
Step 6060: {'loss': 1.0939, 'grad_norm': 0.9586707353591919, 'learning_rate': 0.0021949074074074076, 'epoch': 1.6833333333333333}
Step 6070: {'loss': 1.0102, 'grad_norm': 1.2948763370513916, 'learning_rate': 0.0021902777777777775, 'epoch': 1.6861111111111111}
Step 6080: {'loss': 0.9735, 'grad_norm': 0.5851709246635437, 'learning_rate': 0.0021856481481481484, 'epoch': 1.6888888888888889}
Step 6090: {'loss': 0.8638, 'grad_norm': 0.5272088050842285, 'learning_rate': 0.0021810185185185188, 'epoch': 1.6916666666666667}
Step 6100: {'loss': 0.9456, 'grad_norm': 0.8885396718978882, 'learning_rate': 0.0021763888888888887, 'epoch': 1.6944444444444444}
Step 6110: {'loss': 1.0729, 'grad_norm': 0.4781118929386139, 'learning_rate': 0.0021717592592592596, 'epoch': 1.6972222222222222}
Step 6120: {'loss': 0.9889, 'grad_norm': 1.9143610000610352, 'learning_rate': 0.0021671296296296295, 'epoch': 1.7}
Step 6130: {'loss': 0.8993, 'grad_norm': 0.9681974053382874, 'learning_rate': 0.0021625, 'epoch': 1.7027777777777777}
Step 6140: {'loss': 0.9902, 'grad_norm': 0.44656842947006226, 'learning_rate': 0.0021578703703703703, 'epoch': 1.7055555555555557}
Step 6150: {'loss': 0.8519, 'grad_norm': 0.6411566734313965, 'learning_rate': 0.0021532407407407407, 'epoch': 1.7083333333333335}
Step 6160: {'loss': 1.0133, 'grad_norm': 0.5858054757118225, 'learning_rate': 0.002148611111111111, 'epoch': 1.7111111111111112}
Step 6170: {'loss': 1.0173, 'grad_norm': 0.7707706093788147, 'learning_rate': 0.0021439814814814815, 'epoch': 1.713888888888889}
Step 6180: {'loss': 0.9945, 'grad_norm': 0.43092137575149536, 'learning_rate': 0.002139351851851852, 'epoch': 1.7166666666666668}
Step 6190: {'loss': 1.0146, 'grad_norm': 0.5440536737442017, 'learning_rate': 0.0021347222222222223, 'epoch': 1.7194444444444446}
Step 6200: {'loss': 0.9493, 'grad_norm': 0.5742732882499695, 'learning_rate': 0.0021300925925925927, 'epoch': 1.7222222222222223}
Step 6210: {'loss': 0.9987, 'grad_norm': 0.8329470753669739, 'learning_rate': 0.002125462962962963, 'epoch': 1.725}
Step 6220: {'loss': 0.9229, 'grad_norm': 1.1247292757034302, 'learning_rate': 0.0021208333333333335, 'epoch': 1.7277777777777779}
Step 6230: {'loss': 0.9916, 'grad_norm': 0.6831576228141785, 'learning_rate': 0.002116203703703704, 'epoch': 1.7305555555555556}
Step 6240: {'loss': 0.9586, 'grad_norm': 0.6037518382072449, 'learning_rate': 0.0021115740740740743, 'epoch': 1.7333333333333334}
Step 6250: {'loss': 0.9462, 'grad_norm': 0.7161400318145752, 'learning_rate': 0.0021069444444444443, 'epoch': 1.7361111111111112}
Step 6260: {'loss': 0.9691, 'grad_norm': 0.6057180762290955, 'learning_rate': 0.002102314814814815, 'epoch': 1.738888888888889}
Step 6270: {'loss': 0.9321, 'grad_norm': 1.3651978969573975, 'learning_rate': 0.002097685185185185, 'epoch': 1.7416666666666667}
Step 6280: {'loss': 0.9892, 'grad_norm': 0.5047007203102112, 'learning_rate': 0.0020930555555555555, 'epoch': 1.7444444444444445}
Step 6290: {'loss': 0.9589, 'grad_norm': 0.8030222058296204, 'learning_rate': 0.002088425925925926, 'epoch': 1.7472222222222222}
Step 6300: {'loss': 0.9499, 'grad_norm': 0.6342854499816895, 'learning_rate': 0.0020837962962962963, 'epoch': 1.75}
Step 6310: {'loss': 0.9595, 'grad_norm': 0.7536646723747253, 'learning_rate': 0.0020791666666666667, 'epoch': 1.7527777777777778}
Step 6320: {'loss': 0.8932, 'grad_norm': 0.60810786485672, 'learning_rate': 0.002074537037037037, 'epoch': 1.7555555555555555}
Step 6330: {'loss': 0.963, 'grad_norm': 0.864614725112915, 'learning_rate': 0.0020699074074074075, 'epoch': 1.7583333333333333}
Step 6340: {'loss': 0.9608, 'grad_norm': 1.5959757566452026, 'learning_rate': 0.002065277777777778, 'epoch': 1.761111111111111}
Step 6350: {'loss': 0.8751, 'grad_norm': 1.1441866159439087, 'learning_rate': 0.0020606481481481483, 'epoch': 1.7638888888888888}
Step 6360: {'loss': 1.0379, 'grad_norm': 1.8689079284667969, 'learning_rate': 0.0020560185185185187, 'epoch': 1.7666666666666666}
Step 6370: {'loss': 1.0046, 'grad_norm': 1.322562575340271, 'learning_rate': 0.002051388888888889, 'epoch': 1.7694444444444444}
Step 6380: {'loss': 0.9425, 'grad_norm': 0.7327510118484497, 'learning_rate': 0.0020467592592592595, 'epoch': 1.7722222222222221}
Step 6390: {'loss': 0.9342, 'grad_norm': 0.8558166027069092, 'learning_rate': 0.0020421296296296294, 'epoch': 1.775}
Step 6400: {'loss': 0.9459, 'grad_norm': 0.7069047689437866, 'learning_rate': 0.0020375, 'epoch': 1.7777777777777777}
Step 6410: {'loss': 1.0437, 'grad_norm': 0.825275719165802, 'learning_rate': 0.0020328703703703706, 'epoch': 1.7805555555555554}
Step 6420: {'loss': 0.8983, 'grad_norm': 0.7545620799064636, 'learning_rate': 0.0020282407407407406, 'epoch': 1.7833333333333332}
Step 6430: {'loss': 0.9491, 'grad_norm': 0.4275565445423126, 'learning_rate': 0.002023611111111111, 'epoch': 1.786111111111111}
Step 6440: {'loss': 1.0241, 'grad_norm': 1.2446290254592896, 'learning_rate': 0.002018981481481482, 'epoch': 1.7888888888888888}
Step 6450: {'loss': 0.9187, 'grad_norm': 0.5528396964073181, 'learning_rate': 0.002014351851851852, 'epoch': 1.7916666666666665}
Step 6460: {'loss': 0.9536, 'grad_norm': 0.7947736978530884, 'learning_rate': 0.002009722222222222, 'epoch': 1.7944444444444443}
Step 6470: {'loss': 0.9467, 'grad_norm': 1.059928297996521, 'learning_rate': 0.0020050925925925926, 'epoch': 1.7972222222222223}
Step 6480: {'loss': 0.948, 'grad_norm': 1.030382752418518, 'learning_rate': 0.002000462962962963, 'epoch': 1.8}
Step 6490: {'loss': 0.9633, 'grad_norm': 0.5616987943649292, 'learning_rate': 0.0019958333333333334, 'epoch': 1.8027777777777778}
Step 6500: {'loss': 0.9565, 'grad_norm': 0.6248250603675842, 'learning_rate': 0.001991203703703704, 'epoch': 1.8055555555555556}
Step 6510: {'loss': 0.8895, 'grad_norm': 0.44708654284477234, 'learning_rate': 0.0019865740740740738, 'epoch': 1.8083333333333333}
Step 6520: {'loss': 1.0198, 'grad_norm': 0.7303909659385681, 'learning_rate': 0.0019819444444444446, 'epoch': 1.8111111111111111}
Step 6530: {'loss': 0.9774, 'grad_norm': 0.5233215093612671, 'learning_rate': 0.001977314814814815, 'epoch': 1.8138888888888889}
Step 6540: {'loss': 0.9257, 'grad_norm': 0.5671339631080627, 'learning_rate': 0.001972685185185185, 'epoch': 1.8166666666666667}
Step 6550: {'loss': 0.9901, 'grad_norm': 0.9613161087036133, 'learning_rate': 0.001968055555555556, 'epoch': 1.8194444444444444}
Step 6560: {'loss': 0.9691, 'grad_norm': 1.4750210046768188, 'learning_rate': 0.001963425925925926, 'epoch': 1.8222222222222222}
Step 6570: {'loss': 0.8976, 'grad_norm': 0.7754388451576233, 'learning_rate': 0.001958796296296296, 'epoch': 1.825}
Step 6580: {'loss': 0.9041, 'grad_norm': 0.5449084639549255, 'learning_rate': 0.0019541666666666666, 'epoch': 1.8277777777777777}
Step 6590: {'loss': 0.934, 'grad_norm': 0.6651161313056946, 'learning_rate': 0.0019495370370370372, 'epoch': 1.8305555555555557}
Step 6600: {'loss': 0.8987, 'grad_norm': 0.5869197845458984, 'learning_rate': 0.0019449074074074076, 'epoch': 1.8333333333333335}
Step 6610: {'loss': 0.873, 'grad_norm': 0.6964204907417297, 'learning_rate': 0.0019402777777777777, 'epoch': 1.8361111111111112}
Step 6620: {'loss': 0.8368, 'grad_norm': 0.6496082544326782, 'learning_rate': 0.0019356481481481484, 'epoch': 1.838888888888889}
Step 6630: {'loss': 0.94, 'grad_norm': 0.5558983683586121, 'learning_rate': 0.0019310185185185185, 'epoch': 1.8416666666666668}
Step 6640: {'loss': 0.968, 'grad_norm': 0.7288580536842346, 'learning_rate': 0.001926388888888889, 'epoch': 1.8444444444444446}
Step 6650: {'loss': 0.9115, 'grad_norm': 0.7862935066223145, 'learning_rate': 0.0019217592592592591, 'epoch': 1.8472222222222223}
Step 6660: {'loss': 1.0397, 'grad_norm': 0.5707206130027771, 'learning_rate': 0.0019171296296296297, 'epoch': 1.85}
Step 6670: {'loss': 0.8977, 'grad_norm': 0.6467006802558899, 'learning_rate': 0.0019125000000000001, 'epoch': 1.8527777777777779}
Step 6680: {'loss': 0.872, 'grad_norm': 0.5305130481719971, 'learning_rate': 0.0019078703703703703, 'epoch': 1.8555555555555556}
Step 6690: {'loss': 0.9003, 'grad_norm': 0.453087717294693, 'learning_rate': 0.0019032407407407407, 'epoch': 1.8583333333333334}
Step 6700: {'loss': 0.9672, 'grad_norm': 0.6702544689178467, 'learning_rate': 0.0018986111111111113, 'epoch': 1.8611111111111112}
Step 6710: {'loss': 0.8753, 'grad_norm': 0.7886852622032166, 'learning_rate': 0.0018939814814814815, 'epoch': 1.863888888888889}
Step 6720: {'loss': 0.9329, 'grad_norm': 0.5689331293106079, 'learning_rate': 0.001889351851851852, 'epoch': 1.8666666666666667}
Step 6730: {'loss': 0.9224, 'grad_norm': 1.0513054132461548, 'learning_rate': 0.0018847222222222223, 'epoch': 1.8694444444444445}
Step 6740: {'loss': 1.0077, 'grad_norm': 0.6362541913986206, 'learning_rate': 0.0018800925925925927, 'epoch': 1.8722222222222222}
Step 6750: {'loss': 0.9187, 'grad_norm': 0.8339026570320129, 'learning_rate': 0.0018754629629629629, 'epoch': 1.875}
Step 6760: {'loss': 0.9091, 'grad_norm': 0.8371994495391846, 'learning_rate': 0.0018708333333333333, 'epoch': 1.8777777777777778}
Step 6770: {'loss': 1.0457, 'grad_norm': 0.7988250851631165, 'learning_rate': 0.001866203703703704, 'epoch': 1.8805555555555555}
Step 6780: {'loss': 0.9716, 'grad_norm': 0.7252733111381531, 'learning_rate': 0.001861574074074074, 'epoch': 1.8833333333333333}
Step 6790: {'loss': 1.0046, 'grad_norm': 0.7179559469223022, 'learning_rate': 0.0018569444444444445, 'epoch': 1.886111111111111}
Step 6800: {'loss': 0.8723, 'grad_norm': 0.5841375589370728, 'learning_rate': 0.0018523148148148147, 'epoch': 1.8888888888888888}
Step 6810: {'loss': 0.9924, 'grad_norm': 0.5031728148460388, 'learning_rate': 0.0018476851851851853, 'epoch': 1.8916666666666666}
Step 6820: {'loss': 0.9283, 'grad_norm': 1.5243021249771118, 'learning_rate': 0.0018430555555555557, 'epoch': 1.8944444444444444}
Step 6830: {'loss': 0.9805, 'grad_norm': 1.276016354560852, 'learning_rate': 0.0018384259259259259, 'epoch': 1.8972222222222221}
Step 6840: {'loss': 0.9887, 'grad_norm': 0.7598665356636047, 'learning_rate': 0.0018337962962962965, 'epoch': 1.9}
Step 6850: {'loss': 1.0372, 'grad_norm': 0.6099469065666199, 'learning_rate': 0.0018291666666666667, 'epoch': 1.9027777777777777}
Step 6860: {'loss': 0.9953, 'grad_norm': 0.6771491169929504, 'learning_rate': 0.001824537037037037, 'epoch': 1.9055555555555554}
Step 6870: {'loss': 0.8728, 'grad_norm': 0.44842004776000977, 'learning_rate': 0.0018199074074074072, 'epoch': 1.9083333333333332}
Step 6880: {'loss': 1.018, 'grad_norm': 0.5569671988487244, 'learning_rate': 0.0018152777777777779, 'epoch': 1.911111111111111}
Step 6890: {'loss': 1.0723, 'grad_norm': 0.890884280204773, 'learning_rate': 0.0018106481481481483, 'epoch': 1.9138888888888888}
Step 6900: {'loss': 0.9462, 'grad_norm': 0.5170239210128784, 'learning_rate': 0.0018060185185185184, 'epoch': 1.9166666666666665}
Step 6910: {'loss': 0.901, 'grad_norm': 0.8098617196083069, 'learning_rate': 0.0018013888888888888, 'epoch': 1.9194444444444443}
Step 6920: {'loss': 0.9124, 'grad_norm': 0.6965563893318176, 'learning_rate': 0.0017967592592592594, 'epoch': 1.9222222222222223}
Step 6930: {'loss': 0.8948, 'grad_norm': 0.7493172287940979, 'learning_rate': 0.0017921296296296296, 'epoch': 1.925}
Step 6940: {'loss': 0.955, 'grad_norm': 0.7104393839836121, 'learning_rate': 0.0017875, 'epoch': 1.9277777777777778}
Step 6950: {'loss': 0.8799, 'grad_norm': 0.485643595457077, 'learning_rate': 0.0017828703703703704, 'epoch': 1.9305555555555556}
Step 6960: {'loss': 0.9515, 'grad_norm': 0.8433693647384644, 'learning_rate': 0.0017782407407407408, 'epoch': 1.9333333333333333}
Step 6970: {'loss': 0.9539, 'grad_norm': 1.177676796913147, 'learning_rate': 0.0017736111111111112, 'epoch': 1.9361111111111111}
Step 6980: {'loss': 0.9796, 'grad_norm': 1.5176808834075928, 'learning_rate': 0.0017689814814814814, 'epoch': 1.9388888888888889}
Step 6990: {'loss': 0.9752, 'grad_norm': 1.61104154586792, 'learning_rate': 0.001764351851851852, 'epoch': 1.9416666666666667}
Step 7000: {'loss': 0.9007, 'grad_norm': 0.7261026501655579, 'learning_rate': 0.0017597222222222222, 'epoch': 1.9444444444444444}
Step 7010: {'loss': 0.9877, 'grad_norm': 0.5704547762870789, 'learning_rate': 0.0017550925925925926, 'epoch': 1.9472222222222222}
Step 7020: {'loss': 0.9131, 'grad_norm': 0.44550415873527527, 'learning_rate': 0.0017504629629629628, 'epoch': 1.95}
Step 7030: {'loss': 1.0033, 'grad_norm': 0.7556143999099731, 'learning_rate': 0.0017458333333333334, 'epoch': 1.9527777777777777}
Step 7040: {'loss': 0.9581, 'grad_norm': 0.45798370242118835, 'learning_rate': 0.0017412037037037038, 'epoch': 1.9555555555555557}
Step 7050: {'loss': 0.9395, 'grad_norm': 0.474771112203598, 'learning_rate': 0.001736574074074074, 'epoch': 1.9583333333333335}
Step 7060: {'loss': 0.9079, 'grad_norm': 1.0235660076141357, 'learning_rate': 0.0017319444444444446, 'epoch': 1.9611111111111112}
Step 7070: {'loss': 0.8906, 'grad_norm': 0.5415999293327332, 'learning_rate': 0.001727314814814815, 'epoch': 1.963888888888889}
Step 7080: {'loss': 0.9181, 'grad_norm': 1.311177372932434, 'learning_rate': 0.0017226851851851852, 'epoch': 1.9666666666666668}
Step 7090: {'loss': 1.0025, 'grad_norm': 0.48072919249534607, 'learning_rate': 0.0017180555555555556, 'epoch': 1.9694444444444446}
Step 7100: {'loss': 0.8597, 'grad_norm': 0.542270302772522, 'learning_rate': 0.001713425925925926, 'epoch': 1.9722222222222223}
Step 7110: {'loss': 0.9454, 'grad_norm': 0.7095366716384888, 'learning_rate': 0.0017087962962962964, 'epoch': 1.975}
Step 7120: {'loss': 0.9099, 'grad_norm': 0.5798348188400269, 'learning_rate': 0.0017041666666666665, 'epoch': 1.9777777777777779}
Step 7130: {'loss': 0.911, 'grad_norm': 0.8983389735221863, 'learning_rate': 0.001699537037037037, 'epoch': 1.9805555555555556}
Step 7140: {'loss': 1.0352, 'grad_norm': 0.7622478008270264, 'learning_rate': 0.0016949074074074076, 'epoch': 1.9833333333333334}
Step 7150: {'loss': 0.9459, 'grad_norm': 0.6225858330726624, 'learning_rate': 0.0016902777777777777, 'epoch': 1.9861111111111112}
Step 7160: {'loss': 0.9729, 'grad_norm': 0.5794437527656555, 'learning_rate': 0.0016856481481481481, 'epoch': 1.988888888888889}
Step 7170: {'loss': 0.9506, 'grad_norm': 0.5521824359893799, 'learning_rate': 0.0016810185185185188, 'epoch': 1.9916666666666667}
Step 7180: {'loss': 1.023, 'grad_norm': 0.5357571840286255, 'learning_rate': 0.001676388888888889, 'epoch': 1.9944444444444445}
Step 7190: {'loss': 0.9892, 'grad_norm': 0.7764785289764404, 'learning_rate': 0.0016717592592592593, 'epoch': 1.9972222222222222}
Step 7200: {'loss': 1.0545, 'grad_norm': 0.9242117404937744, 'learning_rate': 0.0016671296296296295, 'epoch': 2.0}
Step 7210: {'loss': 1.0038, 'grad_norm': 1.0092884302139282, 'learning_rate': 0.0016625000000000001, 'epoch': 2.0027777777777778}
Step 7220: {'loss': 0.9199, 'grad_norm': 1.1185827255249023, 'learning_rate': 0.0016578703703703703, 'epoch': 2.0055555555555555}
Step 7230: {'loss': 0.9314, 'grad_norm': 2.063701629638672, 'learning_rate': 0.0016532407407407407, 'epoch': 2.0083333333333333}
Step 7240: {'loss': 1.0201, 'grad_norm': 0.8359583020210266, 'learning_rate': 0.0016486111111111113, 'epoch': 2.011111111111111}
Step 7250: {'loss': 0.9288, 'grad_norm': 0.5461949110031128, 'learning_rate': 0.0016439814814814815, 'epoch': 2.013888888888889}
Step 7260: {'loss': 0.9415, 'grad_norm': 1.291719675064087, 'learning_rate': 0.001639351851851852, 'epoch': 2.0166666666666666}
Step 7270: {'loss': 0.9093, 'grad_norm': 3.672128200531006, 'learning_rate': 0.001634722222222222, 'epoch': 2.0194444444444444}
Step 7280: {'loss': 0.909, 'grad_norm': 0.4427643120288849, 'learning_rate': 0.0016300925925925927, 'epoch': 2.022222222222222}
Step 7290: {'loss': 0.9893, 'grad_norm': 0.5752298831939697, 'learning_rate': 0.001625462962962963, 'epoch': 2.025}
Step 7300: {'loss': 0.9296, 'grad_norm': 1.2242048978805542, 'learning_rate': 0.0016208333333333333, 'epoch': 2.0277777777777777}
Step 7310: {'loss': 1.0838, 'grad_norm': 0.71883225440979, 'learning_rate': 0.0016162037037037037, 'epoch': 2.0305555555555554}
Step 7320: {'loss': 0.9388, 'grad_norm': 0.9170186519622803, 'learning_rate': 0.0016115740740740743, 'epoch': 2.033333333333333}
Step 7330: {'loss': 0.9834, 'grad_norm': 0.8484771251678467, 'learning_rate': 0.0016069444444444445, 'epoch': 2.036111111111111}
Step 7340: {'loss': 0.8927, 'grad_norm': 0.6886072754859924, 'learning_rate': 0.0016023148148148149, 'epoch': 2.0388888888888888}
Step 7350: {'loss': 0.9732, 'grad_norm': 0.8418915867805481, 'learning_rate': 0.0015976851851851853, 'epoch': 2.0416666666666665}
Step 7360: {'loss': 0.9638, 'grad_norm': 0.8862078785896301, 'learning_rate': 0.0015930555555555557, 'epoch': 2.0444444444444443}
Step 7370: {'loss': 0.9332, 'grad_norm': 0.7404569983482361, 'learning_rate': 0.0015884259259259259, 'epoch': 2.047222222222222}
Step 7380: {'loss': 0.9415, 'grad_norm': 0.45655888319015503, 'learning_rate': 0.0015837962962962963, 'epoch': 2.05}
Step 7390: {'loss': 0.9464, 'grad_norm': 0.9335957765579224, 'learning_rate': 0.0015791666666666669, 'epoch': 2.0527777777777776}
Step 7400: {'loss': 0.8872, 'grad_norm': 0.769129753112793, 'learning_rate': 0.001574537037037037, 'epoch': 2.0555555555555554}
Step 7410: {'loss': 0.9092, 'grad_norm': 0.7720823884010315, 'learning_rate': 0.0015699074074074074, 'epoch': 2.058333333333333}
Step 7420: {'loss': 0.9418, 'grad_norm': 1.7014063596725464, 'learning_rate': 0.0015652777777777776, 'epoch': 2.061111111111111}
Step 7430: {'loss': 0.9742, 'grad_norm': 0.7038824558258057, 'learning_rate': 0.0015606481481481482, 'epoch': 2.063888888888889}
Step 7440: {'loss': 0.8442, 'grad_norm': 0.430642306804657, 'learning_rate': 0.0015560185185185186, 'epoch': 2.066666666666667}
Step 7450: {'loss': 0.9214, 'grad_norm': 0.4771341383457184, 'learning_rate': 0.0015513888888888888, 'epoch': 2.0694444444444446}
Step 7460: {'loss': 0.875, 'grad_norm': 0.6333844661712646, 'learning_rate': 0.0015467592592592594, 'epoch': 2.0722222222222224}
Step 7470: {'loss': 0.9657, 'grad_norm': 1.0958776473999023, 'learning_rate': 0.0015421296296296296, 'epoch': 2.075}
Step 7480: {'loss': 0.9301, 'grad_norm': 0.9134036898612976, 'learning_rate': 0.0015375, 'epoch': 2.077777777777778}
Step 7490: {'loss': 1.0183, 'grad_norm': 0.6096910238265991, 'learning_rate': 0.0015328703703703702, 'epoch': 2.0805555555555557}
Step 7500: {'loss': 0.9198, 'grad_norm': 0.5618218779563904, 'learning_rate': 0.0015282407407407408, 'epoch': 2.0833333333333335}
Step 7510: {'loss': 1.0055, 'grad_norm': 0.8284045457839966, 'learning_rate': 0.0015236111111111112, 'epoch': 2.0861111111111112}
Step 7520: {'loss': 1.0137, 'grad_norm': 1.136901617050171, 'learning_rate': 0.0015189814814814814, 'epoch': 2.088888888888889}
Step 7530: {'loss': 0.9121, 'grad_norm': 0.5826315879821777, 'learning_rate': 0.0015143518518518518, 'epoch': 2.091666666666667}
Step 7540: {'loss': 1.0187, 'grad_norm': 0.6993445158004761, 'learning_rate': 0.0015097222222222224, 'epoch': 2.0944444444444446}
Step 7550: {'loss': 1.0025, 'grad_norm': 1.0326184034347534, 'learning_rate': 0.0015050925925925926, 'epoch': 2.0972222222222223}
Step 7560: {'loss': 1.0233, 'grad_norm': 0.7547323107719421, 'learning_rate': 0.001500462962962963, 'epoch': 2.1}
Step 7570: {'loss': 0.9278, 'grad_norm': 0.6450777649879456, 'learning_rate': 0.0014958333333333334, 'epoch': 2.102777777777778}
Step 7580: {'loss': 0.9984, 'grad_norm': 0.510583221912384, 'learning_rate': 0.0014912037037037038, 'epoch': 2.1055555555555556}
Step 7590: {'loss': 0.873, 'grad_norm': 0.46480792760849, 'learning_rate': 0.0014865740740740742, 'epoch': 2.1083333333333334}
Step 7600: {'loss': 0.9622, 'grad_norm': 0.6597659587860107, 'learning_rate': 0.0014819444444444444, 'epoch': 2.111111111111111}
Step 7610: {'loss': 0.9077, 'grad_norm': 0.6657878756523132, 'learning_rate': 0.001477314814814815, 'epoch': 2.113888888888889}
Step 7620: {'loss': 0.8699, 'grad_norm': 0.5313243865966797, 'learning_rate': 0.0014726851851851852, 'epoch': 2.1166666666666667}
Step 7630: {'loss': 1.0226, 'grad_norm': 1.1566251516342163, 'learning_rate': 0.0014680555555555556, 'epoch': 2.1194444444444445}
Step 7640: {'loss': 0.926, 'grad_norm': 0.7817688584327698, 'learning_rate': 0.0014634259259259257, 'epoch': 2.1222222222222222}
Step 7650: {'loss': 0.9243, 'grad_norm': 0.6070982813835144, 'learning_rate': 0.0014587962962962964, 'epoch': 2.125}
Step 7660: {'loss': 0.9971, 'grad_norm': 1.1404964923858643, 'learning_rate': 0.0014541666666666668, 'epoch': 2.1277777777777778}
Step 7670: {'loss': 0.9079, 'grad_norm': 0.9499942064285278, 'learning_rate': 0.001449537037037037, 'epoch': 2.1305555555555555}
Step 7680: {'loss': 0.9789, 'grad_norm': 0.666959822177887, 'learning_rate': 0.0014449074074074076, 'epoch': 2.1333333333333333}
Step 7690: {'loss': 0.9512, 'grad_norm': 0.6234413385391235, 'learning_rate': 0.001440277777777778, 'epoch': 2.136111111111111}
Step 7700: {'loss': 0.9857, 'grad_norm': 0.8548439741134644, 'learning_rate': 0.0014356481481481481, 'epoch': 2.138888888888889}
Step 7710: {'loss': 0.9647, 'grad_norm': 0.5373328328132629, 'learning_rate': 0.0014310185185185185, 'epoch': 2.1416666666666666}
Step 7720: {'loss': 0.8574, 'grad_norm': 0.6701087355613708, 'learning_rate': 0.001426388888888889, 'epoch': 2.1444444444444444}
Step 7730: {'loss': 0.9555, 'grad_norm': 0.6689515709877014, 'learning_rate': 0.0014217592592592593, 'epoch': 2.147222222222222}
Step 7740: {'loss': 0.9321, 'grad_norm': 2.530294418334961, 'learning_rate': 0.0014171296296296295, 'epoch': 2.15}
Step 7750: {'loss': 0.922, 'grad_norm': 0.6081188917160034, 'learning_rate': 0.0014125, 'epoch': 2.1527777777777777}
Step 7760: {'loss': 0.9407, 'grad_norm': 0.5522874593734741, 'learning_rate': 0.0014078703703703705, 'epoch': 2.1555555555555554}
Step 7770: {'loss': 1.0339, 'grad_norm': 0.7876049876213074, 'learning_rate': 0.0014032407407407407, 'epoch': 2.158333333333333}
Step 7780: {'loss': 0.8423, 'grad_norm': 0.9099310040473938, 'learning_rate': 0.001398611111111111, 'epoch': 2.161111111111111}
Step 7790: {'loss': 0.882, 'grad_norm': 0.5840568542480469, 'learning_rate': 0.0013939814814814817, 'epoch': 2.1638888888888888}
Step 7800: {'loss': 1.0063, 'grad_norm': 1.0858590602874756, 'learning_rate': 0.001389351851851852, 'epoch': 2.1666666666666665}
Step 7810: {'loss': 0.9175, 'grad_norm': 1.0561200380325317, 'learning_rate': 0.0013847222222222223, 'epoch': 2.1694444444444443}
Step 7820: {'loss': 0.9204, 'grad_norm': 1.3549110889434814, 'learning_rate': 0.0013800925925925925, 'epoch': 2.172222222222222}
Step 7830: {'loss': 0.9097, 'grad_norm': 1.144193410873413, 'learning_rate': 0.001375462962962963, 'epoch': 2.175}
Step 7840: {'loss': 0.8983, 'grad_norm': 0.41838130354881287, 'learning_rate': 0.0013708333333333333, 'epoch': 2.1777777777777776}
Step 7850: {'loss': 0.9241, 'grad_norm': 0.6066493391990662, 'learning_rate': 0.0013662037037037037, 'epoch': 2.1805555555555554}
Step 7860: {'loss': 1.0311, 'grad_norm': 0.4450872838497162, 'learning_rate': 0.0013615740740740739, 'epoch': 2.183333333333333}
Step 7870: {'loss': 0.9097, 'grad_norm': 0.6748896241188049, 'learning_rate': 0.0013569444444444445, 'epoch': 2.186111111111111}
Step 7880: {'loss': 1.0165, 'grad_norm': 1.2917888164520264, 'learning_rate': 0.0013523148148148149, 'epoch': 2.188888888888889}
Step 7890: {'loss': 0.9316, 'grad_norm': 0.5761563777923584, 'learning_rate': 0.001347685185185185, 'epoch': 2.191666666666667}
Step 7900: {'loss': 0.9874, 'grad_norm': 0.6548261046409607, 'learning_rate': 0.0013430555555555557, 'epoch': 2.1944444444444446}
Step 7910: {'loss': 0.9679, 'grad_norm': 0.4507228136062622, 'learning_rate': 0.001338425925925926, 'epoch': 2.1972222222222224}
Step 7920: {'loss': 1.0293, 'grad_norm': 0.8324399590492249, 'learning_rate': 0.0013337962962962962, 'epoch': 2.2}
Step 7930: {'loss': 0.8888, 'grad_norm': 0.5729785561561584, 'learning_rate': 0.0013291666666666666, 'epoch': 2.202777777777778}
Step 7940: {'loss': 0.9178, 'grad_norm': 1.366418719291687, 'learning_rate': 0.001324537037037037, 'epoch': 2.2055555555555557}
Step 7950: {'loss': 0.8977, 'grad_norm': 0.4301551282405853, 'learning_rate': 0.0013199074074074074, 'epoch': 2.2083333333333335}
Step 7960: {'loss': 0.9877, 'grad_norm': 1.089434027671814, 'learning_rate': 0.0013152777777777778, 'epoch': 2.2111111111111112}
Step 7970: {'loss': 0.9162, 'grad_norm': 0.5476661324501038, 'learning_rate': 0.0013106481481481482, 'epoch': 2.213888888888889}
Step 7980: {'loss': 0.9417, 'grad_norm': 1.7120736837387085, 'learning_rate': 0.0013060185185185186, 'epoch': 2.216666666666667}
Step 7990: {'loss': 0.8551, 'grad_norm': 0.4894856810569763, 'learning_rate': 0.0013013888888888888, 'epoch': 2.2194444444444446}
Step 8000: {'loss': 0.9525, 'grad_norm': 0.4531615972518921, 'learning_rate': 0.0012967592592592592, 'epoch': 2.2222222222222223}
Step 8010: {'loss': 0.9018, 'grad_norm': 1.4017560482025146, 'learning_rate': 0.0012921296296296298, 'epoch': 2.225}
Step 8020: {'loss': 0.9886, 'grad_norm': 1.181223750114441, 'learning_rate': 0.0012875, 'epoch': 2.227777777777778}
Step 8030: {'loss': 0.9199, 'grad_norm': 0.6679280400276184, 'learning_rate': 0.0012828703703703704, 'epoch': 2.2305555555555556}
Step 8040: {'loss': 0.9594, 'grad_norm': 0.6240660548210144, 'learning_rate': 0.0012782407407407406, 'epoch': 2.2333333333333334}
Step 8050: {'loss': 1.0309, 'grad_norm': 0.7647998929023743, 'learning_rate': 0.0012736111111111112, 'epoch': 2.236111111111111}
Step 8060: {'loss': 0.953, 'grad_norm': 0.7700011134147644, 'learning_rate': 0.0012689814814814816, 'epoch': 2.238888888888889}
Step 8070: {'loss': 0.9097, 'grad_norm': 1.0974304676055908, 'learning_rate': 0.0012643518518518518, 'epoch': 2.2416666666666667}
Step 8080: {'loss': 0.9542, 'grad_norm': 0.7010576128959656, 'learning_rate': 0.0012597222222222224, 'epoch': 2.2444444444444445}
Step 8090: {'loss': 0.9988, 'grad_norm': 0.7517632842063904, 'learning_rate': 0.0012550925925925926, 'epoch': 2.2472222222222222}
Step 8100: {'loss': 0.9801, 'grad_norm': 0.6294459700584412, 'learning_rate': 0.001250462962962963, 'epoch': 2.25}
Step 8110: {'loss': 0.9837, 'grad_norm': 0.7520151734352112, 'learning_rate': 0.0012458333333333334, 'epoch': 2.2527777777777778}
Step 8120: {'loss': 0.9516, 'grad_norm': 0.9978694915771484, 'learning_rate': 0.0012412037037037038, 'epoch': 2.2555555555555555}
Step 8130: {'loss': 0.8723, 'grad_norm': 0.5954883098602295, 'learning_rate': 0.0012365740740740742, 'epoch': 2.2583333333333333}
Step 8140: {'loss': 0.9513, 'grad_norm': 0.662273108959198, 'learning_rate': 0.0012319444444444444, 'epoch': 2.261111111111111}
Step 8150: {'loss': 0.9194, 'grad_norm': 0.7850289344787598, 'learning_rate': 0.0012273148148148148, 'epoch': 2.263888888888889}
Step 8160: {'loss': 0.9238, 'grad_norm': 0.8274027109146118, 'learning_rate': 0.0012226851851851852, 'epoch': 2.2666666666666666}
Step 8170: {'loss': 0.9686, 'grad_norm': 0.6556694507598877, 'learning_rate': 0.0012180555555555556, 'epoch': 2.2694444444444444}
Step 8180: {'loss': 0.9012, 'grad_norm': 0.9163112044334412, 'learning_rate': 0.001213425925925926, 'epoch': 2.272222222222222}
Step 8190: {'loss': 0.964, 'grad_norm': 0.6742934584617615, 'learning_rate': 0.0012087962962962964, 'epoch': 2.275}
Step 8200: {'loss': 0.8984, 'grad_norm': 0.724143922328949, 'learning_rate': 0.0012041666666666668, 'epoch': 2.2777777777777777}
Step 8210: {'loss': 0.8519, 'grad_norm': 0.5313547253608704, 'learning_rate': 0.001199537037037037, 'epoch': 2.2805555555555554}
Step 8220: {'loss': 0.9459, 'grad_norm': 1.2083728313446045, 'learning_rate': 0.0011949074074074075, 'epoch': 2.283333333333333}
Step 8230: {'loss': 0.9514, 'grad_norm': 0.7312645316123962, 'learning_rate': 0.0011902777777777777, 'epoch': 2.286111111111111}
Step 8240: {'loss': 0.8862, 'grad_norm': 0.912361741065979, 'learning_rate': 0.0011856481481481481, 'epoch': 2.2888888888888888}
Step 8250: {'loss': 0.9074, 'grad_norm': 0.8016021847724915, 'learning_rate': 0.0011810185185185185, 'epoch': 2.2916666666666665}
Step 8260: {'loss': 0.8585, 'grad_norm': 0.624031662940979, 'learning_rate': 0.001176388888888889, 'epoch': 2.2944444444444443}
Step 8270: {'loss': 0.9414, 'grad_norm': 0.8060621619224548, 'learning_rate': 0.0011717592592592593, 'epoch': 2.297222222222222}
Step 8280: {'loss': 0.9199, 'grad_norm': 0.5311681032180786, 'learning_rate': 0.0011671296296296297, 'epoch': 2.3}
Step 8290: {'loss': 0.9442, 'grad_norm': 0.9278886914253235, 'learning_rate': 0.0011625000000000001, 'epoch': 2.3027777777777776}
Step 8300: {'loss': 0.9503, 'grad_norm': 0.6323524117469788, 'learning_rate': 0.0011578703703703703, 'epoch': 2.3055555555555554}
Step 8310: {'loss': 0.9415, 'grad_norm': 2.223452568054199, 'learning_rate': 0.001153240740740741, 'epoch': 2.3083333333333336}
Step 8320: {'loss': 0.9408, 'grad_norm': 0.9299434423446655, 'learning_rate': 0.001148611111111111, 'epoch': 2.311111111111111}
Step 8330: {'loss': 0.9531, 'grad_norm': 0.8962321877479553, 'learning_rate': 0.0011439814814814815, 'epoch': 2.313888888888889}
Step 8340: {'loss': 0.9792, 'grad_norm': 0.5816237330436707, 'learning_rate': 0.001139351851851852, 'epoch': 2.3166666666666664}
Step 8350: {'loss': 0.8604, 'grad_norm': 0.9258334040641785, 'learning_rate': 0.0011347222222222223, 'epoch': 2.3194444444444446}
Step 8360: {'loss': 0.9408, 'grad_norm': 0.8371824026107788, 'learning_rate': 0.0011300925925925925, 'epoch': 2.3222222222222224}
Step 8370: {'loss': 0.9441, 'grad_norm': 0.6259452104568481, 'learning_rate': 0.001125462962962963, 'epoch': 2.325}
Step 8380: {'loss': 0.9838, 'grad_norm': 1.1498606204986572, 'learning_rate': 0.0011208333333333333, 'epoch': 2.327777777777778}
Step 8390: {'loss': 0.914, 'grad_norm': 0.6928141713142395, 'learning_rate': 0.0011162037037037037, 'epoch': 2.3305555555555557}
Step 8400: {'loss': 0.9189, 'grad_norm': 0.9397886991500854, 'learning_rate': 0.001111574074074074, 'epoch': 2.3333333333333335}
Step 8410: {'loss': 0.9876, 'grad_norm': 1.1727640628814697, 'learning_rate': 0.0011069444444444445, 'epoch': 2.3361111111111112}
Step 8420: {'loss': 1.0518, 'grad_norm': 0.5631185173988342, 'learning_rate': 0.0011023148148148149, 'epoch': 2.338888888888889}
Step 8430: {'loss': 0.9715, 'grad_norm': 0.6378024220466614, 'learning_rate': 0.0010976851851851853, 'epoch': 2.341666666666667}
Step 8440: {'loss': 0.9144, 'grad_norm': 1.2192432880401611, 'learning_rate': 0.0010930555555555557, 'epoch': 2.3444444444444446}
Step 8450: {'loss': 0.9022, 'grad_norm': 0.5543976426124573, 'learning_rate': 0.0010884259259259258, 'epoch': 2.3472222222222223}
Step 8460: {'loss': 0.9576, 'grad_norm': 0.7209756374359131, 'learning_rate': 0.0010837962962962962, 'epoch': 2.35}
Step 8470: {'loss': 0.895, 'grad_norm': 0.6639307737350464, 'learning_rate': 0.0010791666666666666, 'epoch': 2.352777777777778}
Step 8480: {'loss': 0.9954, 'grad_norm': 0.6877062916755676, 'learning_rate': 0.001074537037037037, 'epoch': 2.3555555555555556}
Step 8490: {'loss': 0.9251, 'grad_norm': 0.601505696773529, 'learning_rate': 0.0010699074074074074, 'epoch': 2.3583333333333334}
Step 8500: {'loss': 0.9335, 'grad_norm': 0.6454237103462219, 'learning_rate': 0.0010652777777777778, 'epoch': 2.361111111111111}
Step 8510: {'loss': 0.9141, 'grad_norm': 0.8230634927749634, 'learning_rate': 0.0010606481481481482, 'epoch': 2.363888888888889}
Step 8520: {'loss': 1.0149, 'grad_norm': 1.0374990701675415, 'learning_rate': 0.0010560185185185184, 'epoch': 2.3666666666666667}
Step 8530: {'loss': 0.9176, 'grad_norm': 0.9904072284698486, 'learning_rate': 0.001051388888888889, 'epoch': 2.3694444444444445}
Step 8540: {'loss': 0.8838, 'grad_norm': 0.8566849231719971, 'learning_rate': 0.0010467592592592592, 'epoch': 2.3722222222222222}
Step 8550: {'loss': 0.9247, 'grad_norm': 0.6564648151397705, 'learning_rate': 0.0010421296296296296, 'epoch': 2.375}
Step 8560: {'loss': 1.0485, 'grad_norm': 0.8039534687995911, 'learning_rate': 0.0010375, 'epoch': 2.3777777777777778}
Step 8570: {'loss': 0.9272, 'grad_norm': 0.8375171422958374, 'learning_rate': 0.0010328703703703704, 'epoch': 2.3805555555555555}
Step 8580: {'loss': 0.9833, 'grad_norm': 0.8167377710342407, 'learning_rate': 0.0010282407407407406, 'epoch': 2.3833333333333333}
Step 8590: {'loss': 0.9259, 'grad_norm': 0.4677886664867401, 'learning_rate': 0.0010236111111111112, 'epoch': 2.386111111111111}
Step 8600: {'loss': 1.0186, 'grad_norm': 1.3081600666046143, 'learning_rate': 0.0010189814814814816, 'epoch': 2.388888888888889}
Step 8610: {'loss': 0.9858, 'grad_norm': 0.7227062582969666, 'learning_rate': 0.0010143518518518518, 'epoch': 2.3916666666666666}
Step 8620: {'loss': 0.9771, 'grad_norm': 1.2755248546600342, 'learning_rate': 0.0010097222222222222, 'epoch': 2.3944444444444444}
Step 8630: {'loss': 0.9714, 'grad_norm': 1.155407428741455, 'learning_rate': 0.0010050925925925926, 'epoch': 2.397222222222222}
Step 8640: {'loss': 0.9198, 'grad_norm': 0.6191205382347107, 'learning_rate': 0.001000462962962963, 'epoch': 2.4}
Step 8650: {'loss': 0.9854, 'grad_norm': 0.6483917832374573, 'learning_rate': 0.0009958333333333334, 'epoch': 2.4027777777777777}
Step 8660: {'loss': 0.901, 'grad_norm': 0.5355765223503113, 'learning_rate': 0.0009912037037037038, 'epoch': 2.4055555555555554}
Step 8670: {'loss': 1.0012, 'grad_norm': 0.4515829086303711, 'learning_rate': 0.000986574074074074, 'epoch': 2.408333333333333}
Step 8680: {'loss': 1.0476, 'grad_norm': 1.062070369720459, 'learning_rate': 0.0009819444444444446, 'epoch': 2.411111111111111}
Step 8690: {'loss': 0.9836, 'grad_norm': 0.45941299200057983, 'learning_rate': 0.0009773148148148148, 'epoch': 2.4138888888888888}
Step 8700: {'loss': 0.884, 'grad_norm': 0.708255410194397, 'learning_rate': 0.0009726851851851852, 'epoch': 2.4166666666666665}
Step 8710: {'loss': 0.9637, 'grad_norm': 0.8189176321029663, 'learning_rate': 0.0009680555555555557, 'epoch': 2.4194444444444443}
Step 8720: {'loss': 0.95, 'grad_norm': 0.4611627161502838, 'learning_rate': 0.000963425925925926, 'epoch': 2.422222222222222}
Step 8730: {'loss': 0.7735, 'grad_norm': 0.7467739582061768, 'learning_rate': 0.0009587962962962963, 'epoch': 2.425}
Step 8740: {'loss': 0.8821, 'grad_norm': 0.9097698926925659, 'learning_rate': 0.0009541666666666666, 'epoch': 2.4277777777777776}
Step 8750: {'loss': 0.9457, 'grad_norm': 0.3782452642917633, 'learning_rate': 0.000949537037037037, 'epoch': 2.4305555555555554}
Step 8760: {'loss': 0.9033, 'grad_norm': 0.712513267993927, 'learning_rate': 0.0009449074074074074, 'epoch': 2.4333333333333336}
Step 8770: {'loss': 0.9293, 'grad_norm': 0.6242042779922485, 'learning_rate': 0.0009402777777777778, 'epoch': 2.436111111111111}
Step 8780: {'loss': 0.992, 'grad_norm': 0.7511413097381592, 'learning_rate': 0.0009356481481481481, 'epoch': 2.438888888888889}
Step 8790: {'loss': 0.8785, 'grad_norm': 0.5846637487411499, 'learning_rate': 0.0009310185185185185, 'epoch': 2.4416666666666664}
Step 8800: {'loss': 1.0225, 'grad_norm': 0.7172054052352905, 'learning_rate': 0.0009263888888888889, 'epoch': 2.4444444444444446}
Step 8810: {'loss': 0.8745, 'grad_norm': 0.8094006180763245, 'learning_rate': 0.0009217592592592593, 'epoch': 2.4472222222222224}
Step 8820: {'loss': 0.9731, 'grad_norm': 0.9744852781295776, 'learning_rate': 0.0009171296296296297, 'epoch': 2.45}
Step 8830: {'loss': 0.8828, 'grad_norm': 0.5567485094070435, 'learning_rate': 0.0009125, 'epoch': 2.452777777777778}
Step 8840: {'loss': 1.0167, 'grad_norm': 0.8845450282096863, 'learning_rate': 0.0009078703703703704, 'epoch': 2.4555555555555557}
Step 8850: {'loss': 0.8657, 'grad_norm': 0.6177909970283508, 'learning_rate': 0.0009032407407407407, 'epoch': 2.4583333333333335}
Step 8860: {'loss': 0.9105, 'grad_norm': 0.6872853636741638, 'learning_rate': 0.0008986111111111112, 'epoch': 2.4611111111111112}
Step 8870: {'loss': 0.9755, 'grad_norm': 1.163709282875061, 'learning_rate': 0.0008939814814814815, 'epoch': 2.463888888888889}
Step 8880: {'loss': 1.0223, 'grad_norm': 0.8232021331787109, 'learning_rate': 0.0008893518518518519, 'epoch': 2.466666666666667}
Step 8890: {'loss': 0.8377, 'grad_norm': 0.7414888143539429, 'learning_rate': 0.0008847222222222222, 'epoch': 2.4694444444444446}
Step 8900: {'loss': 0.9758, 'grad_norm': 0.542808473110199, 'learning_rate': 0.0008800925925925926, 'epoch': 2.4722222222222223}
Step 8910: {'loss': 0.8739, 'grad_norm': 0.6603986024856567, 'learning_rate': 0.0008754629629629631, 'epoch': 2.475}
Step 8920: {'loss': 0.9573, 'grad_norm': 0.5485488772392273, 'learning_rate': 0.0008708333333333334, 'epoch': 2.477777777777778}
Step 8930: {'loss': 1.0335, 'grad_norm': 0.9265896677970886, 'learning_rate': 0.0008662037037037038, 'epoch': 2.4805555555555556}
Step 8940: {'loss': 0.8559, 'grad_norm': 0.5633146166801453, 'learning_rate': 0.0008615740740740741, 'epoch': 2.4833333333333334}
Step 8950: {'loss': 0.9734, 'grad_norm': 0.7788000106811523, 'learning_rate': 0.0008569444444444445, 'epoch': 2.486111111111111}
Step 8960: {'loss': 0.9243, 'grad_norm': 0.6286765933036804, 'learning_rate': 0.0008523148148148148, 'epoch': 2.488888888888889}
Step 8970: {'loss': 0.9301, 'grad_norm': 0.842881977558136, 'learning_rate': 0.0008476851851851853, 'epoch': 2.4916666666666667}
Step 8980: {'loss': 0.8842, 'grad_norm': 0.39513570070266724, 'learning_rate': 0.0008430555555555556, 'epoch': 2.4944444444444445}
Step 8990: {'loss': 0.9124, 'grad_norm': 1.7395344972610474, 'learning_rate': 0.000838425925925926, 'epoch': 2.4972222222222222}
Step 9000: {'loss': 0.943, 'grad_norm': 0.5305346846580505, 'learning_rate': 0.0008337962962962962, 'epoch': 2.5}
Step 9010: {'loss': 0.8922, 'grad_norm': 1.0717594623565674, 'learning_rate': 0.0008291666666666666, 'epoch': 2.5027777777777778}
Step 9020: {'loss': 1.0025, 'grad_norm': 0.9874514937400818, 'learning_rate': 0.0008245370370370371, 'epoch': 2.5055555555555555}
Step 9030: {'loss': 0.9277, 'grad_norm': 0.561498761177063, 'learning_rate': 0.0008199074074074074, 'epoch': 2.5083333333333333}
Step 9040: {'loss': 0.931, 'grad_norm': 0.873704195022583, 'learning_rate': 0.0008152777777777778, 'epoch': 2.511111111111111}
Step 9050: {'loss': 0.9929, 'grad_norm': 0.51372230052948, 'learning_rate': 0.0008106481481481481, 'epoch': 2.513888888888889}
Step 9060: {'loss': 0.9467, 'grad_norm': 0.68571537733078, 'learning_rate': 0.0008060185185185185, 'epoch': 2.5166666666666666}
Step 9070: {'loss': 0.9535, 'grad_norm': 0.859540581703186, 'learning_rate': 0.0008013888888888888, 'epoch': 2.5194444444444444}
Step 9080: {'loss': 1.0433, 'grad_norm': 1.113764762878418, 'learning_rate': 0.0007967592592592593, 'epoch': 2.522222222222222}
Step 9090: {'loss': 1.016, 'grad_norm': 0.5345211029052734, 'learning_rate': 0.0007921296296296296, 'epoch': 2.525}
Step 9100: {'loss': 0.8718, 'grad_norm': 0.6409878730773926, 'learning_rate': 0.0007875, 'epoch': 2.5277777777777777}
Step 9110: {'loss': 1.0255, 'grad_norm': 1.3128167390823364, 'learning_rate': 0.0007828703703703704, 'epoch': 2.5305555555555554}
Step 9120: {'loss': 0.8968, 'grad_norm': 0.6889821887016296, 'learning_rate': 0.0007782407407407408, 'epoch': 2.533333333333333}
Step 9130: {'loss': 0.9583, 'grad_norm': 0.8202638030052185, 'learning_rate': 0.0007736111111111112, 'epoch': 2.536111111111111}
Step 9140: {'loss': 0.9569, 'grad_norm': 0.5118162035942078, 'learning_rate': 0.0007689814814814815, 'epoch': 2.5388888888888888}
Step 9150: {'loss': 0.984, 'grad_norm': 0.7692786455154419, 'learning_rate': 0.0007643518518518519, 'epoch': 2.5416666666666665}
Step 9160: {'loss': 1.0248, 'grad_norm': 1.23014497756958, 'learning_rate': 0.0007597222222222222, 'epoch': 2.5444444444444443}
Step 9170: {'loss': 0.8794, 'grad_norm': 0.5776211023330688, 'learning_rate': 0.0007550925925925927, 'epoch': 2.5472222222222225}
Step 9180: {'loss': 0.878, 'grad_norm': 1.3674057722091675, 'learning_rate': 0.000750462962962963, 'epoch': 2.55}
Step 9190: {'loss': 1.0297, 'grad_norm': 1.5377756357192993, 'learning_rate': 0.0007458333333333334, 'epoch': 2.552777777777778}
Step 9200: {'loss': 0.9461, 'grad_norm': 0.5562034249305725, 'learning_rate': 0.0007412037037037037, 'epoch': 2.5555555555555554}
Step 9210: {'loss': 0.9333, 'grad_norm': 0.6590320467948914, 'learning_rate': 0.0007365740740740741, 'epoch': 2.5583333333333336}
Step 9220: {'loss': 0.9337, 'grad_norm': 0.6081616282463074, 'learning_rate': 0.0007319444444444446, 'epoch': 2.561111111111111}
Step 9230: {'loss': 0.9017, 'grad_norm': 0.5570579767227173, 'learning_rate': 0.0007273148148148149, 'epoch': 2.563888888888889}
Step 9240: {'loss': 0.9843, 'grad_norm': 0.6012181043624878, 'learning_rate': 0.0007226851851851853, 'epoch': 2.5666666666666664}
Step 9250: {'loss': 0.9205, 'grad_norm': 0.9474429488182068, 'learning_rate': 0.0007180555555555555, 'epoch': 2.5694444444444446}
Step 9260: {'loss': 0.8834, 'grad_norm': 1.43986177444458, 'learning_rate': 0.000713425925925926, 'epoch': 2.572222222222222}
Step 9270: {'loss': 0.9013, 'grad_norm': 0.851593017578125, 'learning_rate': 0.0007087962962962962, 'epoch': 2.575}
Step 9280: {'loss': 0.9509, 'grad_norm': 0.6465505361557007, 'learning_rate': 0.0007041666666666667, 'epoch': 2.5777777777777775}
Step 9290: {'loss': 0.9969, 'grad_norm': 1.9037026166915894, 'learning_rate': 0.000699537037037037, 'epoch': 2.5805555555555557}
Step 9300: {'loss': 0.8959, 'grad_norm': 0.5133073329925537, 'learning_rate': 0.0006949074074074074, 'epoch': 2.5833333333333335}
Step 9310: {'loss': 0.9542, 'grad_norm': 0.6616167426109314, 'learning_rate': 0.0006902777777777777, 'epoch': 2.5861111111111112}
Step 9320: {'loss': 0.938, 'grad_norm': 0.7963698506355286, 'learning_rate': 0.0006856481481481481, 'epoch': 2.588888888888889}
Step 9330: {'loss': 0.9401, 'grad_norm': 1.007087230682373, 'learning_rate': 0.0006810185185185186, 'epoch': 2.591666666666667}
Step 9340: {'loss': 0.9047, 'grad_norm': 0.5566304922103882, 'learning_rate': 0.0006763888888888889, 'epoch': 2.5944444444444446}
Step 9350: {'loss': 0.8963, 'grad_norm': 0.39681124687194824, 'learning_rate': 0.0006717592592592593, 'epoch': 2.5972222222222223}
Step 9360: {'loss': 0.9945, 'grad_norm': 0.7645761966705322, 'learning_rate': 0.0006671296296296296, 'epoch': 2.6}
Step 9370: {'loss': 0.8473, 'grad_norm': 0.7154533267021179, 'learning_rate': 0.0006625, 'epoch': 2.602777777777778}
Step 9380: {'loss': 0.8899, 'grad_norm': 2.280871629714966, 'learning_rate': 0.0006578703703703703, 'epoch': 2.6055555555555556}
Step 9390: {'loss': 0.9644, 'grad_norm': 0.7628123164176941, 'learning_rate': 0.0006532407407407408, 'epoch': 2.6083333333333334}
Step 9400: {'loss': 0.9231, 'grad_norm': 1.3508092164993286, 'learning_rate': 0.0006486111111111111, 'epoch': 2.611111111111111}
Step 9410: {'loss': 0.9153, 'grad_norm': 0.8090054392814636, 'learning_rate': 0.0006439814814814815, 'epoch': 2.613888888888889}
Step 9420: {'loss': 0.8958, 'grad_norm': 0.5763955116271973, 'learning_rate': 0.0006393518518518519, 'epoch': 2.6166666666666667}
Step 9430: {'loss': 0.914, 'grad_norm': 0.889707088470459, 'learning_rate': 0.0006347222222222222, 'epoch': 2.6194444444444445}
Step 9440: {'loss': 1.0347, 'grad_norm': 0.9596237540245056, 'learning_rate': 0.0006300925925925927, 'epoch': 2.6222222222222222}
Step 9450: {'loss': 0.9263, 'grad_norm': 1.0004359483718872, 'learning_rate': 0.000625462962962963, 'epoch': 2.625}
Step 9460: {'loss': 0.8176, 'grad_norm': 0.5272349715232849, 'learning_rate': 0.0006208333333333334, 'epoch': 2.6277777777777778}
Step 9470: {'loss': 1.0233, 'grad_norm': 0.6384802460670471, 'learning_rate': 0.0006162037037037037, 'epoch': 2.6305555555555555}
Step 9480: {'loss': 0.9339, 'grad_norm': 0.686817467212677, 'learning_rate': 0.0006115740740740742, 'epoch': 2.6333333333333333}
Step 9490: {'loss': 0.9717, 'grad_norm': 0.9982484579086304, 'learning_rate': 0.0006069444444444445, 'epoch': 2.636111111111111}
Step 9500: {'loss': 0.8521, 'grad_norm': 0.7136179804801941, 'learning_rate': 0.0006023148148148149, 'epoch': 2.638888888888889}
Step 9510: {'loss': 0.8873, 'grad_norm': 0.7730231881141663, 'learning_rate': 0.0005976851851851853, 'epoch': 2.6416666666666666}
Step 9520: {'loss': 0.8281, 'grad_norm': 0.49409425258636475, 'learning_rate': 0.0005930555555555555, 'epoch': 2.6444444444444444}
Step 9530: {'loss': 0.9002, 'grad_norm': 0.5176355838775635, 'learning_rate': 0.0005884259259259259, 'epoch': 2.647222222222222}
Step 9540: {'loss': 0.8777, 'grad_norm': 0.5421333312988281, 'learning_rate': 0.0005837962962962963, 'epoch': 2.65}
Step 9550: {'loss': 0.957, 'grad_norm': 0.9960598349571228, 'learning_rate': 0.0005791666666666666, 'epoch': 2.6527777777777777}
Step 9560: {'loss': 0.921, 'grad_norm': 0.9745821356773376, 'learning_rate': 0.000574537037037037, 'epoch': 2.6555555555555554}
Step 9570: {'loss': 0.9162, 'grad_norm': 0.5117774605751038, 'learning_rate': 0.0005699074074074074, 'epoch': 2.658333333333333}
Step 9580: {'loss': 0.9237, 'grad_norm': 0.5526995062828064, 'learning_rate': 0.0005652777777777778, 'epoch': 2.661111111111111}
Step 9590: {'loss': 0.9347, 'grad_norm': 0.8882191777229309, 'learning_rate': 0.0005606481481481482, 'epoch': 2.6638888888888888}
Step 9600: {'loss': 0.8697, 'grad_norm': 0.5033531188964844, 'learning_rate': 0.0005560185185185185, 'epoch': 2.6666666666666665}
Step 9610: {'loss': 0.9352, 'grad_norm': 1.1508251428604126, 'learning_rate': 0.0005513888888888889, 'epoch': 2.6694444444444443}
Step 9620: {'loss': 0.8427, 'grad_norm': 1.131776213645935, 'learning_rate': 0.0005467592592592593, 'epoch': 2.6722222222222225}
Step 9630: {'loss': 0.8323, 'grad_norm': 2.3436808586120605, 'learning_rate': 0.0005421296296296296, 'epoch': 2.675}
Step 9640: {'loss': 0.8206, 'grad_norm': 0.41765695810317993, 'learning_rate': 0.0005375, 'epoch': 2.677777777777778}
Step 9650: {'loss': 0.9468, 'grad_norm': 0.8120386600494385, 'learning_rate': 0.0005328703703703704, 'epoch': 2.6805555555555554}
Step 9660: {'loss': 0.9941, 'grad_norm': 0.7143490314483643, 'learning_rate': 0.0005282407407407407, 'epoch': 2.6833333333333336}
Step 9670: {'loss': 0.9315, 'grad_norm': 0.8728189468383789, 'learning_rate': 0.0005236111111111111, 'epoch': 2.686111111111111}
Step 9680: {'loss': 0.9287, 'grad_norm': 0.6121143698692322, 'learning_rate': 0.0005189814814814815, 'epoch': 2.688888888888889}
Step 9690: {'loss': 0.9171, 'grad_norm': 0.5992828607559204, 'learning_rate': 0.0005143518518518519, 'epoch': 2.6916666666666664}
Step 9700: {'loss': 0.8518, 'grad_norm': 1.3648347854614258, 'learning_rate': 0.0005097222222222223, 'epoch': 2.6944444444444446}
Step 9710: {'loss': 0.8895, 'grad_norm': 0.6534591317176819, 'learning_rate': 0.0005050925925925926, 'epoch': 2.697222222222222}
Step 9720: {'loss': 0.8278, 'grad_norm': 0.7648639678955078, 'learning_rate': 0.000500462962962963, 'epoch': 2.7}
Step 9730: {'loss': 0.9235, 'grad_norm': 1.0015106201171875, 'learning_rate': 0.0004958333333333334, 'epoch': 2.7027777777777775}
Step 9740: {'loss': 0.8495, 'grad_norm': 0.4487658143043518, 'learning_rate': 0.0004912037037037037, 'epoch': 2.7055555555555557}
Step 9750: {'loss': 0.9255, 'grad_norm': 0.49773135781288147, 'learning_rate': 0.00048657407407407406, 'epoch': 2.7083333333333335}
Step 9760: {'loss': 0.8987, 'grad_norm': 0.6145972609519958, 'learning_rate': 0.00048194444444444446, 'epoch': 2.7111111111111112}
Step 9770: {'loss': 0.8876, 'grad_norm': 0.5247430801391602, 'learning_rate': 0.0004773148148148148, 'epoch': 2.713888888888889}
Step 9780: {'loss': 0.9207, 'grad_norm': 0.4467821717262268, 'learning_rate': 0.00047268518518518514, 'epoch': 2.716666666666667}
Step 9790: {'loss': 0.9307, 'grad_norm': 0.8037809133529663, 'learning_rate': 0.0004680555555555556, 'epoch': 2.7194444444444446}
Step 9800: {'loss': 0.8774, 'grad_norm': 1.1504088640213013, 'learning_rate': 0.00046342592592592594, 'epoch': 2.7222222222222223}
Step 9810: {'loss': 0.8999, 'grad_norm': 1.2429569959640503, 'learning_rate': 0.00045879629629629634, 'epoch': 2.725}
Step 9820: {'loss': 0.9448, 'grad_norm': 0.8576600551605225, 'learning_rate': 0.0004541666666666667, 'epoch': 2.727777777777778}
Step 9830: {'loss': 0.9099, 'grad_norm': 0.6078864932060242, 'learning_rate': 0.00044953703703703703, 'epoch': 2.7305555555555556}
Step 9840: {'loss': 0.9397, 'grad_norm': 0.5684414505958557, 'learning_rate': 0.0004449074074074074, 'epoch': 2.7333333333333334}
Step 9850: {'loss': 0.9661, 'grad_norm': 0.6999301314353943, 'learning_rate': 0.00044027777777777777, 'epoch': 2.736111111111111}
Step 9860: {'loss': 1.0251, 'grad_norm': 1.2588287591934204, 'learning_rate': 0.0004356481481481481, 'epoch': 2.738888888888889}
Step 9870: {'loss': 0.979, 'grad_norm': 0.6832688450813293, 'learning_rate': 0.0004310185185185185, 'epoch': 2.7416666666666667}
Step 9880: {'loss': 0.8897, 'grad_norm': 0.7393259406089783, 'learning_rate': 0.00042638888888888886, 'epoch': 2.7444444444444445}
Step 9890: {'loss': 0.9217, 'grad_norm': 1.0836437940597534, 'learning_rate': 0.0004217592592592593, 'epoch': 2.7472222222222222}
Step 9900: {'loss': 0.9647, 'grad_norm': 0.4723881483078003, 'learning_rate': 0.00041712962962962965, 'epoch': 2.75}
Step 9910: {'loss': 0.9457, 'grad_norm': 0.7156240940093994, 'learning_rate': 0.00041250000000000005, 'epoch': 2.7527777777777778}
Step 9920: {'loss': 0.889, 'grad_norm': 0.722499430179596, 'learning_rate': 0.0004078703703703704, 'epoch': 2.7555555555555555}
Step 9930: {'loss': 0.9332, 'grad_norm': 0.6667553782463074, 'learning_rate': 0.00040324074074074074, 'epoch': 2.7583333333333333}
Step 9940: {'loss': 0.8758, 'grad_norm': 0.7207989692687988, 'learning_rate': 0.00039861111111111114, 'epoch': 2.761111111111111}
Step 9950: {'loss': 0.9664, 'grad_norm': 1.078978180885315, 'learning_rate': 0.0003939814814814815, 'epoch': 2.763888888888889}
Step 9960: {'loss': 0.8407, 'grad_norm': 0.7444723844528198, 'learning_rate': 0.0003893518518518518, 'epoch': 2.7666666666666666}
Step 9970: {'loss': 0.851, 'grad_norm': 0.8330644965171814, 'learning_rate': 0.0003847222222222222, 'epoch': 2.7694444444444444}
Step 9980: {'loss': 0.9688, 'grad_norm': 0.712227463722229, 'learning_rate': 0.00038009259259259257, 'epoch': 2.772222222222222}
Step 9990: {'loss': 0.9704, 'grad_norm': 0.7671095132827759, 'learning_rate': 0.0003754629629629629, 'epoch': 2.775}
Step 10000: {'loss': 0.9093, 'grad_norm': 1.2712856531143188, 'learning_rate': 0.00037083333333333337, 'epoch': 2.7777777777777777}
Step 10010: {'loss': 0.8929, 'grad_norm': 0.7052863240242004, 'learning_rate': 0.0003662037037037037, 'epoch': 2.7805555555555554}
Step 10020: {'loss': 0.9466, 'grad_norm': 0.5891629457473755, 'learning_rate': 0.0003615740740740741, 'epoch': 2.783333333333333}
Step 10030: {'loss': 0.82, 'grad_norm': 0.4742896258831024, 'learning_rate': 0.00035694444444444445, 'epoch': 2.786111111111111}
Step 10040: {'loss': 0.9526, 'grad_norm': 0.797350287437439, 'learning_rate': 0.0003523148148148148, 'epoch': 2.7888888888888888}
Step 10050: {'loss': 0.9565, 'grad_norm': 0.44558167457580566, 'learning_rate': 0.0003476851851851852, 'epoch': 2.7916666666666665}
Step 10060: {'loss': 0.888, 'grad_norm': 1.250339150428772, 'learning_rate': 0.00034305555555555554, 'epoch': 2.7944444444444443}
Step 10070: {'loss': 1.019, 'grad_norm': 0.707798957824707, 'learning_rate': 0.00033842592592592594, 'epoch': 2.7972222222222225}
Step 10080: {'loss': 0.9277, 'grad_norm': 0.8258206844329834, 'learning_rate': 0.0003337962962962963, 'epoch': 2.8}
Step 10090: {'loss': 0.9672, 'grad_norm': 0.629921555519104, 'learning_rate': 0.0003291666666666666, 'epoch': 2.802777777777778}
Step 10100: {'loss': 0.9206, 'grad_norm': 0.5945802927017212, 'learning_rate': 0.0003245370370370371, 'epoch': 2.8055555555555554}
Step 10110: {'loss': 0.8846, 'grad_norm': 0.5919332504272461, 'learning_rate': 0.0003199074074074074, 'epoch': 2.8083333333333336}
Step 10120: {'loss': 0.8929, 'grad_norm': 0.6811215281486511, 'learning_rate': 0.0003152777777777778, 'epoch': 2.811111111111111}
Step 10130: {'loss': 0.8645, 'grad_norm': 0.6783750653266907, 'learning_rate': 0.00031064814814814817, 'epoch': 2.813888888888889}
Step 10140: {'loss': 0.8675, 'grad_norm': 0.7708749175071716, 'learning_rate': 0.0003060185185185185, 'epoch': 2.8166666666666664}
Step 10150: {'loss': 0.9572, 'grad_norm': 0.7304461002349854, 'learning_rate': 0.0003013888888888889, 'epoch': 2.8194444444444446}
Step 10160: {'loss': 0.9748, 'grad_norm': 0.9817748665809631, 'learning_rate': 0.00029675925925925925, 'epoch': 2.822222222222222}
Step 10170: {'loss': 0.9276, 'grad_norm': 0.5222152471542358, 'learning_rate': 0.0002921296296296296, 'epoch': 2.825}
Step 10180: {'loss': 0.9596, 'grad_norm': 0.5923540592193604, 'learning_rate': 0.0002875, 'epoch': 2.8277777777777775}
Step 10190: {'loss': 0.8817, 'grad_norm': 0.6436771154403687, 'learning_rate': 0.0002828703703703704, 'epoch': 2.8305555555555557}
Step 10200: {'loss': 0.9599, 'grad_norm': 1.1625547409057617, 'learning_rate': 0.00027824074074074074, 'epoch': 2.8333333333333335}
Step 10210: {'loss': 0.9333, 'grad_norm': 0.6845406889915466, 'learning_rate': 0.00027361111111111114, 'epoch': 2.8361111111111112}
Step 10220: {'loss': 0.9593, 'grad_norm': 1.6514822244644165, 'learning_rate': 0.0002689814814814815, 'epoch': 2.838888888888889}
Step 10230: {'loss': 0.9204, 'grad_norm': 1.0471619367599487, 'learning_rate': 0.0002643518518518519, 'epoch': 2.841666666666667}
Step 10240: {'loss': 0.9325, 'grad_norm': 1.5021382570266724, 'learning_rate': 0.0002597222222222222, 'epoch': 2.8444444444444446}
Step 10250: {'loss': 0.849, 'grad_norm': 1.1041449308395386, 'learning_rate': 0.0002550925925925926, 'epoch': 2.8472222222222223}
Step 10260: {'loss': 0.8123, 'grad_norm': 0.45929330587387085, 'learning_rate': 0.00025046296296296297, 'epoch': 2.85}
Step 10270: {'loss': 0.863, 'grad_norm': 1.0131924152374268, 'learning_rate': 0.0002458333333333333, 'epoch': 2.852777777777778}
Step 10280: {'loss': 0.9678, 'grad_norm': 0.6597927212715149, 'learning_rate': 0.00024120370370370374, 'epoch': 2.8555555555555556}
Step 10290: {'loss': 0.9035, 'grad_norm': 0.716498076915741, 'learning_rate': 0.00023657407407407408, 'epoch': 2.8583333333333334}
Step 10300: {'loss': 0.9095, 'grad_norm': 2.0343823432922363, 'learning_rate': 0.00023194444444444445, 'epoch': 2.861111111111111}
Step 10310: {'loss': 0.9064, 'grad_norm': 0.5908021926879883, 'learning_rate': 0.00022731481481481482, 'epoch': 2.863888888888889}
Step 10320: {'loss': 0.9992, 'grad_norm': 0.6577057242393494, 'learning_rate': 0.00022268518518518517, 'epoch': 2.8666666666666667}
Step 10330: {'loss': 0.8527, 'grad_norm': 0.5715024471282959, 'learning_rate': 0.00021805555555555556, 'epoch': 2.8694444444444445}
Step 10340: {'loss': 0.9376, 'grad_norm': 1.4466328620910645, 'learning_rate': 0.00021342592592592594, 'epoch': 2.8722222222222222}
Step 10350: {'loss': 0.9667, 'grad_norm': 1.292486548423767, 'learning_rate': 0.0002087962962962963, 'epoch': 2.875}
Step 10360: {'loss': 0.9014, 'grad_norm': 0.549673318862915, 'learning_rate': 0.00020416666666666668, 'epoch': 2.8777777777777778}
Step 10370: {'loss': 0.9643, 'grad_norm': 0.5452578663825989, 'learning_rate': 0.00019953703703703702, 'epoch': 2.8805555555555555}
Step 10380: {'loss': 1.0791, 'grad_norm': 0.7886373996734619, 'learning_rate': 0.0001949074074074074, 'epoch': 2.8833333333333333}
Step 10390: {'loss': 0.8469, 'grad_norm': 0.8006559014320374, 'learning_rate': 0.0001902777777777778, 'epoch': 2.886111111111111}
Step 10400: {'loss': 0.947, 'grad_norm': 0.5980682373046875, 'learning_rate': 0.00018564814814814816, 'epoch': 2.888888888888889}
Step 10410: {'loss': 0.955, 'grad_norm': 1.6099600791931152, 'learning_rate': 0.0001810185185185185, 'epoch': 2.8916666666666666}
Step 10420: {'loss': 0.9808, 'grad_norm': 1.010090708732605, 'learning_rate': 0.00017638888888888888, 'epoch': 2.8944444444444444}
Step 10430: {'loss': 0.8626, 'grad_norm': 0.40453216433525085, 'learning_rate': 0.00017175925925925925, 'epoch': 2.897222222222222}
Step 10440: {'loss': 0.8864, 'grad_norm': 0.7360662221908569, 'learning_rate': 0.00016712962962962965, 'epoch': 2.9}
Step 10450: {'loss': 0.9463, 'grad_norm': 0.7684194445610046, 'learning_rate': 0.00016250000000000002, 'epoch': 2.9027777777777777}
Step 10460: {'loss': 0.9232, 'grad_norm': 0.7103614807128906, 'learning_rate': 0.00015787037037037036, 'epoch': 2.9055555555555554}
Step 10470: {'loss': 0.9804, 'grad_norm': 0.7525805234909058, 'learning_rate': 0.00015324074074074074, 'epoch': 2.908333333333333}
Step 10480: {'loss': 0.9504, 'grad_norm': 0.6721153855323792, 'learning_rate': 0.0001486111111111111, 'epoch': 2.911111111111111}
Step 10490: {'loss': 0.8837, 'grad_norm': 1.1267218589782715, 'learning_rate': 0.00014398148148148148, 'epoch': 2.9138888888888888}
Step 10500: {'loss': 0.9246, 'grad_norm': 0.8931413292884827, 'learning_rate': 0.00013935185185185185, 'epoch': 2.9166666666666665}
Step 10510: {'loss': 0.9107, 'grad_norm': 0.47443515062332153, 'learning_rate': 0.00013472222222222222, 'epoch': 2.9194444444444443}
Step 10520: {'loss': 0.9804, 'grad_norm': 0.9467645287513733, 'learning_rate': 0.0001300925925925926, 'epoch': 2.9222222222222225}
Step 10530: {'loss': 0.9646, 'grad_norm': 0.7446088790893555, 'learning_rate': 0.00012546296296296296, 'epoch': 2.925}
Step 10540: {'loss': 0.9172, 'grad_norm': 0.8979041576385498, 'learning_rate': 0.00012083333333333333, 'epoch': 2.927777777777778}
Step 10550: {'loss': 0.9121, 'grad_norm': 0.9180251955986023, 'learning_rate': 0.00011620370370370372, 'epoch': 2.9305555555555554}
Step 10560: {'loss': 0.8949, 'grad_norm': 0.48940303921699524, 'learning_rate': 0.00011157407407407408, 'epoch': 2.9333333333333336}
Step 10570: {'loss': 0.9028, 'grad_norm': 0.6325705051422119, 'learning_rate': 0.00010694444444444445, 'epoch': 2.936111111111111}
Step 10580: {'loss': 0.9202, 'grad_norm': 0.7709694504737854, 'learning_rate': 0.00010231481481481482, 'epoch': 2.938888888888889}
Step 10590: {'loss': 0.9803, 'grad_norm': 1.1507058143615723, 'learning_rate': 9.768518518518519e-05, 'epoch': 2.9416666666666664}
Step 10600: {'loss': 0.8686, 'grad_norm': 1.4178067445755005, 'learning_rate': 9.305555555555555e-05, 'epoch': 2.9444444444444446}
Step 10610: {'loss': 0.9045, 'grad_norm': 0.43223902583122253, 'learning_rate': 8.842592592592593e-05, 'epoch': 2.947222222222222}
Step 10620: {'loss': 0.977, 'grad_norm': 0.7159298062324524, 'learning_rate': 8.379629629629629e-05, 'epoch': 2.95}
Step 10630: {'loss': 0.9688, 'grad_norm': 0.8184689283370972, 'learning_rate': 7.916666666666668e-05, 'epoch': 2.9527777777777775}
Step 10640: {'loss': 0.9157, 'grad_norm': 0.4853339195251465, 'learning_rate': 7.453703703703703e-05, 'epoch': 2.9555555555555557}
Step 10650: {'loss': 1.0433, 'grad_norm': 0.5640790462493896, 'learning_rate': 6.99074074074074e-05, 'epoch': 2.9583333333333335}
Step 10660: {'loss': 0.9671, 'grad_norm': 1.0023066997528076, 'learning_rate': 6.527777777777779e-05, 'epoch': 2.9611111111111112}
Step 10670: {'loss': 0.9584, 'grad_norm': 0.9360733032226562, 'learning_rate': 6.064814814814815e-05, 'epoch': 2.963888888888889}
Step 10680: {'loss': 0.8607, 'grad_norm': 0.39897486567497253, 'learning_rate': 5.601851851851852e-05, 'epoch': 2.966666666666667}
Step 10690: {'loss': 0.901, 'grad_norm': 0.5315811634063721, 'learning_rate': 5.138888888888889e-05, 'epoch': 2.9694444444444446}
Step 10700: {'loss': 0.8673, 'grad_norm': 0.8828896880149841, 'learning_rate': 4.675925925925926e-05, 'epoch': 2.9722222222222223}
Step 10710: {'loss': 0.9249, 'grad_norm': 0.8554227352142334, 'learning_rate': 4.2129629629629625e-05, 'epoch': 2.975}
Step 10720: {'loss': 0.887, 'grad_norm': 0.45907506346702576, 'learning_rate': 3.75e-05, 'epoch': 2.977777777777778}
Step 10730: {'loss': 0.8944, 'grad_norm': 2.4806125164031982, 'learning_rate': 3.2870370370370375e-05, 'epoch': 2.9805555555555556}
Step 10740: {'loss': 0.8996, 'grad_norm': 1.0302599668502808, 'learning_rate': 2.824074074074074e-05, 'epoch': 2.9833333333333334}
Step 10750: {'loss': 0.7954, 'grad_norm': 0.45941126346588135, 'learning_rate': 2.361111111111111e-05, 'epoch': 2.986111111111111}
Step 10760: {'loss': 0.9146, 'grad_norm': 0.7942758798599243, 'learning_rate': 1.8981481481481482e-05, 'epoch': 2.988888888888889}
Step 10770: {'loss': 0.9227, 'grad_norm': 2.0746631622314453, 'learning_rate': 1.4351851851851851e-05, 'epoch': 2.9916666666666667}
Step 10780: {'loss': 0.946, 'grad_norm': 1.1436889171600342, 'learning_rate': 9.722222222222223e-06, 'epoch': 2.9944444444444445}
Step 10790: {'loss': 0.9417, 'grad_norm': 0.8168777227401733, 'learning_rate': 5.092592592592592e-06, 'epoch': 2.9972222222222222}
Step 10800: {'loss': 0.8991, 'grad_norm': 1.210687279701233, 'learning_rate': 4.6296296296296297e-07, 'epoch': 3.0}
Step 10800: {'train_runtime': 1485.0659, 'train_samples_per_second': 14.545, 'train_steps_per_second': 7.272, 'total_flos': 2.29842012970752e+17, 'train_loss': 0.964799658898954, 'epoch': 3.0}
