Step 10: {'loss': 1.356, 'grad_norm': 3.185053825378418, 'learning_rate': 0.0009983333333333333, 'epoch': 0.005555555555555556}
Step 20: {'loss': 0.85, 'grad_norm': 1.277943730354309, 'learning_rate': 0.0009964814814814814, 'epoch': 0.011111111111111112}
Step 30: {'loss': 0.8062, 'grad_norm': 1.1753236055374146, 'learning_rate': 0.0009946296296296296, 'epoch': 0.016666666666666666}
Step 40: {'loss': 0.7397, 'grad_norm': 2.28709077835083, 'learning_rate': 0.0009927777777777778, 'epoch': 0.022222222222222223}
Step 50: {'loss': 0.7196, 'grad_norm': 1.0831834077835083, 'learning_rate': 0.000990925925925926, 'epoch': 0.027777777777777776}
Step 60: {'loss': 0.713, 'grad_norm': 1.2539734840393066, 'learning_rate': 0.000989074074074074, 'epoch': 0.03333333333333333}
Step 70: {'loss': 0.7529, 'grad_norm': 1.0073262453079224, 'learning_rate': 0.0009872222222222222, 'epoch': 0.03888888888888889}
Step 80: {'loss': 0.6903, 'grad_norm': 0.8306481242179871, 'learning_rate': 0.0009853703703703704, 'epoch': 0.044444444444444446}
Step 90: {'loss': 0.6819, 'grad_norm': 2.0302045345306396, 'learning_rate': 0.0009835185185185186, 'epoch': 0.05}
Step 100: {'loss': 0.713, 'grad_norm': 1.1831353902816772, 'learning_rate': 0.0009816666666666667, 'epoch': 0.05555555555555555}
Step 110: {'loss': 0.6622, 'grad_norm': 1.415548324584961, 'learning_rate': 0.0009798148148148149, 'epoch': 0.06111111111111111}
Step 120: {'loss': 0.6828, 'grad_norm': 1.0919013023376465, 'learning_rate': 0.000977962962962963, 'epoch': 0.06666666666666667}
Step 130: {'loss': 0.682, 'grad_norm': 1.196897029876709, 'learning_rate': 0.0009761111111111112, 'epoch': 0.07222222222222222}
Step 140: {'loss': 0.694, 'grad_norm': 1.4931716918945312, 'learning_rate': 0.0009742592592592592, 'epoch': 0.07777777777777778}
Step 150: {'loss': 0.705, 'grad_norm': 1.0255733728408813, 'learning_rate': 0.0009724074074074074, 'epoch': 0.08333333333333333}
Step 160: {'loss': 0.7241, 'grad_norm': 1.1536589860916138, 'learning_rate': 0.0009705555555555556, 'epoch': 0.08888888888888889}
Step 170: {'loss': 0.662, 'grad_norm': 1.0320320129394531, 'learning_rate': 0.0009687037037037037, 'epoch': 0.09444444444444444}
Step 180: {'loss': 0.6726, 'grad_norm': 1.0230134725570679, 'learning_rate': 0.0009668518518518519, 'epoch': 0.1}
Step 190: {'loss': 0.6634, 'grad_norm': 0.9429985880851746, 'learning_rate': 0.000965, 'epoch': 0.10555555555555556}
Step 200: {'loss': 0.6712, 'grad_norm': 1.1831861734390259, 'learning_rate': 0.0009631481481481482, 'epoch': 0.1111111111111111}
Step 210: {'loss': 0.7112, 'grad_norm': 0.9971649646759033, 'learning_rate': 0.0009612962962962964, 'epoch': 0.11666666666666667}
Step 220: {'loss': 0.6732, 'grad_norm': 1.138715386390686, 'learning_rate': 0.0009594444444444444, 'epoch': 0.12222222222222222}
Step 230: {'loss': 0.6729, 'grad_norm': 0.7404812574386597, 'learning_rate': 0.0009575925925925926, 'epoch': 0.12777777777777777}
Step 240: {'loss': 0.6785, 'grad_norm': 0.8830514550209045, 'learning_rate': 0.0009557407407407408, 'epoch': 0.13333333333333333}
Step 250: {'loss': 0.6822, 'grad_norm': 0.9215706586837769, 'learning_rate': 0.0009538888888888889, 'epoch': 0.1388888888888889}
Step 260: {'loss': 0.7262, 'grad_norm': 1.0156248807907104, 'learning_rate': 0.000952037037037037, 'epoch': 0.14444444444444443}
Step 270: {'loss': 0.6685, 'grad_norm': 1.2357509136199951, 'learning_rate': 0.0009501851851851852, 'epoch': 0.15}
Step 280: {'loss': 0.6638, 'grad_norm': 0.8728676438331604, 'learning_rate': 0.0009483333333333334, 'epoch': 0.15555555555555556}
Step 290: {'loss': 0.6967, 'grad_norm': 1.1629161834716797, 'learning_rate': 0.0009464814814814815, 'epoch': 0.16111111111111112}
Step 300: {'loss': 0.66, 'grad_norm': 0.9275152683258057, 'learning_rate': 0.0009446296296296296, 'epoch': 0.16666666666666666}
Step 310: {'loss': 0.6585, 'grad_norm': 1.3778557777404785, 'learning_rate': 0.0009427777777777778, 'epoch': 0.17222222222222222}
Step 320: {'loss': 0.6525, 'grad_norm': 0.9163031578063965, 'learning_rate': 0.000940925925925926, 'epoch': 0.17777777777777778}
Step 330: {'loss': 0.6929, 'grad_norm': 0.9263889193534851, 'learning_rate': 0.000939074074074074, 'epoch': 0.18333333333333332}
Step 340: {'loss': 0.6703, 'grad_norm': 0.8061009049415588, 'learning_rate': 0.0009372222222222222, 'epoch': 0.18888888888888888}
Step 350: {'loss': 0.7176, 'grad_norm': 0.9197353720664978, 'learning_rate': 0.0009353703703703705, 'epoch': 0.19444444444444445}
Step 360: {'loss': 0.7087, 'grad_norm': 0.867313802242279, 'learning_rate': 0.0009335185185185185, 'epoch': 0.2}
Step 370: {'loss': 0.6728, 'grad_norm': 0.9477661848068237, 'learning_rate': 0.0009316666666666667, 'epoch': 0.20555555555555555}
Step 380: {'loss': 0.6563, 'grad_norm': 0.5710556507110596, 'learning_rate': 0.0009298148148148147, 'epoch': 0.2111111111111111}
Step 390: {'loss': 0.638, 'grad_norm': 0.7425103187561035, 'learning_rate': 0.000927962962962963, 'epoch': 0.21666666666666667}
Step 400: {'loss': 0.7063, 'grad_norm': 0.8302301168441772, 'learning_rate': 0.0009261111111111112, 'epoch': 0.2222222222222222}
Step 410: {'loss': 0.6735, 'grad_norm': 1.133362889289856, 'learning_rate': 0.0009242592592592592, 'epoch': 0.22777777777777777}
Step 420: {'loss': 0.6692, 'grad_norm': 0.7141993641853333, 'learning_rate': 0.0009224074074074075, 'epoch': 0.23333333333333334}
Step 430: {'loss': 0.677, 'grad_norm': 1.2031059265136719, 'learning_rate': 0.0009205555555555555, 'epoch': 0.2388888888888889}
Step 440: {'loss': 0.6861, 'grad_norm': 0.9995062351226807, 'learning_rate': 0.0009187037037037037, 'epoch': 0.24444444444444444}
Step 450: {'loss': 0.6812, 'grad_norm': 0.8586083650588989, 'learning_rate': 0.0009168518518518519, 'epoch': 0.25}
Step 460: {'loss': 0.6859, 'grad_norm': 0.7790531516075134, 'learning_rate': 0.000915, 'epoch': 0.25555555555555554}
Step 470: {'loss': 0.6741, 'grad_norm': 1.085947036743164, 'learning_rate': 0.0009131481481481482, 'epoch': 0.2611111111111111}
Step 480: {'loss': 0.6517, 'grad_norm': 0.9113694429397583, 'learning_rate': 0.0009112962962962963, 'epoch': 0.26666666666666666}
Step 490: {'loss': 0.6581, 'grad_norm': 0.7288089394569397, 'learning_rate': 0.0009094444444444445, 'epoch': 0.2722222222222222}
Step 500: {'loss': 0.6966, 'grad_norm': 1.1312068700790405, 'learning_rate': 0.0009075925925925927, 'epoch': 0.2777777777777778}
Step 510: {'loss': 0.6772, 'grad_norm': 0.9351852536201477, 'learning_rate': 0.0009057407407407407, 'epoch': 0.2833333333333333}
Step 520: {'loss': 0.6549, 'grad_norm': 1.200740933418274, 'learning_rate': 0.0009038888888888889, 'epoch': 0.28888888888888886}
Step 530: {'loss': 0.6479, 'grad_norm': 0.8651630878448486, 'learning_rate': 0.0009020370370370371, 'epoch': 0.29444444444444445}
Step 540: {'loss': 0.6684, 'grad_norm': 1.309034824371338, 'learning_rate': 0.0009001851851851852, 'epoch': 0.3}
Step 550: {'loss': 0.6799, 'grad_norm': 0.6774687170982361, 'learning_rate': 0.0008983333333333333, 'epoch': 0.3055555555555556}
Step 560: {'loss': 0.6784, 'grad_norm': 0.994087278842926, 'learning_rate': 0.0008964814814814815, 'epoch': 0.3111111111111111}
Step 570: {'loss': 0.7348, 'grad_norm': 1.0566246509552002, 'learning_rate': 0.0008946296296296297, 'epoch': 0.31666666666666665}
Step 580: {'loss': 0.6618, 'grad_norm': 0.8668630719184875, 'learning_rate': 0.0008927777777777778, 'epoch': 0.32222222222222224}
Step 590: {'loss': 0.6861, 'grad_norm': 0.8434789180755615, 'learning_rate': 0.0008909259259259259, 'epoch': 0.3277777777777778}
Step 600: {'loss': 0.6739, 'grad_norm': 1.0866622924804688, 'learning_rate': 0.0008890740740740741, 'epoch': 0.3333333333333333}
Step 610: {'loss': 0.6982, 'grad_norm': 1.015001893043518, 'learning_rate': 0.0008872222222222223, 'epoch': 0.3388888888888889}
Step 10: {'loss': 1.3376, 'grad_norm': 2.822497844696045, 'learning_rate': 0.0009983333333333333, 'epoch': 0.005555555555555556}
Step 10: {'loss': 1.357, 'grad_norm': 2.4480299949645996, 'learning_rate': 0.0009983333333333333, 'epoch': 0.005555555555555556}
Step 20: {'loss': 0.8874, 'grad_norm': 1.2470860481262207, 'learning_rate': 0.0009964814814814814, 'epoch': 0.011111111111111112}
Step 10: {'loss': 1.4089, 'grad_norm': 2.6997768878936768, 'learning_rate': 0.0009983333333333333, 'epoch': 0.005555555555555556}
Step 20: {'loss': 0.8779, 'grad_norm': 1.7843292951583862, 'learning_rate': 0.0009964814814814814, 'epoch': 0.011111111111111112}
Step 30: {'loss': 0.7708, 'grad_norm': 1.4854775667190552, 'learning_rate': 0.0009946296296296296, 'epoch': 0.016666666666666666}
Step 40: {'loss': 0.7813, 'grad_norm': 0.9539855122566223, 'learning_rate': 0.0009927777777777778, 'epoch': 0.022222222222222223}
Step 50: {'loss': 0.7476, 'grad_norm': 1.090434193611145, 'learning_rate': 0.000990925925925926, 'epoch': 0.027777777777777776}
Step 60: {'loss': 0.6898, 'grad_norm': 1.0880950689315796, 'learning_rate': 0.000989074074074074, 'epoch': 0.03333333333333333}
Step 70: {'loss': 0.7248, 'grad_norm': 1.0952657461166382, 'learning_rate': 0.0009872222222222222, 'epoch': 0.03888888888888889}
Step 80: {'loss': 0.7148, 'grad_norm': 1.2619463205337524, 'learning_rate': 0.0009853703703703704, 'epoch': 0.044444444444444446}
Step 90: {'loss': 0.6637, 'grad_norm': 1.2358366250991821, 'learning_rate': 0.0009835185185185186, 'epoch': 0.05}
Step 100: {'loss': 0.6646, 'grad_norm': 0.7348796129226685, 'learning_rate': 0.0009816666666666667, 'epoch': 0.05555555555555555}
Step 110: {'loss': 0.7245, 'grad_norm': 1.0670056343078613, 'learning_rate': 0.0009798148148148149, 'epoch': 0.06111111111111111}
Step 120: {'loss': 0.6747, 'grad_norm': 1.0326417684555054, 'learning_rate': 0.000977962962962963, 'epoch': 0.06666666666666667}
Step 130: {'loss': 0.6883, 'grad_norm': 0.9235756993293762, 'learning_rate': 0.0009761111111111112, 'epoch': 0.07222222222222222}
Step 140: {'loss': 0.6712, 'grad_norm': 1.2062336206436157, 'learning_rate': 0.0009742592592592592, 'epoch': 0.07777777777777778}
Step 10: {'loss': 1.4073, 'grad_norm': 4.161434650421143, 'learning_rate': 0.0009991666666666666, 'epoch': 0.002777777777777778}
Step 20: {'loss': 1.0372, 'grad_norm': 4.1691694259643555, 'learning_rate': 0.0009982407407407407, 'epoch': 0.005555555555555556}
Step 30: {'loss': 0.8497, 'grad_norm': 2.6872880458831787, 'learning_rate': 0.0009973148148148148, 'epoch': 0.008333333333333333}
Step 40: {'loss': 0.7546, 'grad_norm': 1.5284343957901, 'learning_rate': 0.0009963888888888889, 'epoch': 0.011111111111111112}
Step 50: {'loss': 0.7994, 'grad_norm': 1.3050525188446045, 'learning_rate': 0.000995462962962963, 'epoch': 0.013888888888888888}
Step 60: {'loss': 0.6898, 'grad_norm': 1.1602085828781128, 'learning_rate': 0.000994537037037037, 'epoch': 0.016666666666666666}
Step 70: {'loss': 0.7376, 'grad_norm': 1.785280704498291, 'learning_rate': 0.0009936111111111111, 'epoch': 0.019444444444444445}
Step 80: {'loss': 0.7588, 'grad_norm': 1.7140945196151733, 'learning_rate': 0.0009926851851851852, 'epoch': 0.022222222222222223}
Step 90: {'loss': 0.7673, 'grad_norm': 1.6380974054336548, 'learning_rate': 0.0009917592592592593, 'epoch': 0.025}
Step 100: {'loss': 0.722, 'grad_norm': 2.8929011821746826, 'learning_rate': 0.0009908333333333334, 'epoch': 0.027777777777777776}
Step 110: {'loss': 0.7396, 'grad_norm': 1.7134095430374146, 'learning_rate': 0.0009899074074074074, 'epoch': 0.030555555555555555}
Step 120: {'loss': 0.704, 'grad_norm': 1.7884654998779297, 'learning_rate': 0.0009889814814814815, 'epoch': 0.03333333333333333}
Step 130: {'loss': 0.6746, 'grad_norm': 2.120938539505005, 'learning_rate': 0.0009880555555555556, 'epoch': 0.03611111111111111}
Step 140: {'loss': 0.7482, 'grad_norm': 1.3807592391967773, 'learning_rate': 0.0009871296296296297, 'epoch': 0.03888888888888889}
Step 150: {'loss': 0.6882, 'grad_norm': 1.2629940509796143, 'learning_rate': 0.0009862037037037038, 'epoch': 0.041666666666666664}
Step 160: {'loss': 0.7347, 'grad_norm': 1.2286261320114136, 'learning_rate': 0.0009852777777777778, 'epoch': 0.044444444444444446}
Step 170: {'loss': 0.6853, 'grad_norm': 1.3066073656082153, 'learning_rate': 0.000984351851851852, 'epoch': 0.04722222222222222}
Step 180: {'loss': 0.7276, 'grad_norm': 1.5984536409378052, 'learning_rate': 0.000983425925925926, 'epoch': 0.05}
Step 190: {'loss': 0.7552, 'grad_norm': 1.1924264430999756, 'learning_rate': 0.0009825, 'epoch': 0.05277777777777778}
Step 200: {'loss': 0.673, 'grad_norm': 1.2112102508544922, 'learning_rate': 0.0009815740740740742, 'epoch': 0.05555555555555555}
Step 210: {'loss': 0.7385, 'grad_norm': 1.2637109756469727, 'learning_rate': 0.0009806481481481482, 'epoch': 0.058333333333333334}
Step 220: {'loss': 0.698, 'grad_norm': 1.8409024477005005, 'learning_rate': 0.0009797222222222223, 'epoch': 0.06111111111111111}
Step 230: {'loss': 0.7089, 'grad_norm': 1.0828349590301514, 'learning_rate': 0.0009787962962962964, 'epoch': 0.06388888888888888}
Step 240: {'loss': 0.6843, 'grad_norm': 1.4102728366851807, 'learning_rate': 0.0009778703703703705, 'epoch': 0.06666666666666667}
Step 250: {'loss': 0.743, 'grad_norm': 1.1404815912246704, 'learning_rate': 0.0009769444444444443, 'epoch': 0.06944444444444445}
Step 260: {'loss': 0.7023, 'grad_norm': 1.253859043121338, 'learning_rate': 0.0009760185185185185, 'epoch': 0.07222222222222222}
Step 270: {'loss': 0.6899, 'grad_norm': 1.552195429801941, 'learning_rate': 0.0009750925925925926, 'epoch': 0.075}
Step 280: {'loss': 0.68, 'grad_norm': 1.4440891742706299, 'learning_rate': 0.0009741666666666667, 'epoch': 0.07777777777777778}
Step 290: {'loss': 0.6881, 'grad_norm': 2.009761095046997, 'learning_rate': 0.0009732407407407408, 'epoch': 0.08055555555555556}
Step 300: {'loss': 0.7061, 'grad_norm': 0.9773219227790833, 'learning_rate': 0.0009723148148148149, 'epoch': 0.08333333333333333}
Step 310: {'loss': 0.6788, 'grad_norm': 1.2259061336517334, 'learning_rate': 0.0009713888888888889, 'epoch': 0.08611111111111111}
Step 320: {'loss': 0.7372, 'grad_norm': 1.1273984909057617, 'learning_rate': 0.000970462962962963, 'epoch': 0.08888888888888889}
Step 330: {'loss': 0.7117, 'grad_norm': 1.482029914855957, 'learning_rate': 0.0009695370370370371, 'epoch': 0.09166666666666666}
Step 340: {'loss': 0.697, 'grad_norm': 1.4158029556274414, 'learning_rate': 0.0009686111111111111, 'epoch': 0.09444444444444444}
Step 350: {'loss': 0.6782, 'grad_norm': 0.9189845323562622, 'learning_rate': 0.0009676851851851852, 'epoch': 0.09722222222222222}
Step 360: {'loss': 0.707, 'grad_norm': 1.3850871324539185, 'learning_rate': 0.0009667592592592592, 'epoch': 0.1}
Step 370: {'loss': 0.7764, 'grad_norm': 2.390376567840576, 'learning_rate': 0.0009658333333333333, 'epoch': 0.10277777777777777}
Step 380: {'loss': 0.7096, 'grad_norm': 1.164065957069397, 'learning_rate': 0.0009649074074074075, 'epoch': 0.10555555555555556}
Step 390: {'loss': 0.6579, 'grad_norm': 1.1659444570541382, 'learning_rate': 0.0009639814814814815, 'epoch': 0.10833333333333334}
Step 400: {'loss': 0.7201, 'grad_norm': 1.5875493288040161, 'learning_rate': 0.0009630555555555555, 'epoch': 0.1111111111111111}
Step 410: {'loss': 0.7381, 'grad_norm': 1.9155904054641724, 'learning_rate': 0.0009621296296296297, 'epoch': 0.11388888888888889}
Step 420: {'loss': 0.6724, 'grad_norm': 1.3973084688186646, 'learning_rate': 0.0009612037037037037, 'epoch': 0.11666666666666667}
Step 430: {'loss': 0.6452, 'grad_norm': 1.2688428163528442, 'learning_rate': 0.0009602777777777778, 'epoch': 0.11944444444444445}
Step 440: {'loss': 0.7007, 'grad_norm': 1.9746547937393188, 'learning_rate': 0.000959351851851852, 'epoch': 0.12222222222222222}
Step 450: {'loss': 0.6561, 'grad_norm': 1.1226242780685425, 'learning_rate': 0.0009584259259259259, 'epoch': 0.125}
Step 460: {'loss': 0.7356, 'grad_norm': 1.321235179901123, 'learning_rate': 0.0009575, 'epoch': 0.12777777777777777}
Step 470: {'loss': 0.6486, 'grad_norm': 1.729755163192749, 'learning_rate': 0.000956574074074074, 'epoch': 0.13055555555555556}
Step 480: {'loss': 0.7114, 'grad_norm': 2.250035285949707, 'learning_rate': 0.0009556481481481482, 'epoch': 0.13333333333333333}
Step 490: {'loss': 0.7427, 'grad_norm': 1.1959255933761597, 'learning_rate': 0.0009547222222222223, 'epoch': 0.1361111111111111}
Step 500: {'loss': 0.6959, 'grad_norm': 1.3210258483886719, 'learning_rate': 0.0009537962962962962, 'epoch': 0.1388888888888889}
Step 510: {'loss': 0.6712, 'grad_norm': 1.084916591644287, 'learning_rate': 0.0009528703703703704, 'epoch': 0.14166666666666666}
Step 520: {'loss': 0.6019, 'grad_norm': 1.2262592315673828, 'learning_rate': 0.0009519444444444445, 'epoch': 0.14444444444444443}
Step 530: {'loss': 0.6864, 'grad_norm': 1.093589425086975, 'learning_rate': 0.0009510185185185185, 'epoch': 0.14722222222222223}
Step 540: {'loss': 0.6656, 'grad_norm': 1.4658480882644653, 'learning_rate': 0.0009500925925925927, 'epoch': 0.15}
Step 550: {'loss': 0.6463, 'grad_norm': 1.8092066049575806, 'learning_rate': 0.0009491666666666667, 'epoch': 0.1527777777777778}
Step 560: {'loss': 0.6634, 'grad_norm': 1.9875233173370361, 'learning_rate': 0.0009482407407407407, 'epoch': 0.15555555555555556}
Step 570: {'loss': 0.7105, 'grad_norm': 1.0783908367156982, 'learning_rate': 0.0009473148148148149, 'epoch': 0.15833333333333333}
Step 580: {'loss': 0.7081, 'grad_norm': 0.8065458536148071, 'learning_rate': 0.0009463888888888889, 'epoch': 0.16111111111111112}
Step 590: {'loss': 0.7332, 'grad_norm': 1.9156330823898315, 'learning_rate': 0.0009454629629629629, 'epoch': 0.1638888888888889}
Step 600: {'loss': 0.6919, 'grad_norm': 1.1987768411636353, 'learning_rate': 0.0009445370370370371, 'epoch': 0.16666666666666666}
Step 610: {'loss': 0.6871, 'grad_norm': 1.0551477670669556, 'learning_rate': 0.0009436111111111111, 'epoch': 0.16944444444444445}
Step 620: {'loss': 0.6957, 'grad_norm': 1.9022592306137085, 'learning_rate': 0.0009426851851851852, 'epoch': 0.17222222222222222}
Step 630: {'loss': 0.6901, 'grad_norm': 0.8449017405509949, 'learning_rate': 0.0009417592592592593, 'epoch': 0.175}
Step 640: {'loss': 0.7351, 'grad_norm': 1.0366230010986328, 'learning_rate': 0.0009408333333333333, 'epoch': 0.17777777777777778}
Step 650: {'loss': 0.6521, 'grad_norm': 1.3548468351364136, 'learning_rate': 0.0009399074074074074, 'epoch': 0.18055555555555555}
Step 660: {'loss': 0.6835, 'grad_norm': 1.9005144834518433, 'learning_rate': 0.0009389814814814815, 'epoch': 0.18333333333333332}
Step 670: {'loss': 0.6581, 'grad_norm': 1.3519326448440552, 'learning_rate': 0.0009380555555555556, 'epoch': 0.18611111111111112}
Step 680: {'loss': 0.7228, 'grad_norm': 2.4752681255340576, 'learning_rate': 0.0009371296296296297, 'epoch': 0.18888888888888888}
Step 690: {'loss': 0.7161, 'grad_norm': 1.328296422958374, 'learning_rate': 0.0009362037037037036, 'epoch': 0.19166666666666668}
Step 700: {'loss': 0.705, 'grad_norm': 1.293474793434143, 'learning_rate': 0.0009352777777777778, 'epoch': 0.19444444444444445}
Step 710: {'loss': 0.6718, 'grad_norm': 1.6652467250823975, 'learning_rate': 0.0009343518518518519, 'epoch': 0.19722222222222222}
Step 720: {'loss': 0.6925, 'grad_norm': 1.0469077825546265, 'learning_rate': 0.0009334259259259259, 'epoch': 0.2}
Step 730: {'loss': 0.6801, 'grad_norm': 1.2246328592300415, 'learning_rate': 0.0009325000000000001, 'epoch': 0.20277777777777778}
Step 740: {'loss': 0.6354, 'grad_norm': 1.2542946338653564, 'learning_rate': 0.0009315740740740741, 'epoch': 0.20555555555555555}
Step 750: {'loss': 0.7035, 'grad_norm': 1.8066209554672241, 'learning_rate': 0.0009306481481481481, 'epoch': 0.20833333333333334}
Step 760: {'loss': 0.6795, 'grad_norm': 1.2005316019058228, 'learning_rate': 0.0009297222222222223, 'epoch': 0.2111111111111111}
Step 770: {'loss': 0.6127, 'grad_norm': 2.1024527549743652, 'learning_rate': 0.0009287962962962964, 'epoch': 0.21388888888888888}
Step 780: {'loss': 0.7103, 'grad_norm': 1.0587611198425293, 'learning_rate': 0.0009278703703703704, 'epoch': 0.21666666666666667}
Step 790: {'loss': 0.6896, 'grad_norm': 1.2755929231643677, 'learning_rate': 0.0009269444444444444, 'epoch': 0.21944444444444444}
Step 800: {'loss': 0.681, 'grad_norm': 1.0692640542984009, 'learning_rate': 0.0009260185185185185, 'epoch': 0.2222222222222222}
Step 810: {'loss': 0.7038, 'grad_norm': 1.3123148679733276, 'learning_rate': 0.0009250925925925926, 'epoch': 0.225}
Step 820: {'loss': 0.6678, 'grad_norm': 1.4039287567138672, 'learning_rate': 0.0009241666666666667, 'epoch': 0.22777777777777777}
Step 830: {'loss': 0.7294, 'grad_norm': 1.1909841299057007, 'learning_rate': 0.0009232407407407407, 'epoch': 0.23055555555555557}
Step 840: {'loss': 0.6156, 'grad_norm': 1.1167614459991455, 'learning_rate': 0.0009223148148148148, 'epoch': 0.23333333333333334}
Step 850: {'loss': 0.5952, 'grad_norm': 1.3547356128692627, 'learning_rate': 0.0009213888888888889, 'epoch': 0.2361111111111111}
Step 860: {'loss': 0.7207, 'grad_norm': 1.4473382234573364, 'learning_rate': 0.000920462962962963, 'epoch': 0.2388888888888889}
Step 870: {'loss': 0.6203, 'grad_norm': 1.026612639427185, 'learning_rate': 0.0009195370370370371, 'epoch': 0.24166666666666667}
Step 880: {'loss': 0.6694, 'grad_norm': 1.5074037313461304, 'learning_rate': 0.0009186111111111111, 'epoch': 0.24444444444444444}
Step 890: {'loss': 0.6804, 'grad_norm': 1.0580817461013794, 'learning_rate': 0.0009176851851851852, 'epoch': 0.24722222222222223}
Step 900: {'loss': 0.692, 'grad_norm': 1.1912462711334229, 'learning_rate': 0.0009167592592592593, 'epoch': 0.25}
Step 910: {'loss': 0.6991, 'grad_norm': 1.5152015686035156, 'learning_rate': 0.0009158333333333334, 'epoch': 0.25277777777777777}
Step 920: {'loss': 0.6997, 'grad_norm': 1.2700574398040771, 'learning_rate': 0.0009149074074074074, 'epoch': 0.25555555555555554}
Step 930: {'loss': 0.673, 'grad_norm': 1.2712781429290771, 'learning_rate': 0.0009139814814814815, 'epoch': 0.25833333333333336}
Step 940: {'loss': 0.6969, 'grad_norm': 1.1450114250183105, 'learning_rate': 0.0009130555555555555, 'epoch': 0.2611111111111111}
Step 950: {'loss': 0.7196, 'grad_norm': 1.3177118301391602, 'learning_rate': 0.0009121296296296296, 'epoch': 0.2638888888888889}
Step 960: {'loss': 0.6634, 'grad_norm': 1.2087829113006592, 'learning_rate': 0.0009112037037037038, 'epoch': 0.26666666666666666}
Step 970: {'loss': 0.6551, 'grad_norm': 1.8108781576156616, 'learning_rate': 0.0009102777777777778, 'epoch': 0.26944444444444443}
Step 980: {'loss': 0.7148, 'grad_norm': 1.4762074947357178, 'learning_rate': 0.0009093518518518518, 'epoch': 0.2722222222222222}
Step 990: {'loss': 0.7169, 'grad_norm': 1.3952940702438354, 'learning_rate': 0.000908425925925926, 'epoch': 0.275}
Step 1000: {'loss': 0.6097, 'grad_norm': 1.5015876293182373, 'learning_rate': 0.0009075, 'epoch': 0.2777777777777778}
Step 1010: {'loss': 0.6867, 'grad_norm': 1.0130653381347656, 'learning_rate': 0.0009065740740740741, 'epoch': 0.28055555555555556}
Step 1020: {'loss': 0.6975, 'grad_norm': 1.0423089265823364, 'learning_rate': 0.0009056481481481483, 'epoch': 0.2833333333333333}
Step 1030: {'loss': 0.6824, 'grad_norm': 1.4952812194824219, 'learning_rate': 0.0009047222222222222, 'epoch': 0.2861111111111111}
Step 1040: {'loss': 0.6581, 'grad_norm': 0.9349676370620728, 'learning_rate': 0.0009037962962962963, 'epoch': 0.28888888888888886}
Step 1050: {'loss': 0.7136, 'grad_norm': 1.4309362173080444, 'learning_rate': 0.0009028703703703704, 'epoch': 0.2916666666666667}
Step 1060: {'loss': 0.7034, 'grad_norm': 1.5826987028121948, 'learning_rate': 0.0009019444444444445, 'epoch': 0.29444444444444445}
Step 1070: {'loss': 0.6899, 'grad_norm': 1.0225508213043213, 'learning_rate': 0.0009010185185185186, 'epoch': 0.2972222222222222}
Step 1080: {'loss': 0.6667, 'grad_norm': 1.112353801727295, 'learning_rate': 0.0009000925925925925, 'epoch': 0.3}
Step 1090: {'loss': 0.6391, 'grad_norm': 1.3439222574234009, 'learning_rate': 0.0008991666666666667, 'epoch': 0.30277777777777776}
Step 1100: {'loss': 0.7081, 'grad_norm': 1.8301832675933838, 'learning_rate': 0.0008982407407407408, 'epoch': 0.3055555555555556}
Step 1110: {'loss': 0.7038, 'grad_norm': 1.8355834484100342, 'learning_rate': 0.0008973148148148148, 'epoch': 0.30833333333333335}
Step 1120: {'loss': 0.6669, 'grad_norm': 1.182586669921875, 'learning_rate': 0.000896388888888889, 'epoch': 0.3111111111111111}
Step 1130: {'loss': 0.6822, 'grad_norm': 1.6917868852615356, 'learning_rate': 0.000895462962962963, 'epoch': 0.3138888888888889}
Step 1140: {'loss': 0.6807, 'grad_norm': 1.7632761001586914, 'learning_rate': 0.000894537037037037, 'epoch': 0.31666666666666665}
Step 1150: {'loss': 0.6631, 'grad_norm': 1.2234909534454346, 'learning_rate': 0.0008936111111111112, 'epoch': 0.3194444444444444}
Step 1160: {'loss': 0.6883, 'grad_norm': 1.3696250915527344, 'learning_rate': 0.0008926851851851852, 'epoch': 0.32222222222222224}
Step 1170: {'loss': 0.6655, 'grad_norm': 1.611404299736023, 'learning_rate': 0.0008917592592592592, 'epoch': 0.325}
Step 1180: {'loss': 0.7154, 'grad_norm': 1.2563793659210205, 'learning_rate': 0.0008908333333333334, 'epoch': 0.3277777777777778}
Step 1190: {'loss': 0.6352, 'grad_norm': 2.7619128227233887, 'learning_rate': 0.0008899074074074074, 'epoch': 0.33055555555555555}
Step 1200: {'loss': 0.7528, 'grad_norm': 2.2783896923065186, 'learning_rate': 0.0008889814814814815, 'epoch': 0.3333333333333333}
Step 1210: {'loss': 0.68, 'grad_norm': 0.9696547389030457, 'learning_rate': 0.0008880555555555557, 'epoch': 0.33611111111111114}
Step 1220: {'loss': 0.7235, 'grad_norm': 1.1357321739196777, 'learning_rate': 0.0008871296296296296, 'epoch': 0.3388888888888889}
Step 1230: {'loss': 0.7076, 'grad_norm': 1.2965232133865356, 'learning_rate': 0.0008862037037037037, 'epoch': 0.3416666666666667}
Step 1240: {'loss': 0.6683, 'grad_norm': 1.2350375652313232, 'learning_rate': 0.0008852777777777778, 'epoch': 0.34444444444444444}
Step 1250: {'loss': 0.7435, 'grad_norm': 1.3448110818862915, 'learning_rate': 0.0008843518518518519, 'epoch': 0.3472222222222222}
Step 1260: {'loss': 0.6351, 'grad_norm': 1.798819899559021, 'learning_rate': 0.000883425925925926, 'epoch': 0.35}
Step 1270: {'loss': 0.6997, 'grad_norm': 1.661238431930542, 'learning_rate': 0.0008824999999999999, 'epoch': 0.3527777777777778}
Step 1280: {'loss': 0.6969, 'grad_norm': 1.0893406867980957, 'learning_rate': 0.0008815740740740741, 'epoch': 0.35555555555555557}
Step 1290: {'loss': 0.6543, 'grad_norm': 1.2655302286148071, 'learning_rate': 0.0008806481481481482, 'epoch': 0.35833333333333334}
Step 1300: {'loss': 0.6698, 'grad_norm': 1.233635425567627, 'learning_rate': 0.0008797222222222222, 'epoch': 0.3611111111111111}
Step 1310: {'loss': 0.6676, 'grad_norm': 1.1956732273101807, 'learning_rate': 0.0008787962962962964, 'epoch': 0.3638888888888889}
Step 1320: {'loss': 0.6895, 'grad_norm': 1.4400330781936646, 'learning_rate': 0.0008778703703703704, 'epoch': 0.36666666666666664}
Step 1330: {'loss': 0.6776, 'grad_norm': 1.3957715034484863, 'learning_rate': 0.0008769444444444444, 'epoch': 0.36944444444444446}
Step 1340: {'loss': 0.7236, 'grad_norm': 1.8838013410568237, 'learning_rate': 0.0008760185185185186, 'epoch': 0.37222222222222223}
Step 1350: {'loss': 0.7162, 'grad_norm': 1.573401927947998, 'learning_rate': 0.0008750925925925927, 'epoch': 0.375}
Step 1360: {'loss': 0.6906, 'grad_norm': 1.2965139150619507, 'learning_rate': 0.0008741666666666666, 'epoch': 0.37777777777777777}
Step 1370: {'loss': 0.6916, 'grad_norm': 1.220685362815857, 'learning_rate': 0.0008732407407407407, 'epoch': 0.38055555555555554}
Step 1380: {'loss': 0.6457, 'grad_norm': 1.3817187547683716, 'learning_rate': 0.0008723148148148148, 'epoch': 0.38333333333333336}
Step 1390: {'loss': 0.7297, 'grad_norm': 1.3762496709823608, 'learning_rate': 0.0008713888888888889, 'epoch': 0.3861111111111111}
Step 1400: {'loss': 0.6118, 'grad_norm': 2.0197577476501465, 'learning_rate': 0.000870462962962963, 'epoch': 0.3888888888888889}
Step 1410: {'loss': 0.7082, 'grad_norm': 1.5290467739105225, 'learning_rate': 0.000869537037037037, 'epoch': 0.39166666666666666}
Step 1420: {'loss': 0.6801, 'grad_norm': 1.6866865158081055, 'learning_rate': 0.0008686111111111111, 'epoch': 0.39444444444444443}
Step 1430: {'loss': 0.7496, 'grad_norm': 0.9359914064407349, 'learning_rate': 0.0008676851851851852, 'epoch': 0.3972222222222222}
Step 1440: {'loss': 0.6759, 'grad_norm': 1.5373719930648804, 'learning_rate': 0.0008667592592592593, 'epoch': 0.4}
Step 1450: {'loss': 0.6587, 'grad_norm': 1.4361025094985962, 'learning_rate': 0.0008658333333333334, 'epoch': 0.4027777777777778}
Step 1460: {'loss': 0.6755, 'grad_norm': 1.454375982284546, 'learning_rate': 0.0008649074074074074, 'epoch': 0.40555555555555556}
Step 1470: {'loss': 0.6811, 'grad_norm': 1.3252512216567993, 'learning_rate': 0.0008639814814814815, 'epoch': 0.4083333333333333}
Step 1480: {'loss': 0.737, 'grad_norm': 1.4586180448532104, 'learning_rate': 0.0008630555555555556, 'epoch': 0.4111111111111111}
Step 1490: {'loss': 0.6212, 'grad_norm': 1.2724409103393555, 'learning_rate': 0.0008621296296296296, 'epoch': 0.41388888888888886}
Step 1500: {'loss': 0.7042, 'grad_norm': 1.3202801942825317, 'learning_rate': 0.0008612037037037038, 'epoch': 0.4166666666666667}
Step 1510: {'loss': 0.641, 'grad_norm': 1.3824890851974487, 'learning_rate': 0.0008602777777777778, 'epoch': 0.41944444444444445}
Step 1520: {'loss': 0.6707, 'grad_norm': 1.6457250118255615, 'learning_rate': 0.0008593518518518518, 'epoch': 0.4222222222222222}
Step 1530: {'loss': 0.623, 'grad_norm': 1.8334492444992065, 'learning_rate': 0.0008584259259259259, 'epoch': 0.425}
Step 1540: {'loss': 0.6527, 'grad_norm': 1.3230090141296387, 'learning_rate': 0.0008575000000000001, 'epoch': 0.42777777777777776}
Step 1550: {'loss': 0.6746, 'grad_norm': 1.4613760709762573, 'learning_rate': 0.000856574074074074, 'epoch': 0.4305555555555556}
Step 1560: {'loss': 0.675, 'grad_norm': 2.0201919078826904, 'learning_rate': 0.0008556481481481481, 'epoch': 0.43333333333333335}
Step 1570: {'loss': 0.689, 'grad_norm': 2.424168825149536, 'learning_rate': 0.0008547222222222223, 'epoch': 0.4361111111111111}
Step 1580: {'loss': 0.6976, 'grad_norm': 0.9926238656044006, 'learning_rate': 0.0008537962962962963, 'epoch': 0.4388888888888889}
Step 1590: {'loss': 0.7214, 'grad_norm': 1.523867130279541, 'learning_rate': 0.0008528703703703704, 'epoch': 0.44166666666666665}
Step 1600: {'loss': 0.6736, 'grad_norm': 1.479260802268982, 'learning_rate': 0.0008519444444444445, 'epoch': 0.4444444444444444}
Step 1610: {'loss': 0.6614, 'grad_norm': 1.6343082189559937, 'learning_rate': 0.0008510185185185185, 'epoch': 0.44722222222222224}
Step 1620: {'loss': 0.6548, 'grad_norm': 1.2946662902832031, 'learning_rate': 0.0008500925925925926, 'epoch': 0.45}
Step 1630: {'loss': 0.6952, 'grad_norm': 1.8891562223434448, 'learning_rate': 0.0008491666666666667, 'epoch': 0.4527777777777778}
Step 1640: {'loss': 0.704, 'grad_norm': 1.4095996618270874, 'learning_rate': 0.0008482407407407408, 'epoch': 0.45555555555555555}
Step 1650: {'loss': 0.6573, 'grad_norm': 1.3491336107254028, 'learning_rate': 0.0008473148148148148, 'epoch': 0.4583333333333333}
Step 1660: {'loss': 0.6355, 'grad_norm': 1.1987224817276, 'learning_rate': 0.0008463888888888889, 'epoch': 0.46111111111111114}
Step 1670: {'loss': 0.721, 'grad_norm': 1.2240490913391113, 'learning_rate': 0.000845462962962963, 'epoch': 0.4638888888888889}
Step 1680: {'loss': 0.7109, 'grad_norm': 2.521796703338623, 'learning_rate': 0.0008445370370370371, 'epoch': 0.4666666666666667}
Step 1690: {'loss': 0.684, 'grad_norm': 1.9978654384613037, 'learning_rate': 0.0008436111111111111, 'epoch': 0.46944444444444444}
Step 1700: {'loss': 0.6864, 'grad_norm': 1.3148640394210815, 'learning_rate': 0.0008426851851851852, 'epoch': 0.4722222222222222}
Step 1710: {'loss': 0.6545, 'grad_norm': 1.3921631574630737, 'learning_rate': 0.0008417592592592592, 'epoch': 0.475}
Step 1720: {'loss': 0.6446, 'grad_norm': 1.0197726488113403, 'learning_rate': 0.0008408333333333333, 'epoch': 0.4777777777777778}
Step 1730: {'loss': 0.711, 'grad_norm': 1.1987030506134033, 'learning_rate': 0.0008399074074074075, 'epoch': 0.48055555555555557}
Step 1740: {'loss': 0.6346, 'grad_norm': 1.1090625524520874, 'learning_rate': 0.0008389814814814815, 'epoch': 0.48333333333333334}
Step 1750: {'loss': 0.6459, 'grad_norm': 1.4122697114944458, 'learning_rate': 0.0008380555555555555, 'epoch': 0.4861111111111111}
Step 1760: {'loss': 0.6628, 'grad_norm': 1.0626389980316162, 'learning_rate': 0.0008371296296296297, 'epoch': 0.4888888888888889}
Step 1770: {'loss': 0.6909, 'grad_norm': 1.426689863204956, 'learning_rate': 0.0008362037037037037, 'epoch': 0.49166666666666664}
Step 1780: {'loss': 0.7017, 'grad_norm': 1.164088249206543, 'learning_rate': 0.0008352777777777778, 'epoch': 0.49444444444444446}
Step 1790: {'loss': 0.6681, 'grad_norm': 1.7180075645446777, 'learning_rate': 0.000834351851851852, 'epoch': 0.49722222222222223}
Step 1800: {'loss': 0.6765, 'grad_norm': 0.9043641090393066, 'learning_rate': 0.0008334259259259259, 'epoch': 0.5}
Step 1810: {'loss': 0.6735, 'grad_norm': 0.9490474462509155, 'learning_rate': 0.0008325, 'epoch': 0.5027777777777778}
Step 1820: {'loss': 0.6608, 'grad_norm': 1.203263282775879, 'learning_rate': 0.000831574074074074, 'epoch': 0.5055555555555555}
Step 1830: {'loss': 0.651, 'grad_norm': 2.5038108825683594, 'learning_rate': 0.0008306481481481482, 'epoch': 0.5083333333333333}
Step 1840: {'loss': 0.6764, 'grad_norm': 1.3281421661376953, 'learning_rate': 0.0008297222222222223, 'epoch': 0.5111111111111111}
Step 1850: {'loss': 0.6647, 'grad_norm': 1.4734159708023071, 'learning_rate': 0.0008287962962962962, 'epoch': 0.5138888888888888}
Step 1860: {'loss': 0.6711, 'grad_norm': 1.0683424472808838, 'learning_rate': 0.0008278703703703704, 'epoch': 0.5166666666666667}
Step 1870: {'loss': 0.7235, 'grad_norm': 1.2265069484710693, 'learning_rate': 0.0008269444444444445, 'epoch': 0.5194444444444445}
Step 1880: {'loss': 0.6576, 'grad_norm': 1.5298699140548706, 'learning_rate': 0.0008260185185185185, 'epoch': 0.5222222222222223}
Step 1890: {'loss': 0.6759, 'grad_norm': 1.2694616317749023, 'learning_rate': 0.0008250925925925927, 'epoch': 0.525}
Step 1900: {'loss': 0.6812, 'grad_norm': 1.4051724672317505, 'learning_rate': 0.0008241666666666667, 'epoch': 0.5277777777777778}
Step 1910: {'loss': 0.672, 'grad_norm': 1.6459823846817017, 'learning_rate': 0.0008232407407407407, 'epoch': 0.5305555555555556}
Step 1920: {'loss': 0.6635, 'grad_norm': 1.5968214273452759, 'learning_rate': 0.0008223148148148149, 'epoch': 0.5333333333333333}
Step 1930: {'loss': 0.6583, 'grad_norm': 1.1925888061523438, 'learning_rate': 0.0008213888888888889, 'epoch': 0.5361111111111111}
Step 1940: {'loss': 0.6478, 'grad_norm': 1.3396480083465576, 'learning_rate': 0.0008204629629629629, 'epoch': 0.5388888888888889}
Step 1950: {'loss': 0.6574, 'grad_norm': 1.1437220573425293, 'learning_rate': 0.0008195370370370371, 'epoch': 0.5416666666666666}
Step 1960: {'loss': 0.6717, 'grad_norm': 1.666923999786377, 'learning_rate': 0.0008186111111111111, 'epoch': 0.5444444444444444}
Step 1970: {'loss': 0.6838, 'grad_norm': 1.3516438007354736, 'learning_rate': 0.0008176851851851852, 'epoch': 0.5472222222222223}
Step 1980: {'loss': 0.6671, 'grad_norm': 1.1782890558242798, 'learning_rate': 0.0008167592592592593, 'epoch': 0.55}
Step 1990: {'loss': 0.6567, 'grad_norm': 1.340092658996582, 'learning_rate': 0.0008158333333333333, 'epoch': 0.5527777777777778}
Step 2000: {'loss': 0.6744, 'grad_norm': 1.7335273027420044, 'learning_rate': 0.0008149074074074074, 'epoch': 0.5555555555555556}
Step 2010: {'loss': 0.6582, 'grad_norm': 1.3151975870132446, 'learning_rate': 0.0008139814814814815, 'epoch': 0.5583333333333333}
Step 2020: {'loss': 0.7012, 'grad_norm': 2.3419179916381836, 'learning_rate': 0.0008130555555555556, 'epoch': 0.5611111111111111}
Step 2030: {'loss': 0.6244, 'grad_norm': 2.1417253017425537, 'learning_rate': 0.0008121296296296297, 'epoch': 0.5638888888888889}
Step 2040: {'loss': 0.6604, 'grad_norm': 1.5118684768676758, 'learning_rate': 0.0008112037037037036, 'epoch': 0.5666666666666667}
Step 2050: {'loss': 0.6129, 'grad_norm': 1.3537739515304565, 'learning_rate': 0.0008102777777777778, 'epoch': 0.5694444444444444}
Step 2060: {'loss': 0.6564, 'grad_norm': 1.1662651300430298, 'learning_rate': 0.0008093518518518519, 'epoch': 0.5722222222222222}
Step 2070: {'loss': 0.6488, 'grad_norm': 1.2541130781173706, 'learning_rate': 0.0008084259259259259, 'epoch': 0.575}
Step 2080: {'loss': 0.7124, 'grad_norm': 1.5699892044067383, 'learning_rate': 0.0008075000000000001, 'epoch': 0.5777777777777777}
Step 2090: {'loss': 0.6579, 'grad_norm': 1.5046578645706177, 'learning_rate': 0.0008065740740740741, 'epoch': 0.5805555555555556}
Step 2100: {'loss': 0.6868, 'grad_norm': 1.4630433320999146, 'learning_rate': 0.0008056481481481481, 'epoch': 0.5833333333333334}
Step 2110: {'loss': 0.683, 'grad_norm': 1.18464195728302, 'learning_rate': 0.0008047222222222223, 'epoch': 0.5861111111111111}
Step 2120: {'loss': 0.6837, 'grad_norm': 1.07699716091156, 'learning_rate': 0.0008037962962962964, 'epoch': 0.5888888888888889}
Step 2130: {'loss': 0.6492, 'grad_norm': 1.3392659425735474, 'learning_rate': 0.0008028703703703703, 'epoch': 0.5916666666666667}
Step 2140: {'loss': 0.7359, 'grad_norm': 1.145560622215271, 'learning_rate': 0.0008019444444444444, 'epoch': 0.5944444444444444}
Step 2150: {'loss': 0.6621, 'grad_norm': 1.050405502319336, 'learning_rate': 0.0008010185185185185, 'epoch': 0.5972222222222222}
Step 2160: {'loss': 0.6276, 'grad_norm': 1.0023267269134521, 'learning_rate': 0.0008000925925925926, 'epoch': 0.6}
Step 2170: {'loss': 0.6767, 'grad_norm': 1.0017521381378174, 'learning_rate': 0.0007991666666666667, 'epoch': 0.6027777777777777}
Step 2180: {'loss': 0.6919, 'grad_norm': 1.0374163389205933, 'learning_rate': 0.0007982407407407407, 'epoch': 0.6055555555555555}
Step 2190: {'loss': 0.6907, 'grad_norm': 1.0374418497085571, 'learning_rate': 0.0007973148148148148, 'epoch': 0.6083333333333333}
Step 2200: {'loss': 0.6858, 'grad_norm': 1.1019998788833618, 'learning_rate': 0.0007963888888888889, 'epoch': 0.6111111111111112}
Step 2210: {'loss': 0.6527, 'grad_norm': 1.4000511169433594, 'learning_rate': 0.000795462962962963, 'epoch': 0.6138888888888889}
Step 2220: {'loss': 0.7076, 'grad_norm': 1.066813349723816, 'learning_rate': 0.0007945370370370371, 'epoch': 0.6166666666666667}
Step 2230: {'loss': 0.6681, 'grad_norm': 1.0694020986557007, 'learning_rate': 0.0007936111111111111, 'epoch': 0.6194444444444445}
Step 2240: {'loss': 0.7025, 'grad_norm': 2.0783002376556396, 'learning_rate': 0.0007926851851851852, 'epoch': 0.6222222222222222}
Step 2250: {'loss': 0.6277, 'grad_norm': 1.4010542631149292, 'learning_rate': 0.0007917592592592593, 'epoch': 0.625}
Step 2260: {'loss': 0.6377, 'grad_norm': 0.9436354637145996, 'learning_rate': 0.0007908333333333334, 'epoch': 0.6277777777777778}
Step 2270: {'loss': 0.6511, 'grad_norm': 1.5973681211471558, 'learning_rate': 0.0007899074074074074, 'epoch': 0.6305555555555555}
Step 2280: {'loss': 0.6794, 'grad_norm': 1.2497118711471558, 'learning_rate': 0.0007889814814814815, 'epoch': 0.6333333333333333}
Step 2290: {'loss': 0.6545, 'grad_norm': 1.0785129070281982, 'learning_rate': 0.0007880555555555555, 'epoch': 0.6361111111111111}
Step 2300: {'loss': 0.7295, 'grad_norm': 1.5946320295333862, 'learning_rate': 0.0007871296296296296, 'epoch': 0.6388888888888888}
Step 2310: {'loss': 0.6698, 'grad_norm': 1.3522132635116577, 'learning_rate': 0.0007862037037037038, 'epoch': 0.6416666666666667}
Step 2320: {'loss': 0.6415, 'grad_norm': 1.703900933265686, 'learning_rate': 0.0007852777777777778, 'epoch': 0.6444444444444445}
Step 2330: {'loss': 0.6857, 'grad_norm': 1.5465898513793945, 'learning_rate': 0.0007843518518518518, 'epoch': 0.6472222222222223}
Step 2340: {'loss': 0.6584, 'grad_norm': 1.4896116256713867, 'learning_rate': 0.000783425925925926, 'epoch': 0.65}
Step 2350: {'loss': 0.6012, 'grad_norm': 1.478095293045044, 'learning_rate': 0.0007825, 'epoch': 0.6527777777777778}
Step 2360: {'loss': 0.6478, 'grad_norm': 1.1822509765625, 'learning_rate': 0.0007815740740740741, 'epoch': 0.6555555555555556}
Step 2370: {'loss': 0.6854, 'grad_norm': 1.1243141889572144, 'learning_rate': 0.0007806481481481483, 'epoch': 0.6583333333333333}
Step 2380: {'loss': 0.6549, 'grad_norm': 1.7106045484542847, 'learning_rate': 0.0007797222222222222, 'epoch': 0.6611111111111111}
Step 2390: {'loss': 0.6841, 'grad_norm': 2.033778190612793, 'learning_rate': 0.0007787962962962963, 'epoch': 0.6638888888888889}
Step 2400: {'loss': 0.7012, 'grad_norm': 1.368800163269043, 'learning_rate': 0.0007778703703703704, 'epoch': 0.6666666666666666}
Step 2410: {'loss': 0.641, 'grad_norm': 1.2465027570724487, 'learning_rate': 0.0007769444444444445, 'epoch': 0.6694444444444444}
Step 2420: {'loss': 0.6717, 'grad_norm': 1.2620532512664795, 'learning_rate': 0.0007760185185185186, 'epoch': 0.6722222222222223}
Step 2430: {'loss': 0.6943, 'grad_norm': 2.5602529048919678, 'learning_rate': 0.0007750925925925925, 'epoch': 0.675}
Step 2440: {'loss': 0.6184, 'grad_norm': 1.5919772386550903, 'learning_rate': 0.0007741666666666667, 'epoch': 0.6777777777777778}
Step 2450: {'loss': 0.6549, 'grad_norm': 1.0349586009979248, 'learning_rate': 0.0007732407407407408, 'epoch': 0.6805555555555556}
Step 2460: {'loss': 0.6447, 'grad_norm': 1.3978126049041748, 'learning_rate': 0.0007723148148148148, 'epoch': 0.6833333333333333}
Step 2470: {'loss': 0.6518, 'grad_norm': 1.667372703552246, 'learning_rate': 0.000771388888888889, 'epoch': 0.6861111111111111}
Step 2480: {'loss': 0.6219, 'grad_norm': 1.1251002550125122, 'learning_rate': 0.000770462962962963, 'epoch': 0.6888888888888889}
Step 2490: {'loss': 0.6201, 'grad_norm': 1.251649022102356, 'learning_rate': 0.000769537037037037, 'epoch': 0.6916666666666667}
Step 2500: {'loss': 0.6619, 'grad_norm': 1.362886667251587, 'learning_rate': 0.0007686111111111112, 'epoch': 0.6944444444444444}
Step 2510: {'loss': 0.6408, 'grad_norm': 2.8993008136749268, 'learning_rate': 0.0007676851851851852, 'epoch': 0.6972222222222222}
Step 2520: {'loss': 0.6844, 'grad_norm': 1.1559076309204102, 'learning_rate': 0.0007667592592592592, 'epoch': 0.7}
Step 2530: {'loss': 0.6576, 'grad_norm': 1.2435259819030762, 'learning_rate': 0.0007658333333333334, 'epoch': 0.7027777777777777}
Step 2540: {'loss': 0.6013, 'grad_norm': 1.3628512620925903, 'learning_rate': 0.0007649074074074074, 'epoch': 0.7055555555555556}
Step 2550: {'loss': 0.6325, 'grad_norm': 1.292531132698059, 'learning_rate': 0.0007639814814814815, 'epoch': 0.7083333333333334}
Step 2560: {'loss': 0.6878, 'grad_norm': 1.8465440273284912, 'learning_rate': 0.0007630555555555557, 'epoch': 0.7111111111111111}
Step 2570: {'loss': 0.6394, 'grad_norm': 1.1295377016067505, 'learning_rate': 0.0007621296296296296, 'epoch': 0.7138888888888889}
Step 2580: {'loss': 0.7105, 'grad_norm': 0.8077482581138611, 'learning_rate': 0.0007612037037037037, 'epoch': 0.7166666666666667}
Step 2590: {'loss': 0.6484, 'grad_norm': 1.8288254737854004, 'learning_rate': 0.0007602777777777778, 'epoch': 0.7194444444444444}
Step 2600: {'loss': 0.6518, 'grad_norm': 1.146027684211731, 'learning_rate': 0.0007593518518518519, 'epoch': 0.7222222222222222}
Step 2610: {'loss': 0.641, 'grad_norm': 1.1041878461837769, 'learning_rate': 0.000758425925925926, 'epoch': 0.725}
Step 2620: {'loss': 0.5972, 'grad_norm': 0.8924542665481567, 'learning_rate': 0.0007574999999999999, 'epoch': 0.7277777777777777}
Step 2630: {'loss': 0.7228, 'grad_norm': 1.0246471166610718, 'learning_rate': 0.0007565740740740741, 'epoch': 0.7305555555555555}
Step 2640: {'loss': 0.6002, 'grad_norm': 1.3361440896987915, 'learning_rate': 0.0007556481481481482, 'epoch': 0.7333333333333333}
Step 2650: {'loss': 0.627, 'grad_norm': 1.1861835718154907, 'learning_rate': 0.0007547222222222222, 'epoch': 0.7361111111111112}
Step 2660: {'loss': 0.6614, 'grad_norm': 1.3515143394470215, 'learning_rate': 0.0007537962962962964, 'epoch': 0.7388888888888889}
Step 2670: {'loss': 0.6841, 'grad_norm': 1.647807240486145, 'learning_rate': 0.0007528703703703704, 'epoch': 0.7416666666666667}
Step 2680: {'loss': 0.6062, 'grad_norm': 1.4063276052474976, 'learning_rate': 0.0007519444444444444, 'epoch': 0.7444444444444445}
Step 2690: {'loss': 0.6279, 'grad_norm': 1.4112557172775269, 'learning_rate': 0.0007510185185185186, 'epoch': 0.7472222222222222}
Step 2700: {'loss': 0.681, 'grad_norm': 1.3137611150741577, 'learning_rate': 0.0007500925925925927, 'epoch': 0.75}
Step 2710: {'loss': 0.6679, 'grad_norm': 1.60664701461792, 'learning_rate': 0.0007491666666666666, 'epoch': 0.7527777777777778}
Step 2720: {'loss': 0.6017, 'grad_norm': 1.5269055366516113, 'learning_rate': 0.0007482407407407407, 'epoch': 0.7555555555555555}
Step 2730: {'loss': 0.6899, 'grad_norm': 0.9994379281997681, 'learning_rate': 0.0007473148148148148, 'epoch': 0.7583333333333333}
Step 2740: {'loss': 0.658, 'grad_norm': 1.301710605621338, 'learning_rate': 0.0007463888888888889, 'epoch': 0.7611111111111111}
Step 2750: {'loss': 0.5943, 'grad_norm': 1.6461637020111084, 'learning_rate': 0.000745462962962963, 'epoch': 0.7638888888888888}
Step 2760: {'loss': 0.6226, 'grad_norm': 1.3135874271392822, 'learning_rate': 0.000744537037037037, 'epoch': 0.7666666666666667}
Step 2770: {'loss': 0.6032, 'grad_norm': 1.2124742269515991, 'learning_rate': 0.0007436111111111111, 'epoch': 0.7694444444444445}
Step 2780: {'loss': 0.7003, 'grad_norm': 1.6524955034255981, 'learning_rate': 0.0007426851851851852, 'epoch': 0.7722222222222223}
Step 2790: {'loss': 0.7008, 'grad_norm': 2.9174749851226807, 'learning_rate': 0.0007417592592592593, 'epoch': 0.775}
Step 2800: {'loss': 0.6446, 'grad_norm': 1.4170737266540527, 'learning_rate': 0.0007408333333333334, 'epoch': 0.7777777777777778}
Step 2810: {'loss': 0.6496, 'grad_norm': 1.052481770515442, 'learning_rate': 0.0007399074074074074, 'epoch': 0.7805555555555556}
Step 2820: {'loss': 0.6756, 'grad_norm': 1.440112829208374, 'learning_rate': 0.0007389814814814815, 'epoch': 0.7833333333333333}
Step 2830: {'loss': 0.6781, 'grad_norm': 1.67446768283844, 'learning_rate': 0.0007380555555555556, 'epoch': 0.7861111111111111}
Step 2840: {'loss': 0.662, 'grad_norm': 1.26759672164917, 'learning_rate': 0.0007371296296296296, 'epoch': 0.7888888888888889}
Step 2850: {'loss': 0.6352, 'grad_norm': 1.8344879150390625, 'learning_rate': 0.0007362037037037038, 'epoch': 0.7916666666666666}
Step 2860: {'loss': 0.6916, 'grad_norm': 1.5020216703414917, 'learning_rate': 0.0007352777777777778, 'epoch': 0.7944444444444444}
Step 2870: {'loss': 0.6422, 'grad_norm': 1.5469311475753784, 'learning_rate': 0.0007343518518518518, 'epoch': 0.7972222222222223}
Step 2880: {'loss': 0.6477, 'grad_norm': 1.397771954536438, 'learning_rate': 0.0007334259259259259, 'epoch': 0.8}
Step 2890: {'loss': 0.6302, 'grad_norm': 1.3610275983810425, 'learning_rate': 0.0007325000000000001, 'epoch': 0.8027777777777778}
Step 2900: {'loss': 0.6326, 'grad_norm': 1.3328330516815186, 'learning_rate': 0.000731574074074074, 'epoch': 0.8055555555555556}
Step 2910: {'loss': 0.6532, 'grad_norm': 1.2889580726623535, 'learning_rate': 0.0007306481481481481, 'epoch': 0.8083333333333333}
Step 2920: {'loss': 0.5986, 'grad_norm': 1.055300235748291, 'learning_rate': 0.0007297222222222223, 'epoch': 0.8111111111111111}
Step 2930: {'loss': 0.6607, 'grad_norm': 1.4325013160705566, 'learning_rate': 0.0007287962962962963, 'epoch': 0.8138888888888889}
Step 2940: {'loss': 0.7006, 'grad_norm': 1.3996483087539673, 'learning_rate': 0.0007278703703703704, 'epoch': 0.8166666666666667}
Step 2950: {'loss': 0.7003, 'grad_norm': 1.4444431066513062, 'learning_rate': 0.0007269444444444444, 'epoch': 0.8194444444444444}
Step 2960: {'loss': 0.606, 'grad_norm': 1.6013017892837524, 'learning_rate': 0.0007260185185185185, 'epoch': 0.8222222222222222}
Step 2970: {'loss': 0.6639, 'grad_norm': 1.1213332414627075, 'learning_rate': 0.0007250925925925926, 'epoch': 0.825}
Step 2980: {'loss': 0.7217, 'grad_norm': 1.188657522201538, 'learning_rate': 0.0007241666666666667, 'epoch': 0.8277777777777777}
Step 2990: {'loss': 0.6645, 'grad_norm': 1.2436528205871582, 'learning_rate': 0.0007232407407407408, 'epoch': 0.8305555555555556}
Step 3000: {'loss': 0.5821, 'grad_norm': 1.293133020401001, 'learning_rate': 0.0007223148148148148, 'epoch': 0.8333333333333334}
Step 3010: {'loss': 0.6153, 'grad_norm': 1.3019192218780518, 'learning_rate': 0.0007213888888888889, 'epoch': 0.8361111111111111}
Step 3020: {'loss': 0.6265, 'grad_norm': 1.3039777278900146, 'learning_rate': 0.000720462962962963, 'epoch': 0.8388888888888889}
Step 3030: {'loss': 0.6797, 'grad_norm': 1.087689757347107, 'learning_rate': 0.0007195370370370371, 'epoch': 0.8416666666666667}
Step 3040: {'loss': 0.6703, 'grad_norm': 0.8950591683387756, 'learning_rate': 0.0007186111111111111, 'epoch': 0.8444444444444444}
Step 3050: {'loss': 0.6063, 'grad_norm': 1.316964864730835, 'learning_rate': 0.0007176851851851852, 'epoch': 0.8472222222222222}
Step 3060: {'loss': 0.6585, 'grad_norm': 1.3151482343673706, 'learning_rate': 0.0007167592592592592, 'epoch': 0.85}
Step 3070: {'loss': 0.6902, 'grad_norm': 0.9768524765968323, 'learning_rate': 0.0007158333333333333, 'epoch': 0.8527777777777777}
Step 3080: {'loss': 0.62, 'grad_norm': 1.057326316833496, 'learning_rate': 0.0007149074074074075, 'epoch': 0.8555555555555555}
Step 3090: {'loss': 0.6104, 'grad_norm': 1.1737563610076904, 'learning_rate': 0.0007139814814814815, 'epoch': 0.8583333333333333}
Step 3100: {'loss': 0.6039, 'grad_norm': 0.9332001209259033, 'learning_rate': 0.0007130555555555555, 'epoch': 0.8611111111111112}
Step 3110: {'loss': 0.6893, 'grad_norm': 2.0161192417144775, 'learning_rate': 0.0007121296296296297, 'epoch': 0.8638888888888889}
Step 3120: {'loss': 0.6523, 'grad_norm': 1.021281361579895, 'learning_rate': 0.0007112037037037037, 'epoch': 0.8666666666666667}
Step 3130: {'loss': 0.6457, 'grad_norm': 1.8872405290603638, 'learning_rate': 0.0007102777777777778, 'epoch': 0.8694444444444445}
Step 3140: {'loss': 0.6265, 'grad_norm': 1.3458322286605835, 'learning_rate': 0.000709351851851852, 'epoch': 0.8722222222222222}
Step 3150: {'loss': 0.6583, 'grad_norm': 1.2705121040344238, 'learning_rate': 0.0007084259259259259, 'epoch': 0.875}
Step 3160: {'loss': 0.6203, 'grad_norm': 1.0224695205688477, 'learning_rate': 0.0007075, 'epoch': 0.8777777777777778}
Step 3170: {'loss': 0.6837, 'grad_norm': 1.7543295621871948, 'learning_rate': 0.000706574074074074, 'epoch': 0.8805555555555555}
Step 3180: {'loss': 0.6652, 'grad_norm': 0.9313498139381409, 'learning_rate': 0.0007056481481481482, 'epoch': 0.8833333333333333}
Step 3190: {'loss': 0.655, 'grad_norm': 0.9812782406806946, 'learning_rate': 0.0007047222222222223, 'epoch': 0.8861111111111111}
Step 3200: {'loss': 0.673, 'grad_norm': 1.3119723796844482, 'learning_rate': 0.0007037962962962962, 'epoch': 0.8888888888888888}
Step 3210: {'loss': 0.7133, 'grad_norm': 1.2213295698165894, 'learning_rate': 0.0007028703703703704, 'epoch': 0.8916666666666667}
Step 3220: {'loss': 0.6592, 'grad_norm': 1.2734355926513672, 'learning_rate': 0.0007019444444444445, 'epoch': 0.8944444444444445}
Step 3230: {'loss': 0.6496, 'grad_norm': 1.203277826309204, 'learning_rate': 0.0007010185185185185, 'epoch': 0.8972222222222223}
Step 3240: {'loss': 0.6333, 'grad_norm': 1.339526891708374, 'learning_rate': 0.0007000925925925926, 'epoch': 0.9}
Step 3250: {'loss': 0.6367, 'grad_norm': 1.2539361715316772, 'learning_rate': 0.0006991666666666667, 'epoch': 0.9027777777777778}
Step 3260: {'loss': 0.6872, 'grad_norm': 1.999477744102478, 'learning_rate': 0.0006982407407407407, 'epoch': 0.9055555555555556}
Step 3270: {'loss': 0.6825, 'grad_norm': 1.2334189414978027, 'learning_rate': 0.0006973148148148149, 'epoch': 0.9083333333333333}
Step 3280: {'loss': 0.6642, 'grad_norm': 1.034063458442688, 'learning_rate': 0.0006963888888888889, 'epoch': 0.9111111111111111}
Step 3290: {'loss': 0.6905, 'grad_norm': 1.3607679605484009, 'learning_rate': 0.0006954629629629629, 'epoch': 0.9138888888888889}
Step 3300: {'loss': 0.6102, 'grad_norm': 1.0231585502624512, 'learning_rate': 0.0006945370370370371, 'epoch': 0.9166666666666666}
Step 3310: {'loss': 0.6301, 'grad_norm': 1.081713080406189, 'learning_rate': 0.0006936111111111111, 'epoch': 0.9194444444444444}
Step 3320: {'loss': 0.6288, 'grad_norm': 1.8016622066497803, 'learning_rate': 0.0006926851851851852, 'epoch': 0.9222222222222223}
Step 3330: {'loss': 0.6896, 'grad_norm': 1.1537566184997559, 'learning_rate': 0.0006917592592592593, 'epoch': 0.925}
Step 3340: {'loss': 0.763, 'grad_norm': 1.606461763381958, 'learning_rate': 0.0006908333333333333, 'epoch': 0.9277777777777778}
Step 3350: {'loss': 0.6743, 'grad_norm': 1.2111806869506836, 'learning_rate': 0.0006899074074074074, 'epoch': 0.9305555555555556}
Step 3360: {'loss': 0.6451, 'grad_norm': 1.4804061651229858, 'learning_rate': 0.0006889814814814815, 'epoch': 0.9333333333333333}
Step 3370: {'loss': 0.5829, 'grad_norm': 1.4977772235870361, 'learning_rate': 0.0006880555555555556, 'epoch': 0.9361111111111111}
Step 3380: {'loss': 0.6888, 'grad_norm': 2.0780375003814697, 'learning_rate': 0.0006871296296296297, 'epoch': 0.9388888888888889}
Step 3390: {'loss': 0.6204, 'grad_norm': 1.528656244277954, 'learning_rate': 0.0006862037037037036, 'epoch': 0.9416666666666667}
Step 3400: {'loss': 0.6133, 'grad_norm': 1.302785873413086, 'learning_rate': 0.0006852777777777778, 'epoch': 0.9444444444444444}
Step 3410: {'loss': 0.6201, 'grad_norm': 1.1116458177566528, 'learning_rate': 0.0006843518518518519, 'epoch': 0.9472222222222222}
Step 3420: {'loss': 0.6114, 'grad_norm': 1.0224145650863647, 'learning_rate': 0.0006834259259259259, 'epoch': 0.95}
Step 3430: {'loss': 0.6849, 'grad_norm': 1.4798611402511597, 'learning_rate': 0.0006825000000000001, 'epoch': 0.9527777777777777}
Step 3440: {'loss': 0.6427, 'grad_norm': 0.9009472131729126, 'learning_rate': 0.0006815740740740741, 'epoch': 0.9555555555555556}
Step 3450: {'loss': 0.6038, 'grad_norm': 1.8619505167007446, 'learning_rate': 0.0006806481481481481, 'epoch': 0.9583333333333334}
Step 3460: {'loss': 0.6452, 'grad_norm': 1.8377336263656616, 'learning_rate': 0.0006797222222222223, 'epoch': 0.9611111111111111}
Step 3470: {'loss': 0.6389, 'grad_norm': 1.7680524587631226, 'learning_rate': 0.0006787962962962964, 'epoch': 0.9638888888888889}
Step 3480: {'loss': 0.622, 'grad_norm': 1.0071823596954346, 'learning_rate': 0.0006778703703703703, 'epoch': 0.9666666666666667}
Step 3490: {'loss': 0.6812, 'grad_norm': 1.3404487371444702, 'learning_rate': 0.0006769444444444444, 'epoch': 0.9694444444444444}
Step 3500: {'loss': 0.7075, 'grad_norm': 1.4100037813186646, 'learning_rate': 0.0006760185185185185, 'epoch': 0.9722222222222222}
Step 3510: {'loss': 0.6049, 'grad_norm': 2.113448143005371, 'learning_rate': 0.0006750925925925926, 'epoch': 0.975}
Step 3520: {'loss': 0.7256, 'grad_norm': 1.2901453971862793, 'learning_rate': 0.0006741666666666667, 'epoch': 0.9777777777777777}
Step 3530: {'loss': 0.6882, 'grad_norm': 1.6216663122177124, 'learning_rate': 0.0006732407407407407, 'epoch': 0.9805555555555555}
Step 3540: {'loss': 0.6373, 'grad_norm': 1.2781546115875244, 'learning_rate': 0.0006723148148148148, 'epoch': 0.9833333333333333}
Step 3550: {'loss': 0.7085, 'grad_norm': 1.3704193830490112, 'learning_rate': 0.0006713888888888889, 'epoch': 0.9861111111111112}
Step 3560: {'loss': 0.637, 'grad_norm': 1.6914161443710327, 'learning_rate': 0.000670462962962963, 'epoch': 0.9888888888888889}
Step 3570: {'loss': 0.6349, 'grad_norm': 1.4481227397918701, 'learning_rate': 0.0006695370370370371, 'epoch': 0.9916666666666667}
Step 3580: {'loss': 0.6684, 'grad_norm': 1.677519679069519, 'learning_rate': 0.0006686111111111111, 'epoch': 0.9944444444444445}
Step 3590: {'loss': 0.6459, 'grad_norm': 1.0389050245285034, 'learning_rate': 0.0006676851851851852, 'epoch': 0.9972222222222222}
Step 3600: {'loss': 0.6516, 'grad_norm': 1.2031960487365723, 'learning_rate': 0.0006667592592592593, 'epoch': 1.0}
Step 3610: {'loss': 0.6755, 'grad_norm': 1.0949662923812866, 'learning_rate': 0.0006658333333333334, 'epoch': 1.0027777777777778}
Step 3620: {'loss': 0.636, 'grad_norm': 1.216007947921753, 'learning_rate': 0.0006649074074074074, 'epoch': 1.0055555555555555}
Step 3630: {'loss': 0.6851, 'grad_norm': 1.3485691547393799, 'learning_rate': 0.0006639814814814815, 'epoch': 1.0083333333333333}
Step 3640: {'loss': 0.632, 'grad_norm': 1.4776318073272705, 'learning_rate': 0.0006630555555555555, 'epoch': 1.011111111111111}
Step 3650: {'loss': 0.6986, 'grad_norm': 1.6774671077728271, 'learning_rate': 0.0006621296296296296, 'epoch': 1.0138888888888888}
Step 3660: {'loss': 0.6693, 'grad_norm': 0.918151319026947, 'learning_rate': 0.0006612037037037038, 'epoch': 1.0166666666666666}
Step 3670: {'loss': 0.6666, 'grad_norm': 1.0689148902893066, 'learning_rate': 0.0006602777777777778, 'epoch': 1.0194444444444444}
Step 3680: {'loss': 0.638, 'grad_norm': 1.1919591426849365, 'learning_rate': 0.0006593518518518518, 'epoch': 1.0222222222222221}
Step 3690: {'loss': 0.6042, 'grad_norm': 1.369218349456787, 'learning_rate': 0.000658425925925926, 'epoch': 1.025}
Step 3700: {'loss': 0.6202, 'grad_norm': 1.1242464780807495, 'learning_rate': 0.0006575, 'epoch': 1.0277777777777777}
Step 3710: {'loss': 0.6731, 'grad_norm': 1.1064207553863525, 'learning_rate': 0.0006565740740740741, 'epoch': 1.0305555555555554}
Step 3720: {'loss': 0.6601, 'grad_norm': 1.175451636314392, 'learning_rate': 0.0006556481481481483, 'epoch': 1.0333333333333334}
Step 3730: {'loss': 0.6662, 'grad_norm': 1.2408382892608643, 'learning_rate': 0.0006547222222222222, 'epoch': 1.0361111111111112}
Step 3740: {'loss': 0.6789, 'grad_norm': 1.3700857162475586, 'learning_rate': 0.0006537962962962963, 'epoch': 1.038888888888889}
Step 3750: {'loss': 0.6438, 'grad_norm': 1.9524215459823608, 'learning_rate': 0.0006528703703703704, 'epoch': 1.0416666666666667}
Step 3760: {'loss': 0.6374, 'grad_norm': 1.4406211376190186, 'learning_rate': 0.0006519444444444445, 'epoch': 1.0444444444444445}
Step 3770: {'loss': 0.5973, 'grad_norm': 0.8916115760803223, 'learning_rate': 0.0006510185185185185, 'epoch': 1.0472222222222223}
Step 3780: {'loss': 0.7077, 'grad_norm': 1.496550440788269, 'learning_rate': 0.0006500925925925925, 'epoch': 1.05}
Step 3790: {'loss': 0.6457, 'grad_norm': 1.1807738542556763, 'learning_rate': 0.0006491666666666667, 'epoch': 1.0527777777777778}
Step 3800: {'loss': 0.6203, 'grad_norm': 1.1905689239501953, 'learning_rate': 0.0006482407407407408, 'epoch': 1.0555555555555556}
Step 3810: {'loss': 0.6425, 'grad_norm': 0.7279021739959717, 'learning_rate': 0.0006473148148148148, 'epoch': 1.0583333333333333}
Step 3820: {'loss': 0.6578, 'grad_norm': 1.362952470779419, 'learning_rate': 0.000646388888888889, 'epoch': 1.0611111111111111}
Step 3830: {'loss': 0.6557, 'grad_norm': 1.116424798965454, 'learning_rate': 0.000645462962962963, 'epoch': 1.0638888888888889}
Step 3840: {'loss': 0.5938, 'grad_norm': 1.2855061292648315, 'learning_rate': 0.000644537037037037, 'epoch': 1.0666666666666667}
Step 3850: {'loss': 0.6101, 'grad_norm': 1.1853166818618774, 'learning_rate': 0.0006436111111111112, 'epoch': 1.0694444444444444}
Step 3860: {'loss': 0.666, 'grad_norm': 1.1835908889770508, 'learning_rate': 0.0006426851851851852, 'epoch': 1.0722222222222222}
Step 3870: {'loss': 0.6124, 'grad_norm': 2.7055869102478027, 'learning_rate': 0.0006417592592592592, 'epoch': 1.075}
Step 3880: {'loss': 0.7118, 'grad_norm': 1.2004244327545166, 'learning_rate': 0.0006408333333333334, 'epoch': 1.0777777777777777}
Step 3890: {'loss': 0.63, 'grad_norm': 1.3110681772232056, 'learning_rate': 0.0006399074074074074, 'epoch': 1.0805555555555555}
Step 3900: {'loss': 0.6726, 'grad_norm': 1.33157479763031, 'learning_rate': 0.0006389814814814815, 'epoch': 1.0833333333333333}
Step 3910: {'loss': 0.65, 'grad_norm': 1.3719817399978638, 'learning_rate': 0.0006380555555555557, 'epoch': 1.086111111111111}
Step 3920: {'loss': 0.6562, 'grad_norm': 1.3150808811187744, 'learning_rate': 0.0006371296296296296, 'epoch': 1.0888888888888888}
Step 3930: {'loss': 0.6078, 'grad_norm': 1.3671753406524658, 'learning_rate': 0.0006362037037037037, 'epoch': 1.0916666666666666}
Step 3940: {'loss': 0.6044, 'grad_norm': 2.0018515586853027, 'learning_rate': 0.0006352777777777778, 'epoch': 1.0944444444444446}
Step 3950: {'loss': 0.6305, 'grad_norm': 1.4806365966796875, 'learning_rate': 0.0006343518518518519, 'epoch': 1.0972222222222223}
Step 3960: {'loss': 0.6549, 'grad_norm': 1.9109967947006226, 'learning_rate': 0.000633425925925926, 'epoch': 1.1}
Step 3970: {'loss': 0.6007, 'grad_norm': 1.1914958953857422, 'learning_rate': 0.0006324999999999999, 'epoch': 1.1027777777777779}
Step 3980: {'loss': 0.6011, 'grad_norm': 1.2431879043579102, 'learning_rate': 0.0006315740740740741, 'epoch': 1.1055555555555556}
Step 3990: {'loss': 0.6198, 'grad_norm': 1.2737982273101807, 'learning_rate': 0.0006306481481481482, 'epoch': 1.1083333333333334}
Step 4000: {'loss': 0.6382, 'grad_norm': 1.2255600690841675, 'learning_rate': 0.0006297222222222222, 'epoch': 1.1111111111111112}
Step 4010: {'loss': 0.6099, 'grad_norm': 1.5150970220565796, 'learning_rate': 0.0006287962962962964, 'epoch': 1.113888888888889}
Step 4020: {'loss': 0.6034, 'grad_norm': 1.5407087802886963, 'learning_rate': 0.0006278703703703704, 'epoch': 1.1166666666666667}
Step 4030: {'loss': 0.6961, 'grad_norm': 1.237778663635254, 'learning_rate': 0.0006269444444444444, 'epoch': 1.1194444444444445}
Step 4040: {'loss': 0.6151, 'grad_norm': 1.3995641469955444, 'learning_rate': 0.0006260185185185186, 'epoch': 1.1222222222222222}
Step 4050: {'loss': 0.5755, 'grad_norm': 1.1555662155151367, 'learning_rate': 0.0006250925925925927, 'epoch': 1.125}
Step 4060: {'loss': 0.6391, 'grad_norm': 2.188293218612671, 'learning_rate': 0.0006241666666666666, 'epoch': 1.1277777777777778}
Step 4070: {'loss': 0.6702, 'grad_norm': 1.5808329582214355, 'learning_rate': 0.0006232407407407407, 'epoch': 1.1305555555555555}
Step 4080: {'loss': 0.623, 'grad_norm': 1.2173690795898438, 'learning_rate': 0.0006223148148148148, 'epoch': 1.1333333333333333}
Step 4090: {'loss': 0.6464, 'grad_norm': 1.66629159450531, 'learning_rate': 0.0006213888888888889, 'epoch': 1.136111111111111}
Step 4100: {'loss': 0.6035, 'grad_norm': 1.3541492223739624, 'learning_rate': 0.000620462962962963, 'epoch': 1.1388888888888888}
Step 4110: {'loss': 0.6687, 'grad_norm': 1.1475354433059692, 'learning_rate': 0.000619537037037037, 'epoch': 1.1416666666666666}
Step 4120: {'loss': 0.6006, 'grad_norm': 1.2443652153015137, 'learning_rate': 0.0006186111111111111, 'epoch': 1.1444444444444444}
Step 4130: {'loss': 0.6649, 'grad_norm': 1.2524471282958984, 'learning_rate': 0.0006176851851851852, 'epoch': 1.1472222222222221}
Step 4140: {'loss': 0.6925, 'grad_norm': 1.0665634870529175, 'learning_rate': 0.0006167592592592593, 'epoch': 1.15}
Step 4150: {'loss': 0.6145, 'grad_norm': 1.0297927856445312, 'learning_rate': 0.0006158333333333334, 'epoch': 1.1527777777777777}
Step 4160: {'loss': 0.6105, 'grad_norm': 0.8859186172485352, 'learning_rate': 0.0006149074074074074, 'epoch': 1.1555555555555554}
Step 4170: {'loss': 0.6317, 'grad_norm': 1.5106397867202759, 'learning_rate': 0.0006139814814814815, 'epoch': 1.1583333333333332}
Step 4180: {'loss': 0.6253, 'grad_norm': 1.1607599258422852, 'learning_rate': 0.0006130555555555556, 'epoch': 1.1611111111111112}
Step 4190: {'loss': 0.6454, 'grad_norm': 1.0930323600769043, 'learning_rate': 0.0006121296296296296, 'epoch': 1.163888888888889}
Step 4200: {'loss': 0.6662, 'grad_norm': 0.94362473487854, 'learning_rate': 0.0006112037037037038, 'epoch': 1.1666666666666667}
Step 4210: {'loss': 0.631, 'grad_norm': 1.4389606714248657, 'learning_rate': 0.0006102777777777778, 'epoch': 1.1694444444444445}
Step 4220: {'loss': 0.6722, 'grad_norm': 1.4806398153305054, 'learning_rate': 0.0006093518518518518, 'epoch': 1.1722222222222223}
Step 4230: {'loss': 0.675, 'grad_norm': 1.7581837177276611, 'learning_rate': 0.0006084259259259259, 'epoch': 1.175}
Step 4240: {'loss': 0.5954, 'grad_norm': 1.4723091125488281, 'learning_rate': 0.0006075000000000001, 'epoch': 1.1777777777777778}
Step 4250: {'loss': 0.6517, 'grad_norm': 1.0572891235351562, 'learning_rate': 0.000606574074074074, 'epoch': 1.1805555555555556}
Step 4260: {'loss': 0.6727, 'grad_norm': 1.6559584140777588, 'learning_rate': 0.0006056481481481481, 'epoch': 1.1833333333333333}
Step 4270: {'loss': 0.645, 'grad_norm': 1.9350147247314453, 'learning_rate': 0.0006047222222222223, 'epoch': 1.1861111111111111}
Step 4280: {'loss': 0.6155, 'grad_norm': 1.6450608968734741, 'learning_rate': 0.0006037962962962963, 'epoch': 1.1888888888888889}
Step 4290: {'loss': 0.7071, 'grad_norm': 1.4729055166244507, 'learning_rate': 0.0006028703703703704, 'epoch': 1.1916666666666667}
Step 4300: {'loss': 0.6216, 'grad_norm': 1.2167421579360962, 'learning_rate': 0.0006019444444444444, 'epoch': 1.1944444444444444}
Step 4310: {'loss': 0.6441, 'grad_norm': 1.494011640548706, 'learning_rate': 0.0006010185185185185, 'epoch': 1.1972222222222222}
Step 4320: {'loss': 0.6518, 'grad_norm': 1.0775755643844604, 'learning_rate': 0.0006000925925925926, 'epoch': 1.2}
Step 4330: {'loss': 0.6318, 'grad_norm': 0.9553303718566895, 'learning_rate': 0.0005991666666666667, 'epoch': 1.2027777777777777}
Step 4340: {'loss': 0.649, 'grad_norm': 1.2435874938964844, 'learning_rate': 0.0005982407407407408, 'epoch': 1.2055555555555555}
Step 4350: {'loss': 0.6477, 'grad_norm': 1.3986327648162842, 'learning_rate': 0.0005973148148148148, 'epoch': 1.2083333333333333}
Step 4360: {'loss': 0.6339, 'grad_norm': 1.1359773874282837, 'learning_rate': 0.0005963888888888889, 'epoch': 1.211111111111111}
Step 4370: {'loss': 0.6367, 'grad_norm': 2.3266565799713135, 'learning_rate': 0.000595462962962963, 'epoch': 1.2138888888888888}
Step 4380: {'loss': 0.6018, 'grad_norm': 1.6824830770492554, 'learning_rate': 0.0005945370370370371, 'epoch': 1.2166666666666668}
Step 4390: {'loss': 0.5712, 'grad_norm': 3.3223350048065186, 'learning_rate': 0.000593611111111111, 'epoch': 1.2194444444444446}
Step 4400: {'loss': 0.63, 'grad_norm': 0.950705885887146, 'learning_rate': 0.0005926851851851852, 'epoch': 1.2222222222222223}
Step 4410: {'loss': 0.6252, 'grad_norm': 0.8783802390098572, 'learning_rate': 0.0005917592592592592, 'epoch': 1.225}
Step 4420: {'loss': 0.6756, 'grad_norm': 1.8226262331008911, 'learning_rate': 0.0005908333333333333, 'epoch': 1.2277777777777779}
Step 4430: {'loss': 0.6232, 'grad_norm': 1.771559476852417, 'learning_rate': 0.0005899074074074075, 'epoch': 1.2305555555555556}
Step 4440: {'loss': 0.6377, 'grad_norm': 1.2852822542190552, 'learning_rate': 0.0005889814814814815, 'epoch': 1.2333333333333334}
Step 4450: {'loss': 0.6428, 'grad_norm': 1.7000699043273926, 'learning_rate': 0.0005880555555555555, 'epoch': 1.2361111111111112}
Step 4460: {'loss': 0.6259, 'grad_norm': 0.94922935962677, 'learning_rate': 0.0005871296296296297, 'epoch': 1.238888888888889}
Step 4470: {'loss': 0.6309, 'grad_norm': 1.4701311588287354, 'learning_rate': 0.0005862037037037037, 'epoch': 1.2416666666666667}
Step 4480: {'loss': 0.6195, 'grad_norm': 1.1040515899658203, 'learning_rate': 0.0005852777777777778, 'epoch': 1.2444444444444445}
Step 4490: {'loss': 0.6793, 'grad_norm': 1.2522687911987305, 'learning_rate': 0.000584351851851852, 'epoch': 1.2472222222222222}
Step 4500: {'loss': 0.6613, 'grad_norm': 1.6657127141952515, 'learning_rate': 0.0005834259259259259, 'epoch': 1.25}
Step 4510: {'loss': 0.6135, 'grad_norm': 1.3444485664367676, 'learning_rate': 0.0005825, 'epoch': 1.2527777777777778}
Step 4520: {'loss': 0.6454, 'grad_norm': 1.1330387592315674, 'learning_rate': 0.000581574074074074, 'epoch': 1.2555555555555555}
Step 4530: {'loss': 0.6028, 'grad_norm': 1.204317569732666, 'learning_rate': 0.0005806481481481482, 'epoch': 1.2583333333333333}
Step 4540: {'loss': 0.6748, 'grad_norm': 1.530368447303772, 'learning_rate': 0.0005797222222222222, 'epoch': 1.261111111111111}
Step 4550: {'loss': 0.6583, 'grad_norm': 0.9793873429298401, 'learning_rate': 0.0005787962962962962, 'epoch': 1.2638888888888888}
Step 4560: {'loss': 0.662, 'grad_norm': 0.9574392437934875, 'learning_rate': 0.0005778703703703704, 'epoch': 1.2666666666666666}
Step 4570: {'loss': 0.6219, 'grad_norm': 2.2470643520355225, 'learning_rate': 0.0005769444444444445, 'epoch': 1.2694444444444444}
Step 4580: {'loss': 0.6223, 'grad_norm': 0.8893097043037415, 'learning_rate': 0.0005760185185185185, 'epoch': 1.2722222222222221}
Step 4590: {'loss': 0.653, 'grad_norm': 1.0745965242385864, 'learning_rate': 0.0005750925925925926, 'epoch': 1.275}
Step 4600: {'loss': 0.674, 'grad_norm': 1.2038564682006836, 'learning_rate': 0.0005741666666666667, 'epoch': 1.2777777777777777}
Step 4610: {'loss': 0.6525, 'grad_norm': 1.2475197315216064, 'learning_rate': 0.0005732407407407407, 'epoch': 1.2805555555555554}
Step 4620: {'loss': 0.6531, 'grad_norm': 1.5962048768997192, 'learning_rate': 0.0005723148148148149, 'epoch': 1.2833333333333332}
Step 4630: {'loss': 0.6197, 'grad_norm': 1.0925534963607788, 'learning_rate': 0.0005713888888888889, 'epoch': 1.286111111111111}
Step 4640: {'loss': 0.6005, 'grad_norm': 1.0787835121154785, 'learning_rate': 0.0005704629629629629, 'epoch': 1.2888888888888888}
Step 4650: {'loss': 0.5835, 'grad_norm': 1.1671780347824097, 'learning_rate': 0.0005695370370370371, 'epoch': 1.2916666666666667}
Step 4660: {'loss': 0.6302, 'grad_norm': 1.625551700592041, 'learning_rate': 0.0005686111111111111, 'epoch': 1.2944444444444445}
Step 4670: {'loss': 0.6682, 'grad_norm': 0.9863426685333252, 'learning_rate': 0.0005676851851851852, 'epoch': 1.2972222222222223}
Step 4680: {'loss': 0.5648, 'grad_norm': 1.270480751991272, 'learning_rate': 0.0005667592592592593, 'epoch': 1.3}
Step 4690: {'loss': 0.6252, 'grad_norm': 1.1358433961868286, 'learning_rate': 0.0005658333333333333, 'epoch': 1.3027777777777778}
Step 4700: {'loss': 0.6648, 'grad_norm': 2.0493662357330322, 'learning_rate': 0.0005649074074074074, 'epoch': 1.3055555555555556}
Step 4710: {'loss': 0.6236, 'grad_norm': 1.378881812095642, 'learning_rate': 0.0005639814814814815, 'epoch': 1.3083333333333333}
Step 4720: {'loss': 0.6335, 'grad_norm': 1.2812445163726807, 'learning_rate': 0.0005630555555555556, 'epoch': 1.3111111111111111}
Step 4730: {'loss': 0.6603, 'grad_norm': 1.1342482566833496, 'learning_rate': 0.0005621296296296297, 'epoch': 1.3138888888888889}
Step 4740: {'loss': 0.6752, 'grad_norm': 1.044179081916809, 'learning_rate': 0.0005612037037037036, 'epoch': 1.3166666666666667}
Step 4750: {'loss': 0.6429, 'grad_norm': 1.734918236732483, 'learning_rate': 0.0005602777777777778, 'epoch': 1.3194444444444444}
Step 4760: {'loss': 0.6249, 'grad_norm': 1.1071889400482178, 'learning_rate': 0.0005593518518518519, 'epoch': 1.3222222222222222}
Step 4770: {'loss': 0.668, 'grad_norm': 1.3336784839630127, 'learning_rate': 0.0005584259259259259, 'epoch': 1.325}
Step 4780: {'loss': 0.5929, 'grad_norm': 1.0368561744689941, 'learning_rate': 0.0005575, 'epoch': 1.3277777777777777}
Step 4790: {'loss': 0.6299, 'grad_norm': 0.9803788661956787, 'learning_rate': 0.0005565740740740741, 'epoch': 1.3305555555555555}
Step 4800: {'loss': 0.618, 'grad_norm': 1.0584274530410767, 'learning_rate': 0.0005556481481481481, 'epoch': 1.3333333333333333}
Step 4810: {'loss': 0.592, 'grad_norm': 0.8207072615623474, 'learning_rate': 0.0005547222222222223, 'epoch': 1.3361111111111112}
Step 4820: {'loss': 0.6413, 'grad_norm': 3.099632978439331, 'learning_rate': 0.0005537962962962964, 'epoch': 1.338888888888889}
Step 4830: {'loss': 0.7003, 'grad_norm': 1.1990149021148682, 'learning_rate': 0.0005528703703703703, 'epoch': 1.3416666666666668}
Step 4840: {'loss': 0.6385, 'grad_norm': 1.5556424856185913, 'learning_rate': 0.0005519444444444444, 'epoch': 1.3444444444444446}
Step 4850: {'loss': 0.6461, 'grad_norm': 1.0188194513320923, 'learning_rate': 0.0005510185185185185, 'epoch': 1.3472222222222223}
Step 4860: {'loss': 0.7225, 'grad_norm': 1.189072847366333, 'learning_rate': 0.0005500925925925926, 'epoch': 1.35}
Step 4870: {'loss': 0.5623, 'grad_norm': 1.0552164316177368, 'learning_rate': 0.0005491666666666667, 'epoch': 1.3527777777777779}
Step 4880: {'loss': 0.6014, 'grad_norm': 1.666092872619629, 'learning_rate': 0.0005482407407407407, 'epoch': 1.3555555555555556}
Step 4890: {'loss': 0.6687, 'grad_norm': 1.952885389328003, 'learning_rate': 0.0005473148148148148, 'epoch': 1.3583333333333334}
Step 4900: {'loss': 0.6887, 'grad_norm': 1.557869553565979, 'learning_rate': 0.0005463888888888889, 'epoch': 1.3611111111111112}
Step 4910: {'loss': 0.6135, 'grad_norm': 0.979718029499054, 'learning_rate': 0.000545462962962963, 'epoch': 1.363888888888889}
Step 4920: {'loss': 0.5834, 'grad_norm': 1.2930171489715576, 'learning_rate': 0.0005445370370370371, 'epoch': 1.3666666666666667}
Step 4930: {'loss': 0.5869, 'grad_norm': 1.4174604415893555, 'learning_rate': 0.0005436111111111111, 'epoch': 1.3694444444444445}
Step 4940: {'loss': 0.6376, 'grad_norm': 1.2731256484985352, 'learning_rate': 0.0005426851851851852, 'epoch': 1.3722222222222222}
Step 4950: {'loss': 0.7121, 'grad_norm': 1.2344168424606323, 'learning_rate': 0.0005417592592592593, 'epoch': 1.375}
Step 4960: {'loss': 0.6345, 'grad_norm': 1.51567804813385, 'learning_rate': 0.0005408333333333334, 'epoch': 1.3777777777777778}
Step 4970: {'loss': 0.6387, 'grad_norm': 1.1430546045303345, 'learning_rate': 0.0005399074074074073, 'epoch': 1.3805555555555555}
Step 4980: {'loss': 0.6503, 'grad_norm': 1.0850847959518433, 'learning_rate': 0.0005389814814814815, 'epoch': 1.3833333333333333}
Step 4990: {'loss': 0.6667, 'grad_norm': 1.2899963855743408, 'learning_rate': 0.0005380555555555555, 'epoch': 1.386111111111111}
Step 5000: {'loss': 0.6205, 'grad_norm': 0.9699293375015259, 'learning_rate': 0.0005371296296296296, 'epoch': 1.3888888888888888}
Step 5010: {'loss': 0.6423, 'grad_norm': 0.9738708734512329, 'learning_rate': 0.0005362037037037038, 'epoch': 1.3916666666666666}
Step 5020: {'loss': 0.6563, 'grad_norm': 1.3501920700073242, 'learning_rate': 0.0005352777777777777, 'epoch': 1.3944444444444444}
Step 5030: {'loss': 0.6407, 'grad_norm': 0.8994050025939941, 'learning_rate': 0.0005343518518518518, 'epoch': 1.3972222222222221}
Step 5040: {'loss': 0.6139, 'grad_norm': 1.1027992963790894, 'learning_rate': 0.000533425925925926, 'epoch': 1.4}
Step 5050: {'loss': 0.6117, 'grad_norm': 1.2325284481048584, 'learning_rate': 0.0005325, 'epoch': 1.4027777777777777}
Step 5060: {'loss': 0.6225, 'grad_norm': 1.3202255964279175, 'learning_rate': 0.0005315740740740741, 'epoch': 1.4055555555555554}
Step 5070: {'loss': 0.6487, 'grad_norm': 1.4014344215393066, 'learning_rate': 0.0005306481481481483, 'epoch': 1.4083333333333332}
Step 5080: {'loss': 0.596, 'grad_norm': 1.3322447538375854, 'learning_rate': 0.0005297222222222222, 'epoch': 1.411111111111111}
Step 5090: {'loss': 0.6332, 'grad_norm': 1.2056478261947632, 'learning_rate': 0.0005287962962962963, 'epoch': 1.4138888888888888}
Step 5100: {'loss': 0.6283, 'grad_norm': 0.9575320482254028, 'learning_rate': 0.0005278703703703704, 'epoch': 1.4166666666666667}
Step 5110: {'loss': 0.6823, 'grad_norm': 1.4129626750946045, 'learning_rate': 0.0005269444444444445, 'epoch': 1.4194444444444445}
Step 5120: {'loss': 0.58, 'grad_norm': 1.596574306488037, 'learning_rate': 0.0005260185185185185, 'epoch': 1.4222222222222223}
Step 5130: {'loss': 0.6498, 'grad_norm': 1.1325950622558594, 'learning_rate': 0.0005250925925925925, 'epoch': 1.425}
Step 5140: {'loss': 0.6569, 'grad_norm': 1.607810378074646, 'learning_rate': 0.0005241666666666667, 'epoch': 1.4277777777777778}
Step 5150: {'loss': 0.6334, 'grad_norm': 1.1870417594909668, 'learning_rate': 0.0005232407407407408, 'epoch': 1.4305555555555556}
Step 5160: {'loss': 0.6683, 'grad_norm': 1.3237545490264893, 'learning_rate': 0.0005223148148148148, 'epoch': 1.4333333333333333}
Step 5170: {'loss': 0.6177, 'grad_norm': 1.3862769603729248, 'learning_rate': 0.0005213888888888889, 'epoch': 1.4361111111111111}
Step 5180: {'loss': 0.6224, 'grad_norm': 1.2438386678695679, 'learning_rate': 0.000520462962962963, 'epoch': 1.4388888888888889}
Step 5190: {'loss': 0.6309, 'grad_norm': 1.2199699878692627, 'learning_rate': 0.000519537037037037, 'epoch': 1.4416666666666667}
Step 5200: {'loss': 0.5976, 'grad_norm': 1.3559138774871826, 'learning_rate': 0.0005186111111111112, 'epoch': 1.4444444444444444}
Step 5210: {'loss': 0.6092, 'grad_norm': 1.1962196826934814, 'learning_rate': 0.0005176851851851852, 'epoch': 1.4472222222222222}
Step 5220: {'loss': 0.6521, 'grad_norm': 1.3375568389892578, 'learning_rate': 0.0005167592592592592, 'epoch': 1.45}
Step 5230: {'loss': 0.6385, 'grad_norm': 1.503469705581665, 'learning_rate': 0.0005158333333333334, 'epoch': 1.4527777777777777}
Step 5240: {'loss': 0.694, 'grad_norm': 1.1899207830429077, 'learning_rate': 0.0005149074074074074, 'epoch': 1.4555555555555555}
Step 5250: {'loss': 0.6005, 'grad_norm': 1.0686609745025635, 'learning_rate': 0.0005139814814814815, 'epoch': 1.4583333333333333}
Step 5260: {'loss': 0.6364, 'grad_norm': 1.0726243257522583, 'learning_rate': 0.0005130555555555557, 'epoch': 1.4611111111111112}
Step 5270: {'loss': 0.6766, 'grad_norm': 1.3041186332702637, 'learning_rate': 0.0005121296296296296, 'epoch': 1.463888888888889}
Step 5280: {'loss': 0.5809, 'grad_norm': 1.2890193462371826, 'learning_rate': 0.0005112037037037037, 'epoch': 1.4666666666666668}
Step 5290: {'loss': 0.5963, 'grad_norm': 1.2184776067733765, 'learning_rate': 0.0005102777777777778, 'epoch': 1.4694444444444446}
Step 5300: {'loss': 0.6297, 'grad_norm': 1.268270492553711, 'learning_rate': 0.0005093518518518519, 'epoch': 1.4722222222222223}
Step 5310: {'loss': 0.6279, 'grad_norm': 1.410096287727356, 'learning_rate': 0.000508425925925926, 'epoch': 1.475}
Step 5320: {'loss': 0.6695, 'grad_norm': 1.3500616550445557, 'learning_rate': 0.0005074999999999999, 'epoch': 1.4777777777777779}
Step 5330: {'loss': 0.6724, 'grad_norm': 1.1339191198349, 'learning_rate': 0.0005065740740740741, 'epoch': 1.4805555555555556}
Step 5340: {'loss': 0.6622, 'grad_norm': 1.4479150772094727, 'learning_rate': 0.0005056481481481482, 'epoch': 1.4833333333333334}
Step 5350: {'loss': 0.6136, 'grad_norm': 1.5505503416061401, 'learning_rate': 0.0005047222222222222, 'epoch': 1.4861111111111112}
Step 5360: {'loss': 0.6139, 'grad_norm': 0.8810234069824219, 'learning_rate': 0.0005037962962962963, 'epoch': 1.488888888888889}
Step 5370: {'loss': 0.6765, 'grad_norm': 1.6600388288497925, 'learning_rate': 0.0005028703703703704, 'epoch': 1.4916666666666667}
Step 5380: {'loss': 0.613, 'grad_norm': 1.3223458528518677, 'learning_rate': 0.0005019444444444444, 'epoch': 1.4944444444444445}
Step 5390: {'loss': 0.64, 'grad_norm': 1.2784695625305176, 'learning_rate': 0.0005010185185185186, 'epoch': 1.4972222222222222}
Step 5400: {'loss': 0.6426, 'grad_norm': 1.306458592414856, 'learning_rate': 0.0005000925925925927, 'epoch': 1.5}
Step 5410: {'loss': 0.6602, 'grad_norm': 1.5362029075622559, 'learning_rate': 0.0004991666666666666, 'epoch': 1.5027777777777778}
Step 5420: {'loss': 0.6769, 'grad_norm': 1.6002612113952637, 'learning_rate': 0.0004982407407407407, 'epoch': 1.5055555555555555}
Step 5430: {'loss': 0.6258, 'grad_norm': 1.069128394126892, 'learning_rate': 0.0004973148148148148, 'epoch': 1.5083333333333333}
Step 5440: {'loss': 0.6082, 'grad_norm': 1.0959872007369995, 'learning_rate': 0.0004963888888888889, 'epoch': 1.511111111111111}
Step 5450: {'loss': 0.6869, 'grad_norm': 1.0272538661956787, 'learning_rate': 0.000495462962962963, 'epoch': 1.5138888888888888}
Step 5460: {'loss': 0.6525, 'grad_norm': 1.2869828939437866, 'learning_rate': 0.000494537037037037, 'epoch': 1.5166666666666666}
Step 5470: {'loss': 0.6665, 'grad_norm': 1.5370177030563354, 'learning_rate': 0.0004936111111111111, 'epoch': 1.5194444444444444}
Step 5480: {'loss': 0.5973, 'grad_norm': 1.2936129570007324, 'learning_rate': 0.0004926851851851852, 'epoch': 1.5222222222222221}
Step 5490: {'loss': 0.6574, 'grad_norm': 1.1365265846252441, 'learning_rate': 0.0004917592592592593, 'epoch': 1.525}
Step 5500: {'loss': 0.6502, 'grad_norm': 0.9512130618095398, 'learning_rate': 0.0004908333333333334, 'epoch': 1.5277777777777777}
Step 5510: {'loss': 0.6385, 'grad_norm': 2.040172815322876, 'learning_rate': 0.0004899074074074074, 'epoch': 1.5305555555555554}
Step 5520: {'loss': 0.6301, 'grad_norm': 1.0154917240142822, 'learning_rate': 0.0004889814814814815, 'epoch': 1.5333333333333332}
Step 5530: {'loss': 0.6228, 'grad_norm': 0.9479166269302368, 'learning_rate': 0.0004880555555555556, 'epoch': 1.536111111111111}
Step 5540: {'loss': 0.6989, 'grad_norm': 1.050968885421753, 'learning_rate': 0.0004871296296296296, 'epoch': 1.5388888888888888}
Step 5550: {'loss': 0.665, 'grad_norm': 1.8102456331253052, 'learning_rate': 0.0004862037037037037, 'epoch': 1.5416666666666665}
Step 5560: {'loss': 0.6881, 'grad_norm': 1.184043526649475, 'learning_rate': 0.0004852777777777778, 'epoch': 1.5444444444444443}
Step 5570: {'loss': 0.5947, 'grad_norm': 1.1153305768966675, 'learning_rate': 0.00048435185185185186, 'epoch': 1.5472222222222223}
Step 5580: {'loss': 0.6541, 'grad_norm': 1.6829756498336792, 'learning_rate': 0.00048342592592592594, 'epoch': 1.55}
Step 5590: {'loss': 0.5993, 'grad_norm': 1.2326221466064453, 'learning_rate': 0.0004825, 'epoch': 1.5527777777777778}
Step 5600: {'loss': 0.6549, 'grad_norm': 1.418755054473877, 'learning_rate': 0.0004815740740740741, 'epoch': 1.5555555555555556}
Step 5610: {'loss': 0.6221, 'grad_norm': 1.03130042552948, 'learning_rate': 0.0004806481481481482, 'epoch': 1.5583333333333333}
Step 5620: {'loss': 0.5729, 'grad_norm': 1.1434495449066162, 'learning_rate': 0.0004797222222222222, 'epoch': 1.5611111111111111}
Step 5630: {'loss': 0.717, 'grad_norm': 1.7343336343765259, 'learning_rate': 0.0004787962962962963, 'epoch': 1.5638888888888889}
Step 5640: {'loss': 0.6989, 'grad_norm': 1.2678909301757812, 'learning_rate': 0.0004778703703703704, 'epoch': 1.5666666666666667}
Step 5650: {'loss': 0.6445, 'grad_norm': 1.4187883138656616, 'learning_rate': 0.00047694444444444444, 'epoch': 1.5694444444444444}
Step 5660: {'loss': 0.6698, 'grad_norm': 1.9740418195724487, 'learning_rate': 0.0004760185185185185, 'epoch': 1.5722222222222222}
Step 5670: {'loss': 0.6692, 'grad_norm': 1.3594692945480347, 'learning_rate': 0.0004750925925925926, 'epoch': 1.575}
Step 5680: {'loss': 0.6503, 'grad_norm': 1.569700837135315, 'learning_rate': 0.0004741666666666667, 'epoch': 1.5777777777777777}
Step 5690: {'loss': 0.632, 'grad_norm': 1.3792084455490112, 'learning_rate': 0.00047324074074074076, 'epoch': 1.5805555555555557}
Step 5700: {'loss': 0.5977, 'grad_norm': 1.1576677560806274, 'learning_rate': 0.0004723148148148148, 'epoch': 1.5833333333333335}
Step 5710: {'loss': 0.5605, 'grad_norm': 1.4038829803466797, 'learning_rate': 0.0004713888888888889, 'epoch': 1.5861111111111112}
Step 5720: {'loss': 0.6127, 'grad_norm': 1.9134405851364136, 'learning_rate': 0.000470462962962963, 'epoch': 1.588888888888889}
Step 5730: {'loss': 0.664, 'grad_norm': 1.1903213262557983, 'learning_rate': 0.000469537037037037, 'epoch': 1.5916666666666668}
Step 5740: {'loss': 0.6271, 'grad_norm': 1.5089229345321655, 'learning_rate': 0.0004686111111111111, 'epoch': 1.5944444444444446}
Step 5750: {'loss': 0.5839, 'grad_norm': 0.883469820022583, 'learning_rate': 0.00046768518518518524, 'epoch': 1.5972222222222223}
Step 5760: {'loss': 0.6225, 'grad_norm': 0.9920485615730286, 'learning_rate': 0.00046675925925925926, 'epoch': 1.6}
Step 5770: {'loss': 0.6533, 'grad_norm': 1.5872526168823242, 'learning_rate': 0.00046583333333333334, 'epoch': 1.6027777777777779}
Step 5780: {'loss': 0.6218, 'grad_norm': 1.4652255773544312, 'learning_rate': 0.00046490740740740737, 'epoch': 1.6055555555555556}
Step 5790: {'loss': 0.6332, 'grad_norm': 1.3897022008895874, 'learning_rate': 0.0004639814814814815, 'epoch': 1.6083333333333334}
Step 5800: {'loss': 0.6299, 'grad_norm': 0.9284957051277161, 'learning_rate': 0.0004630555555555556, 'epoch': 1.6111111111111112}
Step 5810: {'loss': 0.6608, 'grad_norm': 1.2288365364074707, 'learning_rate': 0.0004621296296296296, 'epoch': 1.613888888888889}
Step 5820: {'loss': 0.6242, 'grad_norm': 1.495221734046936, 'learning_rate': 0.00046120370370370374, 'epoch': 1.6166666666666667}
Step 5830: {'loss': 0.6632, 'grad_norm': 1.374103307723999, 'learning_rate': 0.00046027777777777777, 'epoch': 1.6194444444444445}
Step 5840: {'loss': 0.724, 'grad_norm': 2.298738479614258, 'learning_rate': 0.00045935185185185185, 'epoch': 1.6222222222222222}
Step 5850: {'loss': 0.6631, 'grad_norm': 0.921551525592804, 'learning_rate': 0.00045842592592592593, 'epoch': 1.625}
Step 5860: {'loss': 0.6281, 'grad_norm': 1.853840947151184, 'learning_rate': 0.0004575, 'epoch': 1.6277777777777778}
Step 5870: {'loss': 0.7097, 'grad_norm': 0.9037609100341797, 'learning_rate': 0.0004565740740740741, 'epoch': 1.6305555555555555}
Step 5880: {'loss': 0.6712, 'grad_norm': 1.352890968322754, 'learning_rate': 0.00045564814814814817, 'epoch': 1.6333333333333333}
Step 5890: {'loss': 0.5799, 'grad_norm': 1.1407915353775024, 'learning_rate': 0.00045472222222222225, 'epoch': 1.636111111111111}
Step 5900: {'loss': 0.6742, 'grad_norm': 1.1459559202194214, 'learning_rate': 0.0004537962962962963, 'epoch': 1.6388888888888888}
Step 5910: {'loss': 0.6068, 'grad_norm': 1.1903717517852783, 'learning_rate': 0.00045287037037037035, 'epoch': 1.6416666666666666}
Step 5920: {'loss': 0.6506, 'grad_norm': 2.019268274307251, 'learning_rate': 0.00045194444444444443, 'epoch': 1.6444444444444444}
Step 5930: {'loss': 0.6969, 'grad_norm': 1.865029215812683, 'learning_rate': 0.00045101851851851857, 'epoch': 1.6472222222222221}
Step 5940: {'loss': 0.6449, 'grad_norm': 1.1194225549697876, 'learning_rate': 0.0004500925925925926, 'epoch': 1.65}
Step 5950: {'loss': 0.6661, 'grad_norm': 1.3754849433898926, 'learning_rate': 0.00044916666666666667, 'epoch': 1.6527777777777777}
Step 5960: {'loss': 0.5478, 'grad_norm': 1.5371098518371582, 'learning_rate': 0.00044824074074074075, 'epoch': 1.6555555555555554}
Step 5970: {'loss': 0.6473, 'grad_norm': 1.3511481285095215, 'learning_rate': 0.00044731481481481483, 'epoch': 1.6583333333333332}
Step 5980: {'loss': 0.655, 'grad_norm': 1.2288727760314941, 'learning_rate': 0.0004463888888888889, 'epoch': 1.661111111111111}
Step 5990: {'loss': 0.6001, 'grad_norm': 1.3490121364593506, 'learning_rate': 0.00044546296296296293, 'epoch': 1.6638888888888888}
Step 6000: {'loss': 0.6786, 'grad_norm': 1.338934063911438, 'learning_rate': 0.00044453703703703707, 'epoch': 1.6666666666666665}
Step 6010: {'loss': 0.6581, 'grad_norm': 5.471953868865967, 'learning_rate': 0.00044361111111111115, 'epoch': 1.6694444444444443}
Step 6020: {'loss': 0.6461, 'grad_norm': 1.2771670818328857, 'learning_rate': 0.0004426851851851852, 'epoch': 1.6722222222222223}
Step 6030: {'loss': 0.6348, 'grad_norm': 1.6344493627548218, 'learning_rate': 0.00044175925925925925, 'epoch': 1.675}
Step 6040: {'loss': 0.6591, 'grad_norm': 1.0875474214553833, 'learning_rate': 0.0004408333333333334, 'epoch': 1.6777777777777778}
Step 6050: {'loss': 0.6172, 'grad_norm': 1.0554929971694946, 'learning_rate': 0.0004399074074074074, 'epoch': 1.6805555555555556}
Step 6060: {'loss': 0.6652, 'grad_norm': 1.0866035223007202, 'learning_rate': 0.0004389814814814815, 'epoch': 1.6833333333333333}
Step 6070: {'loss': 0.5819, 'grad_norm': 1.2713185548782349, 'learning_rate': 0.0004380555555555555, 'epoch': 1.6861111111111111}
Step 6080: {'loss': 0.7037, 'grad_norm': 1.7172192335128784, 'learning_rate': 0.00043712962962962965, 'epoch': 1.6888888888888889}
Step 6090: {'loss': 0.6249, 'grad_norm': 0.7939260005950928, 'learning_rate': 0.00043620370370370373, 'epoch': 1.6916666666666667}
Step 6100: {'loss': 0.5685, 'grad_norm': 1.3356534242630005, 'learning_rate': 0.00043527777777777776, 'epoch': 1.6944444444444444}
Step 6110: {'loss': 0.5941, 'grad_norm': 1.0733917951583862, 'learning_rate': 0.0004343518518518519, 'epoch': 1.6972222222222222}
Step 6120: {'loss': 0.6482, 'grad_norm': 1.1695224046707153, 'learning_rate': 0.00043342592592592597, 'epoch': 1.7}
Step 6130: {'loss': 0.642, 'grad_norm': 1.3096442222595215, 'learning_rate': 0.0004325, 'epoch': 1.7027777777777777}
Step 6140: {'loss': 0.6192, 'grad_norm': 1.1511889696121216, 'learning_rate': 0.0004315740740740741, 'epoch': 1.7055555555555557}
Step 6150: {'loss': 0.6465, 'grad_norm': 1.697863221168518, 'learning_rate': 0.00043064814814814816, 'epoch': 1.7083333333333335}
Step 6160: {'loss': 0.6104, 'grad_norm': 3.6498003005981445, 'learning_rate': 0.00042972222222222224, 'epoch': 1.7111111111111112}
Step 6170: {'loss': 0.5879, 'grad_norm': 1.257453203201294, 'learning_rate': 0.0004287962962962963, 'epoch': 1.713888888888889}
Step 6180: {'loss': 0.6741, 'grad_norm': 1.520135760307312, 'learning_rate': 0.00042787037037037034, 'epoch': 1.7166666666666668}
Step 6190: {'loss': 0.6526, 'grad_norm': 1.1981055736541748, 'learning_rate': 0.0004269444444444445, 'epoch': 1.7194444444444446}
Step 6200: {'loss': 0.6547, 'grad_norm': 1.882363200187683, 'learning_rate': 0.00042601851851851855, 'epoch': 1.7222222222222223}
Step 6210: {'loss': 0.6737, 'grad_norm': 1.53282630443573, 'learning_rate': 0.0004250925925925926, 'epoch': 1.725}
Step 6220: {'loss': 0.6523, 'grad_norm': 0.8569009900093079, 'learning_rate': 0.0004241666666666667, 'epoch': 1.7277777777777779}
Step 6230: {'loss': 0.584, 'grad_norm': 1.5385260581970215, 'learning_rate': 0.00042324074074074074, 'epoch': 1.7305555555555556}
Step 6240: {'loss': 0.6982, 'grad_norm': 1.4815691709518433, 'learning_rate': 0.0004223148148148148, 'epoch': 1.7333333333333334}
Step 6250: {'loss': 0.58, 'grad_norm': 1.3977519273757935, 'learning_rate': 0.0004213888888888889, 'epoch': 1.7361111111111112}
Step 6260: {'loss': 0.6244, 'grad_norm': 1.372273564338684, 'learning_rate': 0.000420462962962963, 'epoch': 1.738888888888889}
Step 6270: {'loss': 0.628, 'grad_norm': 1.2553658485412598, 'learning_rate': 0.00041953703703703706, 'epoch': 1.7416666666666667}
Step 6280: {'loss': 0.6209, 'grad_norm': 1.2390503883361816, 'learning_rate': 0.0004186111111111111, 'epoch': 1.7444444444444445}
Step 6290: {'loss': 0.6726, 'grad_norm': 1.4206984043121338, 'learning_rate': 0.00041768518518518516, 'epoch': 1.7472222222222222}
Step 6300: {'loss': 0.6594, 'grad_norm': 1.684678077697754, 'learning_rate': 0.0004167592592592593, 'epoch': 1.75}
Step 6310: {'loss': 0.6345, 'grad_norm': 1.5310744047164917, 'learning_rate': 0.0004158333333333333, 'epoch': 1.7527777777777778}
Step 6320: {'loss': 0.5705, 'grad_norm': 1.3865588903427124, 'learning_rate': 0.0004149074074074074, 'epoch': 1.7555555555555555}
Step 6330: {'loss': 0.6297, 'grad_norm': 1.3849546909332275, 'learning_rate': 0.00041398148148148154, 'epoch': 1.7583333333333333}
Step 6340: {'loss': 0.6439, 'grad_norm': 1.1700255870819092, 'learning_rate': 0.00041305555555555556, 'epoch': 1.761111111111111}
Step 6350: {'loss': 0.6176, 'grad_norm': 1.166310429573059, 'learning_rate': 0.00041212962962962964, 'epoch': 1.7638888888888888}
Step 6360: {'loss': 0.6488, 'grad_norm': 1.2458150386810303, 'learning_rate': 0.00041120370370370367, 'epoch': 1.7666666666666666}
Step 6370: {'loss': 0.6729, 'grad_norm': 1.4248446226119995, 'learning_rate': 0.0004102777777777778, 'epoch': 1.7694444444444444}
Step 6380: {'loss': 0.6833, 'grad_norm': 1.2657018899917603, 'learning_rate': 0.0004093518518518519, 'epoch': 1.7722222222222221}
Step 6390: {'loss': 0.6699, 'grad_norm': 1.1517449617385864, 'learning_rate': 0.0004084259259259259, 'epoch': 1.775}
Step 6400: {'loss': 0.6638, 'grad_norm': 1.350710153579712, 'learning_rate': 0.0004075, 'epoch': 1.7777777777777777}
Step 6410: {'loss': 0.7211, 'grad_norm': 1.4907373189926147, 'learning_rate': 0.0004065740740740741, 'epoch': 1.7805555555555554}
Step 6420: {'loss': 0.6414, 'grad_norm': 0.9935423135757446, 'learning_rate': 0.00040564814814814814, 'epoch': 1.7833333333333332}
Step 6430: {'loss': 0.5794, 'grad_norm': 1.05710768699646, 'learning_rate': 0.0004047222222222222, 'epoch': 1.786111111111111}
Step 6440: {'loss': 0.6063, 'grad_norm': 1.3615190982818604, 'learning_rate': 0.0004037962962962963, 'epoch': 1.7888888888888888}
Step 6450: {'loss': 0.6197, 'grad_norm': 1.0307393074035645, 'learning_rate': 0.0004028703703703704, 'epoch': 1.7916666666666665}
Step 6460: {'loss': 0.6172, 'grad_norm': 1.0872735977172852, 'learning_rate': 0.00040194444444444446, 'epoch': 1.7944444444444443}
Step 6470: {'loss': 0.6479, 'grad_norm': 1.276924729347229, 'learning_rate': 0.0004010185185185185, 'epoch': 1.7972222222222223}
Step 6480: {'loss': 0.6372, 'grad_norm': 1.1090672016143799, 'learning_rate': 0.0004000925925925926, 'epoch': 1.8}
Step 6490: {'loss': 0.5952, 'grad_norm': 1.094783067703247, 'learning_rate': 0.0003991666666666667, 'epoch': 1.8027777777777778}
Step 6500: {'loss': 0.679, 'grad_norm': 1.1447769403457642, 'learning_rate': 0.00039824074074074073, 'epoch': 1.8055555555555556}
Step 6510: {'loss': 0.6191, 'grad_norm': 0.9188650846481323, 'learning_rate': 0.0003973148148148148, 'epoch': 1.8083333333333333}
Step 6520: {'loss': 0.647, 'grad_norm': 1.1548476219177246, 'learning_rate': 0.0003963888888888889, 'epoch': 1.8111111111111111}
Step 6530: {'loss': 0.6176, 'grad_norm': 1.084563136100769, 'learning_rate': 0.00039546296296296297, 'epoch': 1.8138888888888889}
Step 6540: {'loss': 0.7227, 'grad_norm': 1.8486522436141968, 'learning_rate': 0.00039453703703703705, 'epoch': 1.8166666666666667}
Step 6550: {'loss': 0.619, 'grad_norm': 1.0319504737854004, 'learning_rate': 0.0003936111111111111, 'epoch': 1.8194444444444444}
Step 6560: {'loss': 0.5824, 'grad_norm': 1.736653447151184, 'learning_rate': 0.0003926851851851852, 'epoch': 1.8222222222222222}
Step 6570: {'loss': 0.5999, 'grad_norm': 1.1866639852523804, 'learning_rate': 0.0003917592592592593, 'epoch': 1.825}
Step 6580: {'loss': 0.5995, 'grad_norm': 1.144789695739746, 'learning_rate': 0.0003908333333333333, 'epoch': 1.8277777777777777}
Step 6590: {'loss': 0.6518, 'grad_norm': 1.4919134378433228, 'learning_rate': 0.00038990740740740744, 'epoch': 1.8305555555555557}
Step 6600: {'loss': 0.6565, 'grad_norm': 1.1763641834259033, 'learning_rate': 0.00038898148148148147, 'epoch': 1.8333333333333335}
Step 6610: {'loss': 0.5939, 'grad_norm': 1.2111777067184448, 'learning_rate': 0.00038805555555555555, 'epoch': 1.8361111111111112}
Step 6620: {'loss': 0.6437, 'grad_norm': 1.2405692338943481, 'learning_rate': 0.0003871296296296297, 'epoch': 1.838888888888889}
Step 6630: {'loss': 0.6286, 'grad_norm': 1.1328492164611816, 'learning_rate': 0.0003862037037037037, 'epoch': 1.8416666666666668}
Step 6640: {'loss': 0.6547, 'grad_norm': 1.4515221118927002, 'learning_rate': 0.0003852777777777778, 'epoch': 1.8444444444444446}
Step 6650: {'loss': 0.6048, 'grad_norm': 1.674117088317871, 'learning_rate': 0.00038435185185185187, 'epoch': 1.8472222222222223}
Step 6660: {'loss': 0.6301, 'grad_norm': 0.8067117929458618, 'learning_rate': 0.00038342592592592595, 'epoch': 1.85}
Step 6670: {'loss': 0.6456, 'grad_norm': 1.4302237033843994, 'learning_rate': 0.00038250000000000003, 'epoch': 1.8527777777777779}
Step 6680: {'loss': 0.6206, 'grad_norm': 1.2557473182678223, 'learning_rate': 0.00038157407407407405, 'epoch': 1.8555555555555556}
Step 6690: {'loss': 0.6702, 'grad_norm': 1.211071491241455, 'learning_rate': 0.00038064814814814813, 'epoch': 1.8583333333333334}
Step 6700: {'loss': 0.5931, 'grad_norm': 2.696735143661499, 'learning_rate': 0.00037972222222222227, 'epoch': 1.8611111111111112}
Step 6710: {'loss': 0.6146, 'grad_norm': 1.6758818626403809, 'learning_rate': 0.0003787962962962963, 'epoch': 1.863888888888889}
Step 6720: {'loss': 0.5821, 'grad_norm': 1.4609556198120117, 'learning_rate': 0.00037787037037037037, 'epoch': 1.8666666666666667}
Step 6730: {'loss': 0.5848, 'grad_norm': 1.0272568464279175, 'learning_rate': 0.0003769444444444445, 'epoch': 1.8694444444444445}
Step 6740: {'loss': 0.621, 'grad_norm': 0.879705548286438, 'learning_rate': 0.00037601851851851853, 'epoch': 1.8722222222222222}
Step 6750: {'loss': 0.5972, 'grad_norm': 1.442870020866394, 'learning_rate': 0.0003750925925925926, 'epoch': 1.875}
Step 6760: {'loss': 0.6461, 'grad_norm': 1.289994478225708, 'learning_rate': 0.00037416666666666664, 'epoch': 1.8777777777777778}
Step 6770: {'loss': 0.6328, 'grad_norm': 1.6455967426300049, 'learning_rate': 0.00037324074074074077, 'epoch': 1.8805555555555555}
Step 6780: {'loss': 0.5765, 'grad_norm': 0.9991006255149841, 'learning_rate': 0.00037231481481481485, 'epoch': 1.8833333333333333}
Step 6790: {'loss': 0.6407, 'grad_norm': 1.2199199199676514, 'learning_rate': 0.0003713888888888889, 'epoch': 1.886111111111111}
Step 6800: {'loss': 0.6142, 'grad_norm': 0.9587016105651855, 'learning_rate': 0.00037046296296296295, 'epoch': 1.8888888888888888}
Step 6810: {'loss': 0.6002, 'grad_norm': 0.9579521417617798, 'learning_rate': 0.00036953703703703703, 'epoch': 1.8916666666666666}
Step 6820: {'loss': 0.6106, 'grad_norm': 1.5675722360610962, 'learning_rate': 0.0003686111111111111, 'epoch': 1.8944444444444444}
Step 6830: {'loss': 0.6373, 'grad_norm': 0.9750787019729614, 'learning_rate': 0.0003676851851851852, 'epoch': 1.8972222222222221}
Step 6840: {'loss': 0.6392, 'grad_norm': 0.9357565641403198, 'learning_rate': 0.0003667592592592593, 'epoch': 1.9}
Step 6850: {'loss': 0.6735, 'grad_norm': 1.2712361812591553, 'learning_rate': 0.00036583333333333335, 'epoch': 1.9027777777777777}
Step 6860: {'loss': 0.6855, 'grad_norm': 1.4948673248291016, 'learning_rate': 0.00036490740740740743, 'epoch': 1.9055555555555554}
Step 6870: {'loss': 0.6035, 'grad_norm': 1.3553320169448853, 'learning_rate': 0.00036398148148148146, 'epoch': 1.9083333333333332}
Step 6880: {'loss': 0.6154, 'grad_norm': 1.1061124801635742, 'learning_rate': 0.0003630555555555556, 'epoch': 1.911111111111111}
Step 6890: {'loss': 0.6508, 'grad_norm': 1.140994906425476, 'learning_rate': 0.0003621296296296296, 'epoch': 1.9138888888888888}
Step 6900: {'loss': 0.6336, 'grad_norm': 1.5709794759750366, 'learning_rate': 0.0003612037037037037, 'epoch': 1.9166666666666665}
Step 6910: {'loss': 0.6347, 'grad_norm': 1.2936033010482788, 'learning_rate': 0.0003602777777777778, 'epoch': 1.9194444444444443}
Step 6920: {'loss': 0.6836, 'grad_norm': 1.1649430990219116, 'learning_rate': 0.00035935185185185186, 'epoch': 1.9222222222222223}
Step 6930: {'loss': 0.5984, 'grad_norm': 1.3303622007369995, 'learning_rate': 0.00035842592592592594, 'epoch': 1.925}
Step 6940: {'loss': 0.6137, 'grad_norm': 1.0906274318695068, 'learning_rate': 0.0003575, 'epoch': 1.9277777777777778}
Step 6950: {'loss': 0.6497, 'grad_norm': 1.3790327310562134, 'learning_rate': 0.0003565740740740741, 'epoch': 1.9305555555555556}
Step 6960: {'loss': 0.7104, 'grad_norm': 1.2165300846099854, 'learning_rate': 0.0003556481481481482, 'epoch': 1.9333333333333333}
Step 6970: {'loss': 0.5953, 'grad_norm': 1.1897082328796387, 'learning_rate': 0.0003547222222222222, 'epoch': 1.9361111111111111}
Step 6980: {'loss': 0.5955, 'grad_norm': 1.1552735567092896, 'learning_rate': 0.0003537962962962963, 'epoch': 1.9388888888888889}
Step 6990: {'loss': 0.613, 'grad_norm': 0.9284715056419373, 'learning_rate': 0.0003528703703703704, 'epoch': 1.9416666666666667}
Step 7000: {'loss': 0.625, 'grad_norm': 0.9777101874351501, 'learning_rate': 0.00035194444444444444, 'epoch': 1.9444444444444444}
Step 7010: {'loss': 0.6048, 'grad_norm': 1.4805179834365845, 'learning_rate': 0.0003510185185185185, 'epoch': 1.9472222222222222}
Step 7020: {'loss': 0.6184, 'grad_norm': 1.5591782331466675, 'learning_rate': 0.0003500925925925926, 'epoch': 1.95}
Step 7030: {'loss': 0.6586, 'grad_norm': 0.9495490193367004, 'learning_rate': 0.0003491666666666667, 'epoch': 1.9527777777777777}
Step 7040: {'loss': 0.7097, 'grad_norm': 1.5599663257598877, 'learning_rate': 0.00034824074074074076, 'epoch': 1.9555555555555557}
Step 7050: {'loss': 0.6531, 'grad_norm': 1.3608206510543823, 'learning_rate': 0.0003473148148148148, 'epoch': 1.9583333333333335}
Step 7060: {'loss': 0.6205, 'grad_norm': 0.8258155584335327, 'learning_rate': 0.0003463888888888889, 'epoch': 1.9611111111111112}
Step 7070: {'loss': 0.6091, 'grad_norm': 0.9311746954917908, 'learning_rate': 0.000345462962962963, 'epoch': 1.963888888888889}
Step 7080: {'loss': 0.6341, 'grad_norm': 1.2331902980804443, 'learning_rate': 0.000344537037037037, 'epoch': 1.9666666666666668}
Step 7090: {'loss': 0.614, 'grad_norm': 1.0737754106521606, 'learning_rate': 0.0003436111111111111, 'epoch': 1.9694444444444446}
Step 7100: {'loss': 0.6208, 'grad_norm': 1.2118881940841675, 'learning_rate': 0.00034268518518518524, 'epoch': 1.9722222222222223}
Step 7110: {'loss': 0.6328, 'grad_norm': 1.1739972829818726, 'learning_rate': 0.00034175925925925926, 'epoch': 1.975}
Step 7120: {'loss': 0.6574, 'grad_norm': 1.2641868591308594, 'learning_rate': 0.00034083333333333334, 'epoch': 1.9777777777777779}
Step 7130: {'loss': 0.6408, 'grad_norm': 1.2273902893066406, 'learning_rate': 0.00033990740740740737, 'epoch': 1.9805555555555556}
Step 7140: {'loss': 0.6467, 'grad_norm': 1.418060064315796, 'learning_rate': 0.0003389814814814815, 'epoch': 1.9833333333333334}
Step 7150: {'loss': 0.6305, 'grad_norm': 1.92095947265625, 'learning_rate': 0.0003380555555555556, 'epoch': 1.9861111111111112}
Step 7160: {'loss': 0.6453, 'grad_norm': 1.1644363403320312, 'learning_rate': 0.0003371296296296296, 'epoch': 1.988888888888889}
Step 7170: {'loss': 0.6035, 'grad_norm': 1.1904737949371338, 'learning_rate': 0.00033620370370370374, 'epoch': 1.9916666666666667}
Step 7180: {'loss': 0.6296, 'grad_norm': 1.3513017892837524, 'learning_rate': 0.00033527777777777777, 'epoch': 1.9944444444444445}
Step 7190: {'loss': 0.6228, 'grad_norm': 1.239367961883545, 'learning_rate': 0.00033435185185185185, 'epoch': 1.9972222222222222}
Step 7200: {'loss': 0.627, 'grad_norm': 1.6047430038452148, 'learning_rate': 0.0003334259259259259, 'epoch': 2.0}
Step 7210: {'loss': 0.6382, 'grad_norm': 1.0863374471664429, 'learning_rate': 0.0003325, 'epoch': 2.0027777777777778}
Step 7220: {'loss': 0.6458, 'grad_norm': 1.6940035820007324, 'learning_rate': 0.0003315740740740741, 'epoch': 2.0055555555555555}
Step 7230: {'loss': 0.6623, 'grad_norm': 1.0442472696304321, 'learning_rate': 0.00033064814814814816, 'epoch': 2.0083333333333333}
Step 7240: {'loss': 0.6036, 'grad_norm': 1.1827648878097534, 'learning_rate': 0.00032972222222222224, 'epoch': 2.011111111111111}
Step 7250: {'loss': 0.6606, 'grad_norm': 1.1643013954162598, 'learning_rate': 0.0003287962962962963, 'epoch': 2.013888888888889}
Step 7260: {'loss': 0.6778, 'grad_norm': 1.1390100717544556, 'learning_rate': 0.00032787037037037035, 'epoch': 2.0166666666666666}
Step 7270: {'loss': 0.61, 'grad_norm': 1.352627158164978, 'learning_rate': 0.00032694444444444443, 'epoch': 2.0194444444444444}
Step 7280: {'loss': 0.632, 'grad_norm': 1.24480140209198, 'learning_rate': 0.00032601851851851856, 'epoch': 2.022222222222222}
Step 7290: {'loss': 0.6018, 'grad_norm': 1.1429522037506104, 'learning_rate': 0.0003250925925925926, 'epoch': 2.025}
Step 7300: {'loss': 0.5803, 'grad_norm': 1.2497161626815796, 'learning_rate': 0.00032416666666666667, 'epoch': 2.0277777777777777}
Step 7310: {'loss': 0.6204, 'grad_norm': 1.056307315826416, 'learning_rate': 0.00032324074074074075, 'epoch': 2.0305555555555554}
Step 7320: {'loss': 0.6312, 'grad_norm': 1.4098743200302124, 'learning_rate': 0.0003223148148148148, 'epoch': 2.033333333333333}
Step 7330: {'loss': 0.6271, 'grad_norm': 1.4232392311096191, 'learning_rate': 0.0003213888888888889, 'epoch': 2.036111111111111}
Step 7340: {'loss': 0.5966, 'grad_norm': 1.149889588356018, 'learning_rate': 0.00032046296296296293, 'epoch': 2.0388888888888888}
Step 7350: {'loss': 0.6528, 'grad_norm': 1.1126394271850586, 'learning_rate': 0.00031953703703703707, 'epoch': 2.0416666666666665}
Step 7360: {'loss': 0.6224, 'grad_norm': 1.2728947401046753, 'learning_rate': 0.00031861111111111115, 'epoch': 2.0444444444444443}
Step 7370: {'loss': 0.6077, 'grad_norm': 1.0042613744735718, 'learning_rate': 0.00031768518518518517, 'epoch': 2.047222222222222}
Step 7380: {'loss': 0.6273, 'grad_norm': 1.1894598007202148, 'learning_rate': 0.00031675925925925925, 'epoch': 2.05}
Step 7390: {'loss': 0.6603, 'grad_norm': 1.8252686262130737, 'learning_rate': 0.0003158333333333334, 'epoch': 2.0527777777777776}
Step 7400: {'loss': 0.6634, 'grad_norm': 1.2721443176269531, 'learning_rate': 0.0003149074074074074, 'epoch': 2.0555555555555554}
Step 7410: {'loss': 0.6693, 'grad_norm': 0.8308581709861755, 'learning_rate': 0.0003139814814814815, 'epoch': 2.058333333333333}
Step 7420: {'loss': 0.6241, 'grad_norm': 0.8850691318511963, 'learning_rate': 0.0003130555555555555, 'epoch': 2.061111111111111}
Step 7430: {'loss': 0.5987, 'grad_norm': 1.1773358583450317, 'learning_rate': 0.00031212962962962965, 'epoch': 2.063888888888889}
Step 7440: {'loss': 0.5858, 'grad_norm': 1.4198286533355713, 'learning_rate': 0.00031120370370370373, 'epoch': 2.066666666666667}
Step 7450: {'loss': 0.6154, 'grad_norm': 1.01178777217865, 'learning_rate': 0.00031027777777777775, 'epoch': 2.0694444444444446}
Step 7460: {'loss': 0.6237, 'grad_norm': 1.1969258785247803, 'learning_rate': 0.0003093518518518519, 'epoch': 2.0722222222222224}
Step 7470: {'loss': 0.6108, 'grad_norm': 1.061303973197937, 'learning_rate': 0.00030842592592592597, 'epoch': 2.075}
Step 7480: {'loss': 0.66, 'grad_norm': 0.7681718468666077, 'learning_rate': 0.0003075, 'epoch': 2.077777777777778}
Step 7490: {'loss': 0.5529, 'grad_norm': 1.1749393939971924, 'learning_rate': 0.0003065740740740741, 'epoch': 2.0805555555555557}
Step 7500: {'loss': 0.6023, 'grad_norm': 1.0683259963989258, 'learning_rate': 0.00030564814814814815, 'epoch': 2.0833333333333335}
Step 7510: {'loss': 0.6255, 'grad_norm': 1.4969533681869507, 'learning_rate': 0.00030472222222222223, 'epoch': 2.0861111111111112}
Step 7520: {'loss': 0.631, 'grad_norm': 1.2573364973068237, 'learning_rate': 0.0003037962962962963, 'epoch': 2.088888888888889}
Step 7530: {'loss': 0.6292, 'grad_norm': 1.3660635948181152, 'learning_rate': 0.00030287037037037034, 'epoch': 2.091666666666667}
Step 7540: {'loss': 0.6, 'grad_norm': 0.7483826279640198, 'learning_rate': 0.00030194444444444447, 'epoch': 2.0944444444444446}
Step 7550: {'loss': 0.5992, 'grad_norm': 1.1610329151153564, 'learning_rate': 0.00030101851851851855, 'epoch': 2.0972222222222223}
Step 7560: {'loss': 0.5899, 'grad_norm': 1.0497570037841797, 'learning_rate': 0.0003000925925925926, 'epoch': 2.1}
Step 7570: {'loss': 0.589, 'grad_norm': 1.5266082286834717, 'learning_rate': 0.0002991666666666667, 'epoch': 2.102777777777778}
Step 7580: {'loss': 0.6234, 'grad_norm': 1.3861850500106812, 'learning_rate': 0.00029824074074074074, 'epoch': 2.1055555555555556}
Step 7590: {'loss': 0.6089, 'grad_norm': 1.0400422811508179, 'learning_rate': 0.0002973148148148148, 'epoch': 2.1083333333333334}
Step 7600: {'loss': 0.6504, 'grad_norm': 1.2590086460113525, 'learning_rate': 0.0002963888888888889, 'epoch': 2.111111111111111}
Step 7610: {'loss': 0.6012, 'grad_norm': 1.3575661182403564, 'learning_rate': 0.000295462962962963, 'epoch': 2.113888888888889}
Step 7620: {'loss': 0.5739, 'grad_norm': 1.0862873792648315, 'learning_rate': 0.00029453703703703705, 'epoch': 2.1166666666666667}
Step 7630: {'loss': 0.6436, 'grad_norm': 1.282666563987732, 'learning_rate': 0.0002936111111111111, 'epoch': 2.1194444444444445}
Step 7640: {'loss': 0.6031, 'grad_norm': 1.2743295431137085, 'learning_rate': 0.00029268518518518516, 'epoch': 2.1222222222222222}
Step 7650: {'loss': 0.6916, 'grad_norm': 1.1453725099563599, 'learning_rate': 0.0002917592592592593, 'epoch': 2.125}
Step 7660: {'loss': 0.5897, 'grad_norm': 1.481581449508667, 'learning_rate': 0.0002908333333333333, 'epoch': 2.1277777777777778}
Step 7670: {'loss': 0.6301, 'grad_norm': 1.0075403451919556, 'learning_rate': 0.0002899074074074074, 'epoch': 2.1305555555555555}
Step 7680: {'loss': 0.6, 'grad_norm': 1.4141819477081299, 'learning_rate': 0.00028898148148148153, 'epoch': 2.1333333333333333}
Step 7690: {'loss': 0.6019, 'grad_norm': 1.4425787925720215, 'learning_rate': 0.00028805555555555556, 'epoch': 2.136111111111111}
Step 7700: {'loss': 0.6608, 'grad_norm': 1.1883400678634644, 'learning_rate': 0.00028712962962962964, 'epoch': 2.138888888888889}
Step 7710: {'loss': 0.6566, 'grad_norm': 1.2876484394073486, 'learning_rate': 0.00028620370370370366, 'epoch': 2.1416666666666666}
Step 7720: {'loss': 0.6303, 'grad_norm': 1.3050791025161743, 'learning_rate': 0.0002852777777777778, 'epoch': 2.1444444444444444}
Step 7730: {'loss': 0.6188, 'grad_norm': 1.211682677268982, 'learning_rate': 0.0002843518518518519, 'epoch': 2.147222222222222}
Step 7740: {'loss': 0.6385, 'grad_norm': 1.0644323825836182, 'learning_rate': 0.0002834259259259259, 'epoch': 2.15}
Step 7750: {'loss': 0.5988, 'grad_norm': 1.2108349800109863, 'learning_rate': 0.0002825, 'epoch': 2.1527777777777777}
Step 7760: {'loss': 0.5676, 'grad_norm': 1.1962920427322388, 'learning_rate': 0.0002815740740740741, 'epoch': 2.1555555555555554}
Step 7770: {'loss': 0.6239, 'grad_norm': 1.027575969696045, 'learning_rate': 0.00028064814814814814, 'epoch': 2.158333333333333}
Step 7780: {'loss': 0.5956, 'grad_norm': 1.233831524848938, 'learning_rate': 0.0002797222222222222, 'epoch': 2.161111111111111}
Step 7790: {'loss': 0.6624, 'grad_norm': 1.167362928390503, 'learning_rate': 0.0002787962962962963, 'epoch': 2.1638888888888888}
Step 7800: {'loss': 0.583, 'grad_norm': 1.1203821897506714, 'learning_rate': 0.0002778703703703704, 'epoch': 2.1666666666666665}
Step 7810: {'loss': 0.5668, 'grad_norm': 1.1459935903549194, 'learning_rate': 0.00027694444444444446, 'epoch': 2.1694444444444443}
Step 7820: {'loss': 0.5819, 'grad_norm': 1.5635536909103394, 'learning_rate': 0.0002760185185185185, 'epoch': 2.172222222222222}
Step 7830: {'loss': 0.6412, 'grad_norm': 1.3809584379196167, 'learning_rate': 0.0002750925925925926, 'epoch': 2.175}
Step 7840: {'loss': 0.6179, 'grad_norm': 1.2470006942749023, 'learning_rate': 0.0002741666666666667, 'epoch': 2.1777777777777776}
Step 7850: {'loss': 0.5908, 'grad_norm': 2.167606830596924, 'learning_rate': 0.0002732407407407407, 'epoch': 2.1805555555555554}
Step 7860: {'loss': 0.6465, 'grad_norm': 1.378970742225647, 'learning_rate': 0.0002723148148148148, 'epoch': 2.183333333333333}
Step 7870: {'loss': 0.6394, 'grad_norm': 1.0302549600601196, 'learning_rate': 0.0002713888888888889, 'epoch': 2.186111111111111}
Step 7880: {'loss': 0.5821, 'grad_norm': 0.7188294529914856, 'learning_rate': 0.00027046296296296296, 'epoch': 2.188888888888889}
Step 7890: {'loss': 0.6163, 'grad_norm': 1.2838667631149292, 'learning_rate': 0.00026953703703703704, 'epoch': 2.191666666666667}
Step 7900: {'loss': 0.6245, 'grad_norm': 1.1564979553222656, 'learning_rate': 0.0002686111111111111, 'epoch': 2.1944444444444446}
Step 7910: {'loss': 0.5848, 'grad_norm': 1.7215485572814941, 'learning_rate': 0.0002676851851851852, 'epoch': 2.1972222222222224}
Step 7920: {'loss': 0.625, 'grad_norm': 1.0011155605316162, 'learning_rate': 0.0002667592592592593, 'epoch': 2.2}
Step 7930: {'loss': 0.5998, 'grad_norm': 1.0761048793792725, 'learning_rate': 0.0002658333333333333, 'epoch': 2.202777777777778}
Step 7940: {'loss': 0.6416, 'grad_norm': 1.1639962196350098, 'learning_rate': 0.00026490740740740744, 'epoch': 2.2055555555555557}
Step 7950: {'loss': 0.6167, 'grad_norm': 1.2842069864273071, 'learning_rate': 0.00026398148148148147, 'epoch': 2.2083333333333335}
Step 7960: {'loss': 0.6003, 'grad_norm': 1.009254813194275, 'learning_rate': 0.00026305555555555555, 'epoch': 2.2111111111111112}
Step 7970: {'loss': 0.6756, 'grad_norm': 1.4017548561096191, 'learning_rate': 0.0002621296296296297, 'epoch': 2.213888888888889}
Step 7980: {'loss': 0.6306, 'grad_norm': 1.0540120601654053, 'learning_rate': 0.0002612037037037037, 'epoch': 2.216666666666667}
Step 7990: {'loss': 0.5689, 'grad_norm': 1.0106719732284546, 'learning_rate': 0.0002602777777777778, 'epoch': 2.2194444444444446}
Step 8000: {'loss': 0.6894, 'grad_norm': 1.180759072303772, 'learning_rate': 0.00025935185185185187, 'epoch': 2.2222222222222223}
Step 8010: {'loss': 0.6246, 'grad_norm': 1.2128381729125977, 'learning_rate': 0.00025842592592592595, 'epoch': 2.225}
Step 8020: {'loss': 0.6413, 'grad_norm': 1.261224389076233, 'learning_rate': 0.0002575, 'epoch': 2.227777777777778}
Step 8030: {'loss': 0.6136, 'grad_norm': 1.3690370321273804, 'learning_rate': 0.00025657407407407405, 'epoch': 2.2305555555555556}
Step 8040: {'loss': 0.6233, 'grad_norm': 1.6546238660812378, 'learning_rate': 0.00025564814814814813, 'epoch': 2.2333333333333334}
Step 8050: {'loss': 0.6513, 'grad_norm': 1.3792909383773804, 'learning_rate': 0.00025472222222222226, 'epoch': 2.236111111111111}
Step 8060: {'loss': 0.6419, 'grad_norm': 1.0618666410446167, 'learning_rate': 0.0002537962962962963, 'epoch': 2.238888888888889}
Step 8070: {'loss': 0.5822, 'grad_norm': 1.0403797626495361, 'learning_rate': 0.00025287037037037037, 'epoch': 2.2416666666666667}
Step 8080: {'loss': 0.6322, 'grad_norm': 1.1168322563171387, 'learning_rate': 0.0002519444444444445, 'epoch': 2.2444444444444445}
Step 8090: {'loss': 0.6314, 'grad_norm': 1.1626434326171875, 'learning_rate': 0.00025101851851851853, 'epoch': 2.2472222222222222}
Step 8100: {'loss': 0.5937, 'grad_norm': 1.0861225128173828, 'learning_rate': 0.0002500925925925926, 'epoch': 2.25}
Step 8110: {'loss': 0.6199, 'grad_norm': 1.192132592201233, 'learning_rate': 0.0002491666666666667, 'epoch': 2.2527777777777778}
Step 8120: {'loss': 0.5991, 'grad_norm': 1.2028698921203613, 'learning_rate': 0.0002482407407407407, 'epoch': 2.2555555555555555}
Step 8130: {'loss': 0.6212, 'grad_norm': 1.8606544733047485, 'learning_rate': 0.00024731481481481485, 'epoch': 2.2583333333333333}
Step 8140: {'loss': 0.6274, 'grad_norm': 1.378737211227417, 'learning_rate': 0.00024638888888888887, 'epoch': 2.261111111111111}
Step 8150: {'loss': 0.6655, 'grad_norm': 1.0650882720947266, 'learning_rate': 0.00024546296296296295, 'epoch': 2.263888888888889}
Step 8160: {'loss': 0.6266, 'grad_norm': 0.9704899787902832, 'learning_rate': 0.00024453703703703703, 'epoch': 2.2666666666666666}
Step 8170: {'loss': 0.664, 'grad_norm': 1.5020145177841187, 'learning_rate': 0.0002436111111111111, 'epoch': 2.2694444444444444}
Step 8180: {'loss': 0.626, 'grad_norm': 1.102362036705017, 'learning_rate': 0.0002426851851851852, 'epoch': 2.272222222222222}
Step 8190: {'loss': 0.6027, 'grad_norm': 1.472686767578125, 'learning_rate': 0.00024175925925925927, 'epoch': 2.275}
Step 8200: {'loss': 0.6287, 'grad_norm': 1.5698975324630737, 'learning_rate': 0.00024083333333333335, 'epoch': 2.2777777777777777}
Step 8210: {'loss': 0.6423, 'grad_norm': 1.5214717388153076, 'learning_rate': 0.0002399074074074074, 'epoch': 2.2805555555555554}
Step 8220: {'loss': 0.5975, 'grad_norm': 2.0318892002105713, 'learning_rate': 0.00023898148148148148, 'epoch': 2.283333333333333}
Step 8230: {'loss': 0.6508, 'grad_norm': 0.9103215336799622, 'learning_rate': 0.00023805555555555556, 'epoch': 2.286111111111111}
Step 8240: {'loss': 0.6177, 'grad_norm': 0.9991878867149353, 'learning_rate': 0.00023712962962962964, 'epoch': 2.2888888888888888}
Step 8250: {'loss': 0.6066, 'grad_norm': 1.4207251071929932, 'learning_rate': 0.0002362037037037037, 'epoch': 2.2916666666666665}
Step 8260: {'loss': 0.6663, 'grad_norm': 1.1969677209854126, 'learning_rate': 0.00023527777777777777, 'epoch': 2.2944444444444443}
Step 8270: {'loss': 0.6424, 'grad_norm': 1.2392420768737793, 'learning_rate': 0.00023435185185185185, 'epoch': 2.297222222222222}
Step 8280: {'loss': 0.6444, 'grad_norm': 1.4323877096176147, 'learning_rate': 0.00023342592592592593, 'epoch': 2.3}
Step 8290: {'loss': 0.6077, 'grad_norm': 0.8714242577552795, 'learning_rate': 0.0002325, 'epoch': 2.3027777777777776}
Step 8300: {'loss': 0.6769, 'grad_norm': 0.8996579647064209, 'learning_rate': 0.00023157407407407407, 'epoch': 2.3055555555555554}
Step 8310: {'loss': 0.5987, 'grad_norm': 1.349207878112793, 'learning_rate': 0.00023064814814814817, 'epoch': 2.3083333333333336}
Step 8320: {'loss': 0.6334, 'grad_norm': 1.2806345224380493, 'learning_rate': 0.00022972222222222223, 'epoch': 2.311111111111111}
Step 8330: {'loss': 0.6369, 'grad_norm': 1.1956639289855957, 'learning_rate': 0.0002287962962962963, 'epoch': 2.313888888888889}
Step 8340: {'loss': 0.6151, 'grad_norm': 0.912944495677948, 'learning_rate': 0.00022787037037037036, 'epoch': 2.3166666666666664}
Step 8350: {'loss': 0.6242, 'grad_norm': 1.3622219562530518, 'learning_rate': 0.00022694444444444446, 'epoch': 2.3194444444444446}
Step 8360: {'loss': 0.6013, 'grad_norm': 1.3007891178131104, 'learning_rate': 0.00022601851851851852, 'epoch': 2.3222222222222224}
Step 8370: {'loss': 0.6001, 'grad_norm': 1.1661975383758545, 'learning_rate': 0.0002250925925925926, 'epoch': 2.325}
Step 8380: {'loss': 0.6308, 'grad_norm': 1.583834171295166, 'learning_rate': 0.00022416666666666665, 'epoch': 2.327777777777778}
Step 8390: {'loss': 0.5927, 'grad_norm': 1.163963794708252, 'learning_rate': 0.00022324074074074076, 'epoch': 2.3305555555555557}
Step 8400: {'loss': 0.589, 'grad_norm': 1.194271206855774, 'learning_rate': 0.00022231481481481484, 'epoch': 2.3333333333333335}
Step 8410: {'loss': 0.6338, 'grad_norm': 1.2626478672027588, 'learning_rate': 0.0002213888888888889, 'epoch': 2.3361111111111112}
Step 8420: {'loss': 0.5944, 'grad_norm': 1.218529224395752, 'learning_rate': 0.00022046296296296297, 'epoch': 2.338888888888889}
Step 8430: {'loss': 0.6017, 'grad_norm': 1.413329005241394, 'learning_rate': 0.00021953703703703705, 'epoch': 2.341666666666667}
Step 8440: {'loss': 0.6938, 'grad_norm': 1.2522616386413574, 'learning_rate': 0.00021861111111111113, 'epoch': 2.3444444444444446}
Step 8450: {'loss': 0.5881, 'grad_norm': 0.8297521471977234, 'learning_rate': 0.00021768518518518518, 'epoch': 2.3472222222222223}
Step 8460: {'loss': 0.6568, 'grad_norm': 1.357964277267456, 'learning_rate': 0.00021675925925925926, 'epoch': 2.35}
Step 8470: {'loss': 0.631, 'grad_norm': 0.8642800450325012, 'learning_rate': 0.00021583333333333334, 'epoch': 2.352777777777778}
Step 8480: {'loss': 0.6722, 'grad_norm': 1.1380411386489868, 'learning_rate': 0.00021490740740740742, 'epoch': 2.3555555555555556}
Step 8490: {'loss': 0.6135, 'grad_norm': 1.4169936180114746, 'learning_rate': 0.0002139814814814815, 'epoch': 2.3583333333333334}
Step 8500: {'loss': 0.5994, 'grad_norm': 1.1613609790802002, 'learning_rate': 0.00021305555555555555, 'epoch': 2.361111111111111}
Step 8510: {'loss': 0.6193, 'grad_norm': 1.220797061920166, 'learning_rate': 0.00021212962962962966, 'epoch': 2.363888888888889}
Step 8520: {'loss': 0.5938, 'grad_norm': 1.1433042287826538, 'learning_rate': 0.0002112037037037037, 'epoch': 2.3666666666666667}
Step 8530: {'loss': 0.5889, 'grad_norm': 1.559577226638794, 'learning_rate': 0.0002102777777777778, 'epoch': 2.3694444444444445}
Step 8540: {'loss': 0.7222, 'grad_norm': 1.1516908407211304, 'learning_rate': 0.00020935185185185184, 'epoch': 2.3722222222222222}
Step 8550: {'loss': 0.5885, 'grad_norm': 1.0728058815002441, 'learning_rate': 0.00020842592592592592, 'epoch': 2.375}
Step 8560: {'loss': 0.5972, 'grad_norm': 1.071729063987732, 'learning_rate': 0.0002075, 'epoch': 2.3777777777777778}
Step 8570: {'loss': 0.6348, 'grad_norm': 1.2937426567077637, 'learning_rate': 0.00020657407407407408, 'epoch': 2.3805555555555555}
Step 8580: {'loss': 0.6021, 'grad_norm': 1.084047555923462, 'learning_rate': 0.00020564814814814813, 'epoch': 2.3833333333333333}
Step 8590: {'loss': 0.6605, 'grad_norm': 1.1479358673095703, 'learning_rate': 0.00020472222222222221, 'epoch': 2.386111111111111}
Step 8600: {'loss': 0.5804, 'grad_norm': 1.4981216192245483, 'learning_rate': 0.00020379629629629632, 'epoch': 2.388888888888889}
Step 8610: {'loss': 0.6207, 'grad_norm': 1.0537031888961792, 'learning_rate': 0.00020287037037037037, 'epoch': 2.3916666666666666}
Step 8620: {'loss': 0.6093, 'grad_norm': 1.0123088359832764, 'learning_rate': 0.00020194444444444445, 'epoch': 2.3944444444444444}
Step 8630: {'loss': 0.6152, 'grad_norm': 1.2215516567230225, 'learning_rate': 0.0002010185185185185, 'epoch': 2.397222222222222}
Step 8640: {'loss': 0.5853, 'grad_norm': 0.7910972833633423, 'learning_rate': 0.0002000925925925926, 'epoch': 2.4}
Step 8650: {'loss': 0.625, 'grad_norm': 1.1673871278762817, 'learning_rate': 0.00019916666666666667, 'epoch': 2.4027777777777777}
Step 8660: {'loss': 0.6487, 'grad_norm': 0.9642863869667053, 'learning_rate': 0.00019824074074074074, 'epoch': 2.4055555555555554}
Step 8670: {'loss': 0.5625, 'grad_norm': 1.0803331136703491, 'learning_rate': 0.0001973148148148148, 'epoch': 2.408333333333333}
Step 8680: {'loss': 0.6631, 'grad_norm': 1.0596460103988647, 'learning_rate': 0.0001963888888888889, 'epoch': 2.411111111111111}
Step 8690: {'loss': 0.6511, 'grad_norm': 1.192435383796692, 'learning_rate': 0.00019546296296296296, 'epoch': 2.4138888888888888}
Step 8700: {'loss': 0.6038, 'grad_norm': 1.4483721256256104, 'learning_rate': 0.00019453703703703704, 'epoch': 2.4166666666666665}
Step 8710: {'loss': 0.6269, 'grad_norm': 1.144465684890747, 'learning_rate': 0.00019361111111111112, 'epoch': 2.4194444444444443}
Step 8720: {'loss': 0.6319, 'grad_norm': 1.5113824605941772, 'learning_rate': 0.0001926851851851852, 'epoch': 2.422222222222222}
Step 8730: {'loss': 0.614, 'grad_norm': 1.2977603673934937, 'learning_rate': 0.00019175925925925928, 'epoch': 2.425}
Step 8740: {'loss': 0.6526, 'grad_norm': 1.2373086214065552, 'learning_rate': 0.00019083333333333333, 'epoch': 2.4277777777777776}
Step 8750: {'loss': 0.5694, 'grad_norm': 0.8669878244400024, 'learning_rate': 0.0001899074074074074, 'epoch': 2.4305555555555554}
Step 8760: {'loss': 0.6742, 'grad_norm': 0.9353493452072144, 'learning_rate': 0.0001889814814814815, 'epoch': 2.4333333333333336}
Step 8770: {'loss': 0.6265, 'grad_norm': 1.2460397481918335, 'learning_rate': 0.00018805555555555557, 'epoch': 2.436111111111111}
Step 8780: {'loss': 0.6266, 'grad_norm': 1.2202445268630981, 'learning_rate': 0.00018712962962962962, 'epoch': 2.438888888888889}
Step 8790: {'loss': 0.5928, 'grad_norm': 0.8592935800552368, 'learning_rate': 0.0001862037037037037, 'epoch': 2.4416666666666664}
Step 8800: {'loss': 0.6491, 'grad_norm': 1.3107064962387085, 'learning_rate': 0.0001852777777777778, 'epoch': 2.4444444444444446}
Step 8810: {'loss': 0.6171, 'grad_norm': 1.610183835029602, 'learning_rate': 0.00018435185185185186, 'epoch': 2.4472222222222224}
Step 8820: {'loss': 0.6593, 'grad_norm': 1.3680908679962158, 'learning_rate': 0.00018342592592592594, 'epoch': 2.45}
Step 8830: {'loss': 0.6332, 'grad_norm': 1.2397794723510742, 'learning_rate': 0.0001825, 'epoch': 2.452777777777778}
Step 8840: {'loss': 0.6102, 'grad_norm': 1.121148943901062, 'learning_rate': 0.0001815740740740741, 'epoch': 2.4555555555555557}
Step 8850: {'loss': 0.6039, 'grad_norm': 1.2293331623077393, 'learning_rate': 0.00018064814814814815, 'epoch': 2.4583333333333335}
Step 8860: {'loss': 0.6443, 'grad_norm': 1.211145281791687, 'learning_rate': 0.00017972222222222223, 'epoch': 2.4611111111111112}
Step 8870: {'loss': 0.6601, 'grad_norm': 1.4127696752548218, 'learning_rate': 0.00017879629629629628, 'epoch': 2.463888888888889}
Step 8880: {'loss': 0.6428, 'grad_norm': 1.2619813680648804, 'learning_rate': 0.0001778703703703704, 'epoch': 2.466666666666667}
Step 8890: {'loss': 0.6374, 'grad_norm': 1.019495964050293, 'learning_rate': 0.00017694444444444444, 'epoch': 2.4694444444444446}
Step 8900: {'loss': 0.6326, 'grad_norm': 1.081893801689148, 'learning_rate': 0.00017601851851851852, 'epoch': 2.4722222222222223}
Step 8910: {'loss': 0.6391, 'grad_norm': 1.4600945711135864, 'learning_rate': 0.0001750925925925926, 'epoch': 2.475}
Step 8920: {'loss': 0.5793, 'grad_norm': 1.4236068725585938, 'learning_rate': 0.00017416666666666668, 'epoch': 2.477777777777778}
Step 8930: {'loss': 0.5368, 'grad_norm': 1.333326816558838, 'learning_rate': 0.00017324074074074076, 'epoch': 2.4805555555555556}
Step 8940: {'loss': 0.6287, 'grad_norm': 1.4763267040252686, 'learning_rate': 0.0001723148148148148, 'epoch': 2.4833333333333334}
Step 8950: {'loss': 0.6342, 'grad_norm': 1.7038301229476929, 'learning_rate': 0.0001713888888888889, 'epoch': 2.486111111111111}
Step 8960: {'loss': 0.6634, 'grad_norm': 1.5811632871627808, 'learning_rate': 0.00017046296296296295, 'epoch': 2.488888888888889}
Step 8970: {'loss': 0.639, 'grad_norm': 1.3319059610366821, 'learning_rate': 0.00016953703703703705, 'epoch': 2.4916666666666667}
Step 8980: {'loss': 0.6191, 'grad_norm': 1.0642597675323486, 'learning_rate': 0.0001686111111111111, 'epoch': 2.4944444444444445}
Step 8990: {'loss': 0.6309, 'grad_norm': 1.0524407625198364, 'learning_rate': 0.00016768518518518518, 'epoch': 2.4972222222222222}
Step 9000: {'loss': 0.6359, 'grad_norm': 1.0567009449005127, 'learning_rate': 0.00016675925925925924, 'epoch': 2.5}
Step 9010: {'loss': 0.6191, 'grad_norm': 1.0710365772247314, 'learning_rate': 0.00016583333333333334, 'epoch': 2.5027777777777778}
Step 9020: {'loss': 0.6436, 'grad_norm': 1.2082998752593994, 'learning_rate': 0.00016490740740740742, 'epoch': 2.5055555555555555}
Step 9030: {'loss': 0.6525, 'grad_norm': 1.0966262817382812, 'learning_rate': 0.00016398148148148148, 'epoch': 2.5083333333333333}
Step 9040: {'loss': 0.6489, 'grad_norm': 1.5630676746368408, 'learning_rate': 0.00016305555555555556, 'epoch': 2.511111111111111}
Step 9050: {'loss': 0.672, 'grad_norm': 1.324415922164917, 'learning_rate': 0.00016212962962962964, 'epoch': 2.513888888888889}
Step 9060: {'loss': 0.6786, 'grad_norm': 0.9729698300361633, 'learning_rate': 0.00016120370370370371, 'epoch': 2.5166666666666666}
Step 9070: {'loss': 0.6041, 'grad_norm': 1.374085545539856, 'learning_rate': 0.00016027777777777777, 'epoch': 2.5194444444444444}
Step 9080: {'loss': 0.5801, 'grad_norm': 1.0869865417480469, 'learning_rate': 0.00015935185185185185, 'epoch': 2.522222222222222}
Step 9090: {'loss': 0.6287, 'grad_norm': 1.221680998802185, 'learning_rate': 0.00015842592592592593, 'epoch': 2.525}
Step 9100: {'loss': 0.6273, 'grad_norm': 1.1493815183639526, 'learning_rate': 0.0001575, 'epoch': 2.5277777777777777}
Step 9110: {'loss': 0.5783, 'grad_norm': 1.572473406791687, 'learning_rate': 0.00015657407407407409, 'epoch': 2.5305555555555554}
Step 9120: {'loss': 0.6417, 'grad_norm': 1.6158918142318726, 'learning_rate': 0.00015564814814814814, 'epoch': 2.533333333333333}
Step 9130: {'loss': 0.651, 'grad_norm': 1.0256484746932983, 'learning_rate': 0.00015472222222222225, 'epoch': 2.536111111111111}
Step 9140: {'loss': 0.6577, 'grad_norm': 2.173031806945801, 'learning_rate': 0.0001537962962962963, 'epoch': 2.5388888888888888}
Step 9150: {'loss': 0.6609, 'grad_norm': 1.6230480670928955, 'learning_rate': 0.00015287037037037038, 'epoch': 2.5416666666666665}
Step 9160: {'loss': 0.628, 'grad_norm': 1.195627212524414, 'learning_rate': 0.00015194444444444443, 'epoch': 2.5444444444444443}
Step 9170: {'loss': 0.6026, 'grad_norm': 1.0667201280593872, 'learning_rate': 0.00015101851851851854, 'epoch': 2.5472222222222225}
Step 9180: {'loss': 0.6208, 'grad_norm': 1.3887593746185303, 'learning_rate': 0.0001500925925925926, 'epoch': 2.55}
Step 9190: {'loss': 0.607, 'grad_norm': 0.905049741268158, 'learning_rate': 0.00014916666666666667, 'epoch': 2.552777777777778}
Step 9200: {'loss': 0.624, 'grad_norm': 1.2567871809005737, 'learning_rate': 0.00014824074074074072, 'epoch': 2.5555555555555554}
Step 9210: {'loss': 0.6341, 'grad_norm': 0.8915895223617554, 'learning_rate': 0.00014731481481481483, 'epoch': 2.5583333333333336}
Step 9220: {'loss': 0.6419, 'grad_norm': 1.3564926385879517, 'learning_rate': 0.0001463888888888889, 'epoch': 2.561111111111111}
Step 9230: {'loss': 0.6377, 'grad_norm': 1.355383276939392, 'learning_rate': 0.00014546296296296296, 'epoch': 2.563888888888889}
Step 9240: {'loss': 0.6355, 'grad_norm': 1.313331127166748, 'learning_rate': 0.00014453703703703704, 'epoch': 2.5666666666666664}
Step 9250: {'loss': 0.6117, 'grad_norm': 1.3285938501358032, 'learning_rate': 0.00014361111111111112, 'epoch': 2.5694444444444446}
Step 9260: {'loss': 0.6247, 'grad_norm': 1.1727937459945679, 'learning_rate': 0.0001426851851851852, 'epoch': 2.572222222222222}
Step 9270: {'loss': 0.6038, 'grad_norm': 1.1547917127609253, 'learning_rate': 0.00014175925925925925, 'epoch': 2.575}
Step 9280: {'loss': 0.6029, 'grad_norm': 1.3496242761611938, 'learning_rate': 0.00014083333333333333, 'epoch': 2.5777777777777775}
Step 9290: {'loss': 0.6543, 'grad_norm': 1.7556872367858887, 'learning_rate': 0.0001399074074074074, 'epoch': 2.5805555555555557}
Step 9300: {'loss': 0.5839, 'grad_norm': 1.0077695846557617, 'learning_rate': 0.0001389814814814815, 'epoch': 2.5833333333333335}
Step 9310: {'loss': 0.616, 'grad_norm': 1.1927425861358643, 'learning_rate': 0.00013805555555555554, 'epoch': 2.5861111111111112}
Step 9320: {'loss': 0.6316, 'grad_norm': 1.3198555707931519, 'learning_rate': 0.00013712962962962962, 'epoch': 2.588888888888889}
Step 9330: {'loss': 0.634, 'grad_norm': 1.1347248554229736, 'learning_rate': 0.00013620370370370373, 'epoch': 2.591666666666667}
Step 9340: {'loss': 0.6493, 'grad_norm': 1.3755797147750854, 'learning_rate': 0.00013527777777777778, 'epoch': 2.5944444444444446}
Step 9350: {'loss': 0.5967, 'grad_norm': 1.2359611988067627, 'learning_rate': 0.00013435185185185186, 'epoch': 2.5972222222222223}
Step 9360: {'loss': 0.6143, 'grad_norm': 1.2083361148834229, 'learning_rate': 0.00013342592592592592, 'epoch': 2.6}
Step 9370: {'loss': 0.5939, 'grad_norm': 1.2557873725891113, 'learning_rate': 0.00013250000000000002, 'epoch': 2.602777777777778}
Step 9380: {'loss': 0.6029, 'grad_norm': 0.8860184550285339, 'learning_rate': 0.00013157407407407407, 'epoch': 2.6055555555555556}
Step 9390: {'loss': 0.6146, 'grad_norm': 1.0180697441101074, 'learning_rate': 0.00013064814814814815, 'epoch': 2.6083333333333334}
Step 9400: {'loss': 0.59, 'grad_norm': 0.8864313364028931, 'learning_rate': 0.0001297222222222222, 'epoch': 2.611111111111111}
Step 9410: {'loss': 0.6224, 'grad_norm': 1.0088175535202026, 'learning_rate': 0.0001287962962962963, 'epoch': 2.613888888888889}
Step 9420: {'loss': 0.6193, 'grad_norm': 1.41888427734375, 'learning_rate': 0.0001278703703703704, 'epoch': 2.6166666666666667}
Step 9430: {'loss': 0.5955, 'grad_norm': 0.8732973337173462, 'learning_rate': 0.00012694444444444445, 'epoch': 2.6194444444444445}
Step 9440: {'loss': 0.6454, 'grad_norm': 1.4597487449645996, 'learning_rate': 0.00012601851851851853, 'epoch': 2.6222222222222222}
Step 9450: {'loss': 0.6297, 'grad_norm': 1.0209391117095947, 'learning_rate': 0.00012509259259259258, 'epoch': 2.625}
Step 9460: {'loss': 0.6477, 'grad_norm': 1.0767630338668823, 'learning_rate': 0.00012416666666666666, 'epoch': 2.6277777777777778}
Step 9470: {'loss': 0.6116, 'grad_norm': 1.1623648405075073, 'learning_rate': 0.00012324074074074074, 'epoch': 2.6305555555555555}
Step 9480: {'loss': 0.6028, 'grad_norm': 1.3077590465545654, 'learning_rate': 0.00012231481481481482, 'epoch': 2.6333333333333333}
Step 9490: {'loss': 0.6237, 'grad_norm': 1.2838214635849, 'learning_rate': 0.0001213888888888889, 'epoch': 2.636111111111111}
Step 9500: {'loss': 0.6303, 'grad_norm': 1.2750943899154663, 'learning_rate': 0.00012046296296296296, 'epoch': 2.638888888888889}
Step 9510: {'loss': 0.6695, 'grad_norm': 1.3838378190994263, 'learning_rate': 0.00011953703703703704, 'epoch': 2.6416666666666666}
Step 9520: {'loss': 0.6331, 'grad_norm': 1.2816691398620605, 'learning_rate': 0.00011861111111111111, 'epoch': 2.6444444444444444}
Step 9530: {'loss': 0.6085, 'grad_norm': 1.0432474613189697, 'learning_rate': 0.00011768518518518519, 'epoch': 2.647222222222222}
Step 9540: {'loss': 0.6239, 'grad_norm': 1.1654325723648071, 'learning_rate': 0.00011675925925925925, 'epoch': 2.65}
Step 9550: {'loss': 0.6166, 'grad_norm': 1.3186006546020508, 'learning_rate': 0.00011583333333333333, 'epoch': 2.6527777777777777}
Step 9560: {'loss': 0.638, 'grad_norm': 1.470278024673462, 'learning_rate': 0.0001149074074074074, 'epoch': 2.6555555555555554}
Step 9570: {'loss': 0.5719, 'grad_norm': 1.1140787601470947, 'learning_rate': 0.00011398148148148148, 'epoch': 2.658333333333333}
Step 9580: {'loss': 0.5619, 'grad_norm': 1.0808935165405273, 'learning_rate': 0.00011305555555555556, 'epoch': 2.661111111111111}
Step 9590: {'loss': 0.6624, 'grad_norm': 1.2741963863372803, 'learning_rate': 0.00011212962962962964, 'epoch': 2.6638888888888888}
Step 9600: {'loss': 0.6285, 'grad_norm': 0.9129140377044678, 'learning_rate': 0.0001112037037037037, 'epoch': 2.6666666666666665}
Step 9610: {'loss': 0.6032, 'grad_norm': 0.7983822226524353, 'learning_rate': 0.00011027777777777779, 'epoch': 2.6694444444444443}
Step 9620: {'loss': 0.5895, 'grad_norm': 1.0062415599822998, 'learning_rate': 0.00010935185185185185, 'epoch': 2.6722222222222225}
Step 9630: {'loss': 0.6276, 'grad_norm': 1.2088114023208618, 'learning_rate': 0.00010842592592592593, 'epoch': 2.675}
Step 9640: {'loss': 0.5918, 'grad_norm': 1.1149622201919556, 'learning_rate': 0.0001075, 'epoch': 2.677777777777778}
Step 9650: {'loss': 0.6554, 'grad_norm': 1.2188856601715088, 'learning_rate': 0.00010657407407407408, 'epoch': 2.6805555555555554}
Step 9660: {'loss': 0.6386, 'grad_norm': 1.0696985721588135, 'learning_rate': 0.00010564814814814814, 'epoch': 2.6833333333333336}
Step 9670: {'loss': 0.6606, 'grad_norm': 1.218084692955017, 'learning_rate': 0.00010472222222222222, 'epoch': 2.686111111111111}
Step 9680: {'loss': 0.6277, 'grad_norm': 0.9687331914901733, 'learning_rate': 0.0001037962962962963, 'epoch': 2.688888888888889}
Step 9690: {'loss': 0.6984, 'grad_norm': 1.2964388132095337, 'learning_rate': 0.00010287037037037038, 'epoch': 2.6916666666666664}
Step 9700: {'loss': 0.6245, 'grad_norm': 0.9603760838508606, 'learning_rate': 0.00010194444444444445, 'epoch': 2.6944444444444446}
Step 9710: {'loss': 0.6224, 'grad_norm': 1.3765292167663574, 'learning_rate': 0.00010101851851851853, 'epoch': 2.697222222222222}
Step 9720: {'loss': 0.5854, 'grad_norm': 1.0192348957061768, 'learning_rate': 0.0001000925925925926, 'epoch': 2.7}
Step 9730: {'loss': 0.6346, 'grad_norm': 1.2422236204147339, 'learning_rate': 9.916666666666667e-05, 'epoch': 2.7027777777777775}
Step 9740: {'loss': 0.6281, 'grad_norm': 1.2098720073699951, 'learning_rate': 9.824074074074074e-05, 'epoch': 2.7055555555555557}
Step 9750: {'loss': 0.6118, 'grad_norm': 1.0820729732513428, 'learning_rate': 9.731481481481482e-05, 'epoch': 2.7083333333333335}
Step 9760: {'loss': 0.623, 'grad_norm': 1.2963182926177979, 'learning_rate': 9.638888888888889e-05, 'epoch': 2.7111111111111112}
Step 9770: {'loss': 0.6319, 'grad_norm': 1.725685954093933, 'learning_rate': 9.546296296296297e-05, 'epoch': 2.713888888888889}
Step 9780: {'loss': 0.6482, 'grad_norm': 1.2636196613311768, 'learning_rate': 9.453703703703703e-05, 'epoch': 2.716666666666667}
Step 9790: {'loss': 0.6569, 'grad_norm': 1.3756078481674194, 'learning_rate': 9.361111111111112e-05, 'epoch': 2.7194444444444446}
Step 9800: {'loss': 0.598, 'grad_norm': 1.2603791952133179, 'learning_rate': 9.268518518518519e-05, 'epoch': 2.7222222222222223}
Step 9810: {'loss': 0.5663, 'grad_norm': 1.1564671993255615, 'learning_rate': 9.175925925925927e-05, 'epoch': 2.725}
Step 9820: {'loss': 0.5837, 'grad_norm': 1.4170761108398438, 'learning_rate': 9.083333333333334e-05, 'epoch': 2.727777777777778}
Step 9830: {'loss': 0.6314, 'grad_norm': 0.8780383467674255, 'learning_rate': 8.990740740740742e-05, 'epoch': 2.7305555555555556}
Step 9840: {'loss': 0.6503, 'grad_norm': 1.1343954801559448, 'learning_rate': 8.898148148148148e-05, 'epoch': 2.7333333333333334}
Step 9850: {'loss': 0.5883, 'grad_norm': 1.202752947807312, 'learning_rate': 8.805555555555556e-05, 'epoch': 2.736111111111111}
Step 9860: {'loss': 0.586, 'grad_norm': 1.3401639461517334, 'learning_rate': 8.712962962962963e-05, 'epoch': 2.738888888888889}
Step 9870: {'loss': 0.5918, 'grad_norm': 1.2184115648269653, 'learning_rate': 8.62037037037037e-05, 'epoch': 2.7416666666666667}
Step 9880: {'loss': 0.6661, 'grad_norm': 1.4087423086166382, 'learning_rate': 8.527777777777777e-05, 'epoch': 2.7444444444444445}
Step 9890: {'loss': 0.5955, 'grad_norm': 1.2200359106063843, 'learning_rate': 8.435185185185185e-05, 'epoch': 2.7472222222222222}
Step 9900: {'loss': 0.5887, 'grad_norm': 1.1869540214538574, 'learning_rate': 8.342592592592593e-05, 'epoch': 2.75}
Step 9910: {'loss': 0.5831, 'grad_norm': 1.2964112758636475, 'learning_rate': 8.25e-05, 'epoch': 2.7527777777777778}
Step 9920: {'loss': 0.5766, 'grad_norm': 0.9501935839653015, 'learning_rate': 8.157407407407408e-05, 'epoch': 2.7555555555555555}
Step 9930: {'loss': 0.636, 'grad_norm': 1.112849473953247, 'learning_rate': 8.064814814814815e-05, 'epoch': 2.7583333333333333}
Step 9940: {'loss': 0.6184, 'grad_norm': 0.7434793710708618, 'learning_rate': 7.972222222222223e-05, 'epoch': 2.761111111111111}
Step 9950: {'loss': 0.582, 'grad_norm': 1.1304028034210205, 'learning_rate': 7.879629629629629e-05, 'epoch': 2.763888888888889}
Step 9960: {'loss': 0.621, 'grad_norm': 1.1996935606002808, 'learning_rate': 7.787037037037037e-05, 'epoch': 2.7666666666666666}
Step 9970: {'loss': 0.5952, 'grad_norm': 1.0314985513687134, 'learning_rate': 7.694444444444444e-05, 'epoch': 2.7694444444444444}
Step 9980: {'loss': 0.5778, 'grad_norm': 1.3083961009979248, 'learning_rate': 7.601851851851852e-05, 'epoch': 2.772222222222222}
Step 9990: {'loss': 0.5976, 'grad_norm': 1.374321460723877, 'learning_rate': 7.509259259259258e-05, 'epoch': 2.775}
Step 10000: {'loss': 0.581, 'grad_norm': 1.2497742176055908, 'learning_rate': 7.416666666666668e-05, 'epoch': 2.7777777777777777}
Step 10010: {'loss': 0.6185, 'grad_norm': 1.3879987001419067, 'learning_rate': 7.324074074074074e-05, 'epoch': 2.7805555555555554}
Step 10020: {'loss': 0.6441, 'grad_norm': 1.2511378526687622, 'learning_rate': 7.231481481481482e-05, 'epoch': 2.783333333333333}
Step 10030: {'loss': 0.6314, 'grad_norm': 1.4192302227020264, 'learning_rate': 7.138888888888889e-05, 'epoch': 2.786111111111111}
Step 10040: {'loss': 0.6038, 'grad_norm': 1.3234609365463257, 'learning_rate': 7.046296296296297e-05, 'epoch': 2.7888888888888888}
Step 10050: {'loss': 0.6039, 'grad_norm': 1.2395954132080078, 'learning_rate': 6.953703703703703e-05, 'epoch': 2.7916666666666665}
Step 10060: {'loss': 0.5804, 'grad_norm': 1.4085241556167603, 'learning_rate': 6.861111111111111e-05, 'epoch': 2.7944444444444443}
Step 10070: {'loss': 0.5667, 'grad_norm': 1.1195374727249146, 'learning_rate': 6.768518518518518e-05, 'epoch': 2.7972222222222225}
Step 10080: {'loss': 0.6322, 'grad_norm': 1.2067468166351318, 'learning_rate': 6.675925925925926e-05, 'epoch': 2.8}
Step 10090: {'loss': 0.5746, 'grad_norm': 0.9474177360534668, 'learning_rate': 6.583333333333333e-05, 'epoch': 2.802777777777778}
Step 10100: {'loss': 0.5901, 'grad_norm': 1.0683287382125854, 'learning_rate': 6.490740740740742e-05, 'epoch': 2.8055555555555554}
Step 10110: {'loss': 0.5772, 'grad_norm': 1.2107102870941162, 'learning_rate': 6.398148148148148e-05, 'epoch': 2.8083333333333336}
Step 10120: {'loss': 0.6055, 'grad_norm': 1.0943396091461182, 'learning_rate': 6.305555555555556e-05, 'epoch': 2.811111111111111}
Step 10130: {'loss': 0.629, 'grad_norm': 1.9672940969467163, 'learning_rate': 6.212962962962963e-05, 'epoch': 2.813888888888889}
Step 10140: {'loss': 0.6449, 'grad_norm': 1.6709506511688232, 'learning_rate': 6.120370370370371e-05, 'epoch': 2.8166666666666664}
Step 10150: {'loss': 0.6074, 'grad_norm': 0.837056577205658, 'learning_rate': 6.0277777777777776e-05, 'epoch': 2.8194444444444446}
Step 10160: {'loss': 0.5897, 'grad_norm': 1.0740196704864502, 'learning_rate': 5.935185185185185e-05, 'epoch': 2.822222222222222}
Step 10170: {'loss': 0.6025, 'grad_norm': 1.1204193830490112, 'learning_rate': 5.842592592592592e-05, 'epoch': 2.825}
Step 10180: {'loss': 0.5569, 'grad_norm': 1.1735714673995972, 'learning_rate': 5.75e-05, 'epoch': 2.8277777777777775}
Step 10190: {'loss': 0.5867, 'grad_norm': 0.8716453909873962, 'learning_rate': 5.6574074074074075e-05, 'epoch': 2.8305555555555557}
Step 10200: {'loss': 0.6134, 'grad_norm': 1.0989385843276978, 'learning_rate': 5.564814814814815e-05, 'epoch': 2.8333333333333335}
Step 10210: {'loss': 0.5932, 'grad_norm': 1.3092072010040283, 'learning_rate': 5.472222222222222e-05, 'epoch': 2.8361111111111112}
Step 10220: {'loss': 0.5992, 'grad_norm': 1.3336719274520874, 'learning_rate': 5.379629629629629e-05, 'epoch': 2.838888888888889}
Step 10230: {'loss': 0.599, 'grad_norm': 1.0853782892227173, 'learning_rate': 5.287037037037037e-05, 'epoch': 2.841666666666667}
Step 10240: {'loss': 0.6047, 'grad_norm': 1.3415225744247437, 'learning_rate': 5.1944444444444446e-05, 'epoch': 2.8444444444444446}
Step 10250: {'loss': 0.5902, 'grad_norm': 1.2587473392486572, 'learning_rate': 5.101851851851852e-05, 'epoch': 2.8472222222222223}
Step 10260: {'loss': 0.622, 'grad_norm': 1.744689702987671, 'learning_rate': 5.009259259259259e-05, 'epoch': 2.85}
Step 10270: {'loss': 0.6014, 'grad_norm': 1.113144040107727, 'learning_rate': 4.9166666666666665e-05, 'epoch': 2.852777777777778}
Step 10280: {'loss': 0.6313, 'grad_norm': 1.0500013828277588, 'learning_rate': 4.8240740740740744e-05, 'epoch': 2.8555555555555556}
Step 10290: {'loss': 0.6326, 'grad_norm': 1.3146193027496338, 'learning_rate': 4.731481481481482e-05, 'epoch': 2.8583333333333334}
Step 10300: {'loss': 0.5981, 'grad_norm': 1.202837586402893, 'learning_rate': 4.638888888888889e-05, 'epoch': 2.861111111111111}
Step 10310: {'loss': 0.6414, 'grad_norm': 1.260549545288086, 'learning_rate': 4.546296296296296e-05, 'epoch': 2.863888888888889}
Step 10320: {'loss': 0.646, 'grad_norm': 0.9005154967308044, 'learning_rate': 4.4537037037037036e-05, 'epoch': 2.8666666666666667}
Step 10330: {'loss': 0.6235, 'grad_norm': 1.3325560092926025, 'learning_rate': 4.3611111111111116e-05, 'epoch': 2.8694444444444445}
Step 10340: {'loss': 0.587, 'grad_norm': 1.4885644912719727, 'learning_rate': 4.268518518518519e-05, 'epoch': 2.8722222222222222}
Step 10350: {'loss': 0.6118, 'grad_norm': 0.925266683101654, 'learning_rate': 4.175925925925926e-05, 'epoch': 2.875}
Step 10360: {'loss': 0.6148, 'grad_norm': 1.1124345064163208, 'learning_rate': 4.0833333333333334e-05, 'epoch': 2.8777777777777778}
Step 10370: {'loss': 0.55, 'grad_norm': 0.8758650422096252, 'learning_rate': 3.990740740740741e-05, 'epoch': 2.8805555555555555}
Step 10380: {'loss': 0.6464, 'grad_norm': 0.9353983998298645, 'learning_rate': 3.898148148148148e-05, 'epoch': 2.8833333333333333}
Step 10390: {'loss': 0.6234, 'grad_norm': 1.5025795698165894, 'learning_rate': 3.805555555555556e-05, 'epoch': 2.886111111111111}
Step 10400: {'loss': 0.5454, 'grad_norm': 0.8291535973548889, 'learning_rate': 3.712962962962963e-05, 'epoch': 2.888888888888889}
Step 10410: {'loss': 0.6346, 'grad_norm': 1.4244457483291626, 'learning_rate': 3.6203703703703706e-05, 'epoch': 2.8916666666666666}
Step 10420: {'loss': 0.6075, 'grad_norm': 1.2638235092163086, 'learning_rate': 3.527777777777778e-05, 'epoch': 2.8944444444444444}
Step 10430: {'loss': 0.6035, 'grad_norm': 1.026483416557312, 'learning_rate': 3.435185185185185e-05, 'epoch': 2.897222222222222}
Step 10440: {'loss': 0.5901, 'grad_norm': 1.1493511199951172, 'learning_rate': 3.342592592592593e-05, 'epoch': 2.9}
Step 10450: {'loss': 0.5936, 'grad_norm': 1.1645572185516357, 'learning_rate': 3.2500000000000004e-05, 'epoch': 2.9027777777777777}
Step 10460: {'loss': 0.6162, 'grad_norm': 0.8335521817207336, 'learning_rate': 3.157407407407408e-05, 'epoch': 2.9055555555555554}
Step 10470: {'loss': 0.6048, 'grad_norm': 1.1061698198318481, 'learning_rate': 3.064814814814815e-05, 'epoch': 2.908333333333333}
Step 10480: {'loss': 0.6027, 'grad_norm': 1.2329670190811157, 'learning_rate': 2.9722222222222223e-05, 'epoch': 2.911111111111111}
Step 10490: {'loss': 0.6166, 'grad_norm': 1.3105123043060303, 'learning_rate': 2.8796296296296296e-05, 'epoch': 2.9138888888888888}
Step 10500: {'loss': 0.6295, 'grad_norm': 1.0829850435256958, 'learning_rate': 2.7870370370370372e-05, 'epoch': 2.9166666666666665}
Step 10510: {'loss': 0.5872, 'grad_norm': 1.2975836992263794, 'learning_rate': 2.6944444444444445e-05, 'epoch': 2.9194444444444443}
Step 10520: {'loss': 0.6054, 'grad_norm': 1.1741999387741089, 'learning_rate': 2.6018518518518518e-05, 'epoch': 2.9222222222222225}
Step 10530: {'loss': 0.6048, 'grad_norm': 0.9357525110244751, 'learning_rate': 2.5092592592592594e-05, 'epoch': 2.925}
Step 10540: {'loss': 0.5679, 'grad_norm': 0.6934877038002014, 'learning_rate': 2.4166666666666667e-05, 'epoch': 2.927777777777778}
Step 10550: {'loss': 0.6414, 'grad_norm': 1.1314536333084106, 'learning_rate': 2.3240740740740743e-05, 'epoch': 2.9305555555555554}
Step 10560: {'loss': 0.5704, 'grad_norm': 0.8626025319099426, 'learning_rate': 2.2314814814814816e-05, 'epoch': 2.9333333333333336}
Step 10570: {'loss': 0.6218, 'grad_norm': 1.3847171068191528, 'learning_rate': 2.138888888888889e-05, 'epoch': 2.936111111111111}
Step 10580: {'loss': 0.6062, 'grad_norm': 0.9007899761199951, 'learning_rate': 2.0462962962962965e-05, 'epoch': 2.938888888888889}
Step 10590: {'loss': 0.6156, 'grad_norm': 1.0505211353302002, 'learning_rate': 1.9537037037037038e-05, 'epoch': 2.9416666666666664}
Step 10600: {'loss': 0.6053, 'grad_norm': 0.9477766156196594, 'learning_rate': 1.861111111111111e-05, 'epoch': 2.9444444444444446}
Step 10610: {'loss': 0.6356, 'grad_norm': 1.2687005996704102, 'learning_rate': 1.7685185185185187e-05, 'epoch': 2.947222222222222}
Step 10620: {'loss': 0.6248, 'grad_norm': 1.2974332571029663, 'learning_rate': 1.675925925925926e-05, 'epoch': 2.95}
Step 10630: {'loss': 0.6024, 'grad_norm': 1.056604266166687, 'learning_rate': 1.5833333333333336e-05, 'epoch': 2.9527777777777775}
Step 10640: {'loss': 0.6375, 'grad_norm': 1.114558458328247, 'learning_rate': 1.4907407407407408e-05, 'epoch': 2.9555555555555557}
Step 10650: {'loss': 0.5491, 'grad_norm': 0.9177553653717041, 'learning_rate': 1.3981481481481482e-05, 'epoch': 2.9583333333333335}
Step 10660: {'loss': 0.5542, 'grad_norm': 1.109807014465332, 'learning_rate': 1.3055555555555557e-05, 'epoch': 2.9611111111111112}
Step 10670: {'loss': 0.6472, 'grad_norm': 1.162776231765747, 'learning_rate': 1.212962962962963e-05, 'epoch': 2.963888888888889}
Step 10680: {'loss': 0.6454, 'grad_norm': 1.3188397884368896, 'learning_rate': 1.1203703703703704e-05, 'epoch': 2.966666666666667}
Step 10690: {'loss': 0.6619, 'grad_norm': 1.2463593482971191, 'learning_rate': 1.0277777777777779e-05, 'epoch': 2.9694444444444446}
Step 10700: {'loss': 0.5871, 'grad_norm': 1.2096372842788696, 'learning_rate': 9.351851851851854e-06, 'epoch': 2.9722222222222223}
Step 10710: {'loss': 0.589, 'grad_norm': 1.5348626375198364, 'learning_rate': 8.425925925925925e-06, 'epoch': 2.975}
Step 10720: {'loss': 0.5828, 'grad_norm': 1.5192981958389282, 'learning_rate': 7.5e-06, 'epoch': 2.977777777777778}
Step 10730: {'loss': 0.5977, 'grad_norm': 0.8641830086708069, 'learning_rate': 6.574074074074074e-06, 'epoch': 2.9805555555555556}
Step 10740: {'loss': 0.6289, 'grad_norm': 0.9998703598976135, 'learning_rate': 5.648148148148148e-06, 'epoch': 2.9833333333333334}
Step 10750: {'loss': 0.5831, 'grad_norm': 0.6941900253295898, 'learning_rate': 4.722222222222222e-06, 'epoch': 2.986111111111111}
Step 10760: {'loss': 0.5553, 'grad_norm': 1.177101492881775, 'learning_rate': 3.7962962962962964e-06, 'epoch': 2.988888888888889}
Step 10770: {'loss': 0.5533, 'grad_norm': 0.9331647753715515, 'learning_rate': 2.8703703703703706e-06, 'epoch': 2.9916666666666667}
Step 10780: {'loss': 0.6378, 'grad_norm': 1.1408507823944092, 'learning_rate': 1.9444444444444444e-06, 'epoch': 2.9944444444444445}
Step 10790: {'loss': 0.5611, 'grad_norm': 1.400234341621399, 'learning_rate': 1.0185185185185185e-06, 'epoch': 2.9972222222222222}
Step 10800: {'loss': 0.6101, 'grad_norm': 1.34639310836792, 'learning_rate': 9.259259259259259e-08, 'epoch': 3.0}
Step 10800: {'train_runtime': 1708.2584, 'train_samples_per_second': 12.644, 'train_steps_per_second': 6.322, 'total_flos': 2.8362609831508378e+17, 'train_loss': 0.6440587857917504, 'epoch': 3.0}
