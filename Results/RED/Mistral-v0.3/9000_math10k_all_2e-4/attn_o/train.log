Step 1: {'loss': 1.7356, 'grad_norm': 36.75, 'learning_rate': 0.0, 'num_tokens': 8346.0, 'mean_token_accuracy': 0.7080622911453247, 'epoch': 0.003616636528028933}
Step 2: {'loss': 1.6975, 'grad_norm': 33.0, 'learning_rate': 2.3809523809523808e-06, 'num_tokens': 17185.0, 'mean_token_accuracy': 0.7126031517982483, 'epoch': 0.007233273056057866}
Step 3: {'loss': 1.7511, 'grad_norm': 37.25, 'learning_rate': 4.7619047619047615e-06, 'num_tokens': 25083.0, 'mean_token_accuracy': 0.7148924022912979, 'epoch': 0.0108499095840868}
Step 4: {'loss': 1.8227, 'grad_norm': 28.375, 'learning_rate': 7.142857142857143e-06, 'num_tokens': 32670.0, 'mean_token_accuracy': 0.7013514190912247, 'epoch': 0.014466546112115732}
Step 5: {'loss': 1.6894, 'grad_norm': 25.25, 'learning_rate': 9.523809523809523e-06, 'num_tokens': 40931.0, 'mean_token_accuracy': 0.7145270109176636, 'epoch': 0.018083182640144666}
Step 6: {'loss': 1.7194, 'grad_norm': 27.5, 'learning_rate': 1.1904761904761905e-05, 'num_tokens': 48800.0, 'mean_token_accuracy': 0.7053852528333664, 'epoch': 0.0216998191681736}
Step 7: {'loss': 1.7225, 'grad_norm': 19.75, 'learning_rate': 1.4285714285714285e-05, 'num_tokens': 56638.0, 'mean_token_accuracy': 0.6978476494550705, 'epoch': 0.02531645569620253}
Step 8: {'loss': 1.603, 'grad_norm': 11.4375, 'learning_rate': 1.6666666666666667e-05, 'num_tokens': 64684.0, 'mean_token_accuracy': 0.7247360199689865, 'epoch': 0.028933092224231464}
Step 9: {'loss': 1.5419, 'grad_norm': 14.9375, 'learning_rate': 1.9047619047619046e-05, 'num_tokens': 73190.0, 'mean_token_accuracy': 0.7231251001358032, 'epoch': 0.0325497287522604}
Step 10: {'loss': 1.5875, 'grad_norm': 13.25, 'learning_rate': 2.1428571428571428e-05, 'num_tokens': 81404.0, 'mean_token_accuracy': 0.7100774049758911, 'epoch': 0.03616636528028933}
Step 11: {'loss': 1.5119, 'grad_norm': 11.4375, 'learning_rate': 2.380952380952381e-05, 'num_tokens': 90726.0, 'mean_token_accuracy': 0.717437207698822, 'epoch': 0.039783001808318265}
Step 12: {'loss': 1.4761, 'grad_norm': 10.3125, 'learning_rate': 2.6190476190476192e-05, 'num_tokens': 98842.0, 'mean_token_accuracy': 0.7253800481557846, 'epoch': 0.0433996383363472}
Step 13: {'loss': 1.4234, 'grad_norm': 23.5, 'learning_rate': 2.857142857142857e-05, 'num_tokens': 107106.0, 'mean_token_accuracy': 0.741312175989151, 'epoch': 0.04701627486437613}
Step 14: {'loss': 1.4957, 'grad_norm': 53.75, 'learning_rate': 3.095238095238095e-05, 'num_tokens': 115215.0, 'mean_token_accuracy': 0.7368932813405991, 'epoch': 0.05063291139240506}
Step 15: {'loss': 1.3403, 'grad_norm': 46.75, 'learning_rate': 3.3333333333333335e-05, 'num_tokens': 123657.0, 'mean_token_accuracy': 0.7560893297195435, 'epoch': 0.054249547920433995}
Step 16: {'loss': 1.4409, 'grad_norm': 21.75, 'learning_rate': 3.571428571428572e-05, 'num_tokens': 131526.0, 'mean_token_accuracy': 0.7360446006059647, 'epoch': 0.05786618444846293}
Step 17: {'loss': 1.3172, 'grad_norm': 7.8125, 'learning_rate': 3.809523809523809e-05, 'num_tokens': 139926.0, 'mean_token_accuracy': 0.7461570352315903, 'epoch': 0.06148282097649186}
Step 18: {'loss': 1.3491, 'grad_norm': 8.25, 'learning_rate': 4.047619047619048e-05, 'num_tokens': 148221.0, 'mean_token_accuracy': 0.73761186003685, 'epoch': 0.0650994575045208}
Step 19: {'loss': 1.2594, 'grad_norm': 8.3125, 'learning_rate': 4.2857142857142856e-05, 'num_tokens': 156865.0, 'mean_token_accuracy': 0.7489702999591827, 'epoch': 0.06871609403254973}
Step 20: {'loss': 1.2873, 'grad_norm': 11.25, 'learning_rate': 4.523809523809524e-05, 'num_tokens': 164629.0, 'mean_token_accuracy': 0.748531699180603, 'epoch': 0.07233273056057866}
Step 21: {'loss': 1.1814, 'grad_norm': 8.25, 'learning_rate': 4.761904761904762e-05, 'num_tokens': 173410.0, 'mean_token_accuracy': 0.7615795880556107, 'epoch': 0.0759493670886076}
Step 22: {'loss': 1.1497, 'grad_norm': 7.65625, 'learning_rate': 5e-05, 'num_tokens': 181673.0, 'mean_token_accuracy': 0.7670873999595642, 'epoch': 0.07956600361663653}
Step 23: {'loss': 1.1978, 'grad_norm': 4.96875, 'learning_rate': 5.2380952380952384e-05, 'num_tokens': 190022.0, 'mean_token_accuracy': 0.7497924566268921, 'epoch': 0.08318264014466546}
Step 24: {'loss': 1.1467, 'grad_norm': 5.9375, 'learning_rate': 5.4761904761904766e-05, 'num_tokens': 198123.0, 'mean_token_accuracy': 0.765077069401741, 'epoch': 0.0867992766726944}
Step 25: {'loss': 1.1526, 'grad_norm': 6.15625, 'learning_rate': 5.714285714285714e-05, 'num_tokens': 206280.0, 'mean_token_accuracy': 0.7568462938070297, 'epoch': 0.09041591320072333}
Step 26: {'loss': 1.2025, 'grad_norm': 9.625, 'learning_rate': 5.9523809523809524e-05, 'num_tokens': 213578.0, 'mean_token_accuracy': 0.7490601539611816, 'epoch': 0.09403254972875226}
Step 27: {'loss': 1.1142, 'grad_norm': 17.0, 'learning_rate': 6.19047619047619e-05, 'num_tokens': 221080.0, 'mean_token_accuracy': 0.759089395403862, 'epoch': 0.09764918625678119}
Step 28: {'loss': 1.0612, 'grad_norm': 22.125, 'learning_rate': 6.428571428571429e-05, 'num_tokens': 229476.0, 'mean_token_accuracy': 0.767023578286171, 'epoch': 0.10126582278481013}
Step 29: {'loss': 1.0201, 'grad_norm': 24.375, 'learning_rate': 6.666666666666667e-05, 'num_tokens': 238208.0, 'mean_token_accuracy': 0.780083566904068, 'epoch': 0.10488245931283906}
Step 30: {'loss': 1.0149, 'grad_norm': 16.375, 'learning_rate': 6.904761904761905e-05, 'num_tokens': 246716.0, 'mean_token_accuracy': 0.7777349501848221, 'epoch': 0.10849909584086799}
Step 31: {'loss': 0.9381, 'grad_norm': 7.375, 'learning_rate': 7.142857142857143e-05, 'num_tokens': 256115.0, 'mean_token_accuracy': 0.7920218259096146, 'epoch': 0.11211573236889692}
Step 32: {'loss': 1.0247, 'grad_norm': 7.75, 'learning_rate': 7.380952380952382e-05, 'num_tokens': 264265.0, 'mean_token_accuracy': 0.7769720554351807, 'epoch': 0.11573236889692586}
Step 33: {'loss': 1.0045, 'grad_norm': 5.3125, 'learning_rate': 7.619047619047618e-05, 'num_tokens': 271682.0, 'mean_token_accuracy': 0.7903705686330795, 'epoch': 0.11934900542495479}
Step 34: {'loss': 0.9381, 'grad_norm': 8.8125, 'learning_rate': 7.857142857142858e-05, 'num_tokens': 280639.0, 'mean_token_accuracy': 0.798035204410553, 'epoch': 0.12296564195298372}
Step 35: {'loss': 0.9453, 'grad_norm': 4.0, 'learning_rate': 8.095238095238096e-05, 'num_tokens': 288394.0, 'mean_token_accuracy': 0.7984260022640228, 'epoch': 0.12658227848101267}
Step 36: {'loss': 0.9152, 'grad_norm': 2.75, 'learning_rate': 8.333333333333334e-05, 'num_tokens': 297225.0, 'mean_token_accuracy': 0.7978565692901611, 'epoch': 0.1301989150090416}
Step 37: {'loss': 0.9168, 'grad_norm': 2.328125, 'learning_rate': 8.571428571428571e-05, 'num_tokens': 306421.0, 'mean_token_accuracy': 0.7969881743192673, 'epoch': 0.13381555153707053}
Step 38: {'loss': 0.9561, 'grad_norm': 3.40625, 'learning_rate': 8.80952380952381e-05, 'num_tokens': 314552.0, 'mean_token_accuracy': 0.7912031561136246, 'epoch': 0.13743218806509946}
Step 39: {'loss': 0.9943, 'grad_norm': 4.21875, 'learning_rate': 9.047619047619048e-05, 'num_tokens': 322153.0, 'mean_token_accuracy': 0.7852059751749039, 'epoch': 0.1410488245931284}
Step 40: {'loss': 0.9369, 'grad_norm': 3.859375, 'learning_rate': 9.285714285714286e-05, 'num_tokens': 329823.0, 'mean_token_accuracy': 0.797671303153038, 'epoch': 0.14466546112115733}
Step 41: {'loss': 0.8824, 'grad_norm': 3.515625, 'learning_rate': 9.523809523809524e-05, 'num_tokens': 337378.0, 'mean_token_accuracy': 0.8148497939109802, 'epoch': 0.14828209764918626}
Step 42: {'loss': 0.8975, 'grad_norm': 4.75, 'learning_rate': 9.761904761904762e-05, 'num_tokens': 345418.0, 'mean_token_accuracy': 0.80351322889328, 'epoch': 0.1518987341772152}
Step 43: {'loss': 0.8791, 'grad_norm': 4.09375, 'learning_rate': 0.0001, 'num_tokens': 353295.0, 'mean_token_accuracy': 0.8052192777395248, 'epoch': 0.15551537070524413}
Step 44: {'loss': 0.8404, 'grad_norm': 5.03125, 'learning_rate': 0.00010238095238095237, 'num_tokens': 361107.0, 'mean_token_accuracy': 0.8129069358110428, 'epoch': 0.15913200723327306}
Step 45: {'loss': 0.8576, 'grad_norm': 8.75, 'learning_rate': 0.00010476190476190477, 'num_tokens': 368986.0, 'mean_token_accuracy': 0.807082861661911, 'epoch': 0.162748643761302}
Step 46: {'loss': 0.8411, 'grad_norm': 10.375, 'learning_rate': 0.00010714285714285715, 'num_tokens': 377527.0, 'mean_token_accuracy': 0.8077395707368851, 'epoch': 0.16636528028933092}
Step 47: {'loss': 0.8689, 'grad_norm': 11.125, 'learning_rate': 0.00010952380952380953, 'num_tokens': 385568.0, 'mean_token_accuracy': 0.8039624840021133, 'epoch': 0.16998191681735986}
Step 48: {'loss': 0.8254, 'grad_norm': 3.953125, 'learning_rate': 0.00011190476190476191, 'num_tokens': 394356.0, 'mean_token_accuracy': 0.8136804848909378, 'epoch': 0.1735985533453888}
Step 49: {'loss': 0.8369, 'grad_norm': 6.34375, 'learning_rate': 0.00011428571428571428, 'num_tokens': 402814.0, 'mean_token_accuracy': 0.8168036937713623, 'epoch': 0.17721518987341772}
Step 50: {'loss': 0.8691, 'grad_norm': 4.6875, 'learning_rate': 0.00011666666666666668, 'num_tokens': 410945.0, 'mean_token_accuracy': 0.8080899864435196, 'epoch': 0.18083182640144665}
Step 51: {'loss': 0.7989, 'grad_norm': 3.890625, 'learning_rate': 0.00011904761904761905, 'num_tokens': 419409.0, 'mean_token_accuracy': 0.8224543929100037, 'epoch': 0.1844484629294756}
Step 52: {'loss': 0.8511, 'grad_norm': 4.78125, 'learning_rate': 0.00012142857142857143, 'num_tokens': 426967.0, 'mean_token_accuracy': 0.8062163889408112, 'epoch': 0.18806509945750452}
Step 53: {'loss': 0.7709, 'grad_norm': 3.421875, 'learning_rate': 0.0001238095238095238, 'num_tokens': 434883.0, 'mean_token_accuracy': 0.8263297230005264, 'epoch': 0.19168173598553345}
Step 54: {'loss': 0.8048, 'grad_norm': 5.8125, 'learning_rate': 0.0001261904761904762, 'num_tokens': 442317.0, 'mean_token_accuracy': 0.8156429380178452, 'epoch': 0.19529837251356238}
Step 55: {'loss': 0.8169, 'grad_norm': 3.75, 'learning_rate': 0.00012857142857142858, 'num_tokens': 450246.0, 'mean_token_accuracy': 0.8151804804801941, 'epoch': 0.19891500904159132}
Step 56: {'loss': 0.7353, 'grad_norm': 13.0, 'learning_rate': 0.00013095238095238096, 'num_tokens': 458188.0, 'mean_token_accuracy': 0.825410395860672, 'epoch': 0.20253164556962025}
Step 57: {'loss': 0.8282, 'grad_norm': 11.5625, 'learning_rate': 0.00013333333333333334, 'num_tokens': 465926.0, 'mean_token_accuracy': 0.8045969903469086, 'epoch': 0.20614828209764918}
Step 58: {'loss': 0.7763, 'grad_norm': 4.65625, 'learning_rate': 0.00013571428571428572, 'num_tokens': 474175.0, 'mean_token_accuracy': 0.8132753819227219, 'epoch': 0.20976491862567812}
Step 59: {'loss': 0.7593, 'grad_norm': 3.5, 'learning_rate': 0.0001380952380952381, 'num_tokens': 482850.0, 'mean_token_accuracy': 0.8181809484958649, 'epoch': 0.21338155515370705}
Step 60: {'loss': 0.7194, 'grad_norm': 8.8125, 'learning_rate': 0.00014047619047619049, 'num_tokens': 491624.0, 'mean_token_accuracy': 0.8239763528108597, 'epoch': 0.21699819168173598}
Step 61: {'loss': 0.7141, 'grad_norm': 3.25, 'learning_rate': 0.00014285714285714287, 'num_tokens': 499686.0, 'mean_token_accuracy': 0.8258106857538223, 'epoch': 0.2206148282097649}
Step 62: {'loss': 0.7138, 'grad_norm': 3.390625, 'learning_rate': 0.00014523809523809525, 'num_tokens': 507577.0, 'mean_token_accuracy': 0.8274825364351273, 'epoch': 0.22423146473779385}
Step 63: {'loss': 0.7561, 'grad_norm': 3.78125, 'learning_rate': 0.00014761904761904763, 'num_tokens': 515396.0, 'mean_token_accuracy': 0.8160531669855118, 'epoch': 0.22784810126582278}
Step 64: {'loss': 0.7327, 'grad_norm': 3.0, 'learning_rate': 0.00015000000000000001, 'num_tokens': 523564.0, 'mean_token_accuracy': 0.8232984989881516, 'epoch': 0.2314647377938517}
Step 65: {'loss': 0.7241, 'grad_norm': 2.578125, 'learning_rate': 0.00015238095238095237, 'num_tokens': 530799.0, 'mean_token_accuracy': 0.827638179063797, 'epoch': 0.23508137432188064}
Step 66: {'loss': 0.7576, 'grad_norm': 7.90625, 'learning_rate': 0.00015476190476190478, 'num_tokens': 539076.0, 'mean_token_accuracy': 0.8155798614025116, 'epoch': 0.23869801084990958}
Step 67: {'loss': 0.7184, 'grad_norm': 2.328125, 'learning_rate': 0.00015714285714285716, 'num_tokens': 547650.0, 'mean_token_accuracy': 0.8274867385625839, 'epoch': 0.2423146473779385}
Step 68: {'loss': 0.7331, 'grad_norm': 1.9609375, 'learning_rate': 0.00015952380952380954, 'num_tokens': 555113.0, 'mean_token_accuracy': 0.8269504755735397, 'epoch': 0.24593128390596744}
Step 69: {'loss': 0.7708, 'grad_norm': 2.109375, 'learning_rate': 0.00016190476190476192, 'num_tokens': 563059.0, 'mean_token_accuracy': 0.8106060773134232, 'epoch': 0.24954792043399637}
Step 70: {'loss': 0.7239, 'grad_norm': 1.6953125, 'learning_rate': 0.00016428571428571428, 'num_tokens': 570977.0, 'mean_token_accuracy': 0.8264663517475128, 'epoch': 0.25316455696202533}
Step 71: {'loss': 0.7289, 'grad_norm': 5.8125, 'learning_rate': 0.0001666666666666667, 'num_tokens': 578916.0, 'mean_token_accuracy': 0.822263240814209, 'epoch': 0.25678119349005424}
Step 72: {'loss': 0.6775, 'grad_norm': 2.125, 'learning_rate': 0.00016904761904761904, 'num_tokens': 587397.0, 'mean_token_accuracy': 0.8332807272672653, 'epoch': 0.2603978300180832}
Step 73: {'loss': 0.6915, 'grad_norm': 2.5625, 'learning_rate': 0.00017142857142857143, 'num_tokens': 595236.0, 'mean_token_accuracy': 0.830265000462532, 'epoch': 0.2640144665461121}
Step 74: {'loss': 0.7178, 'grad_norm': 1.5390625, 'learning_rate': 0.00017380952380952383, 'num_tokens': 603211.0, 'mean_token_accuracy': 0.8310236632823944, 'epoch': 0.26763110307414106}
Step 75: {'loss': 0.7228, 'grad_norm': 3.28125, 'learning_rate': 0.0001761904761904762, 'num_tokens': 610639.0, 'mean_token_accuracy': 0.8247397989034653, 'epoch': 0.27124773960216997}
Step 76: {'loss': 0.6702, 'grad_norm': 1.484375, 'learning_rate': 0.0001785714285714286, 'num_tokens': 619569.0, 'mean_token_accuracy': 0.8366878926753998, 'epoch': 0.27486437613019893}
Step 77: {'loss': 0.6935, 'grad_norm': 2.109375, 'learning_rate': 0.00018095238095238095, 'num_tokens': 627971.0, 'mean_token_accuracy': 0.8305175602436066, 'epoch': 0.27848101265822783}
Step 78: {'loss': 0.7143, 'grad_norm': 1.3203125, 'learning_rate': 0.00018333333333333334, 'num_tokens': 636512.0, 'mean_token_accuracy': 0.8241583108901978, 'epoch': 0.2820976491862568}
Step 79: {'loss': 0.6771, 'grad_norm': 1.3125, 'learning_rate': 0.00018571428571428572, 'num_tokens': 645197.0, 'mean_token_accuracy': 0.8360873013734818, 'epoch': 0.2857142857142857}
Step 80: {'loss': 0.7116, 'grad_norm': 1.9140625, 'learning_rate': 0.0001880952380952381, 'num_tokens': 653770.0, 'mean_token_accuracy': 0.8211991339921951, 'epoch': 0.28933092224231466}
Step 81: {'loss': 0.685, 'grad_norm': 2.046875, 'learning_rate': 0.00019047619047619048, 'num_tokens': 661548.0, 'mean_token_accuracy': 0.8354407101869583, 'epoch': 0.29294755877034356}
Step 82: {'loss': 0.6628, 'grad_norm': 1.734375, 'learning_rate': 0.00019285714285714286, 'num_tokens': 670232.0, 'mean_token_accuracy': 0.8374838829040527, 'epoch': 0.2965641952983725}
Step 83: {'loss': 0.7185, 'grad_norm': 1.5078125, 'learning_rate': 0.00019523809523809525, 'num_tokens': 677984.0, 'mean_token_accuracy': 0.8237437158823013, 'epoch': 0.30018083182640143}
Step 84: {'loss': 0.6848, 'grad_norm': 1.484375, 'learning_rate': 0.00019761904761904763, 'num_tokens': 686798.0, 'mean_token_accuracy': 0.8304361253976822, 'epoch': 0.3037974683544304}
Step 85: {'loss': 0.685, 'grad_norm': 1.421875, 'learning_rate': 0.0002, 'num_tokens': 694737.0, 'mean_token_accuracy': 0.8335139602422714, 'epoch': 0.3074141048824593}
Step 86: {'loss': 0.6884, 'grad_norm': 1.3984375, 'learning_rate': 0.00019999911564241312, 'num_tokens': 703531.0, 'mean_token_accuracy': 0.8275825381278992, 'epoch': 0.31103074141048825}
Step 87: {'loss': 0.6918, 'grad_norm': 1.703125, 'learning_rate': 0.00019999646258529424, 'num_tokens': 711353.0, 'mean_token_accuracy': 0.8279900550842285, 'epoch': 0.31464737793851716}
Step 88: {'loss': 0.733, 'grad_norm': 1.609375, 'learning_rate': 0.0001999920408755684, 'num_tokens': 719568.0, 'mean_token_accuracy': 0.8223270773887634, 'epoch': 0.3182640144665461}
Step 89: {'loss': 0.6647, 'grad_norm': 1.4140625, 'learning_rate': 0.000199985850591443, 'num_tokens': 727952.0, 'mean_token_accuracy': 0.8351434618234634, 'epoch': 0.321880650994575}
Step 90: {'loss': 0.671, 'grad_norm': 2.3125, 'learning_rate': 0.0001999778918424066, 'num_tokens': 735908.0, 'mean_token_accuracy': 0.8302910178899765, 'epoch': 0.325497287522604}
Step 91: {'loss': 0.686, 'grad_norm': 1.796875, 'learning_rate': 0.00019996816476922677, 'num_tokens': 743685.0, 'mean_token_accuracy': 0.8307293355464935, 'epoch': 0.3291139240506329}
Step 92: {'loss': 0.691, 'grad_norm': 1.96875, 'learning_rate': 0.0001999566695439477, 'num_tokens': 751204.0, 'mean_token_accuracy': 0.8332337290048599, 'epoch': 0.33273056057866185}
Step 93: {'loss': 0.6986, 'grad_norm': 1.7265625, 'learning_rate': 0.00019994340636988724, 'num_tokens': 759484.0, 'mean_token_accuracy': 0.8250903487205505, 'epoch': 0.33634719710669075}
Step 94: {'loss': 0.6955, 'grad_norm': 1.15625, 'learning_rate': 0.00019992837548163316, 'num_tokens': 767225.0, 'mean_token_accuracy': 0.8301529735326767, 'epoch': 0.3399638336347197}
Step 95: {'loss': 0.7254, 'grad_norm': 1.2265625, 'learning_rate': 0.000199911577145039, 'num_tokens': 775516.0, 'mean_token_accuracy': 0.8195507228374481, 'epoch': 0.3435804701627486}
Step 96: {'loss': 0.6331, 'grad_norm': 1.078125, 'learning_rate': 0.00019989301165721957, 'num_tokens': 784504.0, 'mean_token_accuracy': 0.8434696942567825, 'epoch': 0.3471971066907776}
Step 97: {'loss': 0.7003, 'grad_norm': 1.171875, 'learning_rate': 0.00019987267934654538, 'num_tokens': 792947.0, 'mean_token_accuracy': 0.8277066200971603, 'epoch': 0.3508137432188065}
Step 98: {'loss': 0.6908, 'grad_norm': 1.59375, 'learning_rate': 0.00019985058057263717, 'num_tokens': 800470.0, 'mean_token_accuracy': 0.8286714553833008, 'epoch': 0.35443037974683544}
Step 99: {'loss': 0.697, 'grad_norm': 1.6015625, 'learning_rate': 0.00019982671572635926, 'num_tokens': 808605.0, 'mean_token_accuracy': 0.8294554501771927, 'epoch': 0.35804701627486435}
Step 100: {'loss': 0.6807, 'grad_norm': 1.3828125, 'learning_rate': 0.00019980108522981284, 'num_tokens': 816577.0, 'mean_token_accuracy': 0.8373894393444061, 'epoch': 0.3616636528028933}
Step 101: {'loss': 0.7038, 'grad_norm': 1.15625, 'learning_rate': 0.00019977368953632839, 'num_tokens': 825488.0, 'mean_token_accuracy': 0.8273479640483856, 'epoch': 0.36528028933092227}
Step 102: {'loss': 0.6942, 'grad_norm': 1.625, 'learning_rate': 0.00019974452913045767, 'num_tokens': 833710.0, 'mean_token_accuracy': 0.8291696310043335, 'epoch': 0.3688969258589512}
Step 103: {'loss': 0.6788, 'grad_norm': 1.4609375, 'learning_rate': 0.00019971360452796522, 'num_tokens': 841734.0, 'mean_token_accuracy': 0.8323320895433426, 'epoch': 0.37251356238698013}
Step 104: {'loss': 0.6892, 'grad_norm': 1.25, 'learning_rate': 0.00019968091627581917, 'num_tokens': 849575.0, 'mean_token_accuracy': 0.8370908945798874, 'epoch': 0.37613019891500904}
Step 105: {'loss': 0.6667, 'grad_norm': 1.2265625, 'learning_rate': 0.0001996464649521816, 'num_tokens': 858213.0, 'mean_token_accuracy': 0.8306612223386765, 'epoch': 0.379746835443038}
Step 106: {'loss': 0.6616, 'grad_norm': 1.890625, 'learning_rate': 0.0001996102511663983, 'num_tokens': 865619.0, 'mean_token_accuracy': 0.8416499197483063, 'epoch': 0.3833634719710669}
Step 107: {'loss': 0.6904, 'grad_norm': 1.28125, 'learning_rate': 0.000199572275558988, 'num_tokens': 874056.0, 'mean_token_accuracy': 0.8296097666025162, 'epoch': 0.38698010849909587}
Step 108: {'loss': 0.6674, 'grad_norm': 1.5625, 'learning_rate': 0.00019953253880163104, 'num_tokens': 881701.0, 'mean_token_accuracy': 0.8392436504364014, 'epoch': 0.39059674502712477}
Step 109: {'loss': 0.6634, 'grad_norm': 1.3671875, 'learning_rate': 0.00019949104159715743, 'num_tokens': 889766.0, 'mean_token_accuracy': 0.8352822959423065, 'epoch': 0.39421338155515373}
Step 110: {'loss': 0.6908, 'grad_norm': 1.8203125, 'learning_rate': 0.00019944778467953457, 'num_tokens': 897611.0, 'mean_token_accuracy': 0.8318702429533005, 'epoch': 0.39783001808318263}
Step 111: {'loss': 0.6622, 'grad_norm': 1.296875, 'learning_rate': 0.0001994027688138541, 'num_tokens': 906006.0, 'mean_token_accuracy': 0.8392594307661057, 'epoch': 0.4014466546112116}
Step 112: {'loss': 0.6401, 'grad_norm': 1.2578125, 'learning_rate': 0.0001993559947963185, 'num_tokens': 914379.0, 'mean_token_accuracy': 0.8466888070106506, 'epoch': 0.4050632911392405}
Step 113: {'loss': 0.6631, 'grad_norm': 1.9296875, 'learning_rate': 0.00019930746345422687, 'num_tokens': 922388.0, 'mean_token_accuracy': 0.8387991935014725, 'epoch': 0.40867992766726946}
Step 114: {'loss': 0.6804, 'grad_norm': 1.5546875, 'learning_rate': 0.00019925717564596047, 'num_tokens': 930670.0, 'mean_token_accuracy': 0.8402027636766434, 'epoch': 0.41229656419529837}
Step 115: {'loss': 0.6742, 'grad_norm': 1.359375, 'learning_rate': 0.00019920513226096733, 'num_tokens': 938888.0, 'mean_token_accuracy': 0.8368542790412903, 'epoch': 0.4159132007233273}
Step 116: {'loss': 0.6612, 'grad_norm': 1.3125, 'learning_rate': 0.00019915133421974677, 'num_tokens': 946815.0, 'mean_token_accuracy': 0.8407955467700958, 'epoch': 0.41952983725135623}
Step 117: {'loss': 0.6716, 'grad_norm': 1.8671875, 'learning_rate': 0.00019909578247383284, 'num_tokens': 954839.0, 'mean_token_accuracy': 0.8351261913776398, 'epoch': 0.4231464737793852}
Step 118: {'loss': 0.6897, 'grad_norm': 1.7890625, 'learning_rate': 0.00019903847800577777, 'num_tokens': 962105.0, 'mean_token_accuracy': 0.8292112052440643, 'epoch': 0.4267631103074141}
Step 119: {'loss': 0.652, 'grad_norm': 1.5859375, 'learning_rate': 0.00019897942182913432, 'num_tokens': 970346.0, 'mean_token_accuracy': 0.8347891569137573, 'epoch': 0.43037974683544306}
Step 120: {'loss': 0.692, 'grad_norm': 1.21875, 'learning_rate': 0.00019891861498843807, 'num_tokens': 978701.0, 'mean_token_accuracy': 0.8308912515640259, 'epoch': 0.43399638336347196}
Step 121: {'loss': 0.6817, 'grad_norm': 1.40625, 'learning_rate': 0.00019885605855918885, 'num_tokens': 986511.0, 'mean_token_accuracy': 0.8307782858610153, 'epoch': 0.4376130198915009}
Step 122: {'loss': 0.661, 'grad_norm': 1.3203125, 'learning_rate': 0.00019879175364783172, 'num_tokens': 994741.0, 'mean_token_accuracy': 0.8360387980937958, 'epoch': 0.4412296564195298}
Step 123: {'loss': 0.7211, 'grad_norm': 2.703125, 'learning_rate': 0.00019872570139173738, 'num_tokens': 1002894.0, 'mean_token_accuracy': 0.8247359544038773, 'epoch': 0.4448462929475588}
Step 124: {'loss': 0.6533, 'grad_norm': 1.5703125, 'learning_rate': 0.00019865790295918212, 'num_tokens': 1010848.0, 'mean_token_accuracy': 0.8408122509717941, 'epoch': 0.4484629294755877}
Step 125: {'loss': 0.6627, 'grad_norm': 1.3671875, 'learning_rate': 0.0001985883595493271, 'num_tokens': 1018519.0, 'mean_token_accuracy': 0.8350054621696472, 'epoch': 0.45207956600361665}
Step 126: {'loss': 0.7487, 'grad_norm': 1.2265625, 'learning_rate': 0.00019851707239219716, 'num_tokens': 1026694.0, 'mean_token_accuracy': 0.8177910894155502, 'epoch': 0.45569620253164556}
Step 127: {'loss': 0.7166, 'grad_norm': 1.7734375, 'learning_rate': 0.0001984440427486591, 'num_tokens': 1034365.0, 'mean_token_accuracy': 0.8310264050960541, 'epoch': 0.4593128390596745}
Step 128: {'loss': 0.6537, 'grad_norm': 1.3359375, 'learning_rate': 0.00019836927191039925, 'num_tokens': 1043037.0, 'mean_token_accuracy': 0.8349968791007996, 'epoch': 0.4629294755877034}
Step 129: {'loss': 0.6495, 'grad_norm': 1.1171875, 'learning_rate': 0.00019829276119990078, 'num_tokens': 1051448.0, 'mean_token_accuracy': 0.8364637792110443, 'epoch': 0.4665461121157324}
Step 130: {'loss': 0.654, 'grad_norm': 1.1640625, 'learning_rate': 0.00019821451197042026, 'num_tokens': 1059176.0, 'mean_token_accuracy': 0.837421715259552, 'epoch': 0.4701627486437613}
Step 131: {'loss': 0.6787, 'grad_norm': 1.328125, 'learning_rate': 0.0001981345256059637, 'num_tokens': 1067919.0, 'mean_token_accuracy': 0.8319878131151199, 'epoch': 0.47377938517179025}
Step 132: {'loss': 0.6613, 'grad_norm': 1.3046875, 'learning_rate': 0.00019805280352126205, 'num_tokens': 1075756.0, 'mean_token_accuracy': 0.8339641094207764, 'epoch': 0.47739602169981915}
Step 133: {'loss': 0.6861, 'grad_norm': 1.4375, 'learning_rate': 0.0001979693471617462, 'num_tokens': 1084081.0, 'mean_token_accuracy': 0.8293211311101913, 'epoch': 0.4810126582278481}
Step 134: {'loss': 0.6644, 'grad_norm': 1.21875, 'learning_rate': 0.00019788415800352146, 'num_tokens': 1092778.0, 'mean_token_accuracy': 0.8330710232257843, 'epoch': 0.484629294755877}
Step 135: {'loss': 0.6884, 'grad_norm': 1.125, 'learning_rate': 0.0001977972375533414, 'num_tokens': 1100975.0, 'mean_token_accuracy': 0.8260301053524017, 'epoch': 0.488245931283906}
Step 136: {'loss': 0.6315, 'grad_norm': 1.2265625, 'learning_rate': 0.00019770858734858126, 'num_tokens': 1109496.0, 'mean_token_accuracy': 0.8390386402606964, 'epoch': 0.4918625678119349}
Step 137: {'loss': 0.6453, 'grad_norm': 1.6484375, 'learning_rate': 0.00019761820895721055, 'num_tokens': 1117943.0, 'mean_token_accuracy': 0.8372690826654434, 'epoch': 0.49547920433996384}
Step 138: {'loss': 0.6415, 'grad_norm': 1.2578125, 'learning_rate': 0.00019752610397776564, 'num_tokens': 1126045.0, 'mean_token_accuracy': 0.8351434469223022, 'epoch': 0.49909584086799275}
Step 139: {'loss': 0.6303, 'grad_norm': 1.1953125, 'learning_rate': 0.00019743227403932134, 'num_tokens': 1134069.0, 'mean_token_accuracy': 0.8478660881519318, 'epoch': 0.5027124773960217}
Step 140: {'loss': 0.6643, 'grad_norm': 1.3046875, 'learning_rate': 0.00019733672080146194, 'num_tokens': 1142375.0, 'mean_token_accuracy': 0.8346002697944641, 'epoch': 0.5063291139240507}
Step 141: {'loss': 0.6773, 'grad_norm': 1.4921875, 'learning_rate': 0.0001972394459542521, 'num_tokens': 1149982.0, 'mean_token_accuracy': 0.8345579355955124, 'epoch': 0.5099457504520796}
Step 142: {'loss': 0.7085, 'grad_norm': 1.2421875, 'learning_rate': 0.00019714045121820676, 'num_tokens': 1157703.0, 'mean_token_accuracy': 0.82237808406353, 'epoch': 0.5135623869801085}
Step 143: {'loss': 0.6648, 'grad_norm': 1.4609375, 'learning_rate': 0.00019703973834426088, 'num_tokens': 1165463.0, 'mean_token_accuracy': 0.835750013589859, 'epoch': 0.5171790235081374}
Step 144: {'loss': 0.6572, 'grad_norm': 1.4375, 'learning_rate': 0.00019693730911373832, 'num_tokens': 1172951.0, 'mean_token_accuracy': 0.8402294665575027, 'epoch': 0.5207956600361664}
Step 145: {'loss': 0.7409, 'grad_norm': 1.359375, 'learning_rate': 0.00019683316533832042, 'num_tokens': 1180326.0, 'mean_token_accuracy': 0.8156685531139374, 'epoch': 0.5244122965641953}
Step 146: {'loss': 0.6869, 'grad_norm': 2.1875, 'learning_rate': 0.00019672730886001393, 'num_tokens': 1188275.0, 'mean_token_accuracy': 0.8347951620817184, 'epoch': 0.5280289330922242}
Step 147: {'loss': 0.6739, 'grad_norm': 1.3125, 'learning_rate': 0.00019661974155111848, 'num_tokens': 1195718.0, 'mean_token_accuracy': 0.8334496468305588, 'epoch': 0.5316455696202531}
Step 148: {'loss': 0.6485, 'grad_norm': 1.09375, 'learning_rate': 0.00019651046531419332, 'num_tokens': 1204279.0, 'mean_token_accuracy': 0.8401355147361755, 'epoch': 0.5352622061482821}
Step 149: {'loss': 0.6449, 'grad_norm': 1.28125, 'learning_rate': 0.00019639948208202395, 'num_tokens': 1212429.0, 'mean_token_accuracy': 0.8444821238517761, 'epoch': 0.538878842676311}
Step 150: {'loss': 0.6443, 'grad_norm': 1.2734375, 'learning_rate': 0.0001962867938175875, 'num_tokens': 1220789.0, 'mean_token_accuracy': 0.8380990922451019, 'epoch': 0.5424954792043399}
Step 151: {'loss': 0.6862, 'grad_norm': 1.5234375, 'learning_rate': 0.0001961724025140185, 'num_tokens': 1228036.0, 'mean_token_accuracy': 0.832186296582222, 'epoch': 0.546112115732369}
Step 152: {'loss': 0.7165, 'grad_norm': 1.453125, 'learning_rate': 0.00019605631019457324, 'num_tokens': 1235776.0, 'mean_token_accuracy': 0.827055886387825, 'epoch': 0.5497287522603979}
Step 153: {'loss': 0.6181, 'grad_norm': 1.078125, 'learning_rate': 0.0001959385189125942, 'num_tokens': 1243206.0, 'mean_token_accuracy': 0.8458166718482971, 'epoch': 0.5533453887884268}
Step 154: {'loss': 0.6861, 'grad_norm': 1.109375, 'learning_rate': 0.0001958190307514737, 'num_tokens': 1251131.0, 'mean_token_accuracy': 0.8311118930578232, 'epoch': 0.5569620253164557}
Step 155: {'loss': 0.6872, 'grad_norm': 1.3671875, 'learning_rate': 0.00019569784782461693, 'num_tokens': 1259296.0, 'mean_token_accuracy': 0.8336381614208221, 'epoch': 0.5605786618444847}
Step 156: {'loss': 0.6864, 'grad_norm': 1.2734375, 'learning_rate': 0.0001955749722754047, 'num_tokens': 1267032.0, 'mean_token_accuracy': 0.8292432278394699, 'epoch': 0.5641952983725136}
Step 157: {'loss': 0.6811, 'grad_norm': 1.40625, 'learning_rate': 0.0001954504062771555, 'num_tokens': 1275635.0, 'mean_token_accuracy': 0.8302258402109146, 'epoch': 0.5678119349005425}
Step 158: {'loss': 0.6312, 'grad_norm': 1.078125, 'learning_rate': 0.0001953241520330871, 'num_tokens': 1284106.0, 'mean_token_accuracy': 0.8409279584884644, 'epoch': 0.5714285714285714}
Step 159: {'loss': 0.631, 'grad_norm': 1.0859375, 'learning_rate': 0.00019519621177627742, 'num_tokens': 1292676.0, 'mean_token_accuracy': 0.8447392582893372, 'epoch': 0.5750452079566004}
Step 160: {'loss': 0.6748, 'grad_norm': 1.4375, 'learning_rate': 0.0001950665877696252, 'num_tokens': 1301166.0, 'mean_token_accuracy': 0.8322529047727585, 'epoch': 0.5786618444846293}
Step 161: {'loss': 0.6658, 'grad_norm': 1.4765625, 'learning_rate': 0.00019493528230580992, 'num_tokens': 1310192.0, 'mean_token_accuracy': 0.8312121778726578, 'epoch': 0.5822784810126582}
Step 162: {'loss': 0.6518, 'grad_norm': 1.0, 'learning_rate': 0.00019480229770725124, 'num_tokens': 1318495.0, 'mean_token_accuracy': 0.8407234996557236, 'epoch': 0.5858951175406871}
Step 163: {'loss': 0.712, 'grad_norm': 1.125, 'learning_rate': 0.0001946676363260679, 'num_tokens': 1326259.0, 'mean_token_accuracy': 0.8211874663829803, 'epoch': 0.5895117540687161}
Step 164: {'loss': 0.6978, 'grad_norm': 1.171875, 'learning_rate': 0.00019453130054403626, 'num_tokens': 1333499.0, 'mean_token_accuracy': 0.8289225548505783, 'epoch': 0.593128390596745}
Step 165: {'loss': 0.649, 'grad_norm': 1.0625, 'learning_rate': 0.00019439329277254788, 'num_tokens': 1341902.0, 'mean_token_accuracy': 0.840856283903122, 'epoch': 0.596745027124774}
Step 166: {'loss': 0.6626, 'grad_norm': 0.93359375, 'learning_rate': 0.00019425361545256727, 'num_tokens': 1350355.0, 'mean_token_accuracy': 0.832320362329483, 'epoch': 0.6003616636528029}
Step 167: {'loss': 0.6391, 'grad_norm': 1.125, 'learning_rate': 0.0001941122710545883, 'num_tokens': 1357645.0, 'mean_token_accuracy': 0.8428410887718201, 'epoch': 0.6039783001808319}
Step 168: {'loss': 0.6675, 'grad_norm': 1.0546875, 'learning_rate': 0.00019396926207859084, 'num_tokens': 1365504.0, 'mean_token_accuracy': 0.8349396735429764, 'epoch': 0.6075949367088608}
Step 169: {'loss': 0.6803, 'grad_norm': 1.34375, 'learning_rate': 0.00019382459105399632, 'num_tokens': 1373588.0, 'mean_token_accuracy': 0.8312664031982422, 'epoch': 0.6112115732368897}
Step 170: {'loss': 0.6869, 'grad_norm': 1.234375, 'learning_rate': 0.00019367826053962308, 'num_tokens': 1381683.0, 'mean_token_accuracy': 0.8293314576148987, 'epoch': 0.6148282097649186}
Step 171: {'loss': 0.6727, 'grad_norm': 1.109375, 'learning_rate': 0.00019353027312364116, 'num_tokens': 1389793.0, 'mean_token_accuracy': 0.8375025987625122, 'epoch': 0.6184448462929476}
Step 172: {'loss': 0.6753, 'grad_norm': 1.390625, 'learning_rate': 0.00019338063142352644, 'num_tokens': 1397520.0, 'mean_token_accuracy': 0.8353434354066849, 'epoch': 0.6220614828209765}
Step 173: {'loss': 0.6489, 'grad_norm': 1.125, 'learning_rate': 0.0001932293380860144, 'num_tokens': 1406441.0, 'mean_token_accuracy': 0.8387809693813324, 'epoch': 0.6256781193490054}
Step 174: {'loss': 0.6821, 'grad_norm': 1.0625, 'learning_rate': 0.0001930763957870532, 'num_tokens': 1414262.0, 'mean_token_accuracy': 0.8324494510889053, 'epoch': 0.6292947558770343}
Step 175: {'loss': 0.6571, 'grad_norm': 1.1953125, 'learning_rate': 0.00019292180723175654, 'num_tokens': 1422877.0, 'mean_token_accuracy': 0.8316703736782074, 'epoch': 0.6329113924050633}
Step 176: {'loss': 0.6323, 'grad_norm': 1.1171875, 'learning_rate': 0.0001927655751543556, 'num_tokens': 1430912.0, 'mean_token_accuracy': 0.8478873372077942, 'epoch': 0.6365280289330922}
Step 177: {'loss': 0.6143, 'grad_norm': 1.1875, 'learning_rate': 0.0001926077023181509, 'num_tokens': 1438746.0, 'mean_token_accuracy': 0.851535364985466, 'epoch': 0.6401446654611211}
Step 178: {'loss': 0.6348, 'grad_norm': 0.96484375, 'learning_rate': 0.00019244819151546322, 'num_tokens': 1448323.0, 'mean_token_accuracy': 0.8381128162145615, 'epoch': 0.64376130198915}
Step 179: {'loss': 0.6563, 'grad_norm': 1.1953125, 'learning_rate': 0.00019228704556758434, 'num_tokens': 1456633.0, 'mean_token_accuracy': 0.8346040844917297, 'epoch': 0.6473779385171791}
Step 180: {'loss': 0.6411, 'grad_norm': 1.078125, 'learning_rate': 0.00019212426732472708, 'num_tokens': 1464475.0, 'mean_token_accuracy': 0.8414042145013809, 'epoch': 0.650994575045208}
Step 181: {'loss': 0.6593, 'grad_norm': 1.2265625, 'learning_rate': 0.00019195985966597494, 'num_tokens': 1472503.0, 'mean_token_accuracy': 0.834310993552208, 'epoch': 0.6546112115732369}
Step 182: {'loss': 0.6703, 'grad_norm': 1.015625, 'learning_rate': 0.00019179382549923112, 'num_tokens': 1480520.0, 'mean_token_accuracy': 0.8327518999576569, 'epoch': 0.6582278481012658}
Step 183: {'loss': 0.6982, 'grad_norm': 1.140625, 'learning_rate': 0.0001916261677611671, 'num_tokens': 1488423.0, 'mean_token_accuracy': 0.8260306119918823, 'epoch': 0.6618444846292948}
Step 184: {'loss': 0.6619, 'grad_norm': 1.453125, 'learning_rate': 0.00019145688941717075, 'num_tokens': 1497286.0, 'mean_token_accuracy': 0.835246741771698, 'epoch': 0.6654611211573237}
Step 185: {'loss': 0.6455, 'grad_norm': 1.25, 'learning_rate': 0.0001912859934612938, 'num_tokens': 1504914.0, 'mean_token_accuracy': 0.8363020569086075, 'epoch': 0.6690777576853526}
Step 186: {'loss': 0.6545, 'grad_norm': 1.078125, 'learning_rate': 0.000191113482916199, 'num_tokens': 1513151.0, 'mean_token_accuracy': 0.8366299122571945, 'epoch': 0.6726943942133815}
Step 187: {'loss': 0.6626, 'grad_norm': 1.140625, 'learning_rate': 0.00019093936083310653, 'num_tokens': 1521405.0, 'mean_token_accuracy': 0.8344213664531708, 'epoch': 0.6763110307414105}
Step 188: {'loss': 0.6669, 'grad_norm': 1.2109375, 'learning_rate': 0.00019076363029174007, 'num_tokens': 1529487.0, 'mean_token_accuracy': 0.8316411823034286, 'epoch': 0.6799276672694394}
Step 189: {'loss': 0.646, 'grad_norm': 1.21875, 'learning_rate': 0.00019058629440027236, 'num_tokens': 1537004.0, 'mean_token_accuracy': 0.8371091187000275, 'epoch': 0.6835443037974683}
Step 190: {'loss': 0.6767, 'grad_norm': 1.421875, 'learning_rate': 0.00019040735629527027, 'num_tokens': 1545015.0, 'mean_token_accuracy': 0.8286631405353546, 'epoch': 0.6871609403254972}
Step 191: {'loss': 0.6872, 'grad_norm': 1.2265625, 'learning_rate': 0.00019022681914163917, 'num_tokens': 1552059.0, 'mean_token_accuracy': 0.83485047519207, 'epoch': 0.6907775768535263}
Step 192: {'loss': 0.7008, 'grad_norm': 1.296875, 'learning_rate': 0.0001900446861325671, 'num_tokens': 1559965.0, 'mean_token_accuracy': 0.8287644684314728, 'epoch': 0.6943942133815552}
Step 193: {'loss': 0.6914, 'grad_norm': 1.1328125, 'learning_rate': 0.00018986096048946824, 'num_tokens': 1568840.0, 'mean_token_accuracy': 0.8283108174800873, 'epoch': 0.6980108499095841}
Step 194: {'loss': 0.6191, 'grad_norm': 1.1875, 'learning_rate': 0.00018967564546192592, 'num_tokens': 1577465.0, 'mean_token_accuracy': 0.845394566655159, 'epoch': 0.701627486437613}
Step 195: {'loss': 0.5938, 'grad_norm': 1.109375, 'learning_rate': 0.00018948874432763512, 'num_tokens': 1585734.0, 'mean_token_accuracy': 0.8533122539520264, 'epoch': 0.705244122965642}
Step 196: {'loss': 0.6955, 'grad_norm': 1.1484375, 'learning_rate': 0.0001893002603923446, 'num_tokens': 1593389.0, 'mean_token_accuracy': 0.8278193771839142, 'epoch': 0.7088607594936709}
Step 197: {'loss': 0.6358, 'grad_norm': 1.453125, 'learning_rate': 0.0001891101969897983, 'num_tokens': 1601798.0, 'mean_token_accuracy': 0.8440530151128769, 'epoch': 0.7124773960216998}
Step 198: {'loss': 0.6998, 'grad_norm': 1.3359375, 'learning_rate': 0.00018891855748167649, 'num_tokens': 1609551.0, 'mean_token_accuracy': 0.8262957185506821, 'epoch': 0.7160940325497287}
Step 199: {'loss': 0.6569, 'grad_norm': 1.1484375, 'learning_rate': 0.00018872534525753615, 'num_tokens': 1618055.0, 'mean_token_accuracy': 0.8343593031167984, 'epoch': 0.7197106690777577}
Step 200: {'loss': 0.7009, 'grad_norm': 1.1328125, 'learning_rate': 0.00018853056373475133, 'num_tokens': 1626369.0, 'mean_token_accuracy': 0.8238454610109329, 'epoch': 0.7233273056057866}
Step 201: {'loss': 0.6251, 'grad_norm': 1.0078125, 'learning_rate': 0.0001883342163584523, 'num_tokens': 1635036.0, 'mean_token_accuracy': 0.8459856808185577, 'epoch': 0.7269439421338155}
Step 202: {'loss': 0.67, 'grad_norm': 1.2578125, 'learning_rate': 0.00018813630660146488, 'num_tokens': 1642743.0, 'mean_token_accuracy': 0.8363728076219559, 'epoch': 0.7305605786618445}
Step 203: {'loss': 0.6959, 'grad_norm': 0.98046875, 'learning_rate': 0.00018793683796424903, 'num_tokens': 1650999.0, 'mean_token_accuracy': 0.8276984840631485, 'epoch': 0.7341772151898734}
Step 204: {'loss': 0.66, 'grad_norm': 1.25, 'learning_rate': 0.00018773581397483677, 'num_tokens': 1659226.0, 'mean_token_accuracy': 0.8333583921194077, 'epoch': 0.7377938517179023}
Step 205: {'loss': 0.6505, 'grad_norm': 1.3671875, 'learning_rate': 0.0001875332381887699, 'num_tokens': 1667102.0, 'mean_token_accuracy': 0.8336751163005829, 'epoch': 0.7414104882459313}
Step 206: {'loss': 0.6525, 'grad_norm': 1.1328125, 'learning_rate': 0.00018732911418903713, 'num_tokens': 1675766.0, 'mean_token_accuracy': 0.8343454897403717, 'epoch': 0.7450271247739603}
Step 207: {'loss': 0.6783, 'grad_norm': 1.171875, 'learning_rate': 0.00018712344558601056, 'num_tokens': 1684107.0, 'mean_token_accuracy': 0.8338682651519775, 'epoch': 0.7486437613019892}
Step 208: {'loss': 0.6557, 'grad_norm': 1.0703125, 'learning_rate': 0.00018691623601738199, 'num_tokens': 1692168.0, 'mean_token_accuracy': 0.8361476510763168, 'epoch': 0.7522603978300181}
Step 209: {'loss': 0.6168, 'grad_norm': 1.1171875, 'learning_rate': 0.00018670748914809856, 'num_tokens': 1700143.0, 'mean_token_accuracy': 0.8501090705394745, 'epoch': 0.755877034358047}
Step 210: {'loss': 0.6557, 'grad_norm': 1.046875, 'learning_rate': 0.00018649720867029774, 'num_tokens': 1709254.0, 'mean_token_accuracy': 0.8377736806869507, 'epoch': 0.759493670886076}
Step 211: {'loss': 0.6444, 'grad_norm': 1.109375, 'learning_rate': 0.00018628539830324229, 'num_tokens': 1717107.0, 'mean_token_accuracy': 0.8385031372308731, 'epoch': 0.7631103074141049}
Step 212: {'loss': 0.6701, 'grad_norm': 1.2734375, 'learning_rate': 0.00018607206179325431, 'num_tokens': 1726116.0, 'mean_token_accuracy': 0.8351213186979294, 'epoch': 0.7667269439421338}
Step 213: {'loss': 0.6358, 'grad_norm': 0.99609375, 'learning_rate': 0.00018585720291364904, 'num_tokens': 1734153.0, 'mean_token_accuracy': 0.838577002286911, 'epoch': 0.7703435804701627}
Step 214: {'loss': 0.6763, 'grad_norm': 1.375, 'learning_rate': 0.00018564082546466805, 'num_tokens': 1741651.0, 'mean_token_accuracy': 0.8403173089027405, 'epoch': 0.7739602169981917}
Step 215: {'loss': 0.653, 'grad_norm': 1.015625, 'learning_rate': 0.00018542293327341208, 'num_tokens': 1749994.0, 'mean_token_accuracy': 0.8363102972507477, 'epoch': 0.7775768535262206}
Step 216: {'loss': 0.651, 'grad_norm': 1.0234375, 'learning_rate': 0.00018520353019377348, 'num_tokens': 1757906.0, 'mean_token_accuracy': 0.8424698114395142, 'epoch': 0.7811934900542495}
Step 217: {'loss': 0.6551, 'grad_norm': 1.1796875, 'learning_rate': 0.00018498262010636774, 'num_tokens': 1766578.0, 'mean_token_accuracy': 0.8357950001955032, 'epoch': 0.7848101265822784}
Step 218: {'loss': 0.6642, 'grad_norm': 1.125, 'learning_rate': 0.00018476020691846515, 'num_tokens': 1775135.0, 'mean_token_accuracy': 0.8295414000749588, 'epoch': 0.7884267631103075}
Step 219: {'loss': 0.6669, 'grad_norm': 1.1015625, 'learning_rate': 0.00018453629456392144, 'num_tokens': 1783147.0, 'mean_token_accuracy': 0.8344612866640091, 'epoch': 0.7920433996383364}
Step 220: {'loss': 0.6863, 'grad_norm': 1.09375, 'learning_rate': 0.00018431088700310844, 'num_tokens': 1791132.0, 'mean_token_accuracy': 0.8358092159032822, 'epoch': 0.7956600361663653}
Step 221: {'loss': 0.675, 'grad_norm': 1.5, 'learning_rate': 0.00018408398822284392, 'num_tokens': 1798445.0, 'mean_token_accuracy': 0.8332682996988297, 'epoch': 0.7992766726943942}
Step 222: {'loss': 0.678, 'grad_norm': 1.1171875, 'learning_rate': 0.00018385560223632095, 'num_tokens': 1806960.0, 'mean_token_accuracy': 0.8287836164236069, 'epoch': 0.8028933092224232}
Step 223: {'loss': 0.6829, 'grad_norm': 1.1171875, 'learning_rate': 0.00018362573308303718, 'num_tokens': 1814900.0, 'mean_token_accuracy': 0.828330010175705, 'epoch': 0.8065099457504521}
Step 224: {'loss': 0.6234, 'grad_norm': 1.34375, 'learning_rate': 0.00018339438482872322, 'num_tokens': 1823040.0, 'mean_token_accuracy': 0.8411369621753693, 'epoch': 0.810126582278481}
Step 225: {'loss': 0.661, 'grad_norm': 1.2421875, 'learning_rate': 0.0001831615615652707, 'num_tokens': 1830605.0, 'mean_token_accuracy': 0.8381419330835342, 'epoch': 0.8137432188065099}
Step 226: {'loss': 0.6635, 'grad_norm': 1.6953125, 'learning_rate': 0.00018292726741066007, 'num_tokens': 1838541.0, 'mean_token_accuracy': 0.8322260528802872, 'epoch': 0.8173598553345389}
Step 227: {'loss': 0.6648, 'grad_norm': 1.078125, 'learning_rate': 0.00018269150650888753, 'num_tokens': 1846905.0, 'mean_token_accuracy': 0.8324120789766312, 'epoch': 0.8209764918625678}
Step 228: {'loss': 0.6472, 'grad_norm': 1.0859375, 'learning_rate': 0.00018245428302989195, 'num_tokens': 1855168.0, 'mean_token_accuracy': 0.8421280533075333, 'epoch': 0.8245931283905967}
Step 229: {'loss': 0.6676, 'grad_norm': 1.1171875, 'learning_rate': 0.00018221560116948103, 'num_tokens': 1863852.0, 'mean_token_accuracy': 0.8303764462471008, 'epoch': 0.8282097649186256}
Step 230: {'loss': 0.6566, 'grad_norm': 1.078125, 'learning_rate': 0.000181975465149257, 'num_tokens': 1872296.0, 'mean_token_accuracy': 0.8317880481481552, 'epoch': 0.8318264014466547}
Step 231: {'loss': 0.6759, 'grad_norm': 1.3203125, 'learning_rate': 0.0001817338792165421, 'num_tokens': 1879989.0, 'mean_token_accuracy': 0.829442024230957, 'epoch': 0.8354430379746836}
Step 232: {'loss': 0.6594, 'grad_norm': 1.1640625, 'learning_rate': 0.0001814908476443034, 'num_tokens': 1888736.0, 'mean_token_accuracy': 0.8327993899583817, 'epoch': 0.8390596745027125}
Step 233: {'loss': 0.6742, 'grad_norm': 1.4140625, 'learning_rate': 0.00018124637473107717, 'num_tokens': 1896781.0, 'mean_token_accuracy': 0.833990752696991, 'epoch': 0.8426763110307414}
Step 234: {'loss': 0.6315, 'grad_norm': 1.2421875, 'learning_rate': 0.00018100046480089296, 'num_tokens': 1905269.0, 'mean_token_accuracy': 0.8391531407833099, 'epoch': 0.8462929475587704}
Step 235: {'loss': 0.6534, 'grad_norm': 1.015625, 'learning_rate': 0.000180753122203197, 'num_tokens': 1913994.0, 'mean_token_accuracy': 0.8343954533338547, 'epoch': 0.8499095840867993}
Step 236: {'loss': 0.6433, 'grad_norm': 1.0859375, 'learning_rate': 0.00018050435131277532, 'num_tokens': 1921944.0, 'mean_token_accuracy': 0.8386700749397278, 'epoch': 0.8535262206148282}
Step 237: {'loss': 0.6794, 'grad_norm': 1.3359375, 'learning_rate': 0.00018025415652967645, 'num_tokens': 1929719.0, 'mean_token_accuracy': 0.8323180824518204, 'epoch': 0.8571428571428571}
Step 238: {'loss': 0.6665, 'grad_norm': 1.2109375, 'learning_rate': 0.00018000254227913348, 'num_tokens': 1938483.0, 'mean_token_accuracy': 0.8277459591627121, 'epoch': 0.8607594936708861}
Step 239: {'loss': 0.6398, 'grad_norm': 1.0859375, 'learning_rate': 0.00017974951301148577, 'num_tokens': 1946403.0, 'mean_token_accuracy': 0.8392370343208313, 'epoch': 0.864376130198915}
Step 240: {'loss': 0.6935, 'grad_norm': 1.21875, 'learning_rate': 0.00017949507320210044, 'num_tokens': 1954002.0, 'mean_token_accuracy': 0.826020359992981, 'epoch': 0.8679927667269439}
Step 241: {'loss': 0.6758, 'grad_norm': 1.125, 'learning_rate': 0.00017923922735129302, 'num_tokens': 1962205.0, 'mean_token_accuracy': 0.8300259560346603, 'epoch': 0.8716094032549728}
Step 242: {'loss': 0.6839, 'grad_norm': 1.125, 'learning_rate': 0.00017898197998424782, 'num_tokens': 1970423.0, 'mean_token_accuracy': 0.8294118046760559, 'epoch': 0.8752260397830018}
Step 243: {'loss': 0.6081, 'grad_norm': 1.359375, 'learning_rate': 0.00017872333565093813, 'num_tokens': 1978901.0, 'mean_token_accuracy': 0.8470218777656555, 'epoch': 0.8788426763110307}
Step 244: {'loss': 0.6232, 'grad_norm': 1.7734375, 'learning_rate': 0.00017846329892604547, 'num_tokens': 1987031.0, 'mean_token_accuracy': 0.8388740867376328, 'epoch': 0.8824593128390597}
Step 245: {'loss': 0.6374, 'grad_norm': 1.1171875, 'learning_rate': 0.00017820187440887886, 'num_tokens': 1994904.0, 'mean_token_accuracy': 0.8414137810468674, 'epoch': 0.8860759493670886}
Step 246: {'loss': 0.6818, 'grad_norm': 1.03125, 'learning_rate': 0.0001779390667232934, 'num_tokens': 2003313.0, 'mean_token_accuracy': 0.8285130858421326, 'epoch': 0.8896925858951176}
Step 247: {'loss': 0.63, 'grad_norm': 1.0703125, 'learning_rate': 0.00017767488051760857, 'num_tokens': 2011576.0, 'mean_token_accuracy': 0.8498747795820236, 'epoch': 0.8933092224231465}
Step 248: {'loss': 0.675, 'grad_norm': 1.1953125, 'learning_rate': 0.00017740932046452577, 'num_tokens': 2019018.0, 'mean_token_accuracy': 0.8343466073274612, 'epoch': 0.8969258589511754}
Step 249: {'loss': 0.6705, 'grad_norm': 1.1171875, 'learning_rate': 0.000177142391261046, 'num_tokens': 2027788.0, 'mean_token_accuracy': 0.8307357877492905, 'epoch': 0.9005424954792043}
Step 250: {'loss': 0.637, 'grad_norm': 1.0078125, 'learning_rate': 0.00017687409762838664, 'num_tokens': 2036797.0, 'mean_token_accuracy': 0.8352771401405334, 'epoch': 0.9041591320072333}
Step 251: {'loss': 0.6494, 'grad_norm': 1.3828125, 'learning_rate': 0.0001766044443118978, 'num_tokens': 2044632.0, 'mean_token_accuracy': 0.8380274921655655, 'epoch': 0.9077757685352622}
Step 252: {'loss': 0.6582, 'grad_norm': 1.1796875, 'learning_rate': 0.00017633343608097865, 'num_tokens': 2052598.0, 'mean_token_accuracy': 0.8331261724233627, 'epoch': 0.9113924050632911}
Step 253: {'loss': 0.6972, 'grad_norm': 1.3125, 'learning_rate': 0.00017606107772899287, 'num_tokens': 2060816.0, 'mean_token_accuracy': 0.8258413374423981, 'epoch': 0.9150090415913201}
Step 254: {'loss': 0.6873, 'grad_norm': 1.1015625, 'learning_rate': 0.00017578737407318397, 'num_tokens': 2069268.0, 'mean_token_accuracy': 0.8261941969394684, 'epoch': 0.918625678119349}
Step 255: {'loss': 0.6753, 'grad_norm': 1.1796875, 'learning_rate': 0.00017551232995459003, 'num_tokens': 2077471.0, 'mean_token_accuracy': 0.8293436765670776, 'epoch': 0.9222423146473779}
Step 256: {'loss': 0.622, 'grad_norm': 1.125, 'learning_rate': 0.00017523595023795813, 'num_tokens': 2085752.0, 'mean_token_accuracy': 0.8457155525684357, 'epoch': 0.9258589511754068}
Step 257: {'loss': 0.655, 'grad_norm': 1.2109375, 'learning_rate': 0.00017495823981165823, 'num_tokens': 2093878.0, 'mean_token_accuracy': 0.8368209004402161, 'epoch': 0.9294755877034359}
Step 258: {'loss': 0.711, 'grad_norm': 1.2109375, 'learning_rate': 0.00017467920358759683, 'num_tokens': 2102263.0, 'mean_token_accuracy': 0.8254097700119019, 'epoch': 0.9330922242314648}
Step 259: {'loss': 0.6652, 'grad_norm': 1.3984375, 'learning_rate': 0.00017439884650112989, 'num_tokens': 2109897.0, 'mean_token_accuracy': 0.8329154253005981, 'epoch': 0.9367088607594937}
Step 260: {'loss': 0.6328, 'grad_norm': 1.0078125, 'learning_rate': 0.0001741171735109758, 'num_tokens': 2118808.0, 'mean_token_accuracy': 0.8364604860544205, 'epoch': 0.9403254972875226}
Step 261: {'loss': 0.6854, 'grad_norm': 1.0859375, 'learning_rate': 0.00017383418959912746, 'num_tokens': 2127039.0, 'mean_token_accuracy': 0.8283037096261978, 'epoch': 0.9439421338155516}
Step 262: {'loss': 0.6262, 'grad_norm': 0.96875, 'learning_rate': 0.00017354989977076422, 'num_tokens': 2135786.0, 'mean_token_accuracy': 0.8399356156587601, 'epoch': 0.9475587703435805}
Step 263: {'loss': 0.6657, 'grad_norm': 1.03125, 'learning_rate': 0.00017326430905416346, 'num_tokens': 2143754.0, 'mean_token_accuracy': 0.8337966650724411, 'epoch': 0.9511754068716094}
Step 264: {'loss': 0.6946, 'grad_norm': 1.1484375, 'learning_rate': 0.0001729774225006115, 'num_tokens': 2151285.0, 'mean_token_accuracy': 0.8263353854417801, 'epoch': 0.9547920433996383}
Step 265: {'loss': 0.6768, 'grad_norm': 1.046875, 'learning_rate': 0.00017268924518431438, 'num_tokens': 2159265.0, 'mean_token_accuracy': 0.8325724303722382, 'epoch': 0.9584086799276673}
Step 266: {'loss': 0.6621, 'grad_norm': 1.0546875, 'learning_rate': 0.00017239978220230797, 'num_tokens': 2166511.0, 'mean_token_accuracy': 0.8353382498025894, 'epoch': 0.9620253164556962}
Step 267: {'loss': 0.6427, 'grad_norm': 1.1328125, 'learning_rate': 0.00017210903867436795, 'num_tokens': 2173735.0, 'mean_token_accuracy': 0.8440196216106415, 'epoch': 0.9656419529837251}
Step 268: {'loss': 0.6574, 'grad_norm': 1.234375, 'learning_rate': 0.0001718170197429193, 'num_tokens': 2181749.0, 'mean_token_accuracy': 0.8389811962842941, 'epoch': 0.969258589511754}
Step 269: {'loss': 0.6575, 'grad_norm': 1.078125, 'learning_rate': 0.00017152373057294505, 'num_tokens': 2189238.0, 'mean_token_accuracy': 0.8371729552745819, 'epoch': 0.972875226039783}
Step 270: {'loss': 0.596, 'grad_norm': 1.0546875, 'learning_rate': 0.00017122917635189534, 'num_tokens': 2197541.0, 'mean_token_accuracy': 0.8504781424999237, 'epoch': 0.976491862567812}
Step 271: {'loss': 0.6328, 'grad_norm': 1.296875, 'learning_rate': 0.00017093336228959536, 'num_tokens': 2204806.0, 'mean_token_accuracy': 0.842794880270958, 'epoch': 0.9801084990958409}
Step 272: {'loss': 0.6436, 'grad_norm': 1.2578125, 'learning_rate': 0.00017063629361815328, 'num_tokens': 2212272.0, 'mean_token_accuracy': 0.8404624611139297, 'epoch': 0.9837251356238698}
Step 273: {'loss': 0.6802, 'grad_norm': 1.421875, 'learning_rate': 0.0001703379755918678, 'num_tokens': 2220081.0, 'mean_token_accuracy': 0.8290067464113235, 'epoch': 0.9873417721518988}
Step 274: {'loss': 0.6147, 'grad_norm': 1.1640625, 'learning_rate': 0.0001700384134871351, 'num_tokens': 2228080.0, 'mean_token_accuracy': 0.8461296707391739, 'epoch': 0.9909584086799277}
Step 275: {'loss': 0.601, 'grad_norm': 0.94140625, 'learning_rate': 0.00016973761260235558, 'num_tokens': 2236784.0, 'mean_token_accuracy': 0.8514697700738907, 'epoch': 0.9945750452079566}
Step 276: {'loss': 0.6581, 'grad_norm': 1.390625, 'learning_rate': 0.00016943557825784016, 'num_tokens': 2244842.0, 'mean_token_accuracy': 0.8322362154722214, 'epoch': 0.9981916817359855}
Step 277: {'loss': 0.3223, 'grad_norm': 0.73828125, 'learning_rate': 0.00016913231579571608, 'num_tokens': 2248905.0, 'mean_token_accuracy': 0.8388946354389191, 'epoch': 1.0}
Step 278: {'loss': 0.66, 'grad_norm': 1.1015625, 'learning_rate': 0.0001688278305798326, 'num_tokens': 2257759.0, 'mean_token_accuracy': 0.8329810798168182, 'epoch': 1.003616636528029}
Step 279: {'loss': 0.6376, 'grad_norm': 1.0859375, 'learning_rate': 0.0001685221279956658, 'num_tokens': 2265894.0, 'mean_token_accuracy': 0.8382320702075958, 'epoch': 1.0072332730560578}
Step 280: {'loss': 0.6443, 'grad_norm': 1.203125, 'learning_rate': 0.00016821521345022377, 'num_tokens': 2273960.0, 'mean_token_accuracy': 0.8396140933036804, 'epoch': 1.0108499095840868}
Step 281: {'loss': 0.6579, 'grad_norm': 1.0703125, 'learning_rate': 0.00016790709237195065, 'num_tokens': 2282381.0, 'mean_token_accuracy': 0.8361370116472244, 'epoch': 1.0144665461121158}
Step 282: {'loss': 0.6479, 'grad_norm': 1.125, 'learning_rate': 0.0001675977702106307, 'num_tokens': 2290224.0, 'mean_token_accuracy': 0.8376188427209854, 'epoch': 1.0180831826401446}
Step 283: {'loss': 0.635, 'grad_norm': 1.1484375, 'learning_rate': 0.0001672872524372919, 'num_tokens': 2298865.0, 'mean_token_accuracy': 0.8376675099134445, 'epoch': 1.0216998191681737}
Step 284: {'loss': 0.6402, 'grad_norm': 1.1484375, 'learning_rate': 0.00016697554454410925, 'num_tokens': 2307049.0, 'mean_token_accuracy': 0.8417865037918091, 'epoch': 1.0253164556962024}
Step 285: {'loss': 0.6466, 'grad_norm': 1.1171875, 'learning_rate': 0.00016666265204430747, 'num_tokens': 2315536.0, 'mean_token_accuracy': 0.834496259689331, 'epoch': 1.0289330922242315}
Step 286: {'loss': 0.6306, 'grad_norm': 1.484375, 'learning_rate': 0.00016634858047206378, 'num_tokens': 2322482.0, 'mean_token_accuracy': 0.8436879515647888, 'epoch': 1.0325497287522605}
Step 287: {'loss': 0.6407, 'grad_norm': 0.97265625, 'learning_rate': 0.00016603333538240975, 'num_tokens': 2330487.0, 'mean_token_accuracy': 0.8393655568361282, 'epoch': 1.0361663652802893}
Step 288: {'loss': 0.6503, 'grad_norm': 1.1640625, 'learning_rate': 0.00016571692235113302, 'num_tokens': 2338884.0, 'mean_token_accuracy': 0.8378082662820816, 'epoch': 1.0397830018083183}
Step 289: {'loss': 0.6482, 'grad_norm': 1.046875, 'learning_rate': 0.00016539934697467894, 'num_tokens': 2346571.0, 'mean_token_accuracy': 0.8367979824542999, 'epoch': 1.0433996383363473}
Step 290: {'loss': 0.6405, 'grad_norm': 1.1015625, 'learning_rate': 0.00016508061487005137, 'num_tokens': 2355120.0, 'mean_token_accuracy': 0.840273529291153, 'epoch': 1.047016274864376}
Step 291: {'loss': 0.6249, 'grad_norm': 1.09375, 'learning_rate': 0.00016476073167471345, 'num_tokens': 2363323.0, 'mean_token_accuracy': 0.8432405740022659, 'epoch': 1.0506329113924051}
Step 292: {'loss': 0.6591, 'grad_norm': 0.9921875, 'learning_rate': 0.0001644397030464877, 'num_tokens': 2372252.0, 'mean_token_accuracy': 0.8336566090583801, 'epoch': 1.054249547920434}
Step 293: {'loss': 0.6593, 'grad_norm': 1.3359375, 'learning_rate': 0.00016411753466345628, 'num_tokens': 2379979.0, 'mean_token_accuracy': 0.8393255919218063, 'epoch': 1.057866184448463}
Step 294: {'loss': 0.6484, 'grad_norm': 1.34375, 'learning_rate': 0.00016379423222386024, 'num_tokens': 2388716.0, 'mean_token_accuracy': 0.832715630531311, 'epoch': 1.061482820976492}
Step 295: {'loss': 0.6346, 'grad_norm': 1.0859375, 'learning_rate': 0.0001634698014459988, 'num_tokens': 2397257.0, 'mean_token_accuracy': 0.8398473709821701, 'epoch': 1.0650994575045207}
Step 296: {'loss': 0.6467, 'grad_norm': 1.1484375, 'learning_rate': 0.00016314424806812848, 'num_tokens': 2405455.0, 'mean_token_accuracy': 0.8442057520151138, 'epoch': 1.0687160940325497}
Step 297: {'loss': 0.639, 'grad_norm': 1.5078125, 'learning_rate': 0.0001628175778483612, 'num_tokens': 2413640.0, 'mean_token_accuracy': 0.8408303558826447, 'epoch': 1.0723327305605788}
Step 298: {'loss': 0.6366, 'grad_norm': 1.2421875, 'learning_rate': 0.00016248979656456275, 'num_tokens': 2421776.0, 'mean_token_accuracy': 0.8428573608398438, 'epoch': 1.0759493670886076}
Step 299: {'loss': 0.6386, 'grad_norm': 1.078125, 'learning_rate': 0.00016216091001425035, 'num_tokens': 2430110.0, 'mean_token_accuracy': 0.8356565684080124, 'epoch': 1.0795660036166366}
Step 300: {'loss': 0.603, 'grad_norm': 1.0, 'learning_rate': 0.00016183092401449042, 'num_tokens': 2438022.0, 'mean_token_accuracy': 0.8475423753261566, 'epoch': 1.0831826401446654}
Step 301: {'loss': 0.6695, 'grad_norm': 1.3125, 'learning_rate': 0.00016149984440179537, 'num_tokens': 2445558.0, 'mean_token_accuracy': 0.8324932008981705, 'epoch': 1.0867992766726944}
Step 302: {'loss': 0.6761, 'grad_norm': 1.546875, 'learning_rate': 0.00016116767703202054, 'num_tokens': 2452291.0, 'mean_token_accuracy': 0.8313890546560287, 'epoch': 1.0904159132007234}
Step 303: {'loss': 0.6677, 'grad_norm': 1.2421875, 'learning_rate': 0.0001608344277802606, 'num_tokens': 2459461.0, 'mean_token_accuracy': 0.832894816994667, 'epoch': 1.0940325497287522}
Step 304: {'loss': 0.6126, 'grad_norm': 1.15625, 'learning_rate': 0.00016050010254074564, 'num_tokens': 2467403.0, 'mean_token_accuracy': 0.8418886512517929, 'epoch': 1.0976491862567812}
Step 305: {'loss': 0.6685, 'grad_norm': 1.2265625, 'learning_rate': 0.00016016470722673688, 'num_tokens': 2474882.0, 'mean_token_accuracy': 0.8331159949302673, 'epoch': 1.1012658227848102}
Step 306: {'loss': 0.6166, 'grad_norm': 1.0625, 'learning_rate': 0.00015982824777042217, 'num_tokens': 2483770.0, 'mean_token_accuracy': 0.845273569226265, 'epoch': 1.104882459312839}
Step 307: {'loss': 0.6855, 'grad_norm': 1.125, 'learning_rate': 0.00015949073012281093, 'num_tokens': 2491736.0, 'mean_token_accuracy': 0.8286709934473038, 'epoch': 1.108499095840868}
Step 308: {'loss': 0.6827, 'grad_norm': 1.2734375, 'learning_rate': 0.00015915216025362905, 'num_tokens': 2499844.0, 'mean_token_accuracy': 0.827332615852356, 'epoch': 1.1121157323688968}
Step 309: {'loss': 0.6437, 'grad_norm': 1.15625, 'learning_rate': 0.0001588125441512131, 'num_tokens': 2508152.0, 'mean_token_accuracy': 0.8344348669052124, 'epoch': 1.1157323688969258}
Step 310: {'loss': 0.6723, 'grad_norm': 1.1953125, 'learning_rate': 0.0001584718878224047, 'num_tokens': 2515871.0, 'mean_token_accuracy': 0.8308372646570206, 'epoch': 1.1193490054249549}
Step 311: {'loss': 0.6272, 'grad_norm': 1.0546875, 'learning_rate': 0.00015813019729244405, 'num_tokens': 2524119.0, 'mean_token_accuracy': 0.8397922515869141, 'epoch': 1.1229656419529837}
Step 312: {'loss': 0.676, 'grad_norm': 1.140625, 'learning_rate': 0.0001577874786048633, 'num_tokens': 2532428.0, 'mean_token_accuracy': 0.829012081027031, 'epoch': 1.1265822784810127}
Step 313: {'loss': 0.6251, 'grad_norm': 1.078125, 'learning_rate': 0.00015744373782137992, 'num_tokens': 2541067.0, 'mean_token_accuracy': 0.8399362713098526, 'epoch': 1.1301989150090417}
Step 314: {'loss': 0.6551, 'grad_norm': 1.25, 'learning_rate': 0.00015709898102178932, 'num_tokens': 2549092.0, 'mean_token_accuracy': 0.8360874652862549, 'epoch': 1.1338155515370705}
Step 315: {'loss': 0.6288, 'grad_norm': 1.1640625, 'learning_rate': 0.00015675321430385736, 'num_tokens': 2556671.0, 'mean_token_accuracy': 0.8401961028575897, 'epoch': 1.1374321880650995}
Step 316: {'loss': 0.6322, 'grad_norm': 1.140625, 'learning_rate': 0.00015640644378321235, 'num_tokens': 2565320.0, 'mean_token_accuracy': 0.8387612998485565, 'epoch': 1.1410488245931285}
Step 317: {'loss': 0.6307, 'grad_norm': 0.97265625, 'learning_rate': 0.00015605867559323719, 'num_tokens': 2573808.0, 'mean_token_accuracy': 0.838371679186821, 'epoch': 1.1446654611211573}
Step 318: {'loss': 0.6439, 'grad_norm': 1.390625, 'learning_rate': 0.0001557099158849606, 'num_tokens': 2582132.0, 'mean_token_accuracy': 0.8335812240839005, 'epoch': 1.1482820976491863}
Step 319: {'loss': 0.6364, 'grad_norm': 0.98046875, 'learning_rate': 0.00015536017082694846, 'num_tokens': 2590526.0, 'mean_token_accuracy': 0.8350047618150711, 'epoch': 1.1518987341772151}
Step 320: {'loss': 0.6307, 'grad_norm': 1.0078125, 'learning_rate': 0.0001550094466051947, 'num_tokens': 2597808.0, 'mean_token_accuracy': 0.8470000326633453, 'epoch': 1.1555153707052441}
Step 321: {'loss': 0.6438, 'grad_norm': 1.15625, 'learning_rate': 0.0001546577494230118, 'num_tokens': 2605227.0, 'mean_token_accuracy': 0.8374034315347672, 'epoch': 1.1591320072332731}
Step 322: {'loss': 0.5883, 'grad_norm': 0.9921875, 'learning_rate': 0.00015430508550092124, 'num_tokens': 2612933.0, 'mean_token_accuracy': 0.8532562404870987, 'epoch': 1.162748643761302}
Step 323: {'loss': 0.6391, 'grad_norm': 1.09375, 'learning_rate': 0.00015395146107654328, 'num_tokens': 2621197.0, 'mean_token_accuracy': 0.8357891291379929, 'epoch': 1.166365280289331}
Step 324: {'loss': 0.6073, 'grad_norm': 1.171875, 'learning_rate': 0.00015359688240448678, 'num_tokens': 2630004.0, 'mean_token_accuracy': 0.8378130197525024, 'epoch': 1.1699819168173597}
Step 325: {'loss': 0.5907, 'grad_norm': 1.046875, 'learning_rate': 0.00015324135575623857, 'num_tokens': 2638038.0, 'mean_token_accuracy': 0.8506639450788498, 'epoch': 1.1735985533453888}
Step 326: {'loss': 0.6156, 'grad_norm': 1.0859375, 'learning_rate': 0.00015288488742005237, 'num_tokens': 2646313.0, 'mean_token_accuracy': 0.8419864475727081, 'epoch': 1.1772151898734178}
Step 327: {'loss': 0.602, 'grad_norm': 1.109375, 'learning_rate': 0.00015252748370083772, 'num_tokens': 2655891.0, 'mean_token_accuracy': 0.8430929332971573, 'epoch': 1.1808318264014466}
Step 328: {'loss': 0.6569, 'grad_norm': 1.1171875, 'learning_rate': 0.00015216915092004847, 'num_tokens': 2663685.0, 'mean_token_accuracy': 0.8377160280942917, 'epoch': 1.1844484629294756}
Step 329: {'loss': 0.6279, 'grad_norm': 1.2421875, 'learning_rate': 0.00015180989541557086, 'num_tokens': 2672036.0, 'mean_token_accuracy': 0.8414697647094727, 'epoch': 1.1880650994575046}
Step 330: {'loss': 0.6432, 'grad_norm': 1.140625, 'learning_rate': 0.0001514497235416115, 'num_tokens': 2680460.0, 'mean_token_accuracy': 0.8378136605024338, 'epoch': 1.1916817359855334}
Step 331: {'loss': 0.6578, 'grad_norm': 1.296875, 'learning_rate': 0.00015108864166858506, 'num_tokens': 2689509.0, 'mean_token_accuracy': 0.8250435292720795, 'epoch': 1.1952983725135624}
Step 332: {'loss': 0.6269, 'grad_norm': 1.1171875, 'learning_rate': 0.00015072665618300133, 'num_tokens': 2697889.0, 'mean_token_accuracy': 0.8405360728502274, 'epoch': 1.1989150090415914}
Step 333: {'loss': 0.6344, 'grad_norm': 1.28125, 'learning_rate': 0.00015036377348735255, 'num_tokens': 2705404.0, 'mean_token_accuracy': 0.8481389582157135, 'epoch': 1.2025316455696202}
Step 334: {'loss': 0.6659, 'grad_norm': 1.390625, 'learning_rate': 0.00015000000000000001, 'num_tokens': 2713071.0, 'mean_token_accuracy': 0.8309907913208008, 'epoch': 1.2061482820976492}
Step 335: {'loss': 0.6763, 'grad_norm': 1.3125, 'learning_rate': 0.0001496353421550606, 'num_tokens': 2721227.0, 'mean_token_accuracy': 0.8256070762872696, 'epoch': 1.209764918625678}
Step 336: {'loss': 0.6245, 'grad_norm': 1.640625, 'learning_rate': 0.00014926980640229292, 'num_tokens': 2728612.0, 'mean_token_accuracy': 0.8434150815010071, 'epoch': 1.213381555153707}
Step 337: {'loss': 0.6582, 'grad_norm': 1.421875, 'learning_rate': 0.00014890339920698334, 'num_tokens': 2736490.0, 'mean_token_accuracy': 0.8313819468021393, 'epoch': 1.216998191681736}
Step 338: {'loss': 0.6265, 'grad_norm': 1.5859375, 'learning_rate': 0.0001485361270498315, 'num_tokens': 2744515.0, 'mean_token_accuracy': 0.8403238654136658, 'epoch': 1.2206148282097649}
Step 339: {'loss': 0.6126, 'grad_norm': 1.6171875, 'learning_rate': 0.00014816799642683573, 'num_tokens': 2752836.0, 'mean_token_accuracy': 0.849293902516365, 'epoch': 1.2242314647377939}
Step 340: {'loss': 0.6542, 'grad_norm': 1.1171875, 'learning_rate': 0.0001477990138491783, 'num_tokens': 2760403.0, 'mean_token_accuracy': 0.8335212469100952, 'epoch': 1.2278481012658227}
Step 341: {'loss': 0.6607, 'grad_norm': 1.1796875, 'learning_rate': 0.00014742918584311, 'num_tokens': 2768351.0, 'mean_token_accuracy': 0.8293390423059464, 'epoch': 1.2314647377938517}
Step 342: {'loss': 0.6628, 'grad_norm': 1.09375, 'learning_rate': 0.00014705851894983491, 'num_tokens': 2775582.0, 'mean_token_accuracy': 0.8367438614368439, 'epoch': 1.2350813743218807}
Step 343: {'loss': 0.6157, 'grad_norm': 1.0390625, 'learning_rate': 0.00014668701972539458, 'num_tokens': 2783448.0, 'mean_token_accuracy': 0.8457681685686111, 'epoch': 1.2386980108499095}
Step 344: {'loss': 0.6318, 'grad_norm': 1.125, 'learning_rate': 0.00014631469474055223, 'num_tokens': 2792117.0, 'mean_token_accuracy': 0.8394802361726761, 'epoch': 1.2423146473779385}
Step 345: {'loss': 0.6591, 'grad_norm': 1.3046875, 'learning_rate': 0.00014594155058067626, 'num_tokens': 2799964.0, 'mean_token_accuracy': 0.8351868689060211, 'epoch': 1.2459312839059675}
Step 346: {'loss': 0.6497, 'grad_norm': 1.140625, 'learning_rate': 0.00014556759384562416, 'num_tokens': 2807503.0, 'mean_token_accuracy': 0.8364597707986832, 'epoch': 1.2495479204339963}
Step 347: {'loss': 0.6299, 'grad_norm': 1.4453125, 'learning_rate': 0.00014519283114962538, 'num_tokens': 2815441.0, 'mean_token_accuracy': 0.837340772151947, 'epoch': 1.2531645569620253}
Step 348: {'loss': 0.6492, 'grad_norm': 1.21875, 'learning_rate': 0.0001448172691211646, 'num_tokens': 2823922.0, 'mean_token_accuracy': 0.8313991874456406, 'epoch': 1.2567811934900543}
Step 349: {'loss': 0.6169, 'grad_norm': 1.890625, 'learning_rate': 0.0001444409144028644, 'num_tokens': 2831971.0, 'mean_token_accuracy': 0.8398082107305527, 'epoch': 1.2603978300180831}
Step 350: {'loss': 0.6562, 'grad_norm': 1.1875, 'learning_rate': 0.0001440637736513678, 'num_tokens': 2839671.0, 'mean_token_accuracy': 0.8293885588645935, 'epoch': 1.2640144665461122}
Step 351: {'loss': 0.668, 'grad_norm': 1.109375, 'learning_rate': 0.00014368585353722048, 'num_tokens': 2847760.0, 'mean_token_accuracy': 0.8257249742746353, 'epoch': 1.267631103074141}
Step 352: {'loss': 0.6316, 'grad_norm': 1.1875, 'learning_rate': 0.00014330716074475286, 'num_tokens': 2856095.0, 'mean_token_accuracy': 0.8377149701118469, 'epoch': 1.27124773960217}
Step 353: {'loss': 0.6173, 'grad_norm': 1.1484375, 'learning_rate': 0.00014292770197196184, 'num_tokens': 2864288.0, 'mean_token_accuracy': 0.8435811996459961, 'epoch': 1.274864376130199}
Step 354: {'loss': 0.6628, 'grad_norm': 1.15625, 'learning_rate': 0.00014254748393039232, 'num_tokens': 2872195.0, 'mean_token_accuracy': 0.8269500732421875, 'epoch': 1.2784810126582278}
Step 355: {'loss': 0.6325, 'grad_norm': 1.2109375, 'learning_rate': 0.0001421665133450184, 'num_tokens': 2880282.0, 'mean_token_accuracy': 0.8402366191148758, 'epoch': 1.2820976491862568}
Step 356: {'loss': 0.6505, 'grad_norm': 1.234375, 'learning_rate': 0.00014178479695412473, 'num_tokens': 2888544.0, 'mean_token_accuracy': 0.8306701630353928, 'epoch': 1.2857142857142856}
Step 357: {'loss': 0.6036, 'grad_norm': 1.1640625, 'learning_rate': 0.00014140234150918703, 'num_tokens': 2896741.0, 'mean_token_accuracy': 0.8451235890388489, 'epoch': 1.2893309222423146}
Step 358: {'loss': 0.647, 'grad_norm': 1.0625, 'learning_rate': 0.00014101915377475274, 'num_tokens': 2905210.0, 'mean_token_accuracy': 0.8377054780721664, 'epoch': 1.2929475587703436}
Step 359: {'loss': 0.6227, 'grad_norm': 1.484375, 'learning_rate': 0.00014063524052832147, 'num_tokens': 2913433.0, 'mean_token_accuracy': 0.8398969322443008, 'epoch': 1.2965641952983726}
Step 360: {'loss': 0.6157, 'grad_norm': 1.03125, 'learning_rate': 0.0001402506085602251, 'num_tokens': 2921962.0, 'mean_token_accuracy': 0.8448358476161957, 'epoch': 1.3001808318264014}
Step 361: {'loss': 0.6254, 'grad_norm': 1.6328125, 'learning_rate': 0.0001398652646735076, 'num_tokens': 2929958.0, 'mean_token_accuracy': 0.8376684933900833, 'epoch': 1.3037974683544304}
Step 362: {'loss': 0.6153, 'grad_norm': 2.203125, 'learning_rate': 0.00013947921568380474, 'num_tokens': 2938380.0, 'mean_token_accuracy': 0.8389246463775635, 'epoch': 1.3074141048824592}
Step 363: {'loss': 0.6255, 'grad_norm': 1.1953125, 'learning_rate': 0.00013909246841922358, 'num_tokens': 2947124.0, 'mean_token_accuracy': 0.8389873206615448, 'epoch': 1.3110307414104883}
Step 364: {'loss': 0.6497, 'grad_norm': 1.4609375, 'learning_rate': 0.00013870502972022173, 'num_tokens': 2954598.0, 'mean_token_accuracy': 0.8257369846105576, 'epoch': 1.3146473779385173}
Step 365: {'loss': 0.6404, 'grad_norm': 2.296875, 'learning_rate': 0.0001383169064394862, 'num_tokens': 2963602.0, 'mean_token_accuracy': 0.8322874158620834, 'epoch': 1.318264014466546}
Step 366: {'loss': 0.6285, 'grad_norm': 1.6875, 'learning_rate': 0.00013792810544181234, 'num_tokens': 2971955.0, 'mean_token_accuracy': 0.8337118178606033, 'epoch': 1.321880650994575}
Step 367: {'loss': 0.6538, 'grad_norm': 1.6015625, 'learning_rate': 0.00013753863360398241, 'num_tokens': 2979454.0, 'mean_token_accuracy': 0.8247727155685425, 'epoch': 1.3254972875226039}
Step 368: {'loss': 0.6159, 'grad_norm': 1.3515625, 'learning_rate': 0.00013714849781464389, 'num_tokens': 2987754.0, 'mean_token_accuracy': 0.8327333629131317, 'epoch': 1.3291139240506329}
Step 369: {'loss': 0.6268, 'grad_norm': 1.4296875, 'learning_rate': 0.00013675770497418771, 'num_tokens': 2996310.0, 'mean_token_accuracy': 0.834829792380333, 'epoch': 1.332730560578662}
Step 370: {'loss': 0.5863, 'grad_norm': 1.2734375, 'learning_rate': 0.00013636626199462615, 'num_tokens': 3004426.0, 'mean_token_accuracy': 0.8503564745187759, 'epoch': 1.3363471971066907}
Step 371: {'loss': 0.5931, 'grad_norm': 1.421875, 'learning_rate': 0.00013597417579947054, 'num_tokens': 3012160.0, 'mean_token_accuracy': 0.8466353267431259, 'epoch': 1.3399638336347197}
Step 372: {'loss': 0.639, 'grad_norm': 1.3046875, 'learning_rate': 0.00013558145332360892, 'num_tokens': 3019473.0, 'mean_token_accuracy': 0.8338173031806946, 'epoch': 1.3435804701627485}
Step 373: {'loss': 0.6302, 'grad_norm': 1.25, 'learning_rate': 0.0001351881015131833, 'num_tokens': 3027848.0, 'mean_token_accuracy': 0.8336605578660965, 'epoch': 1.3471971066907775}
Step 374: {'loss': 0.6175, 'grad_norm': 1.34375, 'learning_rate': 0.0001347941273254669, 'num_tokens': 3036467.0, 'mean_token_accuracy': 0.8407945781946182, 'epoch': 1.3508137432188065}
Step 375: {'loss': 0.5917, 'grad_norm': 1.2578125, 'learning_rate': 0.0001343995377287409, 'num_tokens': 3045089.0, 'mean_token_accuracy': 0.839898094534874, 'epoch': 1.3544303797468356}
Step 376: {'loss': 0.6236, 'grad_norm': 1.28125, 'learning_rate': 0.00013400433970217135, 'num_tokens': 3053706.0, 'mean_token_accuracy': 0.832043245434761, 'epoch': 1.3580470162748643}
Step 377: {'loss': 0.636, 'grad_norm': 1.375, 'learning_rate': 0.00013360854023568584, 'num_tokens': 3061504.0, 'mean_token_accuracy': 0.8314289599657059, 'epoch': 1.3616636528028934}
Step 378: {'loss': 0.5917, 'grad_norm': 1.25, 'learning_rate': 0.00013321214632984943, 'num_tokens': 3069556.0, 'mean_token_accuracy': 0.8436180949211121, 'epoch': 1.3652802893309222}
Step 379: {'loss': 0.5973, 'grad_norm': 1.1484375, 'learning_rate': 0.00013281516499574135, 'num_tokens': 3077843.0, 'mean_token_accuracy': 0.8402175158262253, 'epoch': 1.3688969258589512}
Step 380: {'loss': 0.6351, 'grad_norm': 1.3203125, 'learning_rate': 0.00013241760325483068, 'num_tokens': 3085719.0, 'mean_token_accuracy': 0.8288970738649368, 'epoch': 1.3725135623869802}
Step 381: {'loss': 0.6086, 'grad_norm': 1.2890625, 'learning_rate': 0.00013201946813885232, 'num_tokens': 3094450.0, 'mean_token_accuracy': 0.84023118019104, 'epoch': 1.376130198915009}
Step 382: {'loss': 0.6174, 'grad_norm': 1.2890625, 'learning_rate': 0.0001316207666896824, 'num_tokens': 3103127.0, 'mean_token_accuracy': 0.8369697481393814, 'epoch': 1.379746835443038}
Step 383: {'loss': 0.5743, 'grad_norm': 1.2578125, 'learning_rate': 0.00013122150595921397, 'num_tokens': 3112044.0, 'mean_token_accuracy': 0.8413646519184113, 'epoch': 1.3833634719710668}
Step 384: {'loss': 0.6046, 'grad_norm': 1.3046875, 'learning_rate': 0.00013082169300923218, 'num_tokens': 3120354.0, 'mean_token_accuracy': 0.8394400328397751, 'epoch': 1.3869801084990958}
Step 385: {'loss': 0.646, 'grad_norm': 1.8359375, 'learning_rate': 0.00013042133491128935, 'num_tokens': 3127929.0, 'mean_token_accuracy': 0.8283917009830475, 'epoch': 1.3905967450271248}
Step 386: {'loss': 0.6209, 'grad_norm': 1.2734375, 'learning_rate': 0.00013002043874657984, 'num_tokens': 3135697.0, 'mean_token_accuracy': 0.8371178209781647, 'epoch': 1.3942133815551538}
Step 387: {'loss': 0.6365, 'grad_norm': 1.2265625, 'learning_rate': 0.00012961901160581503, 'num_tokens': 3143728.0, 'mean_token_accuracy': 0.83206807076931, 'epoch': 1.3978300180831826}
Step 388: {'loss': 0.6329, 'grad_norm': 1.375, 'learning_rate': 0.00012921706058909756, 'num_tokens': 3150920.0, 'mean_token_accuracy': 0.8387306928634644, 'epoch': 1.4014466546112117}
Step 389: {'loss': 0.6142, 'grad_norm': 1.796875, 'learning_rate': 0.00012881459280579617, 'num_tokens': 3158498.0, 'mean_token_accuracy': 0.833889901638031, 'epoch': 1.4050632911392404}
Step 390: {'loss': 0.6094, 'grad_norm': 1.4296875, 'learning_rate': 0.00012841161537441954, 'num_tokens': 3166391.0, 'mean_token_accuracy': 0.8393020927906036, 'epoch': 1.4086799276672695}
Step 391: {'loss': 0.5825, 'grad_norm': 1.203125, 'learning_rate': 0.00012800813542249072, 'num_tokens': 3174191.0, 'mean_token_accuracy': 0.8499551266431808, 'epoch': 1.4122965641952985}
Step 392: {'loss': 0.5907, 'grad_norm': 1.265625, 'learning_rate': 0.00012760416008642073, 'num_tokens': 3181373.0, 'mean_token_accuracy': 0.8447186797857285, 'epoch': 1.4159132007233273}
Step 393: {'loss': 0.6029, 'grad_norm': 1.25, 'learning_rate': 0.00012719969651138274, 'num_tokens': 3189818.0, 'mean_token_accuracy': 0.844907134771347, 'epoch': 1.4195298372513563}
Step 394: {'loss': 0.6085, 'grad_norm': 1.1484375, 'learning_rate': 0.00012679475185118535, 'num_tokens': 3198042.0, 'mean_token_accuracy': 0.8394840657711029, 'epoch': 1.423146473779385}
Step 395: {'loss': 0.684, 'grad_norm': 1.4609375, 'learning_rate': 0.00012638933326814618, 'num_tokens': 3205618.0, 'mean_token_accuracy': 0.819748044013977, 'epoch': 1.426763110307414}
Step 396: {'loss': 0.5912, 'grad_norm': 1.140625, 'learning_rate': 0.0001259834479329652, 'num_tokens': 3214847.0, 'mean_token_accuracy': 0.8392632454633713, 'epoch': 1.4303797468354431}
Step 397: {'loss': 0.582, 'grad_norm': 1.21875, 'learning_rate': 0.00012557710302459803, 'num_tokens': 3222040.0, 'mean_token_accuracy': 0.8411744832992554, 'epoch': 1.433996383363472}
Step 398: {'loss': 0.5707, 'grad_norm': 1.125, 'learning_rate': 0.0001251703057301286, 'num_tokens': 3230331.0, 'mean_token_accuracy': 0.8515753298997879, 'epoch': 1.437613019891501}
Step 399: {'loss': 0.5861, 'grad_norm': 1.0625, 'learning_rate': 0.00012476306324464247, 'num_tokens': 3238060.0, 'mean_token_accuracy': 0.8466274291276932, 'epoch': 1.4412296564195297}
Step 400: {'loss': 0.6032, 'grad_norm': 1.03125, 'learning_rate': 0.0001243553827710992, 'num_tokens': 3246327.0, 'mean_token_accuracy': 0.8390017598867416, 'epoch': 1.4448462929475587}
Step 401: {'loss': 0.5893, 'grad_norm': 1.1171875, 'learning_rate': 0.00012394727152020528, 'num_tokens': 3255170.0, 'mean_token_accuracy': 0.8400648683309555, 'epoch': 1.4484629294755877}
Step 402: {'loss': 0.6091, 'grad_norm': 1.0703125, 'learning_rate': 0.00012353873671028625, 'num_tokens': 3263894.0, 'mean_token_accuracy': 0.8375901877880096, 'epoch': 1.4520795660036168}
Step 403: {'loss': 0.5907, 'grad_norm': 1.1015625, 'learning_rate': 0.00012312978556715932, 'num_tokens': 3272721.0, 'mean_token_accuracy': 0.8431078940629959, 'epoch': 1.4556962025316456}
Step 404: {'loss': 0.6218, 'grad_norm': 1.3671875, 'learning_rate': 0.00012272042532400542, 'num_tokens': 3280369.0, 'mean_token_accuracy': 0.8347364813089371, 'epoch': 1.4593128390596746}
Step 405: {'loss': 0.6696, 'grad_norm': 1.4375, 'learning_rate': 0.00012231066322124123, 'num_tokens': 3288382.0, 'mean_token_accuracy': 0.8277655839920044, 'epoch': 1.4629294755877034}
Step 406: {'loss': 0.578, 'grad_norm': 1.1171875, 'learning_rate': 0.00012190050650639131, 'num_tokens': 3295934.0, 'mean_token_accuracy': 0.8457117974758148, 'epoch': 1.4665461121157324}
Step 407: {'loss': 0.5886, 'grad_norm': 1.1640625, 'learning_rate': 0.00012148996243395966, 'num_tokens': 3303242.0, 'mean_token_accuracy': 0.8483557105064392, 'epoch': 1.4701627486437614}
Step 408: {'loss': 0.5915, 'grad_norm': 1.125, 'learning_rate': 0.00012107903826530165, 'num_tokens': 3310836.0, 'mean_token_accuracy': 0.8407106399536133, 'epoch': 1.4737793851717902}
Step 409: {'loss': 0.6228, 'grad_norm': 1.3125, 'learning_rate': 0.00012066774126849529, 'num_tokens': 3318370.0, 'mean_token_accuracy': 0.8321873992681503, 'epoch': 1.4773960216998192}
Step 410: {'loss': 0.6147, 'grad_norm': 1.2109375, 'learning_rate': 0.0001202560787182131, 'num_tokens': 3326812.0, 'mean_token_accuracy': 0.8334340006113052, 'epoch': 1.481012658227848}
Step 411: {'loss': 0.5986, 'grad_norm': 1.2890625, 'learning_rate': 0.00011984405789559298, 'num_tokens': 3335265.0, 'mean_token_accuracy': 0.839435338973999, 'epoch': 1.484629294755877}
Step 412: {'loss': 0.6185, 'grad_norm': 1.234375, 'learning_rate': 0.00011943168608810978, 'num_tokens': 3344009.0, 'mean_token_accuracy': 0.8338950872421265, 'epoch': 1.488245931283906}
Step 413: {'loss': 0.6242, 'grad_norm': 1.078125, 'learning_rate': 0.0001190189705894462, 'num_tokens': 3351779.0, 'mean_token_accuracy': 0.8347814679145813, 'epoch': 1.4918625678119348}
Step 414: {'loss': 0.6231, 'grad_norm': 1.1875, 'learning_rate': 0.00011860591869936395, 'num_tokens': 3359439.0, 'mean_token_accuracy': 0.8371942192316055, 'epoch': 1.4954792043399638}
Step 415: {'loss': 0.6027, 'grad_norm': 1.125, 'learning_rate': 0.00011819253772357442, 'num_tokens': 3367051.0, 'mean_token_accuracy': 0.8413814157247543, 'epoch': 1.4990958408679926}
Step 416: {'loss': 0.5996, 'grad_norm': 1.125, 'learning_rate': 0.00011777883497360969, 'num_tokens': 3375158.0, 'mean_token_accuracy': 0.8390109241008759, 'epoch': 1.5027124773960217}
Step 417: {'loss': 0.605, 'grad_norm': 1.1953125, 'learning_rate': 0.00011736481776669306, 'num_tokens': 3383522.0, 'mean_token_accuracy': 0.8377891927957535, 'epoch': 1.5063291139240507}
Step 418: {'loss': 0.6235, 'grad_norm': 1.359375, 'learning_rate': 0.00011695049342560968, 'num_tokens': 3391216.0, 'mean_token_accuracy': 0.8315907418727875, 'epoch': 1.5099457504520797}
Step 419: {'loss': 0.642, 'grad_norm': 1.171875, 'learning_rate': 0.00011653586927857705, 'num_tokens': 3399502.0, 'mean_token_accuracy': 0.8268388956785202, 'epoch': 1.5135623869801085}
Step 420: {'loss': 0.6586, 'grad_norm': 1.2578125, 'learning_rate': 0.0001161209526591154, 'num_tokens': 3407107.0, 'mean_token_accuracy': 0.8321996033191681, 'epoch': 1.5171790235081373}
Step 421: {'loss': 0.5848, 'grad_norm': 1.140625, 'learning_rate': 0.00011570575090591791, 'num_tokens': 3416308.0, 'mean_token_accuracy': 0.8434040993452072, 'epoch': 1.5207956600361663}
Step 422: {'loss': 0.6016, 'grad_norm': 1.0859375, 'learning_rate': 0.00011529027136272099, 'num_tokens': 3423926.0, 'mean_token_accuracy': 0.8402764350175858, 'epoch': 1.5244122965641953}
Step 423: {'loss': 0.6511, 'grad_norm': 1.28125, 'learning_rate': 0.00011487452137817434, 'num_tokens': 3431787.0, 'mean_token_accuracy': 0.8260601162910461, 'epoch': 1.5280289330922243}
Step 424: {'loss': 0.5975, 'grad_norm': 1.2578125, 'learning_rate': 0.0001144585083057111, 'num_tokens': 3439759.0, 'mean_token_accuracy': 0.8386245518922806, 'epoch': 1.5316455696202531}
Step 425: {'loss': 0.6109, 'grad_norm': 1.1875, 'learning_rate': 0.00011404223950341751, 'num_tokens': 3447924.0, 'mean_token_accuracy': 0.8378892540931702, 'epoch': 1.5352622061482821}
Step 426: {'loss': 0.5793, 'grad_norm': 1.1484375, 'learning_rate': 0.0001136257223339031, 'num_tokens': 3456433.0, 'mean_token_accuracy': 0.8436848819255829, 'epoch': 1.538878842676311}
Step 427: {'loss': 0.634, 'grad_norm': 1.3515625, 'learning_rate': 0.00011320896416417026, 'num_tokens': 3464282.0, 'mean_token_accuracy': 0.8289838582277298, 'epoch': 1.54249547920434}
Step 428: {'loss': 0.5777, 'grad_norm': 1.0390625, 'learning_rate': 0.00011279197236548393, 'num_tokens': 3472475.0, 'mean_token_accuracy': 0.8447979092597961, 'epoch': 1.546112115732369}
Step 429: {'loss': 0.5817, 'grad_norm': 1.1484375, 'learning_rate': 0.00011237475431324134, 'num_tokens': 3480680.0, 'mean_token_accuracy': 0.844158798456192, 'epoch': 1.549728752260398}
Step 430: {'loss': 0.5662, 'grad_norm': 1.28125, 'learning_rate': 0.0001119573173868415, 'num_tokens': 3488767.0, 'mean_token_accuracy': 0.8495688736438751, 'epoch': 1.5533453887884268}
Step 431: {'loss': 0.613, 'grad_norm': 1.3828125, 'learning_rate': 0.00011153966896955468, 'num_tokens': 3497143.0, 'mean_token_accuracy': 0.8390434980392456, 'epoch': 1.5569620253164556}
Step 432: {'loss': 0.5904, 'grad_norm': 1.0546875, 'learning_rate': 0.00011112181644839177, 'num_tokens': 3505314.0, 'mean_token_accuracy': 0.8406197726726532, 'epoch': 1.5605786618444846}
Step 433: {'loss': 0.5959, 'grad_norm': 1.1015625, 'learning_rate': 0.00011070376721397373, 'num_tokens': 3512955.0, 'mean_token_accuracy': 0.8481029868125916, 'epoch': 1.5641952983725136}
Step 434: {'loss': 0.6151, 'grad_norm': 1.1875, 'learning_rate': 0.00011028552866040088, 'num_tokens': 3520410.0, 'mean_token_accuracy': 0.839867889881134, 'epoch': 1.5678119349005426}
Step 435: {'loss': 0.6308, 'grad_norm': 1.2578125, 'learning_rate': 0.00010986710818512183, 'num_tokens': 3528303.0, 'mean_token_accuracy': 0.8309421986341476, 'epoch': 1.5714285714285714}
Step 436: {'loss': 0.6207, 'grad_norm': 1.1171875, 'learning_rate': 0.00010944851318880314, 'num_tokens': 3536348.0, 'mean_token_accuracy': 0.8342163264751434, 'epoch': 1.5750452079566004}
Step 437: {'loss': 0.596, 'grad_norm': 1.4296875, 'learning_rate': 0.00010902975107519797, 'num_tokens': 3544310.0, 'mean_token_accuracy': 0.8430838286876678, 'epoch': 1.5786618444846292}
Step 438: {'loss': 0.5948, 'grad_norm': 1.2734375, 'learning_rate': 0.0001086108292510154, 'num_tokens': 3552936.0, 'mean_token_accuracy': 0.8364081680774689, 'epoch': 1.5822784810126582}
Step 439: {'loss': 0.5972, 'grad_norm': 1.3359375, 'learning_rate': 0.00010819175512578926, 'num_tokens': 3561098.0, 'mean_token_accuracy': 0.842145100235939, 'epoch': 1.5858951175406872}
Step 440: {'loss': 0.5843, 'grad_norm': 1.140625, 'learning_rate': 0.0001077725361117472, 'num_tokens': 3569126.0, 'mean_token_accuracy': 0.8472638577222824, 'epoch': 1.5895117540687163}
Step 441: {'loss': 0.6018, 'grad_norm': 1.1796875, 'learning_rate': 0.00010735317962367959, 'num_tokens': 3577536.0, 'mean_token_accuracy': 0.8393289595842361, 'epoch': 1.593128390596745}
Step 442: {'loss': 0.6269, 'grad_norm': 1.4921875, 'learning_rate': 0.00010693369307880816, 'num_tokens': 3586095.0, 'mean_token_accuracy': 0.8324232548475266, 'epoch': 1.5967450271247738}
Step 443: {'loss': 0.6183, 'grad_norm': 1.3203125, 'learning_rate': 0.00010651408389665518, 'num_tokens': 3593608.0, 'mean_token_accuracy': 0.8302096128463745, 'epoch': 1.6003616636528029}
Step 444: {'loss': 0.6251, 'grad_norm': 1.09375, 'learning_rate': 0.00010609435949891191, 'num_tokens': 3601906.0, 'mean_token_accuracy': 0.8345235139131546, 'epoch': 1.6039783001808319}
Step 445: {'loss': 0.6175, 'grad_norm': 1.234375, 'learning_rate': 0.00010567452730930743, 'num_tokens': 3610774.0, 'mean_token_accuracy': 0.8345095664262772, 'epoch': 1.6075949367088609}
Step 446: {'loss': 0.6396, 'grad_norm': 1.1484375, 'learning_rate': 0.00010525459475347735, 'num_tokens': 3619061.0, 'mean_token_accuracy': 0.8358743786811829, 'epoch': 1.6112115732368897}
Step 447: {'loss': 0.5902, 'grad_norm': 1.1484375, 'learning_rate': 0.0001048345692588326, 'num_tokens': 3627429.0, 'mean_token_accuracy': 0.8380572646856308, 'epoch': 1.6148282097649185}
Step 448: {'loss': 0.6354, 'grad_norm': 1.40625, 'learning_rate': 0.00010441445825442772, 'num_tokens': 3635641.0, 'mean_token_accuracy': 0.8340446352958679, 'epoch': 1.6184448462929475}
Step 449: {'loss': 0.5956, 'grad_norm': 1.1640625, 'learning_rate': 0.0001039942691708299, 'num_tokens': 3644184.0, 'mean_token_accuracy': 0.837573230266571, 'epoch': 1.6220614828209765}
Step 450: {'loss': 0.6084, 'grad_norm': 1.140625, 'learning_rate': 0.00010357400943998714, 'num_tokens': 3652442.0, 'mean_token_accuracy': 0.8365984857082367, 'epoch': 1.6256781193490055}
Step 451: {'loss': 0.5717, 'grad_norm': 1.140625, 'learning_rate': 0.00010315368649509716, 'num_tokens': 3661003.0, 'mean_token_accuracy': 0.8457185477018356, 'epoch': 1.6292947558770343}
Step 452: {'loss': 0.6391, 'grad_norm': 1.296875, 'learning_rate': 0.00010273330777047556, 'num_tokens': 3668932.0, 'mean_token_accuracy': 0.8288482874631882, 'epoch': 1.6329113924050633}
Step 453: {'loss': 0.6361, 'grad_norm': 1.1796875, 'learning_rate': 0.00010231288070142469, 'num_tokens': 3676973.0, 'mean_token_accuracy': 0.8296931833028793, 'epoch': 1.6365280289330921}
Step 454: {'loss': 0.5957, 'grad_norm': 1.203125, 'learning_rate': 0.0001018924127241019, 'num_tokens': 3685438.0, 'mean_token_accuracy': 0.8426208794116974, 'epoch': 1.6401446654611211}
Step 455: {'loss': 0.6171, 'grad_norm': 1.078125, 'learning_rate': 0.00010147191127538813, 'num_tokens': 3693752.0, 'mean_token_accuracy': 0.8364716917276382, 'epoch': 1.6437613019891502}
Step 456: {'loss': 0.5756, 'grad_norm': 1.109375, 'learning_rate': 0.00010105138379275625, 'num_tokens': 3702553.0, 'mean_token_accuracy': 0.8441513776779175, 'epoch': 1.6473779385171792}
Step 457: {'loss': 0.5969, 'grad_norm': 1.1953125, 'learning_rate': 0.00010063083771413975, 'num_tokens': 3710738.0, 'mean_token_accuracy': 0.8497410118579865, 'epoch': 1.650994575045208}
Step 458: {'loss': 0.5609, 'grad_norm': 1.0625, 'learning_rate': 0.0001002102804778008, 'num_tokens': 3719486.0, 'mean_token_accuracy': 0.8461988419294357, 'epoch': 1.6546112115732368}
Step 459: {'loss': 0.6248, 'grad_norm': 1.296875, 'learning_rate': 9.97897195221992e-05, 'num_tokens': 3727473.0, 'mean_token_accuracy': 0.838238000869751, 'epoch': 1.6582278481012658}
Step 460: {'loss': 0.6118, 'grad_norm': 1.140625, 'learning_rate': 9.936916228586028e-05, 'num_tokens': 3735613.0, 'mean_token_accuracy': 0.838066890835762, 'epoch': 1.6618444846292948}
Step 461: {'loss': 0.6059, 'grad_norm': 1.25, 'learning_rate': 9.894861620724375e-05, 'num_tokens': 3744614.0, 'mean_token_accuracy': 0.8366351425647736, 'epoch': 1.6654611211573238}
Step 462: {'loss': 0.6292, 'grad_norm': 1.2109375, 'learning_rate': 9.852808872461192e-05, 'num_tokens': 3753393.0, 'mean_token_accuracy': 0.8326152563095093, 'epoch': 1.6690777576853526}
Step 463: {'loss': 0.6102, 'grad_norm': 1.109375, 'learning_rate': 9.810758727589813e-05, 'num_tokens': 3761583.0, 'mean_token_accuracy': 0.8315606713294983, 'epoch': 1.6726943942133814}
Step 464: {'loss': 0.5824, 'grad_norm': 1.0390625, 'learning_rate': 9.768711929857532e-05, 'num_tokens': 3769769.0, 'mean_token_accuracy': 0.8425395637750626, 'epoch': 1.6763110307414104}
Step 465: {'loss': 0.6364, 'grad_norm': 1.1953125, 'learning_rate': 9.726669222952447e-05, 'num_tokens': 3777697.0, 'mean_token_accuracy': 0.8283681869506836, 'epoch': 1.6799276672694394}
Step 466: {'loss': 0.5993, 'grad_norm': 1.359375, 'learning_rate': 9.684631350490287e-05, 'num_tokens': 3785408.0, 'mean_token_accuracy': 0.838729977607727, 'epoch': 1.6835443037974684}
Step 467: {'loss': 0.572, 'grad_norm': 1.1328125, 'learning_rate': 9.642599056001285e-05, 'num_tokens': 3793522.0, 'mean_token_accuracy': 0.8502635806798935, 'epoch': 1.6871609403254972}
Step 468: {'loss': 0.6151, 'grad_norm': 1.5234375, 'learning_rate': 9.60057308291701e-05, 'num_tokens': 3801066.0, 'mean_token_accuracy': 0.8381086587905884, 'epoch': 1.6907775768535263}
Step 469: {'loss': 0.6043, 'grad_norm': 1.2734375, 'learning_rate': 9.55855417455723e-05, 'num_tokens': 3809163.0, 'mean_token_accuracy': 0.8363603204488754, 'epoch': 1.694394213381555}
Step 470: {'loss': 0.6135, 'grad_norm': 1.171875, 'learning_rate': 9.516543074116745e-05, 'num_tokens': 3817724.0, 'mean_token_accuracy': 0.8365553170442581, 'epoch': 1.698010849909584}
Step 471: {'loss': 0.5722, 'grad_norm': 1.1171875, 'learning_rate': 9.474540524652267e-05, 'num_tokens': 3825741.0, 'mean_token_accuracy': 0.8494547605514526, 'epoch': 1.701627486437613}
Step 472: {'loss': 0.5957, 'grad_norm': 1.40625, 'learning_rate': 9.432547269069261e-05, 'num_tokens': 3833423.0, 'mean_token_accuracy': 0.839529886841774, 'epoch': 1.705244122965642}
Step 473: {'loss': 0.5849, 'grad_norm': 1.140625, 'learning_rate': 9.390564050108812e-05, 'num_tokens': 3841886.0, 'mean_token_accuracy': 0.8435649424791336, 'epoch': 1.7088607594936709}
Step 474: {'loss': 0.5853, 'grad_norm': 1.28125, 'learning_rate': 9.348591610334481e-05, 'num_tokens': 3850283.0, 'mean_token_accuracy': 0.8468232303857803, 'epoch': 1.7124773960216997}
Step 475: {'loss': 0.6195, 'grad_norm': 1.28125, 'learning_rate': 9.306630692119182e-05, 'num_tokens': 3857929.0, 'mean_token_accuracy': 0.8376370668411255, 'epoch': 1.7160940325497287}
Step 476: {'loss': 0.6025, 'grad_norm': 1.2109375, 'learning_rate': 9.264682037632046e-05, 'num_tokens': 3865723.0, 'mean_token_accuracy': 0.8400059789419174, 'epoch': 1.7197106690777577}
Step 477: {'loss': 0.6807, 'grad_norm': 1.578125, 'learning_rate': 9.222746388825281e-05, 'num_tokens': 3873155.0, 'mean_token_accuracy': 0.8255472779273987, 'epoch': 1.7233273056057867}
Step 478: {'loss': 0.639, 'grad_norm': 1.53125, 'learning_rate': 9.180824487421077e-05, 'num_tokens': 3881374.0, 'mean_token_accuracy': 0.827655017375946, 'epoch': 1.7269439421338155}
Step 479: {'loss': 0.5724, 'grad_norm': 1.625, 'learning_rate': 9.138917074898461e-05, 'num_tokens': 3889693.0, 'mean_token_accuracy': 0.8475645929574966, 'epoch': 1.7305605786618445}
Step 480: {'loss': 0.6065, 'grad_norm': 1.578125, 'learning_rate': 9.097024892480204e-05, 'num_tokens': 3897247.0, 'mean_token_accuracy': 0.8372930288314819, 'epoch': 1.7341772151898733}
Step 481: {'loss': 0.6346, 'grad_norm': 1.3828125, 'learning_rate': 9.055148681119688e-05, 'num_tokens': 3905011.0, 'mean_token_accuracy': 0.831616684794426, 'epoch': 1.7377938517179023}
Step 482: {'loss': 0.6115, 'grad_norm': 1.390625, 'learning_rate': 9.013289181487821e-05, 'num_tokens': 3912735.0, 'mean_token_accuracy': 0.8312163650989532, 'epoch': 1.7414104882459314}
Step 483: {'loss': 0.6065, 'grad_norm': 1.3046875, 'learning_rate': 8.971447133959919e-05, 'num_tokens': 3920465.0, 'mean_token_accuracy': 0.8394730389118195, 'epoch': 1.7450271247739604}
Step 484: {'loss': 0.5798, 'grad_norm': 1.078125, 'learning_rate': 8.929623278602627e-05, 'num_tokens': 3928581.0, 'mean_token_accuracy': 0.8420510292053223, 'epoch': 1.7486437613019892}
Step 485: {'loss': 0.6244, 'grad_norm': 2.28125, 'learning_rate': 8.887818355160824e-05, 'num_tokens': 3936784.0, 'mean_token_accuracy': 0.83210089802742, 'epoch': 1.752260397830018}
Step 486: {'loss': 0.587, 'grad_norm': 1.3828125, 'learning_rate': 8.846033103044535e-05, 'num_tokens': 3944545.0, 'mean_token_accuracy': 0.8416396528482437, 'epoch': 1.755877034358047}
Step 487: {'loss': 0.581, 'grad_norm': 1.2734375, 'learning_rate': 8.80426826131585e-05, 'num_tokens': 3952715.0, 'mean_token_accuracy': 0.8429349809885025, 'epoch': 1.759493670886076}
Step 488: {'loss': 0.6249, 'grad_norm': 1.6796875, 'learning_rate': 8.762524568675867e-05, 'num_tokens': 3960840.0, 'mean_token_accuracy': 0.8314494490623474, 'epoch': 1.763110307414105}
Step 489: {'loss': 0.5855, 'grad_norm': 1.296875, 'learning_rate': 8.720802763451612e-05, 'num_tokens': 3969275.0, 'mean_token_accuracy': 0.8378151655197144, 'epoch': 1.7667269439421338}
Step 490: {'loss': 0.5823, 'grad_norm': 1.25, 'learning_rate': 8.679103583582979e-05, 'num_tokens': 3977895.0, 'mean_token_accuracy': 0.8465746492147446, 'epoch': 1.7703435804701626}
Step 491: {'loss': 0.608, 'grad_norm': 1.0078125, 'learning_rate': 8.637427766609691e-05, 'num_tokens': 3986509.0, 'mean_token_accuracy': 0.8388150930404663, 'epoch': 1.7739602169981916}
Step 492: {'loss': 0.5657, 'grad_norm': 2.203125, 'learning_rate': 8.595776049658251e-05, 'num_tokens': 3994737.0, 'mean_token_accuracy': 0.8453720510005951, 'epoch': 1.7775768535262206}
Step 493: {'loss': 0.5686, 'grad_norm': 2.171875, 'learning_rate': 8.554149169428894e-05, 'num_tokens': 4003101.0, 'mean_token_accuracy': 0.8425645381212234, 'epoch': 1.7811934900542497}
Step 494: {'loss': 0.6023, 'grad_norm': 1.4453125, 'learning_rate': 8.512547862182567e-05, 'num_tokens': 4011238.0, 'mean_token_accuracy': 0.8351547122001648, 'epoch': 1.7848101265822784}
Step 495: {'loss': 0.572, 'grad_norm': 1.4140625, 'learning_rate': 8.470972863727902e-05, 'num_tokens': 4020290.0, 'mean_token_accuracy': 0.8459530174732208, 'epoch': 1.7884267631103075}
Step 496: {'loss': 0.6311, 'grad_norm': 1.5546875, 'learning_rate': 8.429424909408214e-05, 'num_tokens': 4028170.0, 'mean_token_accuracy': 0.8310339748859406, 'epoch': 1.7920433996383363}
Step 497: {'loss': 0.6042, 'grad_norm': 1.140625, 'learning_rate': 8.387904734088463e-05, 'num_tokens': 4036916.0, 'mean_token_accuracy': 0.8395975232124329, 'epoch': 1.7956600361663653}
Step 498: {'loss': 0.5965, 'grad_norm': 1.921875, 'learning_rate': 8.346413072142297e-05, 'num_tokens': 4044740.0, 'mean_token_accuracy': 0.8365411758422852, 'epoch': 1.7992766726943943}
Step 499: {'loss': 0.6463, 'grad_norm': 2.09375, 'learning_rate': 8.304950657439033e-05, 'num_tokens': 4052486.0, 'mean_token_accuracy': 0.8292936086654663, 'epoch': 1.8028933092224233}
Step 500: {'loss': 0.599, 'grad_norm': 1.3359375, 'learning_rate': 8.263518223330697e-05, 'num_tokens': 4059971.0, 'mean_token_accuracy': 0.8396168351173401, 'epoch': 1.806509945750452}
Step 501: {'loss': 0.6023, 'grad_norm': 1.3515625, 'learning_rate': 8.222116502639032e-05, 'num_tokens': 4067829.0, 'mean_token_accuracy': 0.8454944342374802, 'epoch': 1.810126582278481}
Step 502: {'loss': 0.6084, 'grad_norm': 1.2890625, 'learning_rate': 8.180746227642562e-05, 'num_tokens': 4076526.0, 'mean_token_accuracy': 0.8351162374019623, 'epoch': 1.81374321880651}
Step 503: {'loss': 0.5808, 'grad_norm': 1.109375, 'learning_rate': 8.139408130063609e-05, 'num_tokens': 4084864.0, 'mean_token_accuracy': 0.8470127433538437, 'epoch': 1.817359855334539}
Step 504: {'loss': 0.5989, 'grad_norm': 1.21875, 'learning_rate': 8.098102941055382e-05, 'num_tokens': 4092823.0, 'mean_token_accuracy': 0.8385849148035049, 'epoch': 1.820976491862568}
Step 505: {'loss': 0.6214, 'grad_norm': 4.34375, 'learning_rate': 8.056831391189023e-05, 'num_tokens': 4100531.0, 'mean_token_accuracy': 0.8306955248117447, 'epoch': 1.8245931283905967}
Step 506: {'loss': 0.5927, 'grad_norm': 1.7734375, 'learning_rate': 8.015594210440704e-05, 'num_tokens': 4109628.0, 'mean_token_accuracy': 0.8380113691091537, 'epoch': 1.8282097649186255}
Step 507: {'loss': 0.5812, 'grad_norm': 1.609375, 'learning_rate': 7.974392128178691e-05, 'num_tokens': 4117607.0, 'mean_token_accuracy': 0.845967173576355, 'epoch': 1.8318264014466545}
Step 508: {'loss': 0.6211, 'grad_norm': 1.25, 'learning_rate': 7.93322587315047e-05, 'num_tokens': 4126085.0, 'mean_token_accuracy': 0.8343703895807266, 'epoch': 1.8354430379746836}
Step 509: {'loss': 0.5908, 'grad_norm': 1.3125, 'learning_rate': 7.89209617346984e-05, 'num_tokens': 4135125.0, 'mean_token_accuracy': 0.8414359390735626, 'epoch': 1.8390596745027126}
Step 510: {'loss': 0.6056, 'grad_norm': 1.328125, 'learning_rate': 7.851003756604034e-05, 'num_tokens': 4143902.0, 'mean_token_accuracy': 0.832742914557457, 'epoch': 1.8426763110307414}
Step 511: {'loss': 0.635, 'grad_norm': 1.296875, 'learning_rate': 7.809949349360872e-05, 'num_tokens': 4151749.0, 'mean_token_accuracy': 0.8287511914968491, 'epoch': 1.8462929475587704}
Step 512: {'loss': 0.6034, 'grad_norm': 1.15625, 'learning_rate': 7.768933677875879e-05, 'num_tokens': 4160295.0, 'mean_token_accuracy': 0.8383125215768814, 'epoch': 1.8499095840867992}
Step 513: {'loss': 0.6066, 'grad_norm': 3.828125, 'learning_rate': 7.72795746759946e-05, 'num_tokens': 4168328.0, 'mean_token_accuracy': 0.8332913964986801, 'epoch': 1.8535262206148282}
Step 514: {'loss': 0.6415, 'grad_norm': 1.46875, 'learning_rate': 7.687021443284071e-05, 'num_tokens': 4177430.0, 'mean_token_accuracy': 0.8278858363628387, 'epoch': 1.8571428571428572}
Step 515: {'loss': 0.6053, 'grad_norm': 1.59375, 'learning_rate': 7.646126328971375e-05, 'num_tokens': 4185202.0, 'mean_token_accuracy': 0.8382400274276733, 'epoch': 1.8607594936708862}
Step 516: {'loss': 0.5842, 'grad_norm': 1.5234375, 'learning_rate': 7.605272847979475e-05, 'num_tokens': 4193726.0, 'mean_token_accuracy': 0.840070441365242, 'epoch': 1.864376130198915}
Step 517: {'loss': 0.5871, 'grad_norm': 1.34375, 'learning_rate': 7.564461722890081e-05, 'num_tokens': 4201134.0, 'mean_token_accuracy': 0.8444944620132446, 'epoch': 1.8679927667269438}
Step 518: {'loss': 0.6528, 'grad_norm': 1.1953125, 'learning_rate': 7.523693675535756e-05, 'num_tokens': 4209064.0, 'mean_token_accuracy': 0.8281887173652649, 'epoch': 1.8716094032549728}
Step 519: {'loss': 0.594, 'grad_norm': 2.390625, 'learning_rate': 7.482969426987144e-05, 'num_tokens': 4216973.0, 'mean_token_accuracy': 0.8411335349082947, 'epoch': 1.8752260397830018}
Step 520: {'loss': 0.5703, 'grad_norm': 1.1171875, 'learning_rate': 7.442289697540201e-05, 'num_tokens': 4225232.0, 'mean_token_accuracy': 0.8399359285831451, 'epoch': 1.8788426763110309}
Step 521: {'loss': 0.5859, 'grad_norm': 1.109375, 'learning_rate': 7.401655206703479e-05, 'num_tokens': 4233847.0, 'mean_token_accuracy': 0.8417441248893738, 'epoch': 1.8824593128390597}
Step 522: {'loss': 0.6026, 'grad_norm': 1.2578125, 'learning_rate': 7.361066673185388e-05, 'num_tokens': 4242225.0, 'mean_token_accuracy': 0.8389337062835693, 'epoch': 1.8860759493670884}
Step 523: {'loss': 0.5884, 'grad_norm': 1.1328125, 'learning_rate': 7.32052481488147e-05, 'num_tokens': 4250337.0, 'mean_token_accuracy': 0.8419959843158722, 'epoch': 1.8896925858951175}
Step 524: {'loss': 0.5764, 'grad_norm': 1.1171875, 'learning_rate': 7.280030348861729e-05, 'num_tokens': 4259104.0, 'mean_token_accuracy': 0.8443767130374908, 'epoch': 1.8933092224231465}
Step 525: {'loss': 0.5831, 'grad_norm': 1.2265625, 'learning_rate': 7.239583991357928e-05, 'num_tokens': 4268152.0, 'mean_token_accuracy': 0.842765673995018, 'epoch': 1.8969258589511755}
Step 526: {'loss': 0.5824, 'grad_norm': 1.0625, 'learning_rate': 7.19918645775093e-05, 'num_tokens': 4276061.0, 'mean_token_accuracy': 0.8516747951507568, 'epoch': 1.9005424954792043}
Step 527: {'loss': 0.5893, 'grad_norm': 1.1640625, 'learning_rate': 7.158838462558046e-05, 'num_tokens': 4284829.0, 'mean_token_accuracy': 0.8430275470018387, 'epoch': 1.9041591320072333}
Step 528: {'loss': 0.5908, 'grad_norm': 1.0859375, 'learning_rate': 7.118540719420383e-05, 'num_tokens': 4293868.0, 'mean_token_accuracy': 0.8351135402917862, 'epoch': 1.907775768535262}
Step 529: {'loss': 0.6583, 'grad_norm': 1.1796875, 'learning_rate': 7.078293941090249e-05, 'num_tokens': 4301428.0, 'mean_token_accuracy': 0.8268153220415115, 'epoch': 1.9113924050632911}
Step 530: {'loss': 0.6246, 'grad_norm': 1.296875, 'learning_rate': 7.038098839418503e-05, 'num_tokens': 4309227.0, 'mean_token_accuracy': 0.8293928951025009, 'epoch': 1.9150090415913201}
Step 531: {'loss': 0.5745, 'grad_norm': 0.9765625, 'learning_rate': 6.99795612534202e-05, 'num_tokens': 4317754.0, 'mean_token_accuracy': 0.8438106626272202, 'epoch': 1.9186256781193491}
Step 532: {'loss': 0.6301, 'grad_norm': 1.0625, 'learning_rate': 6.957866508871068e-05, 'num_tokens': 4325617.0, 'mean_token_accuracy': 0.8323457837104797, 'epoch': 1.922242314647378}
Step 533: {'loss': 0.57, 'grad_norm': 1.421875, 'learning_rate': 6.917830699076783e-05, 'num_tokens': 4332863.0, 'mean_token_accuracy': 0.8431806117296219, 'epoch': 1.9258589511754067}
Step 534: {'loss': 0.5747, 'grad_norm': 1.046875, 'learning_rate': 6.877849404078601e-05, 'num_tokens': 4341055.0, 'mean_token_accuracy': 0.8461820334196091, 'epoch': 1.9294755877034357}
Step 535: {'loss': 0.5616, 'grad_norm': 1.21875, 'learning_rate': 6.83792333103176e-05, 'num_tokens': 4348606.0, 'mean_token_accuracy': 0.8477640450000763, 'epoch': 1.9330922242314648}
Step 536: {'loss': 0.6197, 'grad_norm': 1.5, 'learning_rate': 6.798053186114772e-05, 'num_tokens': 4356590.0, 'mean_token_accuracy': 0.831382229924202, 'epoch': 1.9367088607594938}
Step 537: {'loss': 0.6034, 'grad_norm': 1.9921875, 'learning_rate': 6.758239674516933e-05, 'num_tokens': 4364192.0, 'mean_token_accuracy': 0.8399155735969543, 'epoch': 1.9403254972875226}
Step 538: {'loss': 0.5991, 'grad_norm': 1.28125, 'learning_rate': 6.718483500425867e-05, 'num_tokens': 4373055.0, 'mean_token_accuracy': 0.8417132794857025, 'epoch': 1.9439421338155516}
Step 539: {'loss': 0.6289, 'grad_norm': 1.4609375, 'learning_rate': 6.678785367015061e-05, 'num_tokens': 4381070.0, 'mean_token_accuracy': 0.8329360783100128, 'epoch': 1.9475587703435804}
Step 540: {'loss': 0.6262, 'grad_norm': 1.640625, 'learning_rate': 6.639145976431421e-05, 'num_tokens': 4389208.0, 'mean_token_accuracy': 0.8314066380262375, 'epoch': 1.9511754068716094}
Step 541: {'loss': 0.6211, 'grad_norm': 1.34375, 'learning_rate': 6.599566029782863e-05, 'num_tokens': 4396937.0, 'mean_token_accuracy': 0.8306456506252289, 'epoch': 1.9547920433996384}
Step 542: {'loss': 0.5681, 'grad_norm': 1.1484375, 'learning_rate': 6.560046227125914e-05, 'num_tokens': 4404969.0, 'mean_token_accuracy': 0.8461618721485138, 'epoch': 1.9584086799276674}
Step 543: {'loss': 0.5884, 'grad_norm': 1.3671875, 'learning_rate': 6.520587267453312e-05, 'num_tokens': 4413091.0, 'mean_token_accuracy': 0.8370110094547272, 'epoch': 1.9620253164556962}
Step 544: {'loss': 0.5757, 'grad_norm': 1.1015625, 'learning_rate': 6.48118984868167e-05, 'num_tokens': 4421989.0, 'mean_token_accuracy': 0.842031255364418, 'epoch': 1.965641952983725}
Step 545: {'loss': 0.5734, 'grad_norm': 1.1953125, 'learning_rate': 6.44185466763911e-05, 'num_tokens': 4429429.0, 'mean_token_accuracy': 0.8478931784629822, 'epoch': 1.969258589511754}
Step 546: {'loss': 0.5903, 'grad_norm': 1.1171875, 'learning_rate': 6.402582420052948e-05, 'num_tokens': 4437205.0, 'mean_token_accuracy': 0.8411597460508347, 'epoch': 1.972875226039783}
Step 547: {'loss': 0.5736, 'grad_norm': 1.28125, 'learning_rate': 6.363373800537387e-05, 'num_tokens': 4445633.0, 'mean_token_accuracy': 0.8454914093017578, 'epoch': 1.976491862567812}
Step 548: {'loss': 0.5965, 'grad_norm': 1.34375, 'learning_rate': 6.324229502581227e-05, 'num_tokens': 4453561.0, 'mean_token_accuracy': 0.835619792342186, 'epoch': 1.9801084990958409}
Step 549: {'loss': 0.5919, 'grad_norm': 1.21875, 'learning_rate': 6.285150218535612e-05, 'num_tokens': 4462541.0, 'mean_token_accuracy': 0.8408960103988647, 'epoch': 1.9837251356238697}
Step 550: {'loss': 0.5828, 'grad_norm': 1.1796875, 'learning_rate': 6.246136639601764e-05, 'num_tokens': 4470211.0, 'mean_token_accuracy': 0.844131588935852, 'epoch': 1.9873417721518987}
Step 551: {'loss': 0.5766, 'grad_norm': 1.1328125, 'learning_rate': 6.20718945581877e-05, 'num_tokens': 4478124.0, 'mean_token_accuracy': 0.8473980724811554, 'epoch': 1.9909584086799277}
Step 552: {'loss': 0.5986, 'grad_norm': 1.1796875, 'learning_rate': 6.168309356051384e-05, 'num_tokens': 4486346.0, 'mean_token_accuracy': 0.8398633748292923, 'epoch': 1.9945750452079567}
Step 553: {'loss': 0.5728, 'grad_norm': 1.1796875, 'learning_rate': 6.129497027977829e-05, 'num_tokens': 4494059.0, 'mean_token_accuracy': 0.8439520746469498, 'epoch': 1.9981916817359855}
Step 554: {'loss': 0.2932, 'grad_norm': 0.82421875, 'learning_rate': 6.0907531580776425e-05, 'num_tokens': 4497810.0, 'mean_token_accuracy': 0.8499844372272491, 'epoch': 2.0}
Step 555: {'loss': 0.5945, 'grad_norm': 1.125, 'learning_rate': 6.052078431619528e-05, 'num_tokens': 4505943.0, 'mean_token_accuracy': 0.8358603417873383, 'epoch': 2.003616636528029}
Step 556: {'loss': 0.6052, 'grad_norm': 1.2890625, 'learning_rate': 6.0134735326492456e-05, 'num_tokens': 4513991.0, 'mean_token_accuracy': 0.8362824618816376, 'epoch': 2.007233273056058}
Step 557: {'loss': 0.579, 'grad_norm': 1.359375, 'learning_rate': 5.974939143977493e-05, 'num_tokens': 4522337.0, 'mean_token_accuracy': 0.8477122187614441, 'epoch': 2.0108499095840866}
Step 558: {'loss': 0.5875, 'grad_norm': 1.1796875, 'learning_rate': 5.9364759471678556e-05, 'num_tokens': 4530399.0, 'mean_token_accuracy': 0.8401373475790024, 'epoch': 2.0144665461121156}
Step 559: {'loss': 0.6, 'grad_norm': 1.3046875, 'learning_rate': 5.8980846225247286e-05, 'num_tokens': 4538496.0, 'mean_token_accuracy': 0.8387495130300522, 'epoch': 2.0180831826401446}
Step 560: {'loss': 0.5874, 'grad_norm': 1.296875, 'learning_rate': 5.8597658490813e-05, 'num_tokens': 4546968.0, 'mean_token_accuracy': 0.8373221457004547, 'epoch': 2.0216998191681737}
Step 561: {'loss': 0.5927, 'grad_norm': 1.2265625, 'learning_rate': 5.821520304587528e-05, 'num_tokens': 4555496.0, 'mean_token_accuracy': 0.8348565697669983, 'epoch': 2.0253164556962027}
Step 562: {'loss': 0.6443, 'grad_norm': 1.28125, 'learning_rate': 5.7833486654981606e-05, 'num_tokens': 4563728.0, 'mean_token_accuracy': 0.8286545127630234, 'epoch': 2.0289330922242317}
Step 563: {'loss': 0.5814, 'grad_norm': 1.0859375, 'learning_rate': 5.7452516069607734e-05, 'num_tokens': 4572875.0, 'mean_token_accuracy': 0.8427261114120483, 'epoch': 2.0325497287522603}
Step 564: {'loss': 0.5944, 'grad_norm': 1.4765625, 'learning_rate': 5.707229802803817e-05, 'num_tokens': 4580366.0, 'mean_token_accuracy': 0.8431551605463028, 'epoch': 2.0361663652802893}
Step 565: {'loss': 0.5691, 'grad_norm': 1.2890625, 'learning_rate': 5.669283925524715e-05, 'num_tokens': 4588331.0, 'mean_token_accuracy': 0.840119794011116, 'epoch': 2.0397830018083183}
Step 566: {'loss': 0.6021, 'grad_norm': 1.171875, 'learning_rate': 5.63141464627795e-05, 'num_tokens': 4596129.0, 'mean_token_accuracy': 0.8428875505924225, 'epoch': 2.0433996383363473}
Step 567: {'loss': 0.561, 'grad_norm': 1.65625, 'learning_rate': 5.59362263486322e-05, 'num_tokens': 4603984.0, 'mean_token_accuracy': 0.8525980412960052, 'epoch': 2.0470162748643763}
Step 568: {'loss': 0.614, 'grad_norm': 1.6328125, 'learning_rate': 5.555908559713561e-05, 'num_tokens': 4612182.0, 'mean_token_accuracy': 0.8359463214874268, 'epoch': 2.050632911392405}
Step 569: {'loss': 0.6262, 'grad_norm': 1.1953125, 'learning_rate': 5.5182730878835455e-05, 'num_tokens': 4620753.0, 'mean_token_accuracy': 0.8316369950771332, 'epoch': 2.054249547920434}
Step 570: {'loss': 0.5733, 'grad_norm': 1.203125, 'learning_rate': 5.4807168850374647e-05, 'num_tokens': 4629286.0, 'mean_token_accuracy': 0.8458865135908127, 'epoch': 2.057866184448463}
Step 571: {'loss': 0.6059, 'grad_norm': 1.328125, 'learning_rate': 5.443240615437586e-05, 'num_tokens': 4637582.0, 'mean_token_accuracy': 0.8385111689567566, 'epoch': 2.061482820976492}
Step 572: {'loss': 0.5641, 'grad_norm': 1.3828125, 'learning_rate': 5.405844941932374e-05, 'num_tokens': 4645859.0, 'mean_token_accuracy': 0.8449315875768661, 'epoch': 2.065099457504521}
Step 573: {'loss': 0.5763, 'grad_norm': 1.0859375, 'learning_rate': 5.368530525944782e-05, 'num_tokens': 4653885.0, 'mean_token_accuracy': 0.8456213176250458, 'epoch': 2.0687160940325495}
Step 574: {'loss': 0.6197, 'grad_norm': 1.328125, 'learning_rate': 5.331298027460539e-05, 'num_tokens': 4661832.0, 'mean_token_accuracy': 0.836574599146843, 'epoch': 2.0723327305605785}
Step 575: {'loss': 0.5733, 'grad_norm': 1.3984375, 'learning_rate': 5.294148105016509e-05, 'num_tokens': 4669954.0, 'mean_token_accuracy': 0.8444112539291382, 'epoch': 2.0759493670886076}
Step 576: {'loss': 0.6125, 'grad_norm': 1.0546875, 'learning_rate': 5.2570814156890024e-05, 'num_tokens': 4678209.0, 'mean_token_accuracy': 0.8344586193561554, 'epoch': 2.0795660036166366}
Step 577: {'loss': 0.569, 'grad_norm': 1.3359375, 'learning_rate': 5.2200986150821696e-05, 'num_tokens': 4685725.0, 'mean_token_accuracy': 0.8474113494157791, 'epoch': 2.0831826401446656}
Step 578: {'loss': 0.6281, 'grad_norm': 1.328125, 'learning_rate': 5.183200357316427e-05, 'num_tokens': 4693181.0, 'mean_token_accuracy': 0.8339135497808456, 'epoch': 2.0867992766726946}
Step 579: {'loss': 0.6252, 'grad_norm': 1.109375, 'learning_rate': 5.146387295016852e-05, 'num_tokens': 4701640.0, 'mean_token_accuracy': 0.827093780040741, 'epoch': 2.090415913200723}
Step 580: {'loss': 0.6033, 'grad_norm': 3.09375, 'learning_rate': 5.109660079301668e-05, 'num_tokens': 4709848.0, 'mean_token_accuracy': 0.8396764695644379, 'epoch': 2.094032549728752}
Step 581: {'loss': 0.5839, 'grad_norm': 1.1171875, 'learning_rate': 5.07301935977071e-05, 'num_tokens': 4717349.0, 'mean_token_accuracy': 0.8467579931020737, 'epoch': 2.097649186256781}
Step 582: {'loss': 0.57, 'grad_norm': 1.15625, 'learning_rate': 5.0364657844939446e-05, 'num_tokens': 4725782.0, 'mean_token_accuracy': 0.8446459323167801, 'epoch': 2.1012658227848102}
Step 583: {'loss': 0.5928, 'grad_norm': 1.09375, 'learning_rate': 5.000000000000002e-05, 'num_tokens': 4734856.0, 'mean_token_accuracy': 0.838722750544548, 'epoch': 2.1048824593128392}
Step 584: {'loss': 0.5591, 'grad_norm': 1.0859375, 'learning_rate': 4.963622651264749e-05, 'num_tokens': 4743419.0, 'mean_token_accuracy': 0.8463489413261414, 'epoch': 2.108499095840868}
Step 585: {'loss': 0.608, 'grad_norm': 1.2265625, 'learning_rate': 4.9273343816998676e-05, 'num_tokens': 4752216.0, 'mean_token_accuracy': 0.8368601351976395, 'epoch': 2.112115732368897}
Step 586: {'loss': 0.6101, 'grad_norm': 1.140625, 'learning_rate': 4.891135833141495e-05, 'num_tokens': 4760161.0, 'mean_token_accuracy': 0.8334114849567413, 'epoch': 2.115732368896926}
Step 587: {'loss': 0.5805, 'grad_norm': 1.1328125, 'learning_rate': 4.855027645838848e-05, 'num_tokens': 4767468.0, 'mean_token_accuracy': 0.8398609906435013, 'epoch': 2.119349005424955}
Step 588: {'loss': 0.5613, 'grad_norm': 1.109375, 'learning_rate': 4.8190104584429154e-05, 'num_tokens': 4775933.0, 'mean_token_accuracy': 0.8468146473169327, 'epoch': 2.122965641952984}
Step 589: {'loss': 0.5803, 'grad_norm': 1.1171875, 'learning_rate': 4.783084907995156e-05, 'num_tokens': 4784066.0, 'mean_token_accuracy': 0.8429762870073318, 'epoch': 2.1265822784810124}
Step 590: {'loss': 0.5651, 'grad_norm': 1.1171875, 'learning_rate': 4.7472516299162307e-05, 'num_tokens': 4791903.0, 'mean_token_accuracy': 0.8477427810430527, 'epoch': 2.1301989150090415}
Step 591: {'loss': 0.5869, 'grad_norm': 1.2265625, 'learning_rate': 4.7115112579947675e-05, 'num_tokens': 4800076.0, 'mean_token_accuracy': 0.8451334685087204, 'epoch': 2.1338155515370705}
Step 592: {'loss': 0.5825, 'grad_norm': 1.2890625, 'learning_rate': 4.675864424376146e-05, 'num_tokens': 4807608.0, 'mean_token_accuracy': 0.839650571346283, 'epoch': 2.1374321880650995}
Step 593: {'loss': 0.6024, 'grad_norm': 1.4453125, 'learning_rate': 4.6403117595513205e-05, 'num_tokens': 4815136.0, 'mean_token_accuracy': 0.8389283269643784, 'epoch': 2.1410488245931285}
Step 594: {'loss': 0.6107, 'grad_norm': 1.375, 'learning_rate': 4.604853892345672e-05, 'num_tokens': 4822950.0, 'mean_token_accuracy': 0.8349046558141708, 'epoch': 2.1446654611211575}
Step 595: {'loss': 0.5587, 'grad_norm': 1.3828125, 'learning_rate': 4.569491449907878e-05, 'num_tokens': 4830869.0, 'mean_token_accuracy': 0.8493247181177139, 'epoch': 2.148282097649186}
Step 596: {'loss': 0.604, 'grad_norm': 1.4609375, 'learning_rate': 4.534225057698824e-05, 'num_tokens': 4839514.0, 'mean_token_accuracy': 0.8388448357582092, 'epoch': 2.151898734177215}
Step 597: {'loss': 0.6119, 'grad_norm': 1.3671875, 'learning_rate': 4.4990553394805323e-05, 'num_tokens': 4848036.0, 'mean_token_accuracy': 0.8376953601837158, 'epoch': 2.155515370705244}
Step 598: {'loss': 0.576, 'grad_norm': 1.34375, 'learning_rate': 4.4639829173051554e-05, 'num_tokens': 4856737.0, 'mean_token_accuracy': 0.8412370383739471, 'epoch': 2.159132007233273}
Step 599: {'loss': 0.6397, 'grad_norm': 1.7109375, 'learning_rate': 4.429008411503942e-05, 'num_tokens': 4864446.0, 'mean_token_accuracy': 0.829523116350174, 'epoch': 2.162748643761302}
Step 600: {'loss': 0.5747, 'grad_norm': 1.03125, 'learning_rate': 4.394132440676284e-05, 'num_tokens': 4872241.0, 'mean_token_accuracy': 0.846758633852005, 'epoch': 2.1663652802893307}
Step 601: {'loss': 0.5926, 'grad_norm': 1.53125, 'learning_rate': 4.359355621678764e-05, 'num_tokens': 4879475.0, 'mean_token_accuracy': 0.836705356836319, 'epoch': 2.1699819168173597}
Step 602: {'loss': 0.5981, 'grad_norm': 1.5546875, 'learning_rate': 4.324678569614271e-05, 'num_tokens': 4887527.0, 'mean_token_accuracy': 0.8420869410037994, 'epoch': 2.1735985533453888}
Step 603: {'loss': 0.6438, 'grad_norm': 1.5, 'learning_rate': 4.2901018978210714e-05, 'num_tokens': 4894990.0, 'mean_token_accuracy': 0.8266003578901291, 'epoch': 2.1772151898734178}
Step 604: {'loss': 0.5643, 'grad_norm': 1.4765625, 'learning_rate': 4.255626217862013e-05, 'num_tokens': 4902651.0, 'mean_token_accuracy': 0.8485013246536255, 'epoch': 2.180831826401447}
Step 605: {'loss': 0.6341, 'grad_norm': 1.828125, 'learning_rate': 4.221252139513673e-05, 'num_tokens': 4911146.0, 'mean_token_accuracy': 0.832032635807991, 'epoch': 2.184448462929476}
Step 606: {'loss': 0.6073, 'grad_norm': 1.2265625, 'learning_rate': 4.1869802707555985e-05, 'num_tokens': 4918620.0, 'mean_token_accuracy': 0.8380926549434662, 'epoch': 2.1880650994575044}
Step 607: {'loss': 0.6035, 'grad_norm': 1.390625, 'learning_rate': 4.152811217759529e-05, 'num_tokens': 4926092.0, 'mean_token_accuracy': 0.8333533257246017, 'epoch': 2.1916817359855334}
Step 608: {'loss': 0.602, 'grad_norm': 1.2734375, 'learning_rate': 4.1187455848786904e-05, 'num_tokens': 4934344.0, 'mean_token_accuracy': 0.8393799364566803, 'epoch': 2.1952983725135624}
Step 609: {'loss': 0.6155, 'grad_norm': 1.2578125, 'learning_rate': 4.0847839746370995e-05, 'num_tokens': 4942805.0, 'mean_token_accuracy': 0.8346704393625259, 'epoch': 2.1989150090415914}
Step 610: {'loss': 0.618, 'grad_norm': 1.21875, 'learning_rate': 4.0509269877189106e-05, 'num_tokens': 4950821.0, 'mean_token_accuracy': 0.8311421424150467, 'epoch': 2.2025316455696204}
Step 611: {'loss': 0.6151, 'grad_norm': 1.6015625, 'learning_rate': 4.0171752229577875e-05, 'num_tokens': 4957836.0, 'mean_token_accuracy': 0.83897665143013, 'epoch': 2.206148282097649}
Step 612: {'loss': 0.5961, 'grad_norm': 1.5625, 'learning_rate': 3.983529277326315e-05, 'num_tokens': 4965453.0, 'mean_token_accuracy': 0.8397357016801834, 'epoch': 2.209764918625678}
Step 613: {'loss': 0.5997, 'grad_norm': 1.34375, 'learning_rate': 3.9499897459254375e-05, 'num_tokens': 4973744.0, 'mean_token_accuracy': 0.834447979927063, 'epoch': 2.213381555153707}
Step 614: {'loss': 0.5842, 'grad_norm': 1.0234375, 'learning_rate': 3.916557221973942e-05, 'num_tokens': 4982201.0, 'mean_token_accuracy': 0.838198721408844, 'epoch': 2.216998191681736}
Step 615: {'loss': 0.5886, 'grad_norm': 1.453125, 'learning_rate': 3.883232296797947e-05, 'num_tokens': 4989692.0, 'mean_token_accuracy': 0.8409118354320526, 'epoch': 2.220614828209765}
Step 616: {'loss': 0.5533, 'grad_norm': 1.171875, 'learning_rate': 3.8500155598204644e-05, 'num_tokens': 4997447.0, 'mean_token_accuracy': 0.8515207469463348, 'epoch': 2.2242314647377937}
Step 617: {'loss': 0.6231, 'grad_norm': 1.265625, 'learning_rate': 3.8169075985509585e-05, 'num_tokens': 5005469.0, 'mean_token_accuracy': 0.8285920470952988, 'epoch': 2.2278481012658227}
Step 618: {'loss': 0.5509, 'grad_norm': 1.1015625, 'learning_rate': 3.7839089985749654e-05, 'num_tokens': 5013451.0, 'mean_token_accuracy': 0.8550400584936142, 'epoch': 2.2314647377938517}
Step 619: {'loss': 0.5729, 'grad_norm': 1.15625, 'learning_rate': 3.75102034354373e-05, 'num_tokens': 5021665.0, 'mean_token_accuracy': 0.8423675894737244, 'epoch': 2.2350813743218807}
Step 620: {'loss': 0.6044, 'grad_norm': 1.265625, 'learning_rate': 3.718242215163883e-05, 'num_tokens': 5029916.0, 'mean_token_accuracy': 0.8336722105741501, 'epoch': 2.2386980108499097}
Step 621: {'loss': 0.5943, 'grad_norm': 1.203125, 'learning_rate': 3.6855751931871516e-05, 'num_tokens': 5037973.0, 'mean_token_accuracy': 0.8380284011363983, 'epoch': 2.2423146473779383}
Step 622: {'loss': 0.6202, 'grad_norm': 1.1953125, 'learning_rate': 3.653019855400123e-05, 'num_tokens': 5045554.0, 'mean_token_accuracy': 0.8383804261684418, 'epoch': 2.2459312839059673}
Step 623: {'loss': 0.5712, 'grad_norm': 1.171875, 'learning_rate': 3.6205767776139835e-05, 'num_tokens': 5054309.0, 'mean_token_accuracy': 0.8446896821260452, 'epoch': 2.2495479204339963}
Step 624: {'loss': 0.5926, 'grad_norm': 1.2265625, 'learning_rate': 3.5882465336543725e-05, 'num_tokens': 5062276.0, 'mean_token_accuracy': 0.8391639888286591, 'epoch': 2.2531645569620253}
Step 625: {'loss': 0.6024, 'grad_norm': 1.1875, 'learning_rate': 3.5560296953512295e-05, 'num_tokens': 5070555.0, 'mean_token_accuracy': 0.8420195281505585, 'epoch': 2.2567811934900543}
Step 626: {'loss': 0.5842, 'grad_norm': 1.34375, 'learning_rate': 3.523926832528658e-05, 'num_tokens': 5078516.0, 'mean_token_accuracy': 0.8431897908449173, 'epoch': 2.2603978300180834}
Step 627: {'loss': 0.5729, 'grad_norm': 1.1328125, 'learning_rate': 3.491938512994863e-05, 'num_tokens': 5086807.0, 'mean_token_accuracy': 0.8456235229969025, 'epoch': 2.264014466546112}
Step 628: {'loss': 0.6208, 'grad_norm': 1.2265625, 'learning_rate': 3.460065302532108e-05, 'num_tokens': 5094731.0, 'mean_token_accuracy': 0.8300238251686096, 'epoch': 2.267631103074141}
Step 629: {'loss': 0.6197, 'grad_norm': 1.421875, 'learning_rate': 3.4283077648867015e-05, 'num_tokens': 5102469.0, 'mean_token_accuracy': 0.829475149512291, 'epoch': 2.27124773960217}
Step 630: {'loss': 0.6375, 'grad_norm': 1.234375, 'learning_rate': 3.396666461759029e-05, 'num_tokens': 5111269.0, 'mean_token_accuracy': 0.8295326679944992, 'epoch': 2.274864376130199}
Step 631: {'loss': 0.578, 'grad_norm': 1.3125, 'learning_rate': 3.365141952793622e-05, 'num_tokens': 5119606.0, 'mean_token_accuracy': 0.8477795124053955, 'epoch': 2.278481012658228}
Step 632: {'loss': 0.5766, 'grad_norm': 1.15625, 'learning_rate': 3.333734795569251e-05, 'num_tokens': 5127783.0, 'mean_token_accuracy': 0.8451043963432312, 'epoch': 2.282097649186257}
Step 633: {'loss': 0.6117, 'grad_norm': 1.1640625, 'learning_rate': 3.3024455455890776e-05, 'num_tokens': 5135876.0, 'mean_token_accuracy': 0.8324810713529587, 'epoch': 2.2857142857142856}
Step 634: {'loss': 0.6079, 'grad_norm': 1.1484375, 'learning_rate': 3.2712747562708115e-05, 'num_tokens': 5144297.0, 'mean_token_accuracy': 0.8386265933513641, 'epoch': 2.2893309222423146}
Step 635: {'loss': 0.6196, 'grad_norm': 1.359375, 'learning_rate': 3.240222978936931e-05, 'num_tokens': 5152562.0, 'mean_token_accuracy': 0.8344774544239044, 'epoch': 2.2929475587703436}
Step 636: {'loss': 0.565, 'grad_norm': 1.1328125, 'learning_rate': 3.209290762804934e-05, 'num_tokens': 5161544.0, 'mean_token_accuracy': 0.8460593670606613, 'epoch': 2.2965641952983726}
Step 637: {'loss': 0.5912, 'grad_norm': 1.2734375, 'learning_rate': 3.178478654977624e-05, 'num_tokens': 5169897.0, 'mean_token_accuracy': 0.8440756797790527, 'epoch': 2.3001808318264017}
Step 638: {'loss': 0.5948, 'grad_norm': 1.21875, 'learning_rate': 3.147787200433423e-05, 'num_tokens': 5178025.0, 'mean_token_accuracy': 0.8360148221254349, 'epoch': 2.3037974683544302}
Step 639: {'loss': 0.5555, 'grad_norm': 1.265625, 'learning_rate': 3.1172169420167476e-05, 'num_tokens': 5186357.0, 'mean_token_accuracy': 0.8472548425197601, 'epoch': 2.3074141048824592}
Step 640: {'loss': 0.6156, 'grad_norm': 1.1640625, 'learning_rate': 3.086768420428392e-05, 'num_tokens': 5194509.0, 'mean_token_accuracy': 0.8377905040979385, 'epoch': 2.3110307414104883}
Step 641: {'loss': 0.5499, 'grad_norm': 1.234375, 'learning_rate': 3.056442174215985e-05, 'num_tokens': 5202581.0, 'mean_token_accuracy': 0.8521158695220947, 'epoch': 2.3146473779385173}
Step 642: {'loss': 0.5435, 'grad_norm': 1.3046875, 'learning_rate': 3.0262387397644463e-05, 'num_tokens': 5210188.0, 'mean_token_accuracy': 0.8508176356554031, 'epoch': 2.3182640144665463}
Step 643: {'loss': 0.581, 'grad_norm': 1.1484375, 'learning_rate': 2.9961586512864947e-05, 'num_tokens': 5218472.0, 'mean_token_accuracy': 0.842186838388443, 'epoch': 2.321880650994575}
Step 644: {'loss': 0.588, 'grad_norm': 1.5703125, 'learning_rate': 2.9662024408132228e-05, 'num_tokens': 5225701.0, 'mean_token_accuracy': 0.8454922586679459, 'epoch': 2.325497287522604}
Step 645: {'loss': 0.577, 'grad_norm': 1.1640625, 'learning_rate': 2.9363706381846747e-05, 'num_tokens': 5233740.0, 'mean_token_accuracy': 0.8409947156906128, 'epoch': 2.329113924050633}
Step 646: {'loss': 0.5954, 'grad_norm': 1.1171875, 'learning_rate': 2.9066637710404675e-05, 'num_tokens': 5241571.0, 'mean_token_accuracy': 0.8412539064884186, 'epoch': 2.332730560578662}
Step 647: {'loss': 0.6471, 'grad_norm': 1.296875, 'learning_rate': 2.8770823648104683e-05, 'num_tokens': 5249838.0, 'mean_token_accuracy': 0.8209445625543594, 'epoch': 2.336347197106691}
Step 648: {'loss': 0.6264, 'grad_norm': 1.328125, 'learning_rate': 2.8476269427054936e-05, 'num_tokens': 5257871.0, 'mean_token_accuracy': 0.8374459147453308, 'epoch': 2.3399638336347195}
Step 649: {'loss': 0.5999, 'grad_norm': 1.2890625, 'learning_rate': 2.818298025708075e-05, 'num_tokens': 5265951.0, 'mean_token_accuracy': 0.8397017121315002, 'epoch': 2.3435804701627485}
Step 650: {'loss': 0.5879, 'grad_norm': 1.2421875, 'learning_rate': 2.789096132563206e-05, 'num_tokens': 5273855.0, 'mean_token_accuracy': 0.8413403183221817, 'epoch': 2.3471971066907775}
Step 651: {'loss': 0.5941, 'grad_norm': 1.2421875, 'learning_rate': 2.7600217797692042e-05, 'num_tokens': 5281661.0, 'mean_token_accuracy': 0.8416128009557724, 'epoch': 2.3508137432188065}
Step 652: {'loss': 0.5936, 'grad_norm': 1.1953125, 'learning_rate': 2.7310754815685624e-05, 'num_tokens': 5289716.0, 'mean_token_accuracy': 0.8382477164268494, 'epoch': 2.3544303797468356}
Step 653: {'loss': 0.5713, 'grad_norm': 1.2578125, 'learning_rate': 2.702257749938849e-05, 'num_tokens': 5298578.0, 'mean_token_accuracy': 0.8420965820550919, 'epoch': 2.358047016274864}
Step 654: {'loss': 0.5931, 'grad_norm': 1.203125, 'learning_rate': 2.6735690945836543e-05, 'num_tokens': 5305953.0, 'mean_token_accuracy': 0.8416264057159424, 'epoch': 2.361663652802893}
Step 655: {'loss': 0.5503, 'grad_norm': 1.0390625, 'learning_rate': 2.6450100229235795e-05, 'num_tokens': 5314027.0, 'mean_token_accuracy': 0.8520180583000183, 'epoch': 2.365280289330922}
Step 656: {'loss': 0.5722, 'grad_norm': 1.2578125, 'learning_rate': 2.6165810400872583e-05, 'num_tokens': 5321772.0, 'mean_token_accuracy': 0.8469233959913254, 'epoch': 2.368896925858951}
Step 657: {'loss': 0.5858, 'grad_norm': 1.25, 'learning_rate': 2.5882826489024226e-05, 'num_tokens': 5330151.0, 'mean_token_accuracy': 0.8425467759370804, 'epoch': 2.37251356238698}
Step 658: {'loss': 0.5845, 'grad_norm': 1.109375, 'learning_rate': 2.5601153498870134e-05, 'num_tokens': 5338307.0, 'mean_token_accuracy': 0.8430527597665787, 'epoch': 2.376130198915009}
Step 659: {'loss': 0.5689, 'grad_norm': 1.484375, 'learning_rate': 2.532079641240318e-05, 'num_tokens': 5346620.0, 'mean_token_accuracy': 0.8458917438983917, 'epoch': 2.379746835443038}
Step 660: {'loss': 0.5664, 'grad_norm': 1.2890625, 'learning_rate': 2.5041760188341755e-05, 'num_tokens': 5354855.0, 'mean_token_accuracy': 0.8442375063896179, 'epoch': 2.383363471971067}
Step 661: {'loss': 0.6085, 'grad_norm': 1.296875, 'learning_rate': 2.4764049762041874e-05, 'num_tokens': 5363377.0, 'mean_token_accuracy': 0.8365405052900314, 'epoch': 2.386980108499096}
Step 662: {'loss': 0.5752, 'grad_norm': 1.125, 'learning_rate': 2.4487670045410006e-05, 'num_tokens': 5371817.0, 'mean_token_accuracy': 0.8480528295040131, 'epoch': 2.390596745027125}
Step 663: {'loss': 0.5707, 'grad_norm': 1.2734375, 'learning_rate': 2.4212625926816045e-05, 'num_tokens': 5379930.0, 'mean_token_accuracy': 0.842028021812439, 'epoch': 2.394213381555154}
Step 664: {'loss': 0.5827, 'grad_norm': 1.359375, 'learning_rate': 2.3938922271007147e-05, 'num_tokens': 5387820.0, 'mean_token_accuracy': 0.8391063064336777, 'epoch': 2.397830018083183}
Step 665: {'loss': 0.5641, 'grad_norm': 1.2421875, 'learning_rate': 2.3666563919021366e-05, 'num_tokens': 5395952.0, 'mean_token_accuracy': 0.8481676578521729, 'epoch': 2.4014466546112114}
Step 666: {'loss': 0.65, 'grad_norm': 1.296875, 'learning_rate': 2.339555568810221e-05, 'num_tokens': 5403542.0, 'mean_token_accuracy': 0.8302105218172073, 'epoch': 2.4050632911392404}
Step 667: {'loss': 0.5924, 'grad_norm': 1.1875, 'learning_rate': 2.312590237161335e-05, 'num_tokens': 5411456.0, 'mean_token_accuracy': 0.8404993712902069, 'epoch': 2.4086799276672695}
Step 668: {'loss': 0.5712, 'grad_norm': 1.1640625, 'learning_rate': 2.285760873895396e-05, 'num_tokens': 5419107.0, 'mean_token_accuracy': 0.8439407050609589, 'epoch': 2.4122965641952985}
Step 669: {'loss': 0.6052, 'grad_norm': 1.265625, 'learning_rate': 2.2590679535474246e-05, 'num_tokens': 5427196.0, 'mean_token_accuracy': 0.8360464423894882, 'epoch': 2.4159132007233275}
Step 670: {'loss': 0.5787, 'grad_norm': 1.3359375, 'learning_rate': 2.2325119482391467e-05, 'num_tokens': 5435442.0, 'mean_token_accuracy': 0.8378262221813202, 'epoch': 2.419529837251356}
Step 671: {'loss': 0.6065, 'grad_norm': 1.21875, 'learning_rate': 2.2060933276706586e-05, 'num_tokens': 5443401.0, 'mean_token_accuracy': 0.8305012732744217, 'epoch': 2.423146473779385}
Step 672: {'loss': 0.579, 'grad_norm': 1.5078125, 'learning_rate': 2.1798125591121145e-05, 'num_tokens': 5451176.0, 'mean_token_accuracy': 0.8478669971227646, 'epoch': 2.426763110307414}
Step 673: {'loss': 0.5681, 'grad_norm': 1.34375, 'learning_rate': 2.1536701073954558e-05, 'num_tokens': 5458870.0, 'mean_token_accuracy': 0.8418942093849182, 'epoch': 2.430379746835443}
Step 674: {'loss': 0.5835, 'grad_norm': 1.2109375, 'learning_rate': 2.1276664349061902e-05, 'num_tokens': 5467252.0, 'mean_token_accuracy': 0.835906594991684, 'epoch': 2.433996383363472}
Step 675: {'loss': 0.5863, 'grad_norm': 1.1796875, 'learning_rate': 2.1018020015752173e-05, 'num_tokens': 5475035.0, 'mean_token_accuracy': 0.8393665552139282, 'epoch': 2.4376130198915007}
Step 676: {'loss': 0.594, 'grad_norm': 1.3359375, 'learning_rate': 2.0760772648707016e-05, 'num_tokens': 5482436.0, 'mean_token_accuracy': 0.8327431976795197, 'epoch': 2.4412296564195297}
Step 677: {'loss': 0.5876, 'grad_norm': 1.234375, 'learning_rate': 2.0504926797899583e-05, 'num_tokens': 5490468.0, 'mean_token_accuracy': 0.8426152318716049, 'epoch': 2.4448462929475587}
Step 678: {'loss': 0.5642, 'grad_norm': 1.265625, 'learning_rate': 2.025048698851426e-05, 'num_tokens': 5499103.0, 'mean_token_accuracy': 0.8437619656324387, 'epoch': 2.4484629294755877}
Step 679: {'loss': 0.583, 'grad_norm': 1.1953125, 'learning_rate': 1.999745772086655e-05, 'num_tokens': 5506721.0, 'mean_token_accuracy': 0.8402433097362518, 'epoch': 2.4520795660036168}
Step 680: {'loss': 0.61, 'grad_norm': 1.3671875, 'learning_rate': 1.9745843470323556e-05, 'num_tokens': 5514959.0, 'mean_token_accuracy': 0.8388379812240601, 'epoch': 2.4556962025316453}
Step 681: {'loss': 0.5534, 'grad_norm': 1.2421875, 'learning_rate': 1.9495648687224676e-05, 'num_tokens': 5523436.0, 'mean_token_accuracy': 0.845870241522789, 'epoch': 2.4593128390596743}
Step 682: {'loss': 0.5832, 'grad_norm': 1.28125, 'learning_rate': 1.924687779680302e-05, 'num_tokens': 5531182.0, 'mean_token_accuracy': 0.8397562205791473, 'epoch': 2.4629294755877034}
Step 683: {'loss': 0.5983, 'grad_norm': 1.234375, 'learning_rate': 1.8999535199107053e-05, 'num_tokens': 5538992.0, 'mean_token_accuracy': 0.8388585597276688, 'epoch': 2.4665461121157324}
Step 684: {'loss': 0.5748, 'grad_norm': 1.296875, 'learning_rate': 1.8753625268922835e-05, 'num_tokens': 5547991.0, 'mean_token_accuracy': 0.8444245010614395, 'epoch': 2.4701627486437614}
Step 685: {'loss': 0.5946, 'grad_norm': 1.234375, 'learning_rate': 1.8509152355696623e-05, 'num_tokens': 5556195.0, 'mean_token_accuracy': 0.840997651219368, 'epoch': 2.4737793851717904}
Step 686: {'loss': 0.5534, 'grad_norm': 1.125, 'learning_rate': 1.826612078345793e-05, 'num_tokens': 5564240.0, 'mean_token_accuracy': 0.8526159077882767, 'epoch': 2.477396021699819}
Step 687: {'loss': 0.5898, 'grad_norm': 1.140625, 'learning_rate': 1.8024534850743026e-05, 'num_tokens': 5573713.0, 'mean_token_accuracy': 0.8404251337051392, 'epoch': 2.481012658227848}
Step 688: {'loss': 0.5843, 'grad_norm': 1.1796875, 'learning_rate': 1.7784398830519e-05, 'num_tokens': 5582427.0, 'mean_token_accuracy': 0.837518498301506, 'epoch': 2.484629294755877}
Step 689: {'loss': 0.5632, 'grad_norm': 1.1484375, 'learning_rate': 1.7545716970108082e-05, 'num_tokens': 5590857.0, 'mean_token_accuracy': 0.8459927588701248, 'epoch': 2.488245931283906}
Step 690: {'loss': 0.5784, 'grad_norm': 1.2109375, 'learning_rate': 1.7308493491112487e-05, 'num_tokens': 5598830.0, 'mean_token_accuracy': 0.8415483981370926, 'epoch': 2.491862567811935}
Step 691: {'loss': 0.5989, 'grad_norm': 1.1796875, 'learning_rate': 1.7072732589339955e-05, 'num_tokens': 5607031.0, 'mean_token_accuracy': 0.8390201181173325, 'epoch': 2.495479204339964}
Step 692: {'loss': 0.5611, 'grad_norm': 1.40625, 'learning_rate': 1.6838438434729308e-05, 'num_tokens': 5614536.0, 'mean_token_accuracy': 0.8501072227954865, 'epoch': 2.4990958408679926}
Step 693: {'loss': 0.5732, 'grad_norm': 1.1875, 'learning_rate': 1.66056151712768e-05, 'num_tokens': 5622977.0, 'mean_token_accuracy': 0.8448701500892639, 'epoch': 2.5027124773960217}
Step 694: {'loss': 0.5763, 'grad_norm': 1.15625, 'learning_rate': 1.6374266916962832e-05, 'num_tokens': 5630929.0, 'mean_token_accuracy': 0.8405942022800446, 'epoch': 2.5063291139240507}
Step 695: {'loss': 0.5581, 'grad_norm': 1.1953125, 'learning_rate': 1.6144397763679053e-05, 'num_tokens': 5638727.0, 'mean_token_accuracy': 0.8472616076469421, 'epoch': 2.5099457504520797}
Step 696: {'loss': 0.5922, 'grad_norm': 1.1953125, 'learning_rate': 1.5916011777156126e-05, 'num_tokens': 5647109.0, 'mean_token_accuracy': 0.8404716998338699, 'epoch': 2.5135623869801087}
Step 697: {'loss': 0.5663, 'grad_norm': 1.28125, 'learning_rate': 1.5689112996891576e-05, 'num_tokens': 5655128.0, 'mean_token_accuracy': 0.8465738445520401, 'epoch': 2.5171790235081373}
Step 698: {'loss': 0.5435, 'grad_norm': 1.140625, 'learning_rate': 1.5463705436078568e-05, 'num_tokens': 5663805.0, 'mean_token_accuracy': 0.8522245138883591, 'epoch': 2.5207956600361663}
Step 699: {'loss': 0.5887, 'grad_norm': 1.140625, 'learning_rate': 1.523979308153487e-05, 'num_tokens': 5672109.0, 'mean_token_accuracy': 0.8464695662260056, 'epoch': 2.5244122965641953}
Step 700: {'loss': 0.5814, 'grad_norm': 1.34375, 'learning_rate': 1.5017379893632255e-05, 'num_tokens': 5679236.0, 'mean_token_accuracy': 0.8459815382957458, 'epoch': 2.5280289330922243}
Step 701: {'loss': 0.6292, 'grad_norm': 1.2890625, 'learning_rate': 1.4796469806226532e-05, 'num_tokens': 5686785.0, 'mean_token_accuracy': 0.8263316601514816, 'epoch': 2.5316455696202533}
Step 702: {'loss': 0.5664, 'grad_norm': 1.140625, 'learning_rate': 1.4577066726587918e-05, 'num_tokens': 5695117.0, 'mean_token_accuracy': 0.8505492806434631, 'epoch': 2.535262206148282}
Step 703: {'loss': 0.5659, 'grad_norm': 1.09375, 'learning_rate': 1.4359174535331999e-05, 'num_tokens': 5703313.0, 'mean_token_accuracy': 0.8506302982568741, 'epoch': 2.538878842676311}
Step 704: {'loss': 0.5855, 'grad_norm': 1.2109375, 'learning_rate': 1.4142797086351e-05, 'num_tokens': 5711398.0, 'mean_token_accuracy': 0.8455997258424759, 'epoch': 2.54249547920434}
Step 705: {'loss': 0.5775, 'grad_norm': 1.2109375, 'learning_rate': 1.3927938206745716e-05, 'num_tokens': 5719781.0, 'mean_token_accuracy': 0.8428125083446503, 'epoch': 2.546112115732369}
Step 706: {'loss': 0.5821, 'grad_norm': 1.1015625, 'learning_rate': 1.3714601696757712e-05, 'num_tokens': 5727729.0, 'mean_token_accuracy': 0.8441487699747086, 'epoch': 2.549728752260398}
Step 707: {'loss': 0.5804, 'grad_norm': 0.96484375, 'learning_rate': 1.350279132970227e-05, 'num_tokens': 5736802.0, 'mean_token_accuracy': 0.841985285282135, 'epoch': 2.5533453887884265}
Step 708: {'loss': 0.5903, 'grad_norm': 1.2421875, 'learning_rate': 1.3292510851901451e-05, 'num_tokens': 5745193.0, 'mean_token_accuracy': 0.8430035263299942, 'epoch': 2.5569620253164556}
Step 709: {'loss': 0.5964, 'grad_norm': 1.25, 'learning_rate': 1.3083763982618025e-05, 'num_tokens': 5754093.0, 'mean_token_accuracy': 0.8378927856683731, 'epoch': 2.5605786618444846}
Step 710: {'loss': 0.5878, 'grad_norm': 1.2890625, 'learning_rate': 1.287655441398945e-05, 'num_tokens': 5761893.0, 'mean_token_accuracy': 0.8380834609270096, 'epoch': 2.5641952983725136}
Step 711: {'loss': 0.6018, 'grad_norm': 1.3203125, 'learning_rate': 1.2670885810962884e-05, 'num_tokens': 5769522.0, 'mean_token_accuracy': 0.8412509560585022, 'epoch': 2.5678119349005426}
Step 712: {'loss': 0.6263, 'grad_norm': 1.28125, 'learning_rate': 1.2466761811230098e-05, 'num_tokens': 5776899.0, 'mean_token_accuracy': 0.838319256901741, 'epoch': 2.571428571428571}
Step 713: {'loss': 0.5577, 'grad_norm': 1.1484375, 'learning_rate': 1.2264186025163238e-05, 'num_tokens': 5785242.0, 'mean_token_accuracy': 0.8491463512182236, 'epoch': 2.5750452079566006}
Step 714: {'loss': 0.5895, 'grad_norm': 1.2578125, 'learning_rate': 1.2063162035750963e-05, 'num_tokens': 5792681.0, 'mean_token_accuracy': 0.8430019915103912, 'epoch': 2.578661844484629}
Step 715: {'loss': 0.6035, 'grad_norm': 1.0625, 'learning_rate': 1.1863693398535114e-05, 'num_tokens': 5801684.0, 'mean_token_accuracy': 0.839223712682724, 'epoch': 2.5822784810126582}
Step 716: {'loss': 0.6078, 'grad_norm': 1.296875, 'learning_rate': 1.1665783641547734e-05, 'num_tokens': 5810367.0, 'mean_token_accuracy': 0.8343643993139267, 'epoch': 2.5858951175406872}
Step 717: {'loss': 0.5575, 'grad_norm': 1.0703125, 'learning_rate': 1.14694362652487e-05, 'num_tokens': 5818847.0, 'mean_token_accuracy': 0.8499601930379868, 'epoch': 2.5895117540687163}
Step 718: {'loss': 0.5916, 'grad_norm': 1.15625, 'learning_rate': 1.1274654742463841e-05, 'num_tokens': 5826796.0, 'mean_token_accuracy': 0.8376745134592056, 'epoch': 2.5931283905967453}
Step 719: {'loss': 0.6056, 'grad_norm': 1.265625, 'learning_rate': 1.1081442518323549e-05, 'num_tokens': 5834876.0, 'mean_token_accuracy': 0.8392229676246643, 'epoch': 2.596745027124774}
Step 720: {'loss': 0.5882, 'grad_norm': 1.2578125, 'learning_rate': 1.0889803010201716e-05, 'num_tokens': 5843147.0, 'mean_token_accuracy': 0.8418685346841812, 'epoch': 2.600361663652803}
Step 721: {'loss': 0.6108, 'grad_norm': 1.3203125, 'learning_rate': 1.0699739607655435e-05, 'num_tokens': 5851752.0, 'mean_token_accuracy': 0.8366825431585312, 'epoch': 2.603978300180832}
Step 722: {'loss': 0.6412, 'grad_norm': 1.40625, 'learning_rate': 1.0511255672364907e-05, 'num_tokens': 5859565.0, 'mean_token_accuracy': 0.8276883214712143, 'epoch': 2.607594936708861}
Step 723: {'loss': 0.6217, 'grad_norm': 1.4375, 'learning_rate': 1.0324354538074122e-05, 'num_tokens': 5868281.0, 'mean_token_accuracy': 0.8312629461288452, 'epoch': 2.61121157323689}
Step 724: {'loss': 0.6053, 'grad_norm': 1.5390625, 'learning_rate': 1.01390395105318e-05, 'num_tokens': 5876202.0, 'mean_token_accuracy': 0.8336912840604782, 'epoch': 2.6148282097649185}
Step 725: {'loss': 0.5983, 'grad_norm': 1.1171875, 'learning_rate': 9.955313867432914e-06, 'num_tokens': 5885112.0, 'mean_token_accuracy': 0.8350315243005753, 'epoch': 2.6184448462929475}
Step 726: {'loss': 0.5651, 'grad_norm': 1.171875, 'learning_rate': 9.773180858360853e-06, 'num_tokens': 5892794.0, 'mean_token_accuracy': 0.8480809479951859, 'epoch': 2.6220614828209765}
Step 727: {'loss': 0.5978, 'grad_norm': 1.25, 'learning_rate': 9.592643704729753e-06, 'num_tokens': 5901182.0, 'mean_token_accuracy': 0.8374889642000198, 'epoch': 2.6256781193490055}
Step 728: {'loss': 0.6067, 'grad_norm': 1.2734375, 'learning_rate': 9.413705599727651e-06, 'num_tokens': 5910362.0, 'mean_token_accuracy': 0.8384033888578415, 'epoch': 2.6292947558770345}
Step 729: {'loss': 0.5578, 'grad_norm': 1.2578125, 'learning_rate': 9.236369708259962e-06, 'num_tokens': 5918183.0, 'mean_token_accuracy': 0.8523516952991486, 'epoch': 2.632911392405063}
Step 730: {'loss': 0.6445, 'grad_norm': 1.15625, 'learning_rate': 9.060639166893493e-06, 'num_tokens': 5925546.0, 'mean_token_accuracy': 0.8256321847438812, 'epoch': 2.636528028933092}
Step 731: {'loss': 0.6103, 'grad_norm': 1.2265625, 'learning_rate': 8.886517083801015e-06, 'num_tokens': 5933768.0, 'mean_token_accuracy': 0.8389825224876404, 'epoch': 2.640144665461121}
Step 732: {'loss': 0.6256, 'grad_norm': 1.4609375, 'learning_rate': 8.71400653870621e-06, 'num_tokens': 5942103.0, 'mean_token_accuracy': 0.830435648560524, 'epoch': 2.64376130198915}
Step 733: {'loss': 0.6014, 'grad_norm': 1.234375, 'learning_rate': 8.543110582829272e-06, 'num_tokens': 5950016.0, 'mean_token_accuracy': 0.8390062004327774, 'epoch': 2.647377938517179}
Step 734: {'loss': 0.5604, 'grad_norm': 1.171875, 'learning_rate': 8.373832238832913e-06, 'num_tokens': 5958467.0, 'mean_token_accuracy': 0.852035641670227, 'epoch': 2.6509945750452077}
Step 735: {'loss': 0.6313, 'grad_norm': 1.203125, 'learning_rate': 8.206174500768904e-06, 'num_tokens': 5967074.0, 'mean_token_accuracy': 0.8324288576841354, 'epoch': 2.6546112115732368}
Step 736: {'loss': 0.6707, 'grad_norm': 1.28125, 'learning_rate': 8.040140334025082e-06, 'num_tokens': 5975276.0, 'mean_token_accuracy': 0.8190735727548599, 'epoch': 2.6582278481012658}
Step 737: {'loss': 0.6173, 'grad_norm': 2.234375, 'learning_rate': 7.875732675272918e-06, 'num_tokens': 5983562.0, 'mean_token_accuracy': 0.8337292224168777, 'epoch': 2.661844484629295}
Step 738: {'loss': 0.5592, 'grad_norm': 1.1796875, 'learning_rate': 7.712954432415664e-06, 'num_tokens': 5991615.0, 'mean_token_accuracy': 0.844374343752861, 'epoch': 2.665461121157324}
Step 739: {'loss': 0.5478, 'grad_norm': 1.2109375, 'learning_rate': 7.551808484536782e-06, 'num_tokens': 6000860.0, 'mean_token_accuracy': 0.8530874401330948, 'epoch': 2.6690777576853524}
Step 740: {'loss': 0.6284, 'grad_norm': 1.1328125, 'learning_rate': 7.392297681849103e-06, 'num_tokens': 6008603.0, 'mean_token_accuracy': 0.8359423726797104, 'epoch': 2.6726943942133814}
Step 741: {'loss': 0.5957, 'grad_norm': 1.3203125, 'learning_rate': 7.234424845644383e-06, 'num_tokens': 6016899.0, 'mean_token_accuracy': 0.8374871462583542, 'epoch': 2.6763110307414104}
Step 742: {'loss': 0.6122, 'grad_norm': 1.2578125, 'learning_rate': 7.078192768243486e-06, 'num_tokens': 6025395.0, 'mean_token_accuracy': 0.8387321978807449, 'epoch': 2.6799276672694394}
Step 743: {'loss': 0.5873, 'grad_norm': 1.140625, 'learning_rate': 6.923604212946822e-06, 'num_tokens': 6033321.0, 'mean_token_accuracy': 0.842115044593811, 'epoch': 2.6835443037974684}
Step 744: {'loss': 0.6078, 'grad_norm': 1.1484375, 'learning_rate': 6.770661913985632e-06, 'num_tokens': 6041643.0, 'mean_token_accuracy': 0.8367042392492294, 'epoch': 2.687160940325497}
Step 745: {'loss': 0.5738, 'grad_norm': 1.125, 'learning_rate': 6.61936857647355e-06, 'num_tokens': 6049646.0, 'mean_token_accuracy': 0.844278872013092, 'epoch': 2.6907775768535265}
Step 746: {'loss': 0.5654, 'grad_norm': 1.1875, 'learning_rate': 6.469726876358839e-06, 'num_tokens': 6058449.0, 'mean_token_accuracy': 0.8474681824445724, 'epoch': 2.694394213381555}
Step 747: {'loss': 0.5559, 'grad_norm': 1.171875, 'learning_rate': 6.32173946037693e-06, 'num_tokens': 6067069.0, 'mean_token_accuracy': 0.8456750214099884, 'epoch': 2.698010849909584}
Step 748: {'loss': 0.5821, 'grad_norm': 1.1328125, 'learning_rate': 6.175408946003703e-06, 'num_tokens': 6075091.0, 'mean_token_accuracy': 0.8404379189014435, 'epoch': 2.701627486437613}
Step 749: {'loss': 0.579, 'grad_norm': 1.2890625, 'learning_rate': 6.030737921409169e-06, 'num_tokens': 6084111.0, 'mean_token_accuracy': 0.8397874534130096, 'epoch': 2.705244122965642}
Step 750: {'loss': 0.5978, 'grad_norm': 1.296875, 'learning_rate': 5.887728945411697e-06, 'num_tokens': 6091801.0, 'mean_token_accuracy': 0.8377179205417633, 'epoch': 2.708860759493671}
Step 751: {'loss': 0.6019, 'grad_norm': 1.2265625, 'learning_rate': 5.746384547432737e-06, 'num_tokens': 6099264.0, 'mean_token_accuracy': 0.8361388444900513, 'epoch': 2.7124773960216997}
Step 752: {'loss': 0.5835, 'grad_norm': 1.1015625, 'learning_rate': 5.606707227452112e-06, 'num_tokens': 6108246.0, 'mean_token_accuracy': 0.8410059809684753, 'epoch': 2.7160940325497287}
Step 753: {'loss': 0.584, 'grad_norm': 1.1875, 'learning_rate': 5.468699455963766e-06, 'num_tokens': 6116512.0, 'mean_token_accuracy': 0.842697486281395, 'epoch': 2.7197106690777577}
Step 754: {'loss': 0.5554, 'grad_norm': 1.203125, 'learning_rate': 5.332363673932106e-06, 'num_tokens': 6124740.0, 'mean_token_accuracy': 0.8479533344507217, 'epoch': 2.7233273056057867}
Step 755: {'loss': 0.5789, 'grad_norm': 1.0078125, 'learning_rate': 5.197702292748785e-06, 'num_tokens': 6133191.0, 'mean_token_accuracy': 0.8437185734510422, 'epoch': 2.7269439421338157}
Step 756: {'loss': 0.6094, 'grad_norm': 1.1484375, 'learning_rate': 5.064717694190102e-06, 'num_tokens': 6141836.0, 'mean_token_accuracy': 0.8374075889587402, 'epoch': 2.7305605786618443}
Step 757: {'loss': 0.5902, 'grad_norm': 1.34375, 'learning_rate': 4.933412230374812e-06, 'num_tokens': 6149569.0, 'mean_token_accuracy': 0.8379706740379333, 'epoch': 2.7341772151898733}
Step 758: {'loss': 0.5794, 'grad_norm': 1.171875, 'learning_rate': 4.803788223722594e-06, 'num_tokens': 6157560.0, 'mean_token_accuracy': 0.8413214534521103, 'epoch': 2.7377938517179023}
Step 759: {'loss': 0.5872, 'grad_norm': 1.609375, 'learning_rate': 4.675847966912905e-06, 'num_tokens': 6164635.0, 'mean_token_accuracy': 0.8435559123754501, 'epoch': 2.7414104882459314}
Step 760: {'loss': 0.6226, 'grad_norm': 1.296875, 'learning_rate': 4.549593722844492e-06, 'num_tokens': 6172092.0, 'mean_token_accuracy': 0.8302197456359863, 'epoch': 2.7450271247739604}
Step 761: {'loss': 0.5894, 'grad_norm': 1.2421875, 'learning_rate': 4.425027724595298e-06, 'num_tokens': 6180139.0, 'mean_token_accuracy': 0.8424805551767349, 'epoch': 2.748643761301989}
Step 762: {'loss': 0.5738, 'grad_norm': 1.2734375, 'learning_rate': 4.302152175383101e-06, 'num_tokens': 6188515.0, 'mean_token_accuracy': 0.8451708406209946, 'epoch': 2.752260397830018}
Step 763: {'loss': 0.6071, 'grad_norm': 1.171875, 'learning_rate': 4.180969248526334e-06, 'num_tokens': 6196566.0, 'mean_token_accuracy': 0.8402000963687897, 'epoch': 2.755877034358047}
Step 764: {'loss': 0.6249, 'grad_norm': 1.296875, 'learning_rate': 4.0614810874058055e-06, 'num_tokens': 6204702.0, 'mean_token_accuracy': 0.8335590362548828, 'epoch': 2.759493670886076}
Step 765: {'loss': 0.5955, 'grad_norm': 1.296875, 'learning_rate': 3.943689805426776e-06, 'num_tokens': 6212287.0, 'mean_token_accuracy': 0.8380087912082672, 'epoch': 2.763110307414105}
Step 766: {'loss': 0.5761, 'grad_norm': 1.359375, 'learning_rate': 3.827597485981527e-06, 'num_tokens': 6219738.0, 'mean_token_accuracy': 0.839958056807518, 'epoch': 2.7667269439421336}
Step 767: {'loss': 0.5738, 'grad_norm': 1.1796875, 'learning_rate': 3.7132061824125098e-06, 'num_tokens': 6227492.0, 'mean_token_accuracy': 0.8405942469835281, 'epoch': 2.7703435804701626}
Step 768: {'loss': 0.6043, 'grad_norm': 1.875, 'learning_rate': 3.6005179179760875e-06, 'num_tokens': 6234390.0, 'mean_token_accuracy': 0.8355532139539719, 'epoch': 2.7739602169981916}
Step 769: {'loss': 0.5876, 'grad_norm': 1.1328125, 'learning_rate': 3.4895346858066724e-06, 'num_tokens': 6242536.0, 'mean_token_accuracy': 0.8414447456598282, 'epoch': 2.7775768535262206}
Step 770: {'loss': 0.5985, 'grad_norm': 1.3359375, 'learning_rate': 3.380258448881546e-06, 'num_tokens': 6251121.0, 'mean_token_accuracy': 0.8330932855606079, 'epoch': 2.7811934900542497}
Step 771: {'loss': 0.5891, 'grad_norm': 1.1015625, 'learning_rate': 3.2726911399860837e-06, 'num_tokens': 6259483.0, 'mean_token_accuracy': 0.8408491462469101, 'epoch': 2.7848101265822782}
Step 772: {'loss': 0.6035, 'grad_norm': 1.1953125, 'learning_rate': 3.1668346616795963e-06, 'num_tokens': 6266725.0, 'mean_token_accuracy': 0.8380095064640045, 'epoch': 2.7884267631103077}
Step 773: {'loss': 0.5734, 'grad_norm': 1.1015625, 'learning_rate': 3.0626908862616986e-06, 'num_tokens': 6275808.0, 'mean_token_accuracy': 0.8460740894079208, 'epoch': 2.7920433996383363}
Step 774: {'loss': 0.5933, 'grad_norm': 1.171875, 'learning_rate': 2.960261655739127e-06, 'num_tokens': 6283297.0, 'mean_token_accuracy': 0.8428813368082047, 'epoch': 2.7956600361663653}
Step 775: {'loss': 0.5636, 'grad_norm': 1.2109375, 'learning_rate': 2.859548781793242e-06, 'num_tokens': 6291277.0, 'mean_token_accuracy': 0.8471707552671432, 'epoch': 2.7992766726943943}
Step 776: {'loss': 0.572, 'grad_norm': 1.125, 'learning_rate': 2.7605540457479097e-06, 'num_tokens': 6300116.0, 'mean_token_accuracy': 0.8474858701229095, 'epoch': 2.8028933092224233}
Step 777: {'loss': 0.603, 'grad_norm': 1.1171875, 'learning_rate': 2.66327919853806e-06, 'num_tokens': 6309194.0, 'mean_token_accuracy': 0.8366218507289886, 'epoch': 2.8065099457504523}
Step 778: {'loss': 0.5994, 'grad_norm': 1.15625, 'learning_rate': 2.5677259606786684e-06, 'num_tokens': 6317051.0, 'mean_token_accuracy': 0.8408731669187546, 'epoch': 2.810126582278481}
Step 779: {'loss': 0.6003, 'grad_norm': 1.1640625, 'learning_rate': 2.4738960222343566e-06, 'num_tokens': 6324791.0, 'mean_token_accuracy': 0.8375188559293747, 'epoch': 2.81374321880651}
Step 780: {'loss': 0.6002, 'grad_norm': 1.1328125, 'learning_rate': 2.3817910427894717e-06, 'num_tokens': 6332647.0, 'mean_token_accuracy': 0.8426292389631271, 'epoch': 2.817359855334539}
Step 781: {'loss': 0.5783, 'grad_norm': 1.140625, 'learning_rate': 2.291412651418778e-06, 'num_tokens': 6341831.0, 'mean_token_accuracy': 0.8419476747512817, 'epoch': 2.820976491862568}
Step 782: {'loss': 0.5678, 'grad_norm': 1.296875, 'learning_rate': 2.202762446658602e-06, 'num_tokens': 6349710.0, 'mean_token_accuracy': 0.847658634185791, 'epoch': 2.824593128390597}
Step 783: {'loss': 0.5783, 'grad_norm': 1.09375, 'learning_rate': 2.1158419964785516e-06, 'num_tokens': 6358300.0, 'mean_token_accuracy': 0.8444285243749619, 'epoch': 2.8282097649186255}
Step 784: {'loss': 0.6534, 'grad_norm': 1.28125, 'learning_rate': 2.03065283825381e-06, 'num_tokens': 6366298.0, 'mean_token_accuracy': 0.8255581557750702, 'epoch': 2.8318264014466545}
Step 785: {'loss': 0.5347, 'grad_norm': 1.1328125, 'learning_rate': 1.947196478737967e-06, 'num_tokens': 6374382.0, 'mean_token_accuracy': 0.8566354066133499, 'epoch': 2.8354430379746836}
Step 786: {'loss': 0.6152, 'grad_norm': 1.3359375, 'learning_rate': 1.8654743940363039e-06, 'num_tokens': 6382498.0, 'mean_token_accuracy': 0.8287645280361176, 'epoch': 2.8390596745027126}
Step 787: {'loss': 0.547, 'grad_norm': 1.1328125, 'learning_rate': 1.7854880295797405e-06, 'num_tokens': 6391375.0, 'mean_token_accuracy': 0.8491288870573044, 'epoch': 2.8426763110307416}
Step 788: {'loss': 0.579, 'grad_norm': 1.1015625, 'learning_rate': 1.70723880009922e-06, 'num_tokens': 6399065.0, 'mean_token_accuracy': 0.8409265130758286, 'epoch': 2.84629294755877}
Step 789: {'loss': 0.6218, 'grad_norm': 1.09375, 'learning_rate': 1.630728089600775e-06, 'num_tokens': 6407265.0, 'mean_token_accuracy': 0.8342582434415817, 'epoch': 2.849909584086799}
Step 790: {'loss': 0.5858, 'grad_norm': 1.21875, 'learning_rate': 1.5559572513409338e-06, 'num_tokens': 6415448.0, 'mean_token_accuracy': 0.8435855507850647, 'epoch': 2.853526220614828}
Step 791: {'loss': 0.5405, 'grad_norm': 0.99609375, 'learning_rate': 1.482927607802853e-06, 'num_tokens': 6424488.0, 'mean_token_accuracy': 0.8552924990653992, 'epoch': 2.857142857142857}
Step 792: {'loss': 0.5921, 'grad_norm': 1.3046875, 'learning_rate': 1.411640450672902e-06, 'num_tokens': 6432563.0, 'mean_token_accuracy': 0.8439638912677765, 'epoch': 2.8607594936708862}
Step 793: {'loss': 0.5791, 'grad_norm': 1.171875, 'learning_rate': 1.3420970408178913e-06, 'num_tokens': 6439725.0, 'mean_token_accuracy': 0.845550611615181, 'epoch': 2.864376130198915}
Step 794: {'loss': 0.5973, 'grad_norm': 1.3203125, 'learning_rate': 1.2742986082626252e-06, 'num_tokens': 6447602.0, 'mean_token_accuracy': 0.8427644819021225, 'epoch': 2.867992766726944}
Step 795: {'loss': 0.5574, 'grad_norm': 1.1484375, 'learning_rate': 1.2082463521682853e-06, 'num_tokens': 6456134.0, 'mean_token_accuracy': 0.8493899554014206, 'epoch': 2.871609403254973}
Step 796: {'loss': 0.5818, 'grad_norm': 1.4140625, 'learning_rate': 1.143941440811147e-06, 'num_tokens': 6464187.0, 'mean_token_accuracy': 0.8446027934551239, 'epoch': 2.875226039783002}
Step 797: {'loss': 0.6259, 'grad_norm': 1.390625, 'learning_rate': 1.0813850115619306e-06, 'num_tokens': 6471712.0, 'mean_token_accuracy': 0.8256912678480148, 'epoch': 2.878842676311031}
Step 798: {'loss': 0.604, 'grad_norm': 1.6171875, 'learning_rate': 1.0205781708656935e-06, 'num_tokens': 6480072.0, 'mean_token_accuracy': 0.8379739820957184, 'epoch': 2.8824593128390594}
Step 799: {'loss': 0.5867, 'grad_norm': 1.2109375, 'learning_rate': 9.615219942222474e-07, 'num_tokens': 6489162.0, 'mean_token_accuracy': 0.8380226641893387, 'epoch': 2.8860759493670884}
Step 800: {'loss': 0.5714, 'grad_norm': 1.2578125, 'learning_rate': 9.042175261671615e-07, 'num_tokens': 6497542.0, 'mean_token_accuracy': 0.8483365476131439, 'epoch': 2.8896925858951175}
Step 801: {'loss': 0.5915, 'grad_norm': 1.2109375, 'learning_rate': 8.486657802532439e-07, 'num_tokens': 6505647.0, 'mean_token_accuracy': 0.8416405916213989, 'epoch': 2.8933092224231465}
Step 802: {'loss': 0.5774, 'grad_norm': 1.34375, 'learning_rate': 7.948677390326786e-07, 'num_tokens': 6513779.0, 'mean_token_accuracy': 0.843036413192749, 'epoch': 2.8969258589511755}
Step 803: {'loss': 0.596, 'grad_norm': 1.328125, 'learning_rate': 7.428243540395508e-07, 'num_tokens': 6521324.0, 'mean_token_accuracy': 0.8435492962598801, 'epoch': 2.900542495479204}
Step 804: {'loss': 0.6071, 'grad_norm': 1.34375, 'learning_rate': 6.92536545773137e-07, 'num_tokens': 6528869.0, 'mean_token_accuracy': 0.8386490345001221, 'epoch': 2.9041591320072335}
Step 805: {'loss': 0.5667, 'grad_norm': 1.125, 'learning_rate': 6.440052036815081e-07, 'num_tokens': 6537382.0, 'mean_token_accuracy': 0.8446630835533142, 'epoch': 2.907775768535262}
Step 806: {'loss': 0.6079, 'grad_norm': 1.1953125, 'learning_rate': 5.972311861458968e-07, 'num_tokens': 6545401.0, 'mean_token_accuracy': 0.8355303406715393, 'epoch': 2.911392405063291}
Step 807: {'loss': 0.582, 'grad_norm': 1.140625, 'learning_rate': 5.522153204654323e-07, 'num_tokens': 6553755.0, 'mean_token_accuracy': 0.8388776332139969, 'epoch': 2.91500904159132}
Step 808: {'loss': 0.5961, 'grad_norm': 1.2421875, 'learning_rate': 5.089584028425743e-07, 'num_tokens': 6561714.0, 'mean_token_accuracy': 0.8403014540672302, 'epoch': 2.918625678119349}
Step 809: {'loss': 0.5956, 'grad_norm': 1.390625, 'learning_rate': 4.674611983689792e-07, 'num_tokens': 6570313.0, 'mean_token_accuracy': 0.8428746312856674, 'epoch': 2.922242314647378}
Step 810: {'loss': 0.5932, 'grad_norm': 1.1328125, 'learning_rate': 4.277244410120007e-07, 'num_tokens': 6578424.0, 'mean_token_accuracy': 0.8357231914997101, 'epoch': 2.9258589511754067}
Step 811: {'loss': 0.5799, 'grad_norm': 1.234375, 'learning_rate': 3.8974883360169966e-07, 'num_tokens': 6586663.0, 'mean_token_accuracy': 0.840808555483818, 'epoch': 2.9294755877034357}
Step 812: {'loss': 0.5594, 'grad_norm': 1.078125, 'learning_rate': 3.5353504781840965e-07, 'num_tokens': 6594449.0, 'mean_token_accuracy': 0.8506768494844437, 'epoch': 2.9330922242314648}
Step 813: {'loss': 0.5831, 'grad_norm': 1.1640625, 'learning_rate': 3.1908372418084644e-07, 'num_tokens': 6603674.0, 'mean_token_accuracy': 0.8385794907808304, 'epoch': 2.9367088607594938}
Step 814: {'loss': 0.6128, 'grad_norm': 1.1640625, 'learning_rate': 2.86395472034795e-07, 'num_tokens': 6612504.0, 'mean_token_accuracy': 0.8316435515880585, 'epoch': 2.940325497287523}
Step 815: {'loss': 0.6076, 'grad_norm': 1.359375, 'learning_rate': 2.5547086954234024e-07, 'num_tokens': 6620496.0, 'mean_token_accuracy': 0.8379566222429276, 'epoch': 2.9439421338155514}
Step 816: {'loss': 0.5783, 'grad_norm': 1.109375, 'learning_rate': 2.2631046367163067e-07, 'num_tokens': 6629038.0, 'mean_token_accuracy': 0.8440111428499222, 'epoch': 2.9475587703435804}
Step 817: {'loss': 0.5771, 'grad_norm': 1.1640625, 'learning_rate': 1.989147701871641e-07, 'num_tokens': 6637649.0, 'mean_token_accuracy': 0.840430274605751, 'epoch': 2.9511754068716094}
Step 818: {'loss': 0.6005, 'grad_norm': 1.15625, 'learning_rate': 1.7328427364073917e-07, 'num_tokens': 6645947.0, 'mean_token_accuracy': 0.837616577744484, 'epoch': 2.9547920433996384}
Step 819: {'loss': 0.5799, 'grad_norm': 1.1171875, 'learning_rate': 1.494194273628402e-07, 'num_tokens': 6654013.0, 'mean_token_accuracy': 0.8400095701217651, 'epoch': 2.9584086799276674}
Step 820: {'loss': 0.5844, 'grad_norm': 1.15625, 'learning_rate': 1.2732065345462118e-07, 'num_tokens': 6662033.0, 'mean_token_accuracy': 0.838128849864006, 'epoch': 2.962025316455696}
Step 821: {'loss': 0.5774, 'grad_norm': 1.1796875, 'learning_rate': 1.0698834278045633e-07, 'num_tokens': 6669868.0, 'mean_token_accuracy': 0.8380734026432037, 'epoch': 2.965641952983725}
Step 822: {'loss': 0.5433, 'grad_norm': 1.0234375, 'learning_rate': 8.842285496100111e-08, 'num_tokens': 6678144.0, 'mean_token_accuracy': 0.8525518476963043, 'epoch': 2.969258589511754}
Step 823: {'loss': 0.576, 'grad_norm': 1.234375, 'learning_rate': 7.162451836685291e-08, 'num_tokens': 6685967.0, 'mean_token_accuracy': 0.8417794853448868, 'epoch': 2.972875226039783}
Step 824: {'loss': 0.6241, 'grad_norm': 1.2578125, 'learning_rate': 5.659363011275565e-08, 'num_tokens': 6693633.0, 'mean_token_accuracy': 0.8316186517477036, 'epoch': 2.976491862567812}
Step 825: {'loss': 0.6218, 'grad_norm': 1.28125, 'learning_rate': 4.3330456052292914e-08, 'num_tokens': 6701920.0, 'mean_token_accuracy': 0.8372954875230789, 'epoch': 2.9801084990958406}
Step 826: {'loss': 0.5601, 'grad_norm': 1.3046875, 'learning_rate': 3.183523077324724e-08, 'num_tokens': 6710758.0, 'mean_token_accuracy': 0.8464041203260422, 'epoch': 2.9837251356238697}
Step 827: {'loss': 0.5672, 'grad_norm': 1.1796875, 'learning_rate': 2.210815759341456e-08, 'num_tokens': 6718865.0, 'mean_token_accuracy': 0.8464331477880478, 'epoch': 2.9873417721518987}
Step 828: {'loss': 0.5586, 'grad_norm': 1.1328125, 'learning_rate': 1.414940855699598e-08, 'num_tokens': 6726552.0, 'mean_token_accuracy': 0.8460891246795654, 'epoch': 2.9909584086799277}
Step 829: {'loss': 0.6242, 'grad_norm': 2.015625, 'learning_rate': 7.959124431622389e-09, 'num_tokens': 6734560.0, 'mean_token_accuracy': 0.8354198038578033, 'epoch': 2.9945750452079567}
Step 830: {'loss': 0.6214, 'grad_norm': 1.1328125, 'learning_rate': 3.5374147057676276e-09, 'num_tokens': 6743119.0, 'mean_token_accuracy': 0.83045694231987, 'epoch': 2.9981916817359853}
Step 831: {'loss': 0.2997, 'grad_norm': 0.99609375, 'learning_rate': 8.843575868833221e-10, 'num_tokens': 6746715.0, 'mean_token_accuracy': 0.836452066898346, 'epoch': 3.0}
Step 831: {'train_runtime': 2669.1763, 'train_samples_per_second': 9.944, 'train_steps_per_second': 0.311, 'total_flos': 0.0, 'train_loss': 0.6581507523495942, 'epoch': 3.0}
