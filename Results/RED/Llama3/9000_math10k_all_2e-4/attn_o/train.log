Step 1: {'loss': 1.9281, 'grad_norm': 10.75, 'learning_rate': 0.0, 'num_tokens': 6870.0, 'mean_token_accuracy': 0.6471504122018814, 'epoch': 0.0035746201966041107}
Step 2: {'loss': 1.8842, 'grad_norm': 10.5, 'learning_rate': 2.3809523809523808e-06, 'num_tokens': 14029.0, 'mean_token_accuracy': 0.6549101024866104, 'epoch': 0.0071492403932082215}
Step 3: {'loss': 1.7438, 'grad_norm': 9.25, 'learning_rate': 4.7619047619047615e-06, 'num_tokens': 21955.0, 'mean_token_accuracy': 0.6773037612438202, 'epoch': 0.010723860589812333}
Step 4: {'loss': 1.8404, 'grad_norm': 10.0625, 'learning_rate': 7.142857142857143e-06, 'num_tokens': 29279.0, 'mean_token_accuracy': 0.6695668995380402, 'epoch': 0.014298480786416443}
Step 5: {'loss': 1.894, 'grad_norm': 10.625, 'learning_rate': 9.523809523809523e-06, 'num_tokens': 36132.0, 'mean_token_accuracy': 0.6606968641281128, 'epoch': 0.017873100983020553}
Step 6: {'loss': 1.8026, 'grad_norm': 10.0, 'learning_rate': 1.1904761904761905e-05, 'num_tokens': 43261.0, 'mean_token_accuracy': 0.6684628278017044, 'epoch': 0.021447721179624665}
Step 7: {'loss': 1.7598, 'grad_norm': 8.4375, 'learning_rate': 1.4285714285714285e-05, 'num_tokens': 50273.0, 'mean_token_accuracy': 0.6785538345575333, 'epoch': 0.025022341376228777}
Step 8: {'loss': 1.9289, 'grad_norm': 8.1875, 'learning_rate': 1.6666666666666667e-05, 'num_tokens': 56962.0, 'mean_token_accuracy': 0.6506290435791016, 'epoch': 0.028596961572832886}
Step 9: {'loss': 1.7788, 'grad_norm': 6.8125, 'learning_rate': 1.9047619047619046e-05, 'num_tokens': 64111.0, 'mean_token_accuracy': 0.6694523841142654, 'epoch': 0.032171581769437}
Step 10: {'loss': 1.6993, 'grad_norm': 6.25, 'learning_rate': 2.1428571428571428e-05, 'num_tokens': 71541.0, 'mean_token_accuracy': 0.6823606044054031, 'epoch': 0.035746201966041107}
Step 11: {'loss': 1.6275, 'grad_norm': 5.59375, 'learning_rate': 2.380952380952381e-05, 'num_tokens': 79329.0, 'mean_token_accuracy': 0.6953314691781998, 'epoch': 0.03932082216264522}
Step 12: {'loss': 1.7532, 'grad_norm': 5.875, 'learning_rate': 2.6190476190476192e-05, 'num_tokens': 86094.0, 'mean_token_accuracy': 0.6768700927495956, 'epoch': 0.04289544235924933}
Step 13: {'loss': 1.8865, 'grad_norm': 5.8125, 'learning_rate': 2.857142857142857e-05, 'num_tokens': 92383.0, 'mean_token_accuracy': 0.6523146778345108, 'epoch': 0.04647006255585344}
Step 14: {'loss': 1.6453, 'grad_norm': 4.8125, 'learning_rate': 3.095238095238095e-05, 'num_tokens': 99589.0, 'mean_token_accuracy': 0.6859105378389359, 'epoch': 0.050044682752457555}
Step 15: {'loss': 1.7503, 'grad_norm': 5.03125, 'learning_rate': 3.3333333333333335e-05, 'num_tokens': 106075.0, 'mean_token_accuracy': 0.6624821871519089, 'epoch': 0.05361930294906166}
Step 16: {'loss': 1.6403, 'grad_norm': 4.5, 'learning_rate': 3.571428571428572e-05, 'num_tokens': 112984.0, 'mean_token_accuracy': 0.6765303611755371, 'epoch': 0.05719392314566577}
Step 17: {'loss': 1.4866, 'grad_norm': 3.90625, 'learning_rate': 3.809523809523809e-05, 'num_tokens': 120641.0, 'mean_token_accuracy': 0.7059564888477325, 'epoch': 0.06076854334226988}
Step 18: {'loss': 1.5748, 'grad_norm': 4.0, 'learning_rate': 4.047619047619048e-05, 'num_tokens': 127861.0, 'mean_token_accuracy': 0.6800868064165115, 'epoch': 0.064343163538874}
Step 19: {'loss': 1.5155, 'grad_norm': 3.65625, 'learning_rate': 4.2857142857142856e-05, 'num_tokens': 135161.0, 'mean_token_accuracy': 0.685450553894043, 'epoch': 0.0679177837354781}
Step 20: {'loss': 1.4343, 'grad_norm': 3.40625, 'learning_rate': 4.523809523809524e-05, 'num_tokens': 142821.0, 'mean_token_accuracy': 0.6989316046237946, 'epoch': 0.07149240393208221}
Step 21: {'loss': 1.4849, 'grad_norm': 3.71875, 'learning_rate': 4.761904761904762e-05, 'num_tokens': 150258.0, 'mean_token_accuracy': 0.6803610026836395, 'epoch': 0.07506702412868632}
Step 22: {'loss': 1.403, 'grad_norm': 3.65625, 'learning_rate': 5e-05, 'num_tokens': 157944.0, 'mean_token_accuracy': 0.6984232664108276, 'epoch': 0.07864164432529044}
Step 23: {'loss': 1.3964, 'grad_norm': 3.71875, 'learning_rate': 5.2380952380952384e-05, 'num_tokens': 165537.0, 'mean_token_accuracy': 0.7012486606836319, 'epoch': 0.08221626452189455}
Step 24: {'loss': 1.3884, 'grad_norm': 3.8125, 'learning_rate': 5.4761904761904766e-05, 'num_tokens': 172796.0, 'mean_token_accuracy': 0.6996873319149017, 'epoch': 0.08579088471849866}
Step 25: {'loss': 1.4037, 'grad_norm': 3.59375, 'learning_rate': 5.714285714285714e-05, 'num_tokens': 180486.0, 'mean_token_accuracy': 0.6991272419691086, 'epoch': 0.08936550491510277}
Step 26: {'loss': 1.433, 'grad_norm': 3.28125, 'learning_rate': 5.9523809523809524e-05, 'num_tokens': 187614.0, 'mean_token_accuracy': 0.6787369400262833, 'epoch': 0.09294012511170688}
Step 27: {'loss': 1.3921, 'grad_norm': 3.15625, 'learning_rate': 6.19047619047619e-05, 'num_tokens': 194379.0, 'mean_token_accuracy': 0.686480700969696, 'epoch': 0.09651474530831099}
Step 28: {'loss': 1.1892, 'grad_norm': 2.59375, 'learning_rate': 6.428571428571429e-05, 'num_tokens': 202581.0, 'mean_token_accuracy': 0.7297314703464508, 'epoch': 0.10008936550491511}
Step 29: {'loss': 1.406, 'grad_norm': 2.859375, 'learning_rate': 6.666666666666667e-05, 'num_tokens': 209433.0, 'mean_token_accuracy': 0.6897901594638824, 'epoch': 0.10366398570151922}
Step 30: {'loss': 1.2873, 'grad_norm': 2.96875, 'learning_rate': 6.904761904761905e-05, 'num_tokens': 216045.0, 'mean_token_accuracy': 0.7176844775676727, 'epoch': 0.10723860589812333}
Step 31: {'loss': 1.3644, 'grad_norm': 2.859375, 'learning_rate': 7.142857142857143e-05, 'num_tokens': 222500.0, 'mean_token_accuracy': 0.7047875970602036, 'epoch': 0.11081322609472744}
Step 32: {'loss': 1.3107, 'grad_norm': 2.578125, 'learning_rate': 7.380952380952382e-05, 'num_tokens': 229673.0, 'mean_token_accuracy': 0.7053854763507843, 'epoch': 0.11438784629133154}
Step 33: {'loss': 1.2342, 'grad_norm': 2.453125, 'learning_rate': 7.619047619047618e-05, 'num_tokens': 237015.0, 'mean_token_accuracy': 0.717311680316925, 'epoch': 0.11796246648793565}
Step 34: {'loss': 1.2963, 'grad_norm': 2.78125, 'learning_rate': 7.857142857142858e-05, 'num_tokens': 243410.0, 'mean_token_accuracy': 0.7071111351251602, 'epoch': 0.12153708668453976}
Step 35: {'loss': 1.1676, 'grad_norm': 2.484375, 'learning_rate': 8.095238095238096e-05, 'num_tokens': 250568.0, 'mean_token_accuracy': 0.7298548519611359, 'epoch': 0.12511170688114387}
Step 36: {'loss': 1.1249, 'grad_norm': 2.15625, 'learning_rate': 8.333333333333334e-05, 'num_tokens': 258104.0, 'mean_token_accuracy': 0.7270591557025909, 'epoch': 0.128686327077748}
Step 37: {'loss': 1.1613, 'grad_norm': 2.265625, 'learning_rate': 8.571428571428571e-05, 'num_tokens': 265055.0, 'mean_token_accuracy': 0.7248367965221405, 'epoch': 0.1322609472743521}
Step 38: {'loss': 1.1831, 'grad_norm': 2.3125, 'learning_rate': 8.80952380952381e-05, 'num_tokens': 271456.0, 'mean_token_accuracy': 0.7087340503931046, 'epoch': 0.1358355674709562}
Step 39: {'loss': 1.1188, 'grad_norm': 2.171875, 'learning_rate': 9.047619047619048e-05, 'num_tokens': 278331.0, 'mean_token_accuracy': 0.727683424949646, 'epoch': 0.13941018766756033}
Step 40: {'loss': 1.0825, 'grad_norm': 2.375, 'learning_rate': 9.285714285714286e-05, 'num_tokens': 285549.0, 'mean_token_accuracy': 0.728597104549408, 'epoch': 0.14298480786416443}
Step 41: {'loss': 1.1395, 'grad_norm': 2.1875, 'learning_rate': 9.523809523809524e-05, 'num_tokens': 291689.0, 'mean_token_accuracy': 0.7153686434030533, 'epoch': 0.14655942806076855}
Step 42: {'loss': 1.033, 'grad_norm': 1.7265625, 'learning_rate': 9.761904761904762e-05, 'num_tokens': 298692.0, 'mean_token_accuracy': 0.7408218532800674, 'epoch': 0.15013404825737264}
Step 43: {'loss': 0.9382, 'grad_norm': 1.6796875, 'learning_rate': 0.0001, 'num_tokens': 305962.0, 'mean_token_accuracy': 0.7602201998233795, 'epoch': 0.15370866845397677}
Step 44: {'loss': 0.965, 'grad_norm': 1.6015625, 'learning_rate': 0.00010238095238095237, 'num_tokens': 312961.0, 'mean_token_accuracy': 0.7460772097110748, 'epoch': 0.1572832886505809}
Step 45: {'loss': 0.9657, 'grad_norm': 1.640625, 'learning_rate': 0.00010476190476190477, 'num_tokens': 320069.0, 'mean_token_accuracy': 0.7504134178161621, 'epoch': 0.16085790884718498}
Step 46: {'loss': 0.9276, 'grad_norm': 1.8359375, 'learning_rate': 0.00010714285714285715, 'num_tokens': 327715.0, 'mean_token_accuracy': 0.7656335681676865, 'epoch': 0.1644325290437891}
Step 47: {'loss': 0.9415, 'grad_norm': 1.8359375, 'learning_rate': 0.00010952380952380953, 'num_tokens': 334277.0, 'mean_token_accuracy': 0.7691873162984848, 'epoch': 0.1680071492403932}
Step 48: {'loss': 0.9141, 'grad_norm': 1.7890625, 'learning_rate': 0.00011190476190476191, 'num_tokens': 340717.0, 'mean_token_accuracy': 0.769267350435257, 'epoch': 0.17158176943699732}
Step 49: {'loss': 0.9242, 'grad_norm': 1.75, 'learning_rate': 0.00011428571428571428, 'num_tokens': 347823.0, 'mean_token_accuracy': 0.7737926095724106, 'epoch': 0.17515638963360142}
Step 50: {'loss': 0.903, 'grad_norm': 1.609375, 'learning_rate': 0.00011666666666666668, 'num_tokens': 355425.0, 'mean_token_accuracy': 0.7800490409135818, 'epoch': 0.17873100983020554}
Step 51: {'loss': 0.8556, 'grad_norm': 2.140625, 'learning_rate': 0.00011904761904761905, 'num_tokens': 362380.0, 'mean_token_accuracy': 0.7853910475969315, 'epoch': 0.18230563002680966}
Step 52: {'loss': 0.8297, 'grad_norm': 1.671875, 'learning_rate': 0.00012142857142857143, 'num_tokens': 369512.0, 'mean_token_accuracy': 0.8059071600437164, 'epoch': 0.18588025022341376}
Step 53: {'loss': 0.8384, 'grad_norm': 1.3359375, 'learning_rate': 0.0001238095238095238, 'num_tokens': 377204.0, 'mean_token_accuracy': 0.790330246090889, 'epoch': 0.18945487042001788}
Step 54: {'loss': 0.8645, 'grad_norm': 1.4609375, 'learning_rate': 0.0001261904761904762, 'num_tokens': 384066.0, 'mean_token_accuracy': 0.7868016958236694, 'epoch': 0.19302949061662197}
Step 55: {'loss': 0.8343, 'grad_norm': 1.3671875, 'learning_rate': 0.00012857142857142858, 'num_tokens': 390549.0, 'mean_token_accuracy': 0.7933798730373383, 'epoch': 0.1966041108132261}
Step 56: {'loss': 0.7996, 'grad_norm': 1.375, 'learning_rate': 0.00013095238095238096, 'num_tokens': 397925.0, 'mean_token_accuracy': 0.8007360845804214, 'epoch': 0.20017873100983022}
Step 57: {'loss': 0.7945, 'grad_norm': 1.3984375, 'learning_rate': 0.00013333333333333334, 'num_tokens': 406112.0, 'mean_token_accuracy': 0.8070129752159119, 'epoch': 0.2037533512064343}
Step 58: {'loss': 0.8097, 'grad_norm': 0.94921875, 'learning_rate': 0.00013571428571428572, 'num_tokens': 413541.0, 'mean_token_accuracy': 0.8034320324659348, 'epoch': 0.20732797140303844}
Step 59: {'loss': 0.7976, 'grad_norm': 0.99609375, 'learning_rate': 0.0001380952380952381, 'num_tokens': 420547.0, 'mean_token_accuracy': 0.8059718161821365, 'epoch': 0.21090259159964253}
Step 60: {'loss': 0.83, 'grad_norm': 1.09375, 'learning_rate': 0.00014047619047619049, 'num_tokens': 427210.0, 'mean_token_accuracy': 0.7974262237548828, 'epoch': 0.21447721179624665}
Step 61: {'loss': 0.8177, 'grad_norm': 0.89453125, 'learning_rate': 0.00014285714285714287, 'num_tokens': 433957.0, 'mean_token_accuracy': 0.805064857006073, 'epoch': 0.21805183199285075}
Step 62: {'loss': 0.7863, 'grad_norm': 0.90625, 'learning_rate': 0.00014523809523809525, 'num_tokens': 440824.0, 'mean_token_accuracy': 0.8125141263008118, 'epoch': 0.22162645218945487}
Step 63: {'loss': 0.8001, 'grad_norm': 0.859375, 'learning_rate': 0.00014761904761904763, 'num_tokens': 447665.0, 'mean_token_accuracy': 0.7970158457756042, 'epoch': 0.225201072386059}
Step 64: {'loss': 0.7721, 'grad_norm': 0.859375, 'learning_rate': 0.00015000000000000001, 'num_tokens': 454379.0, 'mean_token_accuracy': 0.8018979281187057, 'epoch': 0.2287756925826631}
Step 65: {'loss': 0.8126, 'grad_norm': 0.8671875, 'learning_rate': 0.00015238095238095237, 'num_tokens': 461494.0, 'mean_token_accuracy': 0.7944127172231674, 'epoch': 0.2323503127792672}
Step 66: {'loss': 0.7882, 'grad_norm': 0.91796875, 'learning_rate': 0.00015476190476190478, 'num_tokens': 468302.0, 'mean_token_accuracy': 0.8112179487943649, 'epoch': 0.2359249329758713}
Step 67: {'loss': 0.7577, 'grad_norm': 0.734375, 'learning_rate': 0.00015714285714285716, 'num_tokens': 475372.0, 'mean_token_accuracy': 0.8069047182798386, 'epoch': 0.23949955317247543}
Step 68: {'loss': 0.7241, 'grad_norm': 0.8828125, 'learning_rate': 0.00015952380952380954, 'num_tokens': 482849.0, 'mean_token_accuracy': 0.8170120567083359, 'epoch': 0.24307417336907952}
Step 69: {'loss': 0.7246, 'grad_norm': 0.76953125, 'learning_rate': 0.00016190476190476192, 'num_tokens': 490544.0, 'mean_token_accuracy': 0.8110760748386383, 'epoch': 0.24664879356568364}
Step 70: {'loss': 0.756, 'grad_norm': 0.98046875, 'learning_rate': 0.00016428571428571428, 'num_tokens': 496902.0, 'mean_token_accuracy': 0.8064708560705185, 'epoch': 0.25022341376228774}
Step 71: {'loss': 0.7584, 'grad_norm': 0.8125, 'learning_rate': 0.0001666666666666667, 'num_tokens': 503232.0, 'mean_token_accuracy': 0.8003816306591034, 'epoch': 0.2537980339588919}
Step 72: {'loss': 0.7615, 'grad_norm': 0.71484375, 'learning_rate': 0.00016904761904761904, 'num_tokens': 510214.0, 'mean_token_accuracy': 0.800660714507103, 'epoch': 0.257372654155496}
Step 73: {'loss': 0.6785, 'grad_norm': 0.73828125, 'learning_rate': 0.00017142857142857143, 'num_tokens': 518144.0, 'mean_token_accuracy': 0.8224593847990036, 'epoch': 0.2609472743521001}
Step 74: {'loss': 0.7427, 'grad_norm': 0.97265625, 'learning_rate': 0.00017380952380952383, 'num_tokens': 524981.0, 'mean_token_accuracy': 0.8100967556238174, 'epoch': 0.2645218945487042}
Step 75: {'loss': 0.7116, 'grad_norm': 0.9140625, 'learning_rate': 0.0001761904761904762, 'num_tokens': 531858.0, 'mean_token_accuracy': 0.8175567835569382, 'epoch': 0.2680965147453083}
Step 76: {'loss': 0.6832, 'grad_norm': 0.73828125, 'learning_rate': 0.0001785714285714286, 'num_tokens': 538951.0, 'mean_token_accuracy': 0.8219204246997833, 'epoch': 0.2716711349419124}
Step 77: {'loss': 0.7914, 'grad_norm': 0.8671875, 'learning_rate': 0.00018095238095238095, 'num_tokens': 546345.0, 'mean_token_accuracy': 0.7922574728727341, 'epoch': 0.2752457551385165}
Step 78: {'loss': 0.6935, 'grad_norm': 0.94921875, 'learning_rate': 0.00018333333333333334, 'num_tokens': 552696.0, 'mean_token_accuracy': 0.8173371255397797, 'epoch': 0.27882037533512066}
Step 79: {'loss': 0.713, 'grad_norm': 0.86328125, 'learning_rate': 0.00018571428571428572, 'num_tokens': 559440.0, 'mean_token_accuracy': 0.8124784678220749, 'epoch': 0.28239499553172476}
Step 80: {'loss': 0.6913, 'grad_norm': 0.73828125, 'learning_rate': 0.0001880952380952381, 'num_tokens': 565800.0, 'mean_token_accuracy': 0.8216408938169479, 'epoch': 0.28596961572832885}
Step 81: {'loss': 0.7054, 'grad_norm': 0.8203125, 'learning_rate': 0.00019047619047619048, 'num_tokens': 572904.0, 'mean_token_accuracy': 0.8085333555936813, 'epoch': 0.289544235924933}
Step 82: {'loss': 0.7053, 'grad_norm': 0.86328125, 'learning_rate': 0.00019285714285714286, 'num_tokens': 579748.0, 'mean_token_accuracy': 0.8165672719478607, 'epoch': 0.2931188561215371}
Step 83: {'loss': 0.6734, 'grad_norm': 0.79296875, 'learning_rate': 0.00019523809523809525, 'num_tokens': 587133.0, 'mean_token_accuracy': 0.8226523697376251, 'epoch': 0.2966934763181412}
Step 84: {'loss': 0.6854, 'grad_norm': 0.703125, 'learning_rate': 0.00019761904761904763, 'num_tokens': 593617.0, 'mean_token_accuracy': 0.8127840757369995, 'epoch': 0.3002680965147453}
Step 85: {'loss': 0.6893, 'grad_norm': 0.8359375, 'learning_rate': 0.0002, 'num_tokens': 600153.0, 'mean_token_accuracy': 0.8195479214191437, 'epoch': 0.30384271671134944}
Step 86: {'loss': 0.6559, 'grad_norm': 0.7109375, 'learning_rate': 0.0001999991365731819, 'num_tokens': 606445.0, 'mean_token_accuracy': 0.8235595524311066, 'epoch': 0.30741733690795353}
Step 87: {'loss': 0.6834, 'grad_norm': 0.578125, 'learning_rate': 0.0001999965463076377, 'num_tokens': 614091.0, 'mean_token_accuracy': 0.8203181028366089, 'epoch': 0.3109919571045576}
Step 88: {'loss': 0.6673, 'grad_norm': 0.625, 'learning_rate': 0.0001999922292480975, 'num_tokens': 621001.0, 'mean_token_accuracy': 0.8209480792284012, 'epoch': 0.3145665773011618}
Step 89: {'loss': 0.7187, 'grad_norm': 0.76171875, 'learning_rate': 0.00019998618546911056, 'num_tokens': 627345.0, 'mean_token_accuracy': 0.8070105910301208, 'epoch': 0.31814119749776587}
Step 90: {'loss': 0.6957, 'grad_norm': 0.67578125, 'learning_rate': 0.0001999784150750442, 'num_tokens': 634320.0, 'mean_token_accuracy': 0.8166906833648682, 'epoch': 0.32171581769436997}
Step 91: {'loss': 0.6556, 'grad_norm': 0.61328125, 'learning_rate': 0.00019996891820008164, 'num_tokens': 641831.0, 'mean_token_accuracy': 0.8248830586671829, 'epoch': 0.32529043789097406}
Step 92: {'loss': 0.6545, 'grad_norm': 0.55078125, 'learning_rate': 0.0001999576950082201, 'num_tokens': 649417.0, 'mean_token_accuracy': 0.8231570720672607, 'epoch': 0.3288650580875782}
Step 93: {'loss': 0.7129, 'grad_norm': 0.6640625, 'learning_rate': 0.00019994474569326757, 'num_tokens': 656143.0, 'mean_token_accuracy': 0.8109591007232666, 'epoch': 0.3324396782841823}
Step 94: {'loss': 0.6506, 'grad_norm': 0.59375, 'learning_rate': 0.00019993007047883988, 'num_tokens': 663415.0, 'mean_token_accuracy': 0.8303549140691757, 'epoch': 0.3360142984807864}
Step 95: {'loss': 0.668, 'grad_norm': 0.59765625, 'learning_rate': 0.00019991366961835642, 'num_tokens': 670629.0, 'mean_token_accuracy': 0.8166810721158981, 'epoch': 0.33958891867739055}
Step 96: {'loss': 0.6497, 'grad_norm': 0.60546875, 'learning_rate': 0.00019989554339503612, 'num_tokens': 677789.0, 'mean_token_accuracy': 0.8272779434919357, 'epoch': 0.34316353887399464}
Step 97: {'loss': 0.6876, 'grad_norm': 0.64453125, 'learning_rate': 0.00019987569212189224, 'num_tokens': 686189.0, 'mean_token_accuracy': 0.8120244890451431, 'epoch': 0.34673815907059874}
Step 98: {'loss': 0.6314, 'grad_norm': 0.6171875, 'learning_rate': 0.0001998541161417273, 'num_tokens': 693105.0, 'mean_token_accuracy': 0.8271028399467468, 'epoch': 0.35031277926720283}
Step 99: {'loss': 0.6748, 'grad_norm': 0.6328125, 'learning_rate': 0.00019983081582712685, 'num_tokens': 700450.0, 'mean_token_accuracy': 0.8191751092672348, 'epoch': 0.353887399463807}
Step 100: {'loss': 0.6342, 'grad_norm': 0.65625, 'learning_rate': 0.0001998057915804532, 'num_tokens': 707327.0, 'mean_token_accuracy': 0.8297518044710159, 'epoch': 0.3574620196604111}
Step 101: {'loss': 0.6866, 'grad_norm': 0.7109375, 'learning_rate': 0.0001997790438338385, 'num_tokens': 713719.0, 'mean_token_accuracy': 0.8162469863891602, 'epoch': 0.3610366398570152}
Step 102: {'loss': 0.6448, 'grad_norm': 0.56640625, 'learning_rate': 0.00019975057304917718, 'num_tokens': 720520.0, 'mean_token_accuracy': 0.824910506606102, 'epoch': 0.3646112600536193}
Step 103: {'loss': 0.722, 'grad_norm': 0.69921875, 'learning_rate': 0.00019972037971811802, 'num_tokens': 727397.0, 'mean_token_accuracy': 0.8106105327606201, 'epoch': 0.3681858802502234}
Step 104: {'loss': 0.6617, 'grad_norm': 0.6796875, 'learning_rate': 0.00019968846436205567, 'num_tokens': 734151.0, 'mean_token_accuracy': 0.8292549252510071, 'epoch': 0.3717605004468275}
Step 105: {'loss': 0.7078, 'grad_norm': 0.63671875, 'learning_rate': 0.00019965482753212156, 'num_tokens': 741059.0, 'mean_token_accuracy': 0.816577360033989, 'epoch': 0.3753351206434316}
Step 106: {'loss': 0.6591, 'grad_norm': 0.65625, 'learning_rate': 0.00019961946980917456, 'num_tokens': 748517.0, 'mean_token_accuracy': 0.8249074220657349, 'epoch': 0.37890974084003576}
Step 107: {'loss': 0.6844, 'grad_norm': 0.515625, 'learning_rate': 0.0001995823918037908, 'num_tokens': 756130.0, 'mean_token_accuracy': 0.8186321705579758, 'epoch': 0.38248436103663985}
Step 108: {'loss': 0.6606, 'grad_norm': 0.609375, 'learning_rate': 0.0001995435941562531, 'num_tokens': 762718.0, 'mean_token_accuracy': 0.8291533291339874, 'epoch': 0.38605898123324395}
Step 109: {'loss': 0.6888, 'grad_norm': 0.87109375, 'learning_rate': 0.00019950307753654017, 'num_tokens': 769550.0, 'mean_token_accuracy': 0.8165904134511948, 'epoch': 0.3896336014298481}
Step 110: {'loss': 0.6986, 'grad_norm': 0.515625, 'learning_rate': 0.00019946084264431459, 'num_tokens': 777372.0, 'mean_token_accuracy': 0.810771182179451, 'epoch': 0.3932082216264522}
Step 111: {'loss': 0.6522, 'grad_norm': 0.59375, 'learning_rate': 0.0001994168902089112, 'num_tokens': 784517.0, 'mean_token_accuracy': 0.8279743045568466, 'epoch': 0.3967828418230563}
Step 112: {'loss': 0.645, 'grad_norm': 0.66796875, 'learning_rate': 0.00019937122098932428, 'num_tokens': 791282.0, 'mean_token_accuracy': 0.8262900859117508, 'epoch': 0.40035746201966044}
Step 113: {'loss': 0.6555, 'grad_norm': 0.609375, 'learning_rate': 0.00019932383577419432, 'num_tokens': 798706.0, 'mean_token_accuracy': 0.8246958255767822, 'epoch': 0.40393208221626453}
Step 114: {'loss': 0.7118, 'grad_norm': 0.68359375, 'learning_rate': 0.00019927473538179467, 'num_tokens': 805743.0, 'mean_token_accuracy': 0.8128745555877686, 'epoch': 0.4075067024128686}
Step 115: {'loss': 0.6484, 'grad_norm': 0.53125, 'learning_rate': 0.00019922392066001722, 'num_tokens': 813174.0, 'mean_token_accuracy': 0.8263526409864426, 'epoch': 0.4110813226094727}
Step 116: {'loss': 0.6685, 'grad_norm': 0.53515625, 'learning_rate': 0.00019917139248635786, 'num_tokens': 820564.0, 'mean_token_accuracy': 0.8247490972280502, 'epoch': 0.41465594280607687}
Step 117: {'loss': 0.6346, 'grad_norm': 0.765625, 'learning_rate': 0.0001991171517679013, 'num_tokens': 827058.0, 'mean_token_accuracy': 0.8286167979240417, 'epoch': 0.41823056300268097}
Step 118: {'loss': 0.6351, 'grad_norm': 0.60546875, 'learning_rate': 0.0001990611994413053, 'num_tokens': 833700.0, 'mean_token_accuracy': 0.8313601762056351, 'epoch': 0.42180518319928506}
Step 119: {'loss': 0.68, 'grad_norm': 0.65625, 'learning_rate': 0.00019900353647278466, 'num_tokens': 840606.0, 'mean_token_accuracy': 0.8175671398639679, 'epoch': 0.4253798033958892}
Step 120: {'loss': 0.6831, 'grad_norm': 0.8203125, 'learning_rate': 0.00019894416385809444, 'num_tokens': 847708.0, 'mean_token_accuracy': 0.8179950267076492, 'epoch': 0.4289544235924933}
Step 121: {'loss': 0.6761, 'grad_norm': 0.6796875, 'learning_rate': 0.00019888308262251285, 'num_tokens': 855032.0, 'mean_token_accuracy': 0.8223768025636673, 'epoch': 0.4325290437890974}
Step 122: {'loss': 0.6729, 'grad_norm': 0.6015625, 'learning_rate': 0.0001988202938208234, 'num_tokens': 861902.0, 'mean_token_accuracy': 0.8198434263467789, 'epoch': 0.4361036639857015}
Step 123: {'loss': 0.6967, 'grad_norm': 0.6796875, 'learning_rate': 0.00019875579853729676, 'num_tokens': 868591.0, 'mean_token_accuracy': 0.8160652816295624, 'epoch': 0.43967828418230565}
Step 124: {'loss': 0.6554, 'grad_norm': 0.70703125, 'learning_rate': 0.00019868959788567212, 'num_tokens': 875872.0, 'mean_token_accuracy': 0.8297679722309113, 'epoch': 0.44325290437890974}
Step 125: {'loss': 0.6539, 'grad_norm': 0.63671875, 'learning_rate': 0.00019862169300913785, 'num_tokens': 882918.0, 'mean_token_accuracy': 0.8288920521736145, 'epoch': 0.44682752457551383}
Step 126: {'loss': 0.6398, 'grad_norm': 0.59375, 'learning_rate': 0.0001985520850803117, 'num_tokens': 889987.0, 'mean_token_accuracy': 0.8312947601079941, 'epoch': 0.450402144772118}
Step 127: {'loss': 0.7311, 'grad_norm': 0.8046875, 'learning_rate': 0.00019848077530122083, 'num_tokens': 896503.0, 'mean_token_accuracy': 0.8132105022668839, 'epoch': 0.4539767649687221}
Step 128: {'loss': 0.6661, 'grad_norm': 0.70703125, 'learning_rate': 0.00019840776490328066, 'num_tokens': 903382.0, 'mean_token_accuracy': 0.824618935585022, 'epoch': 0.4575513851653262}
Step 129: {'loss': 0.652, 'grad_norm': 0.62109375, 'learning_rate': 0.00019833305514727395, 'num_tokens': 910142.0, 'mean_token_accuracy': 0.8308051824569702, 'epoch': 0.46112600536193027}
Step 130: {'loss': 0.6393, 'grad_norm': 0.68359375, 'learning_rate': 0.00019825664732332884, 'num_tokens': 916463.0, 'mean_token_accuracy': 0.8271943777799606, 'epoch': 0.4647006255585344}
Step 131: {'loss': 0.6896, 'grad_norm': 0.671875, 'learning_rate': 0.0001981785427508966, 'num_tokens': 923361.0, 'mean_token_accuracy': 0.8135822862386703, 'epoch': 0.4682752457551385}
Step 132: {'loss': 0.6964, 'grad_norm': 0.66015625, 'learning_rate': 0.00019809874277872886, 'num_tokens': 930173.0, 'mean_token_accuracy': 0.8161501288414001, 'epoch': 0.4718498659517426}
Step 133: {'loss': 0.6389, 'grad_norm': 0.6171875, 'learning_rate': 0.00019801724878485438, 'num_tokens': 937229.0, 'mean_token_accuracy': 0.8248115479946136, 'epoch': 0.47542448614834676}
Step 134: {'loss': 0.6701, 'grad_norm': 0.71875, 'learning_rate': 0.00019793406217655517, 'num_tokens': 944276.0, 'mean_token_accuracy': 0.8247141093015671, 'epoch': 0.47899910634495085}
Step 135: {'loss': 0.7026, 'grad_norm': 0.73828125, 'learning_rate': 0.00019784918439034216, 'num_tokens': 952105.0, 'mean_token_accuracy': 0.8157166242599487, 'epoch': 0.48257372654155495}
Step 136: {'loss': 0.637, 'grad_norm': 0.54296875, 'learning_rate': 0.00019776261689193048, 'num_tokens': 959133.0, 'mean_token_accuracy': 0.8281835168600082, 'epoch': 0.48614834673815904}
Step 137: {'loss': 0.6594, 'grad_norm': 0.703125, 'learning_rate': 0.00019767436117621413, 'num_tokens': 965871.0, 'mean_token_accuracy': 0.825066864490509, 'epoch': 0.4897229669347632}
Step 138: {'loss': 0.6741, 'grad_norm': 0.6484375, 'learning_rate': 0.00019758441876724017, 'num_tokens': 972706.0, 'mean_token_accuracy': 0.8242108076810837, 'epoch': 0.4932975871313673}
Step 139: {'loss': 0.6722, 'grad_norm': 0.6328125, 'learning_rate': 0.00019749279121818235, 'num_tokens': 979544.0, 'mean_token_accuracy': 0.8205322176218033, 'epoch': 0.4968722073279714}
Step 140: {'loss': 0.6555, 'grad_norm': 0.76171875, 'learning_rate': 0.00019739948011131438, 'num_tokens': 986332.0, 'mean_token_accuracy': 0.8321613967418671, 'epoch': 0.5004468275245755}
Step 141: {'loss': 0.6621, 'grad_norm': 0.6875, 'learning_rate': 0.00019730448705798239, 'num_tokens': 992715.0, 'mean_token_accuracy': 0.8339578807353973, 'epoch': 0.5040214477211796}
Step 142: {'loss': 0.6417, 'grad_norm': 0.6015625, 'learning_rate': 0.00019720781369857746, 'num_tokens': 999937.0, 'mean_token_accuracy': 0.8310750126838684, 'epoch': 0.5075960679177838}
Step 143: {'loss': 0.6074, 'grad_norm': 0.6953125, 'learning_rate': 0.000197109461702507, 'num_tokens': 1007578.0, 'mean_token_accuracy': 0.8336659222841263, 'epoch': 0.5111706881143878}
Step 144: {'loss': 0.6462, 'grad_norm': 0.54296875, 'learning_rate': 0.00019700943276816603, 'num_tokens': 1014991.0, 'mean_token_accuracy': 0.8247860968112946, 'epoch': 0.514745308310992}
Step 145: {'loss': 0.6586, 'grad_norm': 0.66015625, 'learning_rate': 0.0001969077286229078, 'num_tokens': 1021699.0, 'mean_token_accuracy': 0.8209440112113953, 'epoch': 0.5183199285075961}
Step 146: {'loss': 0.7, 'grad_norm': 0.68359375, 'learning_rate': 0.00019680435102301412, 'num_tokens': 1028584.0, 'mean_token_accuracy': 0.8185215145349503, 'epoch': 0.5218945487042002}
Step 147: {'loss': 0.6239, 'grad_norm': 0.66796875, 'learning_rate': 0.00019669930175366472, 'num_tokens': 1035374.0, 'mean_token_accuracy': 0.8335137814283371, 'epoch': 0.5254691689008043}
Step 148: {'loss': 0.6673, 'grad_norm': 0.83203125, 'learning_rate': 0.00019659258262890683, 'num_tokens': 1042837.0, 'mean_token_accuracy': 0.821174144744873, 'epoch': 0.5290437890974083}
Step 149: {'loss': 0.6653, 'grad_norm': 0.6796875, 'learning_rate': 0.00019648419549162348, 'num_tokens': 1050223.0, 'mean_token_accuracy': 0.8191363215446472, 'epoch': 0.5326184092940125}
Step 150: {'loss': 0.6352, 'grad_norm': 0.640625, 'learning_rate': 0.00019637414221350196, 'num_tokens': 1057220.0, 'mean_token_accuracy': 0.8251620680093765, 'epoch': 0.5361930294906166}
Step 151: {'loss': 0.6467, 'grad_norm': 0.61328125, 'learning_rate': 0.0001962624246950012, 'num_tokens': 1063913.0, 'mean_token_accuracy': 0.8276655226945877, 'epoch': 0.5397676496872207}
Step 152: {'loss': 0.6475, 'grad_norm': 0.68359375, 'learning_rate': 0.00019614904486531934, 'num_tokens': 1071184.0, 'mean_token_accuracy': 0.8295164406299591, 'epoch': 0.5433422698838248}
Step 153: {'loss': 0.6412, 'grad_norm': 0.65625, 'learning_rate': 0.00019603400468235998, 'num_tokens': 1078012.0, 'mean_token_accuracy': 0.8280434757471085, 'epoch': 0.546916890080429}
Step 154: {'loss': 0.6058, 'grad_norm': 0.66015625, 'learning_rate': 0.0001959173061326988, 'num_tokens': 1085363.0, 'mean_token_accuracy': 0.8385254442691803, 'epoch': 0.550491510277033}
Step 155: {'loss': 0.6841, 'grad_norm': 0.62890625, 'learning_rate': 0.0001957989512315489, 'num_tokens': 1092329.0, 'mean_token_accuracy': 0.8218822032213211, 'epoch': 0.5540661304736372}
Step 156: {'loss': 0.6302, 'grad_norm': 0.67578125, 'learning_rate': 0.0001956789420227262, 'num_tokens': 1099141.0, 'mean_token_accuracy': 0.83278788626194, 'epoch': 0.5576407506702413}
Step 157: {'loss': 0.6394, 'grad_norm': 0.6328125, 'learning_rate': 0.0001955572805786141, 'num_tokens': 1106104.0, 'mean_token_accuracy': 0.8284991532564163, 'epoch': 0.5612153708668454}
Step 158: {'loss': 0.684, 'grad_norm': 0.984375, 'learning_rate': 0.00019543396900012763, 'num_tokens': 1112633.0, 'mean_token_accuracy': 0.8166691809892654, 'epoch': 0.5647899910634495}
Step 159: {'loss': 0.6069, 'grad_norm': 0.59765625, 'learning_rate': 0.0001953090094166773, 'num_tokens': 1120037.0, 'mean_token_accuracy': 0.8357307314872742, 'epoch': 0.5683646112600537}
Step 160: {'loss': 0.6812, 'grad_norm': 0.59765625, 'learning_rate': 0.00019518240398613227, 'num_tokens': 1128153.0, 'mean_token_accuracy': 0.8092121928930283, 'epoch': 0.5719392314566577}
Step 161: {'loss': 0.64, 'grad_norm': 0.6171875, 'learning_rate': 0.0001950541548947829, 'num_tokens': 1135132.0, 'mean_token_accuracy': 0.8312367498874664, 'epoch': 0.5755138516532619}
Step 162: {'loss': 0.7046, 'grad_norm': 0.76953125, 'learning_rate': 0.0001949242643573034, 'num_tokens': 1141706.0, 'mean_token_accuracy': 0.8139436691999435, 'epoch': 0.579088471849866}
Step 163: {'loss': 0.6771, 'grad_norm': 0.5546875, 'learning_rate': 0.0001947927346167132, 'num_tokens': 1149137.0, 'mean_token_accuracy': 0.822582796216011, 'epoch': 0.58266309204647}
Step 164: {'loss': 0.6456, 'grad_norm': 0.70703125, 'learning_rate': 0.00019465956794433836, 'num_tokens': 1156176.0, 'mean_token_accuracy': 0.8282889425754547, 'epoch': 0.5862377122430742}
Step 165: {'loss': 0.6176, 'grad_norm': 0.62109375, 'learning_rate': 0.00019452476663977248, 'num_tokens': 1163058.0, 'mean_token_accuracy': 0.8367817997932434, 'epoch': 0.5898123324396782}
Step 166: {'loss': 0.6019, 'grad_norm': 0.74609375, 'learning_rate': 0.00019438833303083678, 'num_tokens': 1170061.0, 'mean_token_accuracy': 0.8420358151197433, 'epoch': 0.5933869526362824}
Step 167: {'loss': 0.6449, 'grad_norm': 0.703125, 'learning_rate': 0.00019425026947353992, 'num_tokens': 1177558.0, 'mean_token_accuracy': 0.8276980966329575, 'epoch': 0.5969615728328865}
Step 168: {'loss': 0.6426, 'grad_norm': 0.5625, 'learning_rate': 0.00019411057835203756, 'num_tokens': 1185133.0, 'mean_token_accuracy': 0.8277457356452942, 'epoch': 0.6005361930294906}
Step 169: {'loss': 0.6906, 'grad_norm': 0.87890625, 'learning_rate': 0.00019396926207859084, 'num_tokens': 1192082.0, 'mean_token_accuracy': 0.8181239515542984, 'epoch': 0.6041108132260947}
Step 170: {'loss': 0.6625, 'grad_norm': 0.62109375, 'learning_rate': 0.00019382632309352502, 'num_tokens': 1199529.0, 'mean_token_accuracy': 0.8231994807720184, 'epoch': 0.6076854334226989}
Step 171: {'loss': 0.6732, 'grad_norm': 0.76171875, 'learning_rate': 0.0001936817638651871, 'num_tokens': 1206034.0, 'mean_token_accuracy': 0.8242385685443878, 'epoch': 0.6112600536193029}
Step 172: {'loss': 0.6534, 'grad_norm': 0.82421875, 'learning_rate': 0.0001935355868899034, 'num_tokens': 1212184.0, 'mean_token_accuracy': 0.821699321269989, 'epoch': 0.6148346738159071}
Step 173: {'loss': 0.6271, 'grad_norm': 0.57421875, 'learning_rate': 0.00019338779469193639, 'num_tokens': 1219381.0, 'mean_token_accuracy': 0.8343948870897293, 'epoch': 0.6184092940125112}
Step 174: {'loss': 0.6759, 'grad_norm': 0.76171875, 'learning_rate': 0.00019323838982344092, 'num_tokens': 1225463.0, 'mean_token_accuracy': 0.8191428780555725, 'epoch': 0.6219839142091153}
Step 175: {'loss': 0.6378, 'grad_norm': 0.84765625, 'learning_rate': 0.00019308737486442045, 'num_tokens': 1232279.0, 'mean_token_accuracy': 0.8388029038906097, 'epoch': 0.6255585344057194}
Step 176: {'loss': 0.6461, 'grad_norm': 0.765625, 'learning_rate': 0.00019293475242268223, 'num_tokens': 1238705.0, 'mean_token_accuracy': 0.8268957585096359, 'epoch': 0.6291331546023236}
Step 177: {'loss': 0.6454, 'grad_norm': 0.69140625, 'learning_rate': 0.00019278052513379255, 'num_tokens': 1245519.0, 'mean_token_accuracy': 0.8285118192434311, 'epoch': 0.6327077747989276}
Step 178: {'loss': 0.6528, 'grad_norm': 0.75390625, 'learning_rate': 0.0001926246956610309, 'num_tokens': 1252270.0, 'mean_token_accuracy': 0.824802964925766, 'epoch': 0.6362823949955317}
Step 179: {'loss': 0.6506, 'grad_norm': 0.7265625, 'learning_rate': 0.00019246726669534415, 'num_tokens': 1258782.0, 'mean_token_accuracy': 0.8237873166799545, 'epoch': 0.6398570151921358}
Step 180: {'loss': 0.6382, 'grad_norm': 0.6015625, 'learning_rate': 0.0001923082409553002, 'num_tokens': 1265623.0, 'mean_token_accuracy': 0.8296671062707901, 'epoch': 0.6434316353887399}
Step 181: {'loss': 0.6096, 'grad_norm': 0.7890625, 'learning_rate': 0.00019214762118704076, 'num_tokens': 1272678.0, 'mean_token_accuracy': 0.8334728479385376, 'epoch': 0.6470062555853441}
Step 182: {'loss': 0.6236, 'grad_norm': 0.75, 'learning_rate': 0.0001919854101642342, 'num_tokens': 1279763.0, 'mean_token_accuracy': 0.8319319635629654, 'epoch': 0.6505808757819481}
Step 183: {'loss': 0.6484, 'grad_norm': 0.6796875, 'learning_rate': 0.00019182161068802741, 'num_tokens': 1287266.0, 'mean_token_accuracy': 0.8261329829692841, 'epoch': 0.6541554959785523}
Step 184: {'loss': 0.6013, 'grad_norm': 0.7890625, 'learning_rate': 0.00019165622558699763, 'num_tokens': 1294102.0, 'mean_token_accuracy': 0.8353032618761063, 'epoch': 0.6577301161751564}
Step 185: {'loss': 0.6461, 'grad_norm': 0.6015625, 'learning_rate': 0.00019148925771710347, 'num_tokens': 1301127.0, 'mean_token_accuracy': 0.8267246186733246, 'epoch': 0.6613047363717605}
Step 186: {'loss': 0.7181, 'grad_norm': 0.64453125, 'learning_rate': 0.00019132070996163568, 'num_tokens': 1308133.0, 'mean_token_accuracy': 0.8129422068595886, 'epoch': 0.6648793565683646}
Step 187: {'loss': 0.6305, 'grad_norm': 0.66796875, 'learning_rate': 0.00019115058523116733, 'num_tokens': 1315464.0, 'mean_token_accuracy': 0.8289641290903091, 'epoch': 0.6684539767649688}
Step 188: {'loss': 0.6721, 'grad_norm': 0.796875, 'learning_rate': 0.00019097888646350345, 'num_tokens': 1321716.0, 'mean_token_accuracy': 0.8236453831195831, 'epoch': 0.6720285969615728}
Step 189: {'loss': 0.6505, 'grad_norm': 0.64453125, 'learning_rate': 0.0001908056166236305, 'num_tokens': 1328343.0, 'mean_token_accuracy': 0.8268140256404877, 'epoch': 0.675603217158177}
Step 190: {'loss': 0.6393, 'grad_norm': 0.6953125, 'learning_rate': 0.000190630778703665, 'num_tokens': 1334881.0, 'mean_token_accuracy': 0.8306754231452942, 'epoch': 0.6791778373547811}
Step 191: {'loss': 0.7073, 'grad_norm': 0.765625, 'learning_rate': 0.00019045437572280194, 'num_tokens': 1342003.0, 'mean_token_accuracy': 0.8107288628816605, 'epoch': 0.6827524575513851}
Step 192: {'loss': 0.615, 'grad_norm': 0.64453125, 'learning_rate': 0.00019027641072726258, 'num_tokens': 1348819.0, 'mean_token_accuracy': 0.8374683707952499, 'epoch': 0.6863270777479893}
Step 193: {'loss': 0.6686, 'grad_norm': 0.62109375, 'learning_rate': 0.0001900968867902419, 'num_tokens': 1355392.0, 'mean_token_accuracy': 0.822965607047081, 'epoch': 0.6899016979445934}
Step 194: {'loss': 0.6291, 'grad_norm': 0.57421875, 'learning_rate': 0.00018991580701185562, 'num_tokens': 1362767.0, 'mean_token_accuracy': 0.8296021670103073, 'epoch': 0.6934763181411975}
Step 195: {'loss': 0.6826, 'grad_norm': 0.6953125, 'learning_rate': 0.00018973317451908642, 'num_tokens': 1369642.0, 'mean_token_accuracy': 0.8270293623209, 'epoch': 0.6970509383378016}
Step 196: {'loss': 0.7256, 'grad_norm': 0.80859375, 'learning_rate': 0.0001895489924657301, 'num_tokens': 1376719.0, 'mean_token_accuracy': 0.8117304593324661, 'epoch': 0.7006255585344057}
Step 197: {'loss': 0.6014, 'grad_norm': 0.66796875, 'learning_rate': 0.00018936326403234125, 'num_tokens': 1384016.0, 'mean_token_accuracy': 0.8327536433935165, 'epoch': 0.7042001787310098}
Step 198: {'loss': 0.6509, 'grad_norm': 0.53125, 'learning_rate': 0.00018917599242617797, 'num_tokens': 1390809.0, 'mean_token_accuracy': 0.8293238431215286, 'epoch': 0.707774798927614}
Step 199: {'loss': 0.6124, 'grad_norm': 0.60546875, 'learning_rate': 0.0001889871808811469, 'num_tokens': 1397951.0, 'mean_token_accuracy': 0.8295943588018417, 'epoch': 0.711349419124218}
Step 200: {'loss': 0.6568, 'grad_norm': 0.703125, 'learning_rate': 0.00018879683265774695, 'num_tokens': 1404537.0, 'mean_token_accuracy': 0.8220623284578323, 'epoch': 0.7149240393208222}
Step 201: {'loss': 0.6363, 'grad_norm': 0.66015625, 'learning_rate': 0.00018860495104301345, 'num_tokens': 1411676.0, 'mean_token_accuracy': 0.8301090747117996, 'epoch': 0.7184986595174263}
Step 202: {'loss': 0.6721, 'grad_norm': 0.578125, 'learning_rate': 0.00018841153935046098, 'num_tokens': 1418631.0, 'mean_token_accuracy': 0.8144266158342361, 'epoch': 0.7220732797140303}
Step 203: {'loss': 0.6656, 'grad_norm': 0.69921875, 'learning_rate': 0.00018821660092002641, 'num_tokens': 1425345.0, 'mean_token_accuracy': 0.8263262510299683, 'epoch': 0.7256478999106345}
Step 204: {'loss': 0.6363, 'grad_norm': 0.60546875, 'learning_rate': 0.00018802013911801112, 'num_tokens': 1432419.0, 'mean_token_accuracy': 0.8327935636043549, 'epoch': 0.7292225201072386}
Step 205: {'loss': 0.6245, 'grad_norm': 0.6171875, 'learning_rate': 0.00018782215733702286, 'num_tokens': 1439356.0, 'mean_token_accuracy': 0.825022503733635, 'epoch': 0.7327971403038427}
Step 206: {'loss': 0.6169, 'grad_norm': 0.5703125, 'learning_rate': 0.00018762265899591722, 'num_tokens': 1446689.0, 'mean_token_accuracy': 0.8285880088806152, 'epoch': 0.7363717605004468}
Step 207: {'loss': 0.664, 'grad_norm': 0.640625, 'learning_rate': 0.00018742164753973855, 'num_tokens': 1453143.0, 'mean_token_accuracy': 0.8207703232765198, 'epoch': 0.739946380697051}
Step 208: {'loss': 0.6218, 'grad_norm': 0.62890625, 'learning_rate': 0.00018721912643966055, 'num_tokens': 1459567.0, 'mean_token_accuracy': 0.8254743218421936, 'epoch': 0.743521000893655}
Step 209: {'loss': 0.6174, 'grad_norm': 0.5703125, 'learning_rate': 0.00018701509919292613, 'num_tokens': 1466203.0, 'mean_token_accuracy': 0.8363111615180969, 'epoch': 0.7470956210902592}
Step 210: {'loss': 0.6567, 'grad_norm': 0.55859375, 'learning_rate': 0.0001868095693227872, 'num_tokens': 1473302.0, 'mean_token_accuracy': 0.8258266150951385, 'epoch': 0.7506702412868632}
Step 211: {'loss': 0.6753, 'grad_norm': 0.69140625, 'learning_rate': 0.00018660254037844388, 'num_tokens': 1479535.0, 'mean_token_accuracy': 0.82199726998806, 'epoch': 0.7542448614834674}
Step 212: {'loss': 0.6519, 'grad_norm': 0.78125, 'learning_rate': 0.00018639401593498298, 'num_tokens': 1486692.0, 'mean_token_accuracy': 0.8282057195901871, 'epoch': 0.7578194816800715}
Step 213: {'loss': 0.6353, 'grad_norm': 0.5703125, 'learning_rate': 0.0001861839995933164, 'num_tokens': 1493651.0, 'mean_token_accuracy': 0.8273303657770157, 'epoch': 0.7613941018766756}
Step 214: {'loss': 0.7016, 'grad_norm': 0.69140625, 'learning_rate': 0.00018597249498011903, 'num_tokens': 1500079.0, 'mean_token_accuracy': 0.811843141913414, 'epoch': 0.7649687220732797}
Step 215: {'loss': 0.6576, 'grad_norm': 0.59375, 'learning_rate': 0.00018575950574776595, 'num_tokens': 1506614.0, 'mean_token_accuracy': 0.825633242726326, 'epoch': 0.7685433422698839}
Step 216: {'loss': 0.6572, 'grad_norm': 0.66796875, 'learning_rate': 0.00018554503557426948, 'num_tokens': 1514072.0, 'mean_token_accuracy': 0.8223528563976288, 'epoch': 0.7721179624664879}
Step 217: {'loss': 0.6685, 'grad_norm': 0.73828125, 'learning_rate': 0.00018532908816321558, 'num_tokens': 1520934.0, 'mean_token_accuracy': 0.8255711197853088, 'epoch': 0.775692582663092}
Step 218: {'loss': 0.5951, 'grad_norm': 0.66015625, 'learning_rate': 0.00018511166724369997, 'num_tokens': 1528622.0, 'mean_token_accuracy': 0.8422878831624985, 'epoch': 0.7792672028596962}
Step 219: {'loss': 0.702, 'grad_norm': 0.59375, 'learning_rate': 0.00018489277657026375, 'num_tokens': 1535640.0, 'mean_token_accuracy': 0.8142092078924179, 'epoch': 0.7828418230563002}
Step 220: {'loss': 0.6017, 'grad_norm': 0.5859375, 'learning_rate': 0.00018467241992282843, 'num_tokens': 1543449.0, 'mean_token_accuracy': 0.8315289914608002, 'epoch': 0.7864164432529044}
Step 221: {'loss': 0.5758, 'grad_norm': 0.6171875, 'learning_rate': 0.0001844506011066308, 'num_tokens': 1550946.0, 'mean_token_accuracy': 0.8373656719923019, 'epoch': 0.7899910634495085}
Step 222: {'loss': 0.6506, 'grad_norm': 0.70703125, 'learning_rate': 0.00018422732395215717, 'num_tokens': 1557813.0, 'mean_token_accuracy': 0.8275381922721863, 'epoch': 0.7935656836461126}
Step 223: {'loss': 0.6667, 'grad_norm': 0.75, 'learning_rate': 0.00018400259231507717, 'num_tokens': 1565225.0, 'mean_token_accuracy': 0.8138528317213058, 'epoch': 0.7971403038427167}
Step 224: {'loss': 0.6678, 'grad_norm': 0.67578125, 'learning_rate': 0.00018377641007617722, 'num_tokens': 1572344.0, 'mean_token_accuracy': 0.8200840055942535, 'epoch': 0.8007149240393209}
Step 225: {'loss': 0.6919, 'grad_norm': 0.640625, 'learning_rate': 0.00018354878114129367, 'num_tokens': 1579319.0, 'mean_token_accuracy': 0.816659539937973, 'epoch': 0.8042895442359249}
Step 226: {'loss': 0.6842, 'grad_norm': 0.6875, 'learning_rate': 0.0001833197094412449, 'num_tokens': 1586336.0, 'mean_token_accuracy': 0.8142296671867371, 'epoch': 0.8078641644325291}
Step 227: {'loss': 0.5938, 'grad_norm': 0.6640625, 'learning_rate': 0.00018308919893176396, 'num_tokens': 1593594.0, 'mean_token_accuracy': 0.8378836214542389, 'epoch': 0.8114387846291331}
Step 228: {'loss': 0.6656, 'grad_norm': 0.625, 'learning_rate': 0.00018285725359343, 'num_tokens': 1600326.0, 'mean_token_accuracy': 0.820324718952179, 'epoch': 0.8150134048257373}
Step 229: {'loss': 0.6225, 'grad_norm': 0.7265625, 'learning_rate': 0.0001826238774315995, 'num_tokens': 1606713.0, 'mean_token_accuracy': 0.8383585512638092, 'epoch': 0.8185880250223414}
Step 230: {'loss': 0.657, 'grad_norm': 0.578125, 'learning_rate': 0.00018238907447633716, 'num_tokens': 1613421.0, 'mean_token_accuracy': 0.8182238638401031, 'epoch': 0.8221626452189454}
Step 231: {'loss': 0.6529, 'grad_norm': 0.76171875, 'learning_rate': 0.00018215284878234642, 'num_tokens': 1620286.0, 'mean_token_accuracy': 0.8257066607475281, 'epoch': 0.8257372654155496}
Step 232: {'loss': 0.616, 'grad_norm': 0.6328125, 'learning_rate': 0.0001819152044288992, 'num_tokens': 1627379.0, 'mean_token_accuracy': 0.832334965467453, 'epoch': 0.8293118856121537}
Step 233: {'loss': 0.6646, 'grad_norm': 0.77734375, 'learning_rate': 0.00018167614551976567, 'num_tokens': 1634973.0, 'mean_token_accuracy': 0.8266243040561676, 'epoch': 0.8328865058087578}
Step 234: {'loss': 0.6518, 'grad_norm': 0.74609375, 'learning_rate': 0.00018143567618314333, 'num_tokens': 1641744.0, 'mean_token_accuracy': 0.8274715840816498, 'epoch': 0.8364611260053619}
Step 235: {'loss': 0.6772, 'grad_norm': 0.6328125, 'learning_rate': 0.00018119380057158568, 'num_tokens': 1648623.0, 'mean_token_accuracy': 0.8176508694887161, 'epoch': 0.8400357462019661}
Step 236: {'loss': 0.6495, 'grad_norm': 0.62890625, 'learning_rate': 0.0001809505228619304, 'num_tokens': 1655254.0, 'mean_token_accuracy': 0.8272945284843445, 'epoch': 0.8436103663985701}
Step 237: {'loss': 0.6181, 'grad_norm': 0.68359375, 'learning_rate': 0.00018070584725522762, 'num_tokens': 1662104.0, 'mean_token_accuracy': 0.8379857838153839, 'epoch': 0.8471849865951743}
Step 238: {'loss': 0.656, 'grad_norm': 0.82421875, 'learning_rate': 0.00018045977797666684, 'num_tokens': 1668726.0, 'mean_token_accuracy': 0.8214875161647797, 'epoch': 0.8507596067917784}
Step 239: {'loss': 0.6046, 'grad_norm': 0.578125, 'learning_rate': 0.0001802123192755044, 'num_tokens': 1675529.0, 'mean_token_accuracy': 0.8350933492183685, 'epoch': 0.8543342269883825}
Step 240: {'loss': 0.6345, 'grad_norm': 0.70703125, 'learning_rate': 0.00017996347542498985, 'num_tokens': 1682760.0, 'mean_token_accuracy': 0.8260590881109238, 'epoch': 0.8579088471849866}
Step 241: {'loss': 0.6927, 'grad_norm': 0.72265625, 'learning_rate': 0.00017971325072229226, 'num_tokens': 1689764.0, 'mean_token_accuracy': 0.8181971311569214, 'epoch': 0.8614834673815907}
Step 242: {'loss': 0.631, 'grad_norm': 0.63671875, 'learning_rate': 0.00017946164948842602, 'num_tokens': 1696835.0, 'mean_token_accuracy': 0.8323894888162613, 'epoch': 0.8650580875781948}
Step 243: {'loss': 0.6522, 'grad_norm': 0.86328125, 'learning_rate': 0.00017920867606817625, 'num_tokens': 1704046.0, 'mean_token_accuracy': 0.8289367854595184, 'epoch': 0.868632707774799}
Step 244: {'loss': 0.6295, 'grad_norm': 0.69921875, 'learning_rate': 0.00017895433483002354, 'num_tokens': 1711086.0, 'mean_token_accuracy': 0.833201065659523, 'epoch': 0.872207327971403}
Step 245: {'loss': 0.662, 'grad_norm': 0.56640625, 'learning_rate': 0.0001786986301660689, 'num_tokens': 1718664.0, 'mean_token_accuracy': 0.822191059589386, 'epoch': 0.8757819481680071}
Step 246: {'loss': 0.6502, 'grad_norm': 0.5859375, 'learning_rate': 0.00017844156649195759, 'num_tokens': 1725946.0, 'mean_token_accuracy': 0.8235044479370117, 'epoch': 0.8793565683646113}
Step 247: {'loss': 0.6176, 'grad_norm': 0.72265625, 'learning_rate': 0.000178183148246803, 'num_tokens': 1733258.0, 'mean_token_accuracy': 0.8334923684597015, 'epoch': 0.8829311885612153}
Step 248: {'loss': 0.6301, 'grad_norm': 0.59765625, 'learning_rate': 0.00017792337989311, 'num_tokens': 1741353.0, 'mean_token_accuracy': 0.8265145570039749, 'epoch': 0.8865058087578195}
Step 249: {'loss': 0.6775, 'grad_norm': 0.6015625, 'learning_rate': 0.00017766226591669785, 'num_tokens': 1748987.0, 'mean_token_accuracy': 0.820665255188942, 'epoch': 0.8900804289544236}
Step 250: {'loss': 0.6495, 'grad_norm': 0.6796875, 'learning_rate': 0.00017739981082662276, 'num_tokens': 1756986.0, 'mean_token_accuracy': 0.8244116306304932, 'epoch': 0.8936550491510277}
Step 251: {'loss': 0.6062, 'grad_norm': 0.671875, 'learning_rate': 0.0001771360191551, 'num_tokens': 1763583.0, 'mean_token_accuracy': 0.8323104530572891, 'epoch': 0.8972296693476318}
Step 252: {'loss': 0.67, 'grad_norm': 0.7109375, 'learning_rate': 0.00017687089545742558, 'num_tokens': 1770640.0, 'mean_token_accuracy': 0.825374186038971, 'epoch': 0.900804289544236}
Step 253: {'loss': 0.6556, 'grad_norm': 0.7265625, 'learning_rate': 0.0001766044443118978, 'num_tokens': 1778506.0, 'mean_token_accuracy': 0.8209580183029175, 'epoch': 0.90437890974084}
Step 254: {'loss': 0.614, 'grad_norm': 0.72265625, 'learning_rate': 0.00017633667031973792, 'num_tokens': 1785557.0, 'mean_token_accuracy': 0.8351401388645172, 'epoch': 0.9079535299374442}
Step 255: {'loss': 0.6042, 'grad_norm': 0.6484375, 'learning_rate': 0.00017606757810501088, 'num_tokens': 1793100.0, 'mean_token_accuracy': 0.8351765424013138, 'epoch': 0.9115281501340483}
Step 256: {'loss': 0.593, 'grad_norm': 0.578125, 'learning_rate': 0.0001757971723145453, 'num_tokens': 1801301.0, 'mean_token_accuracy': 0.8373968601226807, 'epoch': 0.9151027703306523}
Step 257: {'loss': 0.662, 'grad_norm': 0.65625, 'learning_rate': 0.0001755254576178535, 'num_tokens': 1808723.0, 'mean_token_accuracy': 0.8211925327777863, 'epoch': 0.9186773905272565}
Step 258: {'loss': 0.6541, 'grad_norm': 0.6875, 'learning_rate': 0.00017525243870705051, 'num_tokens': 1815646.0, 'mean_token_accuracy': 0.8206506669521332, 'epoch': 0.9222520107238605}
Step 259: {'loss': 0.6255, 'grad_norm': 0.60546875, 'learning_rate': 0.00017497812029677344, 'num_tokens': 1823298.0, 'mean_token_accuracy': 0.8348080068826675, 'epoch': 0.9258266309204647}
Step 260: {'loss': 0.6287, 'grad_norm': 0.58203125, 'learning_rate': 0.0001747025071240996, 'num_tokens': 1830327.0, 'mean_token_accuracy': 0.833214282989502, 'epoch': 0.9294012511170688}
Step 261: {'loss': 0.6616, 'grad_norm': 0.66015625, 'learning_rate': 0.00017442560394846516, 'num_tokens': 1837344.0, 'mean_token_accuracy': 0.8213386535644531, 'epoch': 0.9329758713136729}
Step 262: {'loss': 0.6051, 'grad_norm': 0.625, 'learning_rate': 0.00017414741555158266, 'num_tokens': 1844287.0, 'mean_token_accuracy': 0.8368996828794479, 'epoch': 0.936550491510277}
Step 263: {'loss': 0.639, 'grad_norm': 0.75, 'learning_rate': 0.0001738679467373586, 'num_tokens': 1851404.0, 'mean_token_accuracy': 0.8325133323669434, 'epoch': 0.9401251117068812}
Step 264: {'loss': 0.689, 'grad_norm': 0.57421875, 'learning_rate': 0.00017358720233181022, 'num_tokens': 1858531.0, 'mean_token_accuracy': 0.8179986923933029, 'epoch': 0.9436997319034852}
Step 265: {'loss': 0.6202, 'grad_norm': 0.53515625, 'learning_rate': 0.00017330518718298264, 'num_tokens': 1865479.0, 'mean_token_accuracy': 0.8368898928165436, 'epoch': 0.9472743521000894}
Step 266: {'loss': 0.6679, 'grad_norm': 0.60546875, 'learning_rate': 0.00017302190616086464, 'num_tokens': 1872902.0, 'mean_token_accuracy': 0.8258229941129684, 'epoch': 0.9508489722966935}
Step 267: {'loss': 0.6566, 'grad_norm': 0.61328125, 'learning_rate': 0.00017273736415730488, 'num_tokens': 1880725.0, 'mean_token_accuracy': 0.824826180934906, 'epoch': 0.9544235924932976}
Step 268: {'loss': 0.6039, 'grad_norm': 0.703125, 'learning_rate': 0.00017245156608592727, 'num_tokens': 1887508.0, 'mean_token_accuracy': 0.8372777700424194, 'epoch': 0.9579982126899017}
Step 269: {'loss': 0.5874, 'grad_norm': 0.52734375, 'learning_rate': 0.0001721645168820462, 'num_tokens': 1895275.0, 'mean_token_accuracy': 0.8384279161691666, 'epoch': 0.9615728328865059}
Step 270: {'loss': 0.5587, 'grad_norm': 0.55078125, 'learning_rate': 0.0001718762215025813, 'num_tokens': 1902986.0, 'mean_token_accuracy': 0.843104898929596, 'epoch': 0.9651474530831099}
Step 271: {'loss': 0.6202, 'grad_norm': 0.59375, 'learning_rate': 0.00017158668492597186, 'num_tokens': 1909563.0, 'mean_token_accuracy': 0.8316759616136551, 'epoch': 0.968722073279714}
Step 272: {'loss': 0.6478, 'grad_norm': 0.734375, 'learning_rate': 0.0001712959121520907, 'num_tokens': 1916237.0, 'mean_token_accuracy': 0.8268061727285385, 'epoch': 0.9722966934763181}
Step 273: {'loss': 0.6481, 'grad_norm': 0.7421875, 'learning_rate': 0.00017100390820215804, 'num_tokens': 1923472.0, 'mean_token_accuracy': 0.8322817981243134, 'epoch': 0.9758713136729222}
Step 274: {'loss': 0.6676, 'grad_norm': 0.84765625, 'learning_rate': 0.00017071067811865476, 'num_tokens': 1930731.0, 'mean_token_accuracy': 0.8144926577806473, 'epoch': 0.9794459338695264}
Step 275: {'loss': 0.6216, 'grad_norm': 0.67578125, 'learning_rate': 0.00017041622696523518, 'num_tokens': 1937339.0, 'mean_token_accuracy': 0.8347611278295517, 'epoch': 0.9830205540661304}
Step 276: {'loss': 0.6429, 'grad_norm': 0.6484375, 'learning_rate': 0.0001701205598266398, 'num_tokens': 1945102.0, 'mean_token_accuracy': 0.8316303938627243, 'epoch': 0.9865951742627346}
Step 277: {'loss': 0.6523, 'grad_norm': 0.640625, 'learning_rate': 0.00016982368180860728, 'num_tokens': 1951918.0, 'mean_token_accuracy': 0.8244926333427429, 'epoch': 0.9901697944593387}
Step 278: {'loss': 0.6238, 'grad_norm': 0.69921875, 'learning_rate': 0.00016952559803778657, 'num_tokens': 1958939.0, 'mean_token_accuracy': 0.8259170800447464, 'epoch': 0.9937444146559428}
Step 279: {'loss': 0.6535, 'grad_norm': 0.6875, 'learning_rate': 0.00016922631366164797, 'num_tokens': 1965866.0, 'mean_token_accuracy': 0.8204317837953568, 'epoch': 0.9973190348525469}
Step 280: {'loss': 0.4487, 'grad_norm': 0.6484375, 'learning_rate': 0.0001689258338483947, 'num_tokens': 1970045.0, 'mean_token_accuracy': 0.8373257319132487, 'epoch': 1.0}
Step 281: {'loss': 0.6081, 'grad_norm': 0.57421875, 'learning_rate': 0.0001686241637868734, 'num_tokens': 1977369.0, 'mean_token_accuracy': 0.8382477760314941, 'epoch': 1.003574620196604}
Step 282: {'loss': 0.6229, 'grad_norm': 0.61328125, 'learning_rate': 0.00016832130868648434, 'num_tokens': 1984050.0, 'mean_token_accuracy': 0.8292492181062698, 'epoch': 1.0071492403932083}
Step 283: {'loss': 0.5962, 'grad_norm': 0.69140625, 'learning_rate': 0.00016801727377709194, 'num_tokens': 1991148.0, 'mean_token_accuracy': 0.8401339948177338, 'epoch': 1.0107238605898123}
Step 284: {'loss': 0.5995, 'grad_norm': 0.6640625, 'learning_rate': 0.00016771206430893408, 'num_tokens': 1998135.0, 'mean_token_accuracy': 0.8332959413528442, 'epoch': 1.0142984807864164}
Step 285: {'loss': 0.6335, 'grad_norm': 0.703125, 'learning_rate': 0.00016740568555253155, 'num_tokens': 2004985.0, 'mean_token_accuracy': 0.8351942598819733, 'epoch': 1.0178731009830206}
Step 286: {'loss': 0.6468, 'grad_norm': 0.6328125, 'learning_rate': 0.00016709814279859702, 'num_tokens': 2011815.0, 'mean_token_accuracy': 0.8278169631958008, 'epoch': 1.0214477211796247}
Step 287: {'loss': 0.631, 'grad_norm': 0.6796875, 'learning_rate': 0.00016678944135794374, 'num_tokens': 2018837.0, 'mean_token_accuracy': 0.8277769535779953, 'epoch': 1.0250223413762287}
Step 288: {'loss': 0.6235, 'grad_norm': 0.7265625, 'learning_rate': 0.00016647958656139378, 'num_tokens': 2025660.0, 'mean_token_accuracy': 0.8372099250555038, 'epoch': 1.028596961572833}
Step 289: {'loss': 0.5993, 'grad_norm': 0.671875, 'learning_rate': 0.00016616858375968595, 'num_tokens': 2032408.0, 'mean_token_accuracy': 0.8377906978130341, 'epoch': 1.032171581769437}
Step 290: {'loss': 0.6353, 'grad_norm': 0.83984375, 'learning_rate': 0.00016585643832338343, 'num_tokens': 2039288.0, 'mean_token_accuracy': 0.8227658718824387, 'epoch': 1.035746201966041}
Step 291: {'loss': 0.6563, 'grad_norm': 0.83203125, 'learning_rate': 0.000165543155642781, 'num_tokens': 2046617.0, 'mean_token_accuracy': 0.8235680609941483, 'epoch': 1.0393208221626453}
Step 292: {'loss': 0.6491, 'grad_norm': 0.7421875, 'learning_rate': 0.00016522874112781213, 'num_tokens': 2053752.0, 'mean_token_accuracy': 0.8252004086971283, 'epoch': 1.0428954423592494}
Step 293: {'loss': 0.6392, 'grad_norm': 0.69921875, 'learning_rate': 0.0001649132002079552, 'num_tokens': 2061027.0, 'mean_token_accuracy': 0.8303465247154236, 'epoch': 1.0464700625558534}
Step 294: {'loss': 0.5978, 'grad_norm': 0.625, 'learning_rate': 0.0001645965383321401, 'num_tokens': 2068229.0, 'mean_token_accuracy': 0.8358600437641144, 'epoch': 1.0500446827524577}
Step 295: {'loss': 0.6149, 'grad_norm': 0.80078125, 'learning_rate': 0.00016427876096865394, 'num_tokens': 2075802.0, 'mean_token_accuracy': 0.832482635974884, 'epoch': 1.0536193029490617}
Step 296: {'loss': 0.6325, 'grad_norm': 0.62109375, 'learning_rate': 0.00016395987360504668, 'num_tokens': 2083482.0, 'mean_token_accuracy': 0.8321602791547775, 'epoch': 1.0571939231456657}
Step 297: {'loss': 0.5737, 'grad_norm': 0.7109375, 'learning_rate': 0.00016363988174803638, 'num_tokens': 2090170.0, 'mean_token_accuracy': 0.8458491563796997, 'epoch': 1.0607685433422698}
Step 298: {'loss': 0.6272, 'grad_norm': 0.71484375, 'learning_rate': 0.000163318790923414, 'num_tokens': 2097313.0, 'mean_token_accuracy': 0.8275811076164246, 'epoch': 1.064343163538874}
Step 299: {'loss': 0.6359, 'grad_norm': 0.51171875, 'learning_rate': 0.00016299660667594814, 'num_tokens': 2104683.0, 'mean_token_accuracy': 0.827076256275177, 'epoch': 1.067917783735478}
Step 300: {'loss': 0.6549, 'grad_norm': 0.6015625, 'learning_rate': 0.00016267333456928922, 'num_tokens': 2111987.0, 'mean_token_accuracy': 0.8250886499881744, 'epoch': 1.0714924039320821}
Step 301: {'loss': 0.6591, 'grad_norm': 0.6875, 'learning_rate': 0.00016234898018587337, 'num_tokens': 2118836.0, 'mean_token_accuracy': 0.8232588022947311, 'epoch': 1.0750670241286864}
Step 302: {'loss': 0.6544, 'grad_norm': 0.578125, 'learning_rate': 0.000162023549126826, 'num_tokens': 2126103.0, 'mean_token_accuracy': 0.8263901025056839, 'epoch': 1.0786416443252904}
Step 303: {'loss': 0.6806, 'grad_norm': 0.9296875, 'learning_rate': 0.00016169704701186527, 'num_tokens': 2131991.0, 'mean_token_accuracy': 0.8182977885007858, 'epoch': 1.0822162645218945}
Step 304: {'loss': 0.6403, 'grad_norm': 0.90234375, 'learning_rate': 0.00016136947947920476, 'num_tokens': 2138218.0, 'mean_token_accuracy': 0.8273479342460632, 'epoch': 1.0857908847184987}
Step 305: {'loss': 0.621, 'grad_norm': 0.77734375, 'learning_rate': 0.00016104085218545633, 'num_tokens': 2145175.0, 'mean_token_accuracy': 0.8393133878707886, 'epoch': 1.0893655049151028}
Step 306: {'loss': 0.5839, 'grad_norm': 0.828125, 'learning_rate': 0.00016071117080553236, 'num_tokens': 2151596.0, 'mean_token_accuracy': 0.8438932597637177, 'epoch': 1.0929401251117068}
Step 307: {'loss': 0.6462, 'grad_norm': 0.59765625, 'learning_rate': 0.00016038044103254775, 'num_tokens': 2158704.0, 'mean_token_accuracy': 0.8240531533956528, 'epoch': 1.096514745308311}
Step 308: {'loss': 0.6207, 'grad_norm': 0.890625, 'learning_rate': 0.0001600486685777216, 'num_tokens': 2166370.0, 'mean_token_accuracy': 0.8338975757360458, 'epoch': 1.100089365504915}
Step 309: {'loss': 0.6333, 'grad_norm': 0.8046875, 'learning_rate': 0.00015971585917027862, 'num_tokens': 2172872.0, 'mean_token_accuracy': 0.83463454246521, 'epoch': 1.1036639857015191}
Step 310: {'loss': 0.6475, 'grad_norm': 0.6953125, 'learning_rate': 0.00015938201855735014, 'num_tokens': 2179894.0, 'mean_token_accuracy': 0.8275118768215179, 'epoch': 1.1072386058981234}
Step 311: {'loss': 0.6271, 'grad_norm': 0.69140625, 'learning_rate': 0.00015904715250387498, 'num_tokens': 2187192.0, 'mean_token_accuracy': 0.8305117636919022, 'epoch': 1.1108132260947274}
Step 312: {'loss': 0.6401, 'grad_norm': 0.671875, 'learning_rate': 0.00015871126679249976, 'num_tokens': 2193819.0, 'mean_token_accuracy': 0.8278932720422745, 'epoch': 1.1143878462913315}
Step 313: {'loss': 0.6438, 'grad_norm': 0.76953125, 'learning_rate': 0.000158374367223479, 'num_tokens': 2200363.0, 'mean_token_accuracy': 0.8327091336250305, 'epoch': 1.1179624664879357}
Step 314: {'loss': 0.6499, 'grad_norm': 0.6953125, 'learning_rate': 0.00015803645961457523, 'num_tokens': 2206662.0, 'mean_token_accuracy': 0.821482926607132, 'epoch': 1.1215370866845398}
Step 315: {'loss': 0.6703, 'grad_norm': 0.6328125, 'learning_rate': 0.0001576975498009583, 'num_tokens': 2214270.0, 'mean_token_accuracy': 0.8206651359796524, 'epoch': 1.1251117068811438}
Step 316: {'loss': 0.6179, 'grad_norm': 0.6015625, 'learning_rate': 0.0001573576436351046, 'num_tokens': 2221211.0, 'mean_token_accuracy': 0.8343525528907776, 'epoch': 1.128686327077748}
Step 317: {'loss': 0.6012, 'grad_norm': 0.70703125, 'learning_rate': 0.0001570167469866962, 'num_tokens': 2228273.0, 'mean_token_accuracy': 0.838072881102562, 'epoch': 1.1322609472743521}
Step 318: {'loss': 0.6508, 'grad_norm': 0.796875, 'learning_rate': 0.00015667486574251916, 'num_tokens': 2235417.0, 'mean_token_accuracy': 0.82508984208107, 'epoch': 1.1358355674709562}
Step 319: {'loss': 0.6206, 'grad_norm': 0.64453125, 'learning_rate': 0.0001563320058063622, 'num_tokens': 2242383.0, 'mean_token_accuracy': 0.8342783004045486, 'epoch': 1.1394101876675604}
Step 320: {'loss': 0.6245, 'grad_norm': 0.56640625, 'learning_rate': 0.00015598817309891465, 'num_tokens': 2249379.0, 'mean_token_accuracy': 0.8273451924324036, 'epoch': 1.1429848078641645}
Step 321: {'loss': 0.6624, 'grad_norm': 0.69921875, 'learning_rate': 0.00015564337355766412, 'num_tokens': 2256800.0, 'mean_token_accuracy': 0.8227005898952484, 'epoch': 1.1465594280607685}
Step 322: {'loss': 0.6141, 'grad_norm': 0.65234375, 'learning_rate': 0.00015529761313679393, 'num_tokens': 2264179.0, 'mean_token_accuracy': 0.8319224864244461, 'epoch': 1.1501340482573728}
Step 323: {'loss': 0.5995, 'grad_norm': 0.6953125, 'learning_rate': 0.0001549508978070806, 'num_tokens': 2270835.0, 'mean_token_accuracy': 0.8355732560157776, 'epoch': 1.1537086684539768}
Step 324: {'loss': 0.6688, 'grad_norm': 0.94140625, 'learning_rate': 0.00015460323355579036, 'num_tokens': 2277496.0, 'mean_token_accuracy': 0.8216145783662796, 'epoch': 1.1572832886505808}
Step 325: {'loss': 0.5906, 'grad_norm': 0.66796875, 'learning_rate': 0.00015425462638657595, 'num_tokens': 2283590.0, 'mean_token_accuracy': 0.8409363627433777, 'epoch': 1.160857908847185}
Step 326: {'loss': 0.6453, 'grad_norm': 0.64453125, 'learning_rate': 0.00015390508231937297, 'num_tokens': 2290279.0, 'mean_token_accuracy': 0.8280633985996246, 'epoch': 1.1644325290437891}
Step 327: {'loss': 0.6389, 'grad_norm': 0.72265625, 'learning_rate': 0.00015355460739029586, 'num_tokens': 2296868.0, 'mean_token_accuracy': 0.8380047231912613, 'epoch': 1.1680071492403932}
Step 328: {'loss': 0.6064, 'grad_norm': 0.69921875, 'learning_rate': 0.00015320320765153367, 'num_tokens': 2303928.0, 'mean_token_accuracy': 0.8368301093578339, 'epoch': 1.1715817694369974}
Step 329: {'loss': 0.5919, 'grad_norm': 0.66796875, 'learning_rate': 0.00015285088917124556, 'num_tokens': 2311484.0, 'mean_token_accuracy': 0.8371270596981049, 'epoch': 1.1751563896336015}
Step 330: {'loss': 0.6206, 'grad_norm': 0.59375, 'learning_rate': 0.000152497658033456, 'num_tokens': 2319024.0, 'mean_token_accuracy': 0.8338636308908463, 'epoch': 1.1787310098302055}
Step 331: {'loss': 0.6628, 'grad_norm': 0.7421875, 'learning_rate': 0.0001521435203379498, 'num_tokens': 2325363.0, 'mean_token_accuracy': 0.8269105702638626, 'epoch': 1.1823056300268098}
Step 332: {'loss': 0.6767, 'grad_norm': 0.6796875, 'learning_rate': 0.0001517884822001666, 'num_tokens': 2331720.0, 'mean_token_accuracy': 0.8223647624254227, 'epoch': 1.1858802502234138}
Step 333: {'loss': 0.6157, 'grad_norm': 0.66796875, 'learning_rate': 0.00015143254975109538, 'num_tokens': 2338788.0, 'mean_token_accuracy': 0.8303301930427551, 'epoch': 1.1894548704200179}
Step 334: {'loss': 0.6351, 'grad_norm': 0.63671875, 'learning_rate': 0.00015107572913716858, 'num_tokens': 2346475.0, 'mean_token_accuracy': 0.8273787796497345, 'epoch': 1.193029490616622}
Step 335: {'loss': 0.6147, 'grad_norm': 0.63671875, 'learning_rate': 0.0001507180265201559, 'num_tokens': 2353116.0, 'mean_token_accuracy': 0.8356465995311737, 'epoch': 1.1966041108132262}
Step 336: {'loss': 0.6498, 'grad_norm': 0.61328125, 'learning_rate': 0.0001503594480770581, 'num_tokens': 2359564.0, 'mean_token_accuracy': 0.8302795588970184, 'epoch': 1.2001787310098302}
Step 337: {'loss': 0.6195, 'grad_norm': 0.55859375, 'learning_rate': 0.00015000000000000001, 'num_tokens': 2367237.0, 'mean_token_accuracy': 0.8340136259794235, 'epoch': 1.2037533512064342}
Step 338: {'loss': 0.6578, 'grad_norm': 0.67578125, 'learning_rate': 0.0001496396884961238, 'num_tokens': 2374566.0, 'mean_token_accuracy': 0.8153748065233231, 'epoch': 1.2073279714030385}
Step 339: {'loss': 0.6444, 'grad_norm': 0.75390625, 'learning_rate': 0.00014927851978748178, 'num_tokens': 2381980.0, 'mean_token_accuracy': 0.8277138769626617, 'epoch': 1.2109025915996425}
Step 340: {'loss': 0.6237, 'grad_norm': 0.6953125, 'learning_rate': 0.00014891650011092896, 'num_tokens': 2388928.0, 'mean_token_accuracy': 0.8291880190372467, 'epoch': 1.2144772117962466}
Step 341: {'loss': 0.617, 'grad_norm': 0.6484375, 'learning_rate': 0.00014855363571801523, 'num_tokens': 2395934.0, 'mean_token_accuracy': 0.8316779881715775, 'epoch': 1.2180518319928508}
Step 342: {'loss': 0.5962, 'grad_norm': 0.62109375, 'learning_rate': 0.0001481899328748776, 'num_tokens': 2402216.0, 'mean_token_accuracy': 0.831967368721962, 'epoch': 1.2216264521894549}
Step 343: {'loss': 0.6078, 'grad_norm': 0.6171875, 'learning_rate': 0.00014782539786213183, 'num_tokens': 2409593.0, 'mean_token_accuracy': 0.8358206748962402, 'epoch': 1.225201072386059}
Step 344: {'loss': 0.6171, 'grad_norm': 0.58984375, 'learning_rate': 0.00014746003697476404, 'num_tokens': 2416468.0, 'mean_token_accuracy': 0.8324341922998428, 'epoch': 1.2287756925826632}
Step 345: {'loss': 0.6687, 'grad_norm': 0.6875, 'learning_rate': 0.00014709385652202203, 'num_tokens': 2424202.0, 'mean_token_accuracy': 0.8235089182853699, 'epoch': 1.2323503127792672}
Step 346: {'loss': 0.6271, 'grad_norm': 0.67578125, 'learning_rate': 0.0001467268628273062, 'num_tokens': 2431088.0, 'mean_token_accuracy': 0.823681965470314, 'epoch': 1.2359249329758712}
Step 347: {'loss': 0.646, 'grad_norm': 0.671875, 'learning_rate': 0.00014635906222806058, 'num_tokens': 2438883.0, 'mean_token_accuracy': 0.8241864144802094, 'epoch': 1.2394995531724755}
Step 348: {'loss': 0.6285, 'grad_norm': 0.80859375, 'learning_rate': 0.00014599046107566314, 'num_tokens': 2445354.0, 'mean_token_accuracy': 0.8346180617809296, 'epoch': 1.2430741733690795}
Step 349: {'loss': 0.617, 'grad_norm': 0.66796875, 'learning_rate': 0.0001456210657353163, 'num_tokens': 2452228.0, 'mean_token_accuracy': 0.832968458533287, 'epoch': 1.2466487935656836}
Step 350: {'loss': 0.6202, 'grad_norm': 0.5859375, 'learning_rate': 0.00014525088258593696, 'num_tokens': 2459410.0, 'mean_token_accuracy': 0.827303022146225, 'epoch': 1.2502234137622876}
Step 351: {'loss': 0.6549, 'grad_norm': 0.70703125, 'learning_rate': 0.00014487991802004623, 'num_tokens': 2465886.0, 'mean_token_accuracy': 0.8250890076160431, 'epoch': 1.2537980339588919}
Step 352: {'loss': 0.6465, 'grad_norm': 0.83984375, 'learning_rate': 0.00014450817844365921, 'num_tokens': 2472462.0, 'mean_token_accuracy': 0.825502023100853, 'epoch': 1.257372654155496}
Step 353: {'loss': 0.6193, 'grad_norm': 0.6953125, 'learning_rate': 0.0001441356702761744, 'num_tokens': 2479984.0, 'mean_token_accuracy': 0.8336396217346191, 'epoch': 1.2609472743521}
Step 354: {'loss': 0.6006, 'grad_norm': 0.6875, 'learning_rate': 0.00014376239995026252, 'num_tokens': 2487020.0, 'mean_token_accuracy': 0.8358736634254456, 'epoch': 1.2645218945487042}
Step 355: {'loss': 0.5889, 'grad_norm': 0.64453125, 'learning_rate': 0.00014338837391175582, 'num_tokens': 2493644.0, 'mean_token_accuracy': 0.8391240984201431, 'epoch': 1.2680965147453083}
Step 356: {'loss': 0.5941, 'grad_norm': 0.72265625, 'learning_rate': 0.0001430135986195365, 'num_tokens': 2500463.0, 'mean_token_accuracy': 0.8395209163427353, 'epoch': 1.2716711349419123}
Step 357: {'loss': 0.6529, 'grad_norm': 0.66015625, 'learning_rate': 0.0001426380805454254, 'num_tokens': 2508103.0, 'mean_token_accuracy': 0.8194067180156708, 'epoch': 1.2752457551385166}
Step 358: {'loss': 0.6142, 'grad_norm': 0.71484375, 'learning_rate': 0.00014226182617406996, 'num_tokens': 2515523.0, 'mean_token_accuracy': 0.8308827131986618, 'epoch': 1.2788203753351206}
Step 359: {'loss': 0.6251, 'grad_norm': 0.625, 'learning_rate': 0.0001418848420028325, 'num_tokens': 2522611.0, 'mean_token_accuracy': 0.8283546715974808, 'epoch': 1.2823949955317246}
Step 360: {'loss': 0.5983, 'grad_norm': 0.73828125, 'learning_rate': 0.00014150713454167787, 'num_tokens': 2529321.0, 'mean_token_accuracy': 0.8411479741334915, 'epoch': 1.285969615728329}
Step 361: {'loss': 0.6296, 'grad_norm': 0.6171875, 'learning_rate': 0.00014112871031306119, 'num_tokens': 2536334.0, 'mean_token_accuracy': 0.8314404785633087, 'epoch': 1.289544235924933}
Step 362: {'loss': 0.6369, 'grad_norm': 0.52734375, 'learning_rate': 0.00014074957585181487, 'num_tokens': 2543595.0, 'mean_token_accuracy': 0.8251775652170181, 'epoch': 1.293118856121537}
Step 363: {'loss': 0.6195, 'grad_norm': 0.62109375, 'learning_rate': 0.00014036973770503624, 'num_tokens': 2550499.0, 'mean_token_accuracy': 0.8369875550270081, 'epoch': 1.2966934763181412}
Step 364: {'loss': 0.6427, 'grad_norm': 0.84375, 'learning_rate': 0.00013998920243197407, 'num_tokens': 2557474.0, 'mean_token_accuracy': 0.8300534039735794, 'epoch': 1.3002680965147453}
Step 365: {'loss': 0.5811, 'grad_norm': 0.72265625, 'learning_rate': 0.0001396079766039157, 'num_tokens': 2564931.0, 'mean_token_accuracy': 0.8398203104734421, 'epoch': 1.3038427167113493}
Step 366: {'loss': 0.6768, 'grad_norm': 0.640625, 'learning_rate': 0.00013922606680407307, 'num_tokens': 2572203.0, 'mean_token_accuracy': 0.8194981962442398, 'epoch': 1.3074173369079536}
Step 367: {'loss': 0.5807, 'grad_norm': 0.6640625, 'learning_rate': 0.00013884347962746948, 'num_tokens': 2579692.0, 'mean_token_accuracy': 0.8391681611537933, 'epoch': 1.3109919571045576}
Step 368: {'loss': 0.6812, 'grad_norm': 0.7890625, 'learning_rate': 0.00013846022168082552, 'num_tokens': 2587132.0, 'mean_token_accuracy': 0.8188678026199341, 'epoch': 1.3145665773011617}
Step 369: {'loss': 0.6135, 'grad_norm': 0.6015625, 'learning_rate': 0.00013807629958244498, 'num_tokens': 2594403.0, 'mean_token_accuracy': 0.833098292350769, 'epoch': 1.318141197497766}
Step 370: {'loss': 0.6457, 'grad_norm': 0.68359375, 'learning_rate': 0.00013769171996210052, 'num_tokens': 2600631.0, 'mean_token_accuracy': 0.8230044990777969, 'epoch': 1.32171581769437}
Step 371: {'loss': 0.6153, 'grad_norm': 0.765625, 'learning_rate': 0.0001373064894609194, 'num_tokens': 2607388.0, 'mean_token_accuracy': 0.8292296528816223, 'epoch': 1.325290437890974}
Step 372: {'loss': 0.6123, 'grad_norm': 0.59765625, 'learning_rate': 0.00013692061473126845, 'num_tokens': 2614589.0, 'mean_token_accuracy': 0.8294532001018524, 'epoch': 1.3288650580875783}
Step 373: {'loss': 0.6037, 'grad_norm': 0.6875, 'learning_rate': 0.00013653410243663952, 'num_tokens': 2621806.0, 'mean_token_accuracy': 0.8332705497741699, 'epoch': 1.3324396782841823}
Step 374: {'loss': 0.6451, 'grad_norm': 0.765625, 'learning_rate': 0.0001361469592515342, 'num_tokens': 2629590.0, 'mean_token_accuracy': 0.8307884633541107, 'epoch': 1.3360142984807863}
Step 375: {'loss': 0.6685, 'grad_norm': 0.6875, 'learning_rate': 0.0001357591918613486, 'num_tokens': 2637084.0, 'mean_token_accuracy': 0.8211323171854019, 'epoch': 1.3395889186773906}
Step 376: {'loss': 0.6386, 'grad_norm': 0.67578125, 'learning_rate': 0.00013537080696225814, 'num_tokens': 2644434.0, 'mean_token_accuracy': 0.8303705602884293, 'epoch': 1.3431635388739946}
Step 377: {'loss': 0.6049, 'grad_norm': 0.6015625, 'learning_rate': 0.0001349818112611015, 'num_tokens': 2651463.0, 'mean_token_accuracy': 0.8361025601625443, 'epoch': 1.3467381590705987}
Step 378: {'loss': 0.635, 'grad_norm': 0.71875, 'learning_rate': 0.00013459221147526504, 'num_tokens': 2658807.0, 'mean_token_accuracy': 0.8303477466106415, 'epoch': 1.350312779267203}
Step 379: {'loss': 0.6341, 'grad_norm': 0.68359375, 'learning_rate': 0.00013420201433256689, 'num_tokens': 2665550.0, 'mean_token_accuracy': 0.8307327479124069, 'epoch': 1.353887399463807}
Step 380: {'loss': 0.65, 'grad_norm': 0.53515625, 'learning_rate': 0.00013381122657114058, 'num_tokens': 2672487.0, 'mean_token_accuracy': 0.8280852138996124, 'epoch': 1.357462019660411}
Step 381: {'loss': 0.6657, 'grad_norm': 0.66796875, 'learning_rate': 0.00013341985493931877, 'num_tokens': 2680041.0, 'mean_token_accuracy': 0.8195061385631561, 'epoch': 1.3610366398570153}
Step 382: {'loss': 0.6427, 'grad_norm': 0.71484375, 'learning_rate': 0.00013302790619551674, 'num_tokens': 2686878.0, 'mean_token_accuracy': 0.8237323611974716, 'epoch': 1.3646112600536193}
Step 383: {'loss': 0.6059, 'grad_norm': 0.68359375, 'learning_rate': 0.0001326353871081156, 'num_tokens': 2693452.0, 'mean_token_accuracy': 0.8370873779058456, 'epoch': 1.3681858802502234}
Step 384: {'loss': 0.6329, 'grad_norm': 0.58203125, 'learning_rate': 0.00013224230445534545, 'num_tokens': 2701374.0, 'mean_token_accuracy': 0.8262389153242111, 'epoch': 1.3717605004468276}
Step 385: {'loss': 0.661, 'grad_norm': 0.7578125, 'learning_rate': 0.00013184866502516845, 'num_tokens': 2707856.0, 'mean_token_accuracy': 0.8227909505367279, 'epoch': 1.3753351206434317}
Step 386: {'loss': 0.6462, 'grad_norm': 0.63671875, 'learning_rate': 0.00013145447561516138, 'num_tokens': 2715054.0, 'mean_token_accuracy': 0.8242949098348618, 'epoch': 1.3789097408400357}
Step 387: {'loss': 0.659, 'grad_norm': 0.76171875, 'learning_rate': 0.00013105974303239838, 'num_tokens': 2721888.0, 'mean_token_accuracy': 0.8259525001049042, 'epoch': 1.38248436103664}
Step 388: {'loss': 0.6196, 'grad_norm': 0.58984375, 'learning_rate': 0.00013066447409333345, 'num_tokens': 2729703.0, 'mean_token_accuracy': 0.8315169662237167, 'epoch': 1.386058981233244}
Step 389: {'loss': 0.616, 'grad_norm': 0.64453125, 'learning_rate': 0.0001302686756236826, 'num_tokens': 2736822.0, 'mean_token_accuracy': 0.8410689830780029, 'epoch': 1.389633601429848}
Step 390: {'loss': 0.6704, 'grad_norm': 0.63671875, 'learning_rate': 0.0001298723544583061, 'num_tokens': 2744243.0, 'mean_token_accuracy': 0.8229356706142426, 'epoch': 1.3932082216264523}
Step 391: {'loss': 0.5987, 'grad_norm': 0.62890625, 'learning_rate': 0.00012947551744109043, 'num_tokens': 2750525.0, 'mean_token_accuracy': 0.837499663233757, 'epoch': 1.3967828418230563}
Step 392: {'loss': 0.5967, 'grad_norm': 0.640625, 'learning_rate': 0.00012907817142483, 'num_tokens': 2756987.0, 'mean_token_accuracy': 0.8363188803195953, 'epoch': 1.4003574620196604}
Step 393: {'loss': 0.6042, 'grad_norm': 0.71875, 'learning_rate': 0.00012868032327110904, 'num_tokens': 2764264.0, 'mean_token_accuracy': 0.8363217562437057, 'epoch': 1.4039320822162646}
Step 394: {'loss': 0.6373, 'grad_norm': 0.68359375, 'learning_rate': 0.00012828197985018276, 'num_tokens': 2771377.0, 'mean_token_accuracy': 0.823126494884491, 'epoch': 1.4075067024128687}
Step 395: {'loss': 0.6189, 'grad_norm': 0.734375, 'learning_rate': 0.00012788314804085903, 'num_tokens': 2778152.0, 'mean_token_accuracy': 0.8279798626899719, 'epoch': 1.4110813226094727}
Step 396: {'loss': 0.6128, 'grad_norm': 0.6640625, 'learning_rate': 0.00012748383473037948, 'num_tokens': 2785004.0, 'mean_token_accuracy': 0.8369440585374832, 'epoch': 1.414655942806077}
Step 397: {'loss': 0.6361, 'grad_norm': 0.78515625, 'learning_rate': 0.00012708404681430053, 'num_tokens': 2792076.0, 'mean_token_accuracy': 0.8329528123140335, 'epoch': 1.418230563002681}
Step 398: {'loss': 0.6335, 'grad_norm': 0.71484375, 'learning_rate': 0.0001266837911963743, 'num_tokens': 2799028.0, 'mean_token_accuracy': 0.8275431841611862, 'epoch': 1.421805183199285}
Step 399: {'loss': 0.6452, 'grad_norm': 0.8125, 'learning_rate': 0.00012628307478842953, 'num_tokens': 2806008.0, 'mean_token_accuracy': 0.8288718312978745, 'epoch': 1.4253798033958893}
Step 400: {'loss': 0.6316, 'grad_norm': 0.75, 'learning_rate': 0.00012588190451025207, 'num_tokens': 2812437.0, 'mean_token_accuracy': 0.830902099609375, 'epoch': 1.4289544235924934}
Step 401: {'loss': 0.6693, 'grad_norm': 0.75, 'learning_rate': 0.0001254802872894655, 'num_tokens': 2819546.0, 'mean_token_accuracy': 0.8191305249929428, 'epoch': 1.4325290437890974}
Step 402: {'loss': 0.6773, 'grad_norm': 0.6796875, 'learning_rate': 0.00012507823006141128, 'num_tokens': 2825800.0, 'mean_token_accuracy': 0.8200982809066772, 'epoch': 1.4361036639857014}
Step 403: {'loss': 0.5876, 'grad_norm': 0.640625, 'learning_rate': 0.00012467573976902935, 'num_tokens': 2832478.0, 'mean_token_accuracy': 0.8373169600963593, 'epoch': 1.4396782841823057}
Step 404: {'loss': 0.6483, 'grad_norm': 0.640625, 'learning_rate': 0.000124272823362738, 'num_tokens': 2840084.0, 'mean_token_accuracy': 0.8254862576723099, 'epoch': 1.4432529043789097}
Step 405: {'loss': 0.5788, 'grad_norm': 0.61328125, 'learning_rate': 0.0001238694878003138, 'num_tokens': 2847614.0, 'mean_token_accuracy': 0.8451459556818008, 'epoch': 1.4468275245755138}
Step 406: {'loss': 0.6409, 'grad_norm': 0.6171875, 'learning_rate': 0.00012346574004677154, 'num_tokens': 2854537.0, 'mean_token_accuracy': 0.82790507376194, 'epoch': 1.450402144772118}
Step 407: {'loss': 0.6092, 'grad_norm': 0.6015625, 'learning_rate': 0.00012306158707424403, 'num_tokens': 2861602.0, 'mean_token_accuracy': 0.8342805653810501, 'epoch': 1.453976764968722}
Step 408: {'loss': 0.6366, 'grad_norm': 0.625, 'learning_rate': 0.00012265703586186158, 'num_tokens': 2868498.0, 'mean_token_accuracy': 0.8340804576873779, 'epoch': 1.4575513851653261}
Step 409: {'loss': 0.6394, 'grad_norm': 0.62109375, 'learning_rate': 0.00012225209339563145, 'num_tokens': 2875928.0, 'mean_token_accuracy': 0.827851802110672, 'epoch': 1.4611260053619302}
Step 410: {'loss': 0.6527, 'grad_norm': 0.71484375, 'learning_rate': 0.0001218467666683174, 'num_tokens': 2883002.0, 'mean_token_accuracy': 0.8270480632781982, 'epoch': 1.4647006255585344}
Step 411: {'loss': 0.629, 'grad_norm': 0.546875, 'learning_rate': 0.00012144106267931876, 'num_tokens': 2889986.0, 'mean_token_accuracy': 0.8305439352989197, 'epoch': 1.4682752457551385}
Step 412: {'loss': 0.6413, 'grad_norm': 0.85546875, 'learning_rate': 0.00012103498843454959, 'num_tokens': 2896718.0, 'mean_token_accuracy': 0.8282238095998764, 'epoch': 1.4718498659517425}
Step 413: {'loss': 0.6548, 'grad_norm': 0.69921875, 'learning_rate': 0.00012062855094631778, 'num_tokens': 2902988.0, 'mean_token_accuracy': 0.8287563920021057, 'epoch': 1.4754244861483468}
Step 414: {'loss': 0.5856, 'grad_norm': 0.6328125, 'learning_rate': 0.00012022175723320381, 'num_tokens': 2909862.0, 'mean_token_accuracy': 0.8422903269529343, 'epoch': 1.4789991063449508}
Step 415: {'loss': 0.6032, 'grad_norm': 0.66796875, 'learning_rate': 0.00011981461431993977, 'num_tokens': 2916230.0, 'mean_token_accuracy': 0.8370690792798996, 'epoch': 1.4825737265415548}
Step 416: {'loss': 0.6305, 'grad_norm': 0.88671875, 'learning_rate': 0.00011940712923728783, 'num_tokens': 2923196.0, 'mean_token_accuracy': 0.8337442576885223, 'epoch': 1.486148346738159}
Step 417: {'loss': 0.6006, 'grad_norm': 0.703125, 'learning_rate': 0.00011899930902191902, 'num_tokens': 2930338.0, 'mean_token_accuracy': 0.8377654403448105, 'epoch': 1.4897229669347631}
Step 418: {'loss': 0.6532, 'grad_norm': 0.87890625, 'learning_rate': 0.00011859116071629149, 'num_tokens': 2937636.0, 'mean_token_accuracy': 0.8205392062664032, 'epoch': 1.4932975871313672}
Step 419: {'loss': 0.6579, 'grad_norm': 0.8125, 'learning_rate': 0.00011818269136852909, 'num_tokens': 2944691.0, 'mean_token_accuracy': 0.8271670192480087, 'epoch': 1.4968722073279714}
Step 420: {'loss': 0.6274, 'grad_norm': 0.69140625, 'learning_rate': 0.00011777390803229965, 'num_tokens': 2951241.0, 'mean_token_accuracy': 0.8283763974905014, 'epoch': 1.5004468275245755}
Step 421: {'loss': 0.5559, 'grad_norm': 0.625, 'learning_rate': 0.00011736481776669306, 'num_tokens': 2958240.0, 'mean_token_accuracy': 0.8431021422147751, 'epoch': 1.5040214477211795}
Step 422: {'loss': 0.5937, 'grad_norm': 0.65625, 'learning_rate': 0.00011695542763609943, 'num_tokens': 2964828.0, 'mean_token_accuracy': 0.8412037491798401, 'epoch': 1.5075960679177838}
Step 423: {'loss': 0.6636, 'grad_norm': 0.65234375, 'learning_rate': 0.00011654574471008713, 'num_tokens': 2971704.0, 'mean_token_accuracy': 0.8259049504995346, 'epoch': 1.5111706881143878}
Step 424: {'loss': 0.6987, 'grad_norm': 0.71484375, 'learning_rate': 0.00011613577606328068, 'num_tokens': 2978705.0, 'mean_token_accuracy': 0.8175061941146851, 'epoch': 1.5147453083109919}
Step 425: {'loss': 0.6465, 'grad_norm': 0.72265625, 'learning_rate': 0.00011572552877523854, 'num_tokens': 2986116.0, 'mean_token_accuracy': 0.8205180913209915, 'epoch': 1.5183199285075961}
Step 426: {'loss': 0.6355, 'grad_norm': 0.76171875, 'learning_rate': 0.00011531500993033093, 'num_tokens': 2992839.0, 'mean_token_accuracy': 0.8268113881349564, 'epoch': 1.5218945487042002}
Step 427: {'loss': 0.5815, 'grad_norm': 0.6640625, 'learning_rate': 0.00011490422661761744, 'num_tokens': 3000002.0, 'mean_token_accuracy': 0.8392964899539948, 'epoch': 1.5254691689008042}
Step 428: {'loss': 0.6305, 'grad_norm': 0.66796875, 'learning_rate': 0.00011449318593072466, 'num_tokens': 3006954.0, 'mean_token_accuracy': 0.833612397313118, 'epoch': 1.5290437890974085}
Step 429: {'loss': 0.6306, 'grad_norm': 0.66796875, 'learning_rate': 0.00011408189496772368, 'num_tokens': 3014244.0, 'mean_token_accuracy': 0.8264822959899902, 'epoch': 1.5326184092940125}
Step 430: {'loss': 0.6319, 'grad_norm': 0.6640625, 'learning_rate': 0.00011367036083100735, 'num_tokens': 3022055.0, 'mean_token_accuracy': 0.8346481621265411, 'epoch': 1.5361930294906165}
Step 431: {'loss': 0.6302, 'grad_norm': 0.625, 'learning_rate': 0.00011325859062716795, 'num_tokens': 3028833.0, 'mean_token_accuracy': 0.8289318382740021, 'epoch': 1.5397676496872208}
Step 432: {'loss': 0.6642, 'grad_norm': 0.7109375, 'learning_rate': 0.00011284659146687415, 'num_tokens': 3035824.0, 'mean_token_accuracy': 0.8233172297477722, 'epoch': 1.5433422698838248}
Step 433: {'loss': 0.636, 'grad_norm': 0.58203125, 'learning_rate': 0.00011243437046474853, 'num_tokens': 3043442.0, 'mean_token_accuracy': 0.8269410133361816, 'epoch': 1.5469168900804289}
Step 434: {'loss': 0.5677, 'grad_norm': 0.6640625, 'learning_rate': 0.00011202193473924438, 'num_tokens': 3050665.0, 'mean_token_accuracy': 0.8424568623304367, 'epoch': 1.5504915102770331}
Step 435: {'loss': 0.604, 'grad_norm': 0.90234375, 'learning_rate': 0.00011160929141252303, 'num_tokens': 3057442.0, 'mean_token_accuracy': 0.8356612771749496, 'epoch': 1.5540661304736372}
Step 436: {'loss': 0.6438, 'grad_norm': 0.69140625, 'learning_rate': 0.00011119644761033078, 'num_tokens': 3064219.0, 'mean_token_accuracy': 0.8224972039461136, 'epoch': 1.5576407506702412}
Step 437: {'loss': 0.6046, 'grad_norm': 0.8671875, 'learning_rate': 0.00011078341046187589, 'num_tokens': 3071251.0, 'mean_token_accuracy': 0.8318877965211868, 'epoch': 1.5612153708668455}
Step 438: {'loss': 0.6318, 'grad_norm': 0.7734375, 'learning_rate': 0.00011037018709970528, 'num_tokens': 3078239.0, 'mean_token_accuracy': 0.827822282910347, 'epoch': 1.5647899910634495}
Step 439: {'loss': 0.6368, 'grad_norm': 0.6484375, 'learning_rate': 0.00010995678465958168, 'num_tokens': 3085322.0, 'mean_token_accuracy': 0.8265822380781174, 'epoch': 1.5683646112600536}
Step 440: {'loss': 0.5909, 'grad_norm': 0.63671875, 'learning_rate': 0.00010954321028036012, 'num_tokens': 3092495.0, 'mean_token_accuracy': 0.8418488502502441, 'epoch': 1.5719392314566578}
Step 441: {'loss': 0.645, 'grad_norm': 0.7734375, 'learning_rate': 0.00010912947110386484, 'num_tokens': 3099294.0, 'mean_token_accuracy': 0.8281891942024231, 'epoch': 1.5755138516532619}
Step 442: {'loss': 0.6123, 'grad_norm': 0.640625, 'learning_rate': 0.00010871557427476583, 'num_tokens': 3106466.0, 'mean_token_accuracy': 0.8296225368976593, 'epoch': 1.579088471849866}
Step 443: {'loss': 0.6591, 'grad_norm': 0.66796875, 'learning_rate': 0.00010830152694045552, 'num_tokens': 3115285.0, 'mean_token_accuracy': 0.8207129091024399, 'epoch': 1.5826630920464702}
Step 444: {'loss': 0.6218, 'grad_norm': 0.94921875, 'learning_rate': 0.0001078873362509254, 'num_tokens': 3121799.0, 'mean_token_accuracy': 0.8380125910043716, 'epoch': 1.5862377122430742}
Step 445: {'loss': 0.6378, 'grad_norm': 0.75390625, 'learning_rate': 0.00010747300935864243, 'num_tokens': 3128686.0, 'mean_token_accuracy': 0.8212957680225372, 'epoch': 1.5898123324396782}
Step 446: {'loss': 0.6645, 'grad_norm': 0.6640625, 'learning_rate': 0.00010705855341842563, 'num_tokens': 3136051.0, 'mean_token_accuracy': 0.8224116861820221, 'epoch': 1.5933869526362825}
Step 447: {'loss': 0.6519, 'grad_norm': 0.609375, 'learning_rate': 0.00010664397558732244, 'num_tokens': 3143686.0, 'mean_token_accuracy': 0.8250418901443481, 'epoch': 1.5969615728328865}
Step 448: {'loss': 0.6103, 'grad_norm': 0.6796875, 'learning_rate': 0.00010622928302448523, 'num_tokens': 3150883.0, 'mean_token_accuracy': 0.8342649340629578, 'epoch': 1.6005361930294906}
Step 449: {'loss': 0.6374, 'grad_norm': 0.66796875, 'learning_rate': 0.00010581448289104758, 'num_tokens': 3159011.0, 'mean_token_accuracy': 0.8249816447496414, 'epoch': 1.6041108132260948}
Step 450: {'loss': 0.6102, 'grad_norm': 0.96484375, 'learning_rate': 0.00010539958235000074, 'num_tokens': 3165353.0, 'mean_token_accuracy': 0.8359273374080658, 'epoch': 1.6076854334226989}
Step 451: {'loss': 0.6348, 'grad_norm': 0.78515625, 'learning_rate': 0.00010498458856606972, 'num_tokens': 3171874.0, 'mean_token_accuracy': 0.8254444450139999, 'epoch': 1.611260053619303}
Step 452: {'loss': 0.6365, 'grad_norm': 0.70703125, 'learning_rate': 0.00010456950870558981, 'num_tokens': 3178679.0, 'mean_token_accuracy': 0.8228977918624878, 'epoch': 1.6148346738159072}
Step 453: {'loss': 0.6366, 'grad_norm': 0.6640625, 'learning_rate': 0.00010415434993638269, 'num_tokens': 3185512.0, 'mean_token_accuracy': 0.8323312252759933, 'epoch': 1.6184092940125112}
Step 454: {'loss': 0.611, 'grad_norm': 0.61328125, 'learning_rate': 0.0001037391194276326, 'num_tokens': 3192435.0, 'mean_token_accuracy': 0.8361631631851196, 'epoch': 1.6219839142091153}
Step 455: {'loss': 0.6108, 'grad_norm': 0.75, 'learning_rate': 0.00010332382434976266, 'num_tokens': 3200031.0, 'mean_token_accuracy': 0.8296021521091461, 'epoch': 1.6255585344057195}
Step 456: {'loss': 0.6439, 'grad_norm': 0.62109375, 'learning_rate': 0.00010290847187431113, 'num_tokens': 3207348.0, 'mean_token_accuracy': 0.8218091875314713, 'epoch': 1.6291331546023236}
Step 457: {'loss': 0.6499, 'grad_norm': 0.84375, 'learning_rate': 0.0001024930691738073, 'num_tokens': 3214536.0, 'mean_token_accuracy': 0.8187630325555801, 'epoch': 1.6327077747989276}
Step 458: {'loss': 0.5828, 'grad_norm': 0.74609375, 'learning_rate': 0.00010207762342164777, 'num_tokens': 3221724.0, 'mean_token_accuracy': 0.8415232449769974, 'epoch': 1.6362823949955319}
Step 459: {'loss': 0.6113, 'grad_norm': 0.7265625, 'learning_rate': 0.00010166214179197264, 'num_tokens': 3228816.0, 'mean_token_accuracy': 0.8387569785118103, 'epoch': 1.6398570151921357}
Step 460: {'loss': 0.686, 'grad_norm': 0.65625, 'learning_rate': 0.00010124663145954152, 'num_tokens': 3236041.0, 'mean_token_accuracy': 0.81489597260952, 'epoch': 1.64343163538874}
Step 461: {'loss': 0.5729, 'grad_norm': 0.67578125, 'learning_rate': 0.00010083109959960973, 'num_tokens': 3242941.0, 'mean_token_accuracy': 0.8405153155326843, 'epoch': 1.6470062555853442}
Step 462: {'loss': 0.6201, 'grad_norm': 0.7265625, 'learning_rate': 0.00010041555338780426, 'num_tokens': 3249682.0, 'mean_token_accuracy': 0.8283464759588242, 'epoch': 1.650580875781948}
Step 463: {'loss': 0.6339, 'grad_norm': 0.69140625, 'learning_rate': 0.0001, 'num_tokens': 3256465.0, 'mean_token_accuracy': 0.8337883502244949, 'epoch': 1.6541554959785523}
Step 464: {'loss': 0.6238, 'grad_norm': 0.6328125, 'learning_rate': 9.958444661219578e-05, 'num_tokens': 3263308.0, 'mean_token_accuracy': 0.8281580954790115, 'epoch': 1.6577301161751565}
Step 465: {'loss': 0.6281, 'grad_norm': 0.6171875, 'learning_rate': 9.916890040039031e-05, 'num_tokens': 3270643.0, 'mean_token_accuracy': 0.8266607224941254, 'epoch': 1.6613047363717603}
Step 466: {'loss': 0.6336, 'grad_norm': 0.67578125, 'learning_rate': 9.875336854045851e-05, 'num_tokens': 3277849.0, 'mean_token_accuracy': 0.8297896087169647, 'epoch': 1.6648793565683646}
Step 467: {'loss': 0.5988, 'grad_norm': 0.6171875, 'learning_rate': 9.833785820802739e-05, 'num_tokens': 3285566.0, 'mean_token_accuracy': 0.835076242685318, 'epoch': 1.6684539767649689}
Step 468: {'loss': 0.6116, 'grad_norm': 0.57421875, 'learning_rate': 9.792237657835224e-05, 'num_tokens': 3292769.0, 'mean_token_accuracy': 0.8330405950546265, 'epoch': 1.6720285969615727}
Step 469: {'loss': 0.6309, 'grad_norm': 0.62890625, 'learning_rate': 9.750693082619273e-05, 'num_tokens': 3299674.0, 'mean_token_accuracy': 0.8339169174432755, 'epoch': 1.675603217158177}
Step 470: {'loss': 0.621, 'grad_norm': 0.66796875, 'learning_rate': 9.709152812568886e-05, 'num_tokens': 3306510.0, 'mean_token_accuracy': 0.8319272398948669, 'epoch': 1.6791778373547812}
Step 471: {'loss': 0.6344, 'grad_norm': 0.63671875, 'learning_rate': 9.667617565023735e-05, 'num_tokens': 3313849.0, 'mean_token_accuracy': 0.8306918442249298, 'epoch': 1.682752457551385}
Step 472: {'loss': 0.6407, 'grad_norm': 0.65234375, 'learning_rate': 9.626088057236745e-05, 'num_tokens': 3321297.0, 'mean_token_accuracy': 0.8300704061985016, 'epoch': 1.6863270777479893}
Step 473: {'loss': 0.6232, 'grad_norm': 0.68359375, 'learning_rate': 9.584565006361734e-05, 'num_tokens': 3329246.0, 'mean_token_accuracy': 0.8316116183996201, 'epoch': 1.6899016979445936}
Step 474: {'loss': 0.6348, 'grad_norm': 0.61328125, 'learning_rate': 9.543049129441021e-05, 'num_tokens': 3336960.0, 'mean_token_accuracy': 0.8266024589538574, 'epoch': 1.6934763181411974}
Step 475: {'loss': 0.6104, 'grad_norm': 0.65234375, 'learning_rate': 9.501541143393028e-05, 'num_tokens': 3344519.0, 'mean_token_accuracy': 0.8311120420694351, 'epoch': 1.6970509383378016}
Step 476: {'loss': 0.6193, 'grad_norm': 0.63671875, 'learning_rate': 9.460041764999928e-05, 'num_tokens': 3351362.0, 'mean_token_accuracy': 0.8307115137577057, 'epoch': 1.7006255585344057}
Step 477: {'loss': 0.6643, 'grad_norm': 0.8125, 'learning_rate': 9.418551710895243e-05, 'num_tokens': 3357895.0, 'mean_token_accuracy': 0.8220054060220718, 'epoch': 1.7042001787310097}
Step 478: {'loss': 0.6197, 'grad_norm': 0.62109375, 'learning_rate': 9.37707169755148e-05, 'num_tokens': 3364951.0, 'mean_token_accuracy': 0.8329513221979141, 'epoch': 1.707774798927614}
Step 479: {'loss': 0.6259, 'grad_norm': 0.8203125, 'learning_rate': 9.335602441267759e-05, 'num_tokens': 3371514.0, 'mean_token_accuracy': 0.8316084891557693, 'epoch': 1.711349419124218}
Step 480: {'loss': 0.6424, 'grad_norm': 0.75, 'learning_rate': 9.294144658157442e-05, 'num_tokens': 3378145.0, 'mean_token_accuracy': 0.8288857489824295, 'epoch': 1.714924039320822}
Step 481: {'loss': 0.6208, 'grad_norm': 0.890625, 'learning_rate': 9.252699064135758e-05, 'num_tokens': 3384986.0, 'mean_token_accuracy': 0.8345069140195847, 'epoch': 1.7184986595174263}
Step 482: {'loss': 0.604, 'grad_norm': 0.734375, 'learning_rate': 9.211266374907463e-05, 'num_tokens': 3392631.0, 'mean_token_accuracy': 0.8436948955059052, 'epoch': 1.7220732797140303}
Step 483: {'loss': 0.6703, 'grad_norm': 0.671875, 'learning_rate': 9.169847305954447e-05, 'num_tokens': 3399775.0, 'mean_token_accuracy': 0.8194270879030228, 'epoch': 1.7256478999106344}
Step 484: {'loss': 0.5572, 'grad_norm': 0.66796875, 'learning_rate': 9.128442572523417e-05, 'num_tokens': 3406650.0, 'mean_token_accuracy': 0.8501825481653214, 'epoch': 1.7292225201072386}
Step 485: {'loss': 0.6524, 'grad_norm': 0.6171875, 'learning_rate': 9.087052889613518e-05, 'num_tokens': 3414229.0, 'mean_token_accuracy': 0.8217077702283859, 'epoch': 1.7327971403038427}
Step 486: {'loss': 0.6112, 'grad_norm': 0.6796875, 'learning_rate': 9.045678971963988e-05, 'num_tokens': 3421019.0, 'mean_token_accuracy': 0.8375879377126694, 'epoch': 1.7363717605004467}
Step 487: {'loss': 0.6296, 'grad_norm': 0.76953125, 'learning_rate': 9.004321534041835e-05, 'num_tokens': 3427538.0, 'mean_token_accuracy': 0.8297932893037796, 'epoch': 1.739946380697051}
Step 488: {'loss': 0.6244, 'grad_norm': 0.6171875, 'learning_rate': 8.962981290029474e-05, 'num_tokens': 3434252.0, 'mean_token_accuracy': 0.8337816596031189, 'epoch': 1.743521000893655}
Step 489: {'loss': 0.5983, 'grad_norm': 0.796875, 'learning_rate': 8.921658953812415e-05, 'num_tokens': 3441625.0, 'mean_token_accuracy': 0.837708979845047, 'epoch': 1.747095621090259}
Step 490: {'loss': 0.6345, 'grad_norm': 0.6171875, 'learning_rate': 8.880355238966923e-05, 'num_tokens': 3448351.0, 'mean_token_accuracy': 0.8292791843414307, 'epoch': 1.7506702412868633}
Step 491: {'loss': 0.6021, 'grad_norm': 0.55859375, 'learning_rate': 8.839070858747697e-05, 'num_tokens': 3455704.0, 'mean_token_accuracy': 0.8347097933292389, 'epoch': 1.7542448614834674}
Step 492: {'loss': 0.6679, 'grad_norm': 0.7265625, 'learning_rate': 8.797806526075565e-05, 'num_tokens': 3463046.0, 'mean_token_accuracy': 0.8200940489768982, 'epoch': 1.7578194816800714}
Step 493: {'loss': 0.615, 'grad_norm': 0.7421875, 'learning_rate': 8.756562953525152e-05, 'num_tokens': 3469958.0, 'mean_token_accuracy': 0.8355281949043274, 'epoch': 1.7613941018766757}
Step 494: {'loss': 0.6152, 'grad_norm': 0.6796875, 'learning_rate': 8.715340853312585e-05, 'num_tokens': 3477230.0, 'mean_token_accuracy': 0.8363722264766693, 'epoch': 1.7649687220732797}
Step 495: {'loss': 0.6014, 'grad_norm': 0.578125, 'learning_rate': 8.674140937283208e-05, 'num_tokens': 3485473.0, 'mean_token_accuracy': 0.8330794423818588, 'epoch': 1.7685433422698837}
Step 496: {'loss': 0.6009, 'grad_norm': 0.6953125, 'learning_rate': 8.632963916899268e-05, 'num_tokens': 3492416.0, 'mean_token_accuracy': 0.8402307778596878, 'epoch': 1.772117962466488}
Step 497: {'loss': 0.6488, 'grad_norm': 0.7265625, 'learning_rate': 8.591810503227635e-05, 'num_tokens': 3498899.0, 'mean_token_accuracy': 0.8257144242525101, 'epoch': 1.775692582663092}
Step 498: {'loss': 0.583, 'grad_norm': 0.59765625, 'learning_rate': 8.550681406927535e-05, 'num_tokens': 3505792.0, 'mean_token_accuracy': 0.8310617953538895, 'epoch': 1.779267202859696}
Step 499: {'loss': 0.6285, 'grad_norm': 0.62109375, 'learning_rate': 8.509577338238255e-05, 'num_tokens': 3512713.0, 'mean_token_accuracy': 0.836579903960228, 'epoch': 1.7828418230563003}
Step 500: {'loss': 0.6897, 'grad_norm': 0.78515625, 'learning_rate': 8.46849900696691e-05, 'num_tokens': 3519709.0, 'mean_token_accuracy': 0.8153598755598068, 'epoch': 1.7864164432529044}
Step 501: {'loss': 0.6673, 'grad_norm': 0.70703125, 'learning_rate': 8.427447122476148e-05, 'num_tokens': 3527829.0, 'mean_token_accuracy': 0.8170109391212463, 'epoch': 1.7899910634495084}
Step 502: {'loss': 0.616, 'grad_norm': 0.71875, 'learning_rate': 8.386422393671933e-05, 'num_tokens': 3534655.0, 'mean_token_accuracy': 0.8353001475334167, 'epoch': 1.7935656836461127}
Step 503: {'loss': 0.6373, 'grad_norm': 0.69921875, 'learning_rate': 8.345425528991288e-05, 'num_tokens': 3541293.0, 'mean_token_accuracy': 0.8264419138431549, 'epoch': 1.7971403038427167}
Step 504: {'loss': 0.5607, 'grad_norm': 0.70703125, 'learning_rate': 8.304457236390062e-05, 'num_tokens': 3547934.0, 'mean_token_accuracy': 0.8471044898033142, 'epoch': 1.8007149240393208}
Step 505: {'loss': 0.6728, 'grad_norm': 0.734375, 'learning_rate': 8.263518223330697e-05, 'num_tokens': 3555133.0, 'mean_token_accuracy': 0.814793735742569, 'epoch': 1.804289544235925}
Step 506: {'loss': 0.6435, 'grad_norm': 0.7578125, 'learning_rate': 8.222609196770036e-05, 'num_tokens': 3561504.0, 'mean_token_accuracy': 0.8253336995840073, 'epoch': 1.807864164432529}
Step 507: {'loss': 0.6276, 'grad_norm': 0.7109375, 'learning_rate': 8.181730863147093e-05, 'num_tokens': 3568731.0, 'mean_token_accuracy': 0.8342457115650177, 'epoch': 1.811438784629133}
Step 508: {'loss': 0.6626, 'grad_norm': 0.73828125, 'learning_rate': 8.140883928370855e-05, 'num_tokens': 3575587.0, 'mean_token_accuracy': 0.8156827837228775, 'epoch': 1.8150134048257374}
Step 509: {'loss': 0.6194, 'grad_norm': 0.69140625, 'learning_rate': 8.100069097808103e-05, 'num_tokens': 3582995.0, 'mean_token_accuracy': 0.8370514810085297, 'epoch': 1.8185880250223414}
Step 510: {'loss': 0.6264, 'grad_norm': 0.73828125, 'learning_rate': 8.059287076271216e-05, 'num_tokens': 3589755.0, 'mean_token_accuracy': 0.8341372460126877, 'epoch': 1.8221626452189454}
Step 511: {'loss': 0.6312, 'grad_norm': 0.78125, 'learning_rate': 8.018538568006027e-05, 'num_tokens': 3596732.0, 'mean_token_accuracy': 0.8273930996656418, 'epoch': 1.8257372654155497}
Step 512: {'loss': 0.596, 'grad_norm': 0.71484375, 'learning_rate': 7.977824276679623e-05, 'num_tokens': 3603529.0, 'mean_token_accuracy': 0.8341917097568512, 'epoch': 1.8293118856121537}
Step 513: {'loss': 0.605, 'grad_norm': 0.75, 'learning_rate': 7.937144905368226e-05, 'num_tokens': 3610763.0, 'mean_token_accuracy': 0.8385581970214844, 'epoch': 1.8328865058087578}
Step 514: {'loss': 0.6019, 'grad_norm': 0.69921875, 'learning_rate': 7.896501156545045e-05, 'num_tokens': 3617485.0, 'mean_token_accuracy': 0.8366889655590057, 'epoch': 1.836461126005362}
Step 515: {'loss': 0.6759, 'grad_norm': 0.66796875, 'learning_rate': 7.855893732068125e-05, 'num_tokens': 3624722.0, 'mean_token_accuracy': 0.8226854205131531, 'epoch': 1.840035746201966}
Step 516: {'loss': 0.6026, 'grad_norm': 0.62109375, 'learning_rate': 7.815323333168262e-05, 'num_tokens': 3632505.0, 'mean_token_accuracy': 0.8372167497873306, 'epoch': 1.8436103663985701}
Step 517: {'loss': 0.6159, 'grad_norm': 0.8203125, 'learning_rate': 7.774790660436858e-05, 'num_tokens': 3639265.0, 'mean_token_accuracy': 0.8356474190950394, 'epoch': 1.8471849865951744}
Step 518: {'loss': 0.6202, 'grad_norm': 0.7734375, 'learning_rate': 7.734296413813846e-05, 'num_tokens': 3646646.0, 'mean_token_accuracy': 0.8344980329275131, 'epoch': 1.8507596067917784}
Step 519: {'loss': 0.6009, 'grad_norm': 0.73046875, 'learning_rate': 7.693841292575598e-05, 'num_tokens': 3653239.0, 'mean_token_accuracy': 0.8383153229951859, 'epoch': 1.8543342269883825}
Step 520: {'loss': 0.6174, 'grad_norm': 0.703125, 'learning_rate': 7.653425995322851e-05, 'num_tokens': 3660236.0, 'mean_token_accuracy': 0.8334595263004303, 'epoch': 1.8579088471849867}
Step 521: {'loss': 0.6277, 'grad_norm': 0.63671875, 'learning_rate': 7.613051219968623e-05, 'num_tokens': 3667714.0, 'mean_token_accuracy': 0.8254815936088562, 'epoch': 1.8614834673815905}
Step 522: {'loss': 0.5986, 'grad_norm': 0.6796875, 'learning_rate': 7.572717663726203e-05, 'num_tokens': 3674922.0, 'mean_token_accuracy': 0.8334463834762573, 'epoch': 1.8650580875781948}
Step 523: {'loss': 0.6638, 'grad_norm': 0.671875, 'learning_rate': 7.532426023097063e-05, 'num_tokens': 3682215.0, 'mean_token_accuracy': 0.8158411085605621, 'epoch': 1.868632707774799}
Step 524: {'loss': 0.6388, 'grad_norm': 0.66796875, 'learning_rate': 7.492176993858873e-05, 'num_tokens': 3689306.0, 'mean_token_accuracy': 0.8294187486171722, 'epoch': 1.8722073279714029}
Step 525: {'loss': 0.5874, 'grad_norm': 0.6328125, 'learning_rate': 7.451971271053455e-05, 'num_tokens': 3696045.0, 'mean_token_accuracy': 0.838336393237114, 'epoch': 1.8757819481680071}
Step 526: {'loss': 0.6536, 'grad_norm': 0.8046875, 'learning_rate': 7.411809548974792e-05, 'num_tokens': 3703433.0, 'mean_token_accuracy': 0.8263331353664398, 'epoch': 1.8793565683646114}
Step 527: {'loss': 0.6885, 'grad_norm': 0.69140625, 'learning_rate': 7.371692521157048e-05, 'num_tokens': 3710606.0, 'mean_token_accuracy': 0.8188546001911163, 'epoch': 1.8829311885612152}
Step 528: {'loss': 0.6116, 'grad_norm': 0.62109375, 'learning_rate': 7.33162088036257e-05, 'num_tokens': 3717849.0, 'mean_token_accuracy': 0.8316086083650589, 'epoch': 1.8865058087578195}
Step 529: {'loss': 0.6363, 'grad_norm': 0.734375, 'learning_rate': 7.291595318569951e-05, 'num_tokens': 3725202.0, 'mean_token_accuracy': 0.8280453681945801, 'epoch': 1.8900804289544237}
Step 530: {'loss': 0.6421, 'grad_norm': 0.66015625, 'learning_rate': 7.251616526962053e-05, 'num_tokens': 3732371.0, 'mean_token_accuracy': 0.8226893693208694, 'epoch': 1.8936550491510276}
Step 531: {'loss': 0.6257, 'grad_norm': 0.76171875, 'learning_rate': 7.211685195914097e-05, 'num_tokens': 3739688.0, 'mean_token_accuracy': 0.8302295655012131, 'epoch': 1.8972296693476318}
Step 532: {'loss': 0.6364, 'grad_norm': 0.62109375, 'learning_rate': 7.171802014981726e-05, 'num_tokens': 3746328.0, 'mean_token_accuracy': 0.8245806097984314, 'epoch': 1.900804289544236}
Step 533: {'loss': 0.6407, 'grad_norm': 0.57421875, 'learning_rate': 7.131967672889101e-05, 'num_tokens': 3753743.0, 'mean_token_accuracy': 0.8267208933830261, 'epoch': 1.90437890974084}
Step 534: {'loss': 0.66, 'grad_norm': 0.6953125, 'learning_rate': 7.092182857516998e-05, 'num_tokens': 3760046.0, 'mean_token_accuracy': 0.8248918652534485, 'epoch': 1.9079535299374442}
Step 535: {'loss': 0.6022, 'grad_norm': 0.63671875, 'learning_rate': 7.052448255890957e-05, 'num_tokens': 3766881.0, 'mean_token_accuracy': 0.8355745673179626, 'epoch': 1.9115281501340484}
Step 536: {'loss': 0.603, 'grad_norm': 0.66796875, 'learning_rate': 7.012764554169393e-05, 'num_tokens': 3773544.0, 'mean_token_accuracy': 0.8390022069215775, 'epoch': 1.9151027703306522}
Step 537: {'loss': 0.5787, 'grad_norm': 0.609375, 'learning_rate': 6.973132437631742e-05, 'num_tokens': 3780890.0, 'mean_token_accuracy': 0.8412420749664307, 'epoch': 1.9186773905272565}
Step 538: {'loss': 0.6059, 'grad_norm': 0.64453125, 'learning_rate': 6.933552590666659e-05, 'num_tokens': 3787580.0, 'mean_token_accuracy': 0.8360189646482468, 'epoch': 1.9222520107238605}
Step 539: {'loss': 0.6231, 'grad_norm': 0.68359375, 'learning_rate': 6.894025696760163e-05, 'num_tokens': 3794883.0, 'mean_token_accuracy': 0.8292631059885025, 'epoch': 1.9258266309204646}
Step 540: {'loss': 0.5858, 'grad_norm': 0.640625, 'learning_rate': 6.854552438483865e-05, 'num_tokens': 3801631.0, 'mean_token_accuracy': 0.8395306169986725, 'epoch': 1.9294012511170688}
Step 541: {'loss': 0.6444, 'grad_norm': 0.703125, 'learning_rate': 6.815133497483157e-05, 'num_tokens': 3808439.0, 'mean_token_accuracy': 0.8281753063201904, 'epoch': 1.9329758713136729}
Step 542: {'loss': 0.6316, 'grad_norm': 0.671875, 'learning_rate': 6.775769554465454e-05, 'num_tokens': 3815678.0, 'mean_token_accuracy': 0.829397663474083, 'epoch': 1.936550491510277}
Step 543: {'loss': 0.6315, 'grad_norm': 0.68359375, 'learning_rate': 6.736461289188445e-05, 'num_tokens': 3822107.0, 'mean_token_accuracy': 0.8315889239311218, 'epoch': 1.9401251117068812}
Step 544: {'loss': 0.6631, 'grad_norm': 0.76953125, 'learning_rate': 6.697209380448333e-05, 'num_tokens': 3829020.0, 'mean_token_accuracy': 0.8258896768093109, 'epoch': 1.9436997319034852}
Step 545: {'loss': 0.6315, 'grad_norm': 0.62890625, 'learning_rate': 6.658014506068126e-05, 'num_tokens': 3836461.0, 'mean_token_accuracy': 0.8290911614894867, 'epoch': 1.9472743521000893}
Step 546: {'loss': 0.5822, 'grad_norm': 0.66796875, 'learning_rate': 6.618877342885945e-05, 'num_tokens': 3843307.0, 'mean_token_accuracy': 0.8435959070920944, 'epoch': 1.9508489722966935}
Step 547: {'loss': 0.6232, 'grad_norm': 0.73046875, 'learning_rate': 6.579798566743314e-05, 'num_tokens': 3850788.0, 'mean_token_accuracy': 0.8303278684616089, 'epoch': 1.9544235924932976}
Step 548: {'loss': 0.6003, 'grad_norm': 0.7421875, 'learning_rate': 6.540778852473497e-05, 'num_tokens': 3857797.0, 'mean_token_accuracy': 0.8381865918636322, 'epoch': 1.9579982126899016}
Step 549: {'loss': 0.5848, 'grad_norm': 0.62890625, 'learning_rate': 6.501818873889855e-05, 'num_tokens': 3864626.0, 'mean_token_accuracy': 0.8418698906898499, 'epoch': 1.9615728328865059}
Step 550: {'loss': 0.6088, 'grad_norm': 0.65234375, 'learning_rate': 6.462919303774186e-05, 'num_tokens': 3871355.0, 'mean_token_accuracy': 0.8324623256921768, 'epoch': 1.96514745308311}
Step 551: {'loss': 0.626, 'grad_norm': 0.6796875, 'learning_rate': 6.424080813865138e-05, 'num_tokens': 3877796.0, 'mean_token_accuracy': 0.8253237158060074, 'epoch': 1.968722073279714}
Step 552: {'loss': 0.6495, 'grad_norm': 0.7265625, 'learning_rate': 6.385304074846585e-05, 'num_tokens': 3884907.0, 'mean_token_accuracy': 0.8195400089025497, 'epoch': 1.9722966934763182}
Step 553: {'loss': 0.5831, 'grad_norm': 0.65625, 'learning_rate': 6.34658975633605e-05, 'num_tokens': 3892389.0, 'mean_token_accuracy': 0.8380276560783386, 'epoch': 1.9758713136729222}
Step 554: {'loss': 0.6278, 'grad_norm': 0.5625, 'learning_rate': 6.307938526873157e-05, 'num_tokens': 3899667.0, 'mean_token_accuracy': 0.8265780657529831, 'epoch': 1.9794459338695263}
Step 555: {'loss': 0.6305, 'grad_norm': 0.78515625, 'learning_rate': 6.269351053908061e-05, 'num_tokens': 3906742.0, 'mean_token_accuracy': 0.8339315801858902, 'epoch': 1.9830205540661305}
Step 556: {'loss': 0.6335, 'grad_norm': 0.6328125, 'learning_rate': 6.230828003789949e-05, 'num_tokens': 3914300.0, 'mean_token_accuracy': 0.8289615511894226, 'epoch': 1.9865951742627346}
Step 557: {'loss': 0.6491, 'grad_norm': 0.7265625, 'learning_rate': 6.192370041755505e-05, 'num_tokens': 3921259.0, 'mean_token_accuracy': 0.8255602121353149, 'epoch': 1.9901697944593386}
Step 558: {'loss': 0.6304, 'grad_norm': 0.6796875, 'learning_rate': 6.15397783191745e-05, 'num_tokens': 3928539.0, 'mean_token_accuracy': 0.8285812884569168, 'epoch': 1.9937444146559429}
Step 559: {'loss': 0.6215, 'grad_norm': 0.71484375, 'learning_rate': 6.115652037253053e-05, 'num_tokens': 3935649.0, 'mean_token_accuracy': 0.8346456289291382, 'epoch': 1.997319034852547}
Step 560: {'loss': 0.4792, 'grad_norm': 0.7109375, 'learning_rate': 6.077393319592697e-05, 'num_tokens': 3940090.0, 'mean_token_accuracy': 0.8304336667060852, 'epoch': 2.0}
Step 561: {'loss': 0.6166, 'grad_norm': 0.58984375, 'learning_rate': 6.039202339608432e-05, 'num_tokens': 3947175.0, 'mean_token_accuracy': 0.8324636220932007, 'epoch': 2.0035746201966043}
Step 562: {'loss': 0.6418, 'grad_norm': 0.60546875, 'learning_rate': 6.001079756802592e-05, 'num_tokens': 3954891.0, 'mean_token_accuracy': 0.8188408315181732, 'epoch': 2.007149240393208}
Step 563: {'loss': 0.6197, 'grad_norm': 0.61328125, 'learning_rate': 5.963026229496378e-05, 'num_tokens': 3961841.0, 'mean_token_accuracy': 0.831304520368576, 'epoch': 2.0107238605898123}
Step 564: {'loss': 0.6674, 'grad_norm': 0.65625, 'learning_rate': 5.925042414818514e-05, 'num_tokens': 3968882.0, 'mean_token_accuracy': 0.8190011978149414, 'epoch': 2.0142984807864166}
Step 565: {'loss': 0.609, 'grad_norm': 0.64453125, 'learning_rate': 5.887128968693887e-05, 'num_tokens': 3976547.0, 'mean_token_accuracy': 0.8332952260971069, 'epoch': 2.0178731009830204}
Step 566: {'loss': 0.6122, 'grad_norm': 0.6796875, 'learning_rate': 5.849286545832211e-05, 'num_tokens': 3984150.0, 'mean_token_accuracy': 0.8338613361120224, 'epoch': 2.0214477211796247}
Step 567: {'loss': 0.632, 'grad_norm': 0.609375, 'learning_rate': 5.8115157997167536e-05, 'num_tokens': 3991654.0, 'mean_token_accuracy': 0.8303449004888535, 'epoch': 2.025022341376229}
Step 568: {'loss': 0.6105, 'grad_norm': 0.60546875, 'learning_rate': 5.773817382593008e-05, 'num_tokens': 3998293.0, 'mean_token_accuracy': 0.8367068618535995, 'epoch': 2.0285969615728328}
Step 569: {'loss': 0.6323, 'grad_norm': 0.7109375, 'learning_rate': 5.736191945457463e-05, 'num_tokens': 4005234.0, 'mean_token_accuracy': 0.8295879811048508, 'epoch': 2.032171581769437}
Step 570: {'loss': 0.5965, 'grad_norm': 0.671875, 'learning_rate': 5.698640138046349e-05, 'num_tokens': 4012099.0, 'mean_token_accuracy': 0.843607485294342, 'epoch': 2.0357462019660413}
Step 571: {'loss': 0.641, 'grad_norm': 0.73828125, 'learning_rate': 5.6611626088244194e-05, 'num_tokens': 4019124.0, 'mean_token_accuracy': 0.8236638158559799, 'epoch': 2.039320822162645}
Step 572: {'loss': 0.5848, 'grad_norm': 0.59765625, 'learning_rate': 5.623760004973748e-05, 'num_tokens': 4026767.0, 'mean_token_accuracy': 0.8368106186389923, 'epoch': 2.0428954423592494}
Step 573: {'loss': 0.6668, 'grad_norm': 0.7109375, 'learning_rate': 5.58643297238256e-05, 'num_tokens': 4034337.0, 'mean_token_accuracy': 0.8224663585424423, 'epoch': 2.0464700625558536}
Step 574: {'loss': 0.6191, 'grad_norm': 0.71875, 'learning_rate': 5.549182155634076e-05, 'num_tokens': 4041496.0, 'mean_token_accuracy': 0.8331897854804993, 'epoch': 2.0500446827524574}
Step 575: {'loss': 0.6462, 'grad_norm': 0.671875, 'learning_rate': 5.5120081979953785e-05, 'num_tokens': 4048326.0, 'mean_token_accuracy': 0.8280438333749771, 'epoch': 2.0536193029490617}
Step 576: {'loss': 0.6429, 'grad_norm': 0.7109375, 'learning_rate': 5.47491174140631e-05, 'num_tokens': 4055055.0, 'mean_token_accuracy': 0.8228525519371033, 'epoch': 2.057193923145666}
Step 577: {'loss': 0.6387, 'grad_norm': 0.7265625, 'learning_rate': 5.43789342646837e-05, 'num_tokens': 4062545.0, 'mean_token_accuracy': 0.821467787027359, 'epoch': 2.0607685433422698}
Step 578: {'loss': 0.6394, 'grad_norm': 0.66796875, 'learning_rate': 5.400953892433687e-05, 'num_tokens': 4069357.0, 'mean_token_accuracy': 0.8325211107730865, 'epoch': 2.064343163538874}
Step 579: {'loss': 0.6414, 'grad_norm': 0.73828125, 'learning_rate': 5.3640937771939436e-05, 'num_tokens': 4076462.0, 'mean_token_accuracy': 0.8264894634485245, 'epoch': 2.0679177837354783}
Step 580: {'loss': 0.5902, 'grad_norm': 0.69140625, 'learning_rate': 5.32731371726938e-05, 'num_tokens': 4083664.0, 'mean_token_accuracy': 0.8357989341020584, 'epoch': 2.071492403932082}
Step 581: {'loss': 0.6322, 'grad_norm': 0.75390625, 'learning_rate': 5.290614347797802e-05, 'num_tokens': 4090666.0, 'mean_token_accuracy': 0.8220258206129074, 'epoch': 2.0750670241286864}
Step 582: {'loss': 0.6176, 'grad_norm': 0.8515625, 'learning_rate': 5.253996302523596e-05, 'num_tokens': 4097166.0, 'mean_token_accuracy': 0.8374854922294617, 'epoch': 2.0786416443252906}
Step 583: {'loss': 0.5835, 'grad_norm': 0.68359375, 'learning_rate': 5.217460213786821e-05, 'num_tokens': 4104192.0, 'mean_token_accuracy': 0.8352121263742447, 'epoch': 2.0822162645218945}
Step 584: {'loss': 0.6106, 'grad_norm': 0.65234375, 'learning_rate': 5.1810067125122443e-05, 'num_tokens': 4111372.0, 'mean_token_accuracy': 0.8301422297954559, 'epoch': 2.0857908847184987}
Step 585: {'loss': 0.6447, 'grad_norm': 0.68359375, 'learning_rate': 5.1446364281984774e-05, 'num_tokens': 4118163.0, 'mean_token_accuracy': 0.8289355039596558, 'epoch': 2.089365504915103}
Step 586: {'loss': 0.5646, 'grad_norm': 0.6171875, 'learning_rate': 5.108349988907111e-05, 'num_tokens': 4125061.0, 'mean_token_accuracy': 0.8473774641752243, 'epoch': 2.092940125111707}
Step 587: {'loss': 0.611, 'grad_norm': 0.71875, 'learning_rate': 5.072148021251821e-05, 'num_tokens': 4132338.0, 'mean_token_accuracy': 0.8285608738660812, 'epoch': 2.096514745308311}
Step 588: {'loss': 0.5991, 'grad_norm': 0.59375, 'learning_rate': 5.0360311503876234e-05, 'num_tokens': 4139618.0, 'mean_token_accuracy': 0.8379294872283936, 'epoch': 2.1000893655049153}
Step 589: {'loss': 0.6342, 'grad_norm': 0.73828125, 'learning_rate': 5.000000000000002e-05, 'num_tokens': 4146040.0, 'mean_token_accuracy': 0.8286861926317215, 'epoch': 2.103663985701519}
Step 590: {'loss': 0.6077, 'grad_norm': 0.6328125, 'learning_rate': 4.964055192294187e-05, 'num_tokens': 4153420.0, 'mean_token_accuracy': 0.8339000791311264, 'epoch': 2.1072386058981234}
Step 591: {'loss': 0.6135, 'grad_norm': 0.8046875, 'learning_rate': 4.92819734798441e-05, 'num_tokens': 4160762.0, 'mean_token_accuracy': 0.8390012085437775, 'epoch': 2.110813226094727}
Step 592: {'loss': 0.5719, 'grad_norm': 0.64453125, 'learning_rate': 4.892427086283147e-05, 'num_tokens': 4168215.0, 'mean_token_accuracy': 0.842653751373291, 'epoch': 2.1143878462913315}
Step 593: {'loss': 0.5676, 'grad_norm': 0.72265625, 'learning_rate': 4.856745024890466e-05, 'num_tokens': 4175179.0, 'mean_token_accuracy': 0.8480403572320938, 'epoch': 2.1179624664879357}
Step 594: {'loss': 0.5862, 'grad_norm': 0.6171875, 'learning_rate': 4.821151779983343e-05, 'num_tokens': 4182804.0, 'mean_token_accuracy': 0.8419446647167206, 'epoch': 2.1215370866845396}
Step 595: {'loss': 0.6238, 'grad_norm': 0.76171875, 'learning_rate': 4.78564796620502e-05, 'num_tokens': 4189519.0, 'mean_token_accuracy': 0.8234293311834335, 'epoch': 2.125111706881144}
Step 596: {'loss': 0.6174, 'grad_norm': 0.703125, 'learning_rate': 4.7502341966544e-05, 'num_tokens': 4196150.0, 'mean_token_accuracy': 0.8310612589120865, 'epoch': 2.128686327077748}
Step 597: {'loss': 0.6574, 'grad_norm': 0.8046875, 'learning_rate': 4.7149110828754464e-05, 'num_tokens': 4203422.0, 'mean_token_accuracy': 0.8227216899394989, 'epoch': 2.132260947274352}
Step 598: {'loss': 0.6325, 'grad_norm': 0.75, 'learning_rate': 4.6796792348466356e-05, 'num_tokens': 4210293.0, 'mean_token_accuracy': 0.8292419463396072, 'epoch': 2.135835567470956}
Step 599: {'loss': 0.6257, 'grad_norm': 0.85546875, 'learning_rate': 4.644539260970416e-05, 'num_tokens': 4217232.0, 'mean_token_accuracy': 0.8292177021503448, 'epoch': 2.1394101876675604}
Step 600: {'loss': 0.642, 'grad_norm': 0.734375, 'learning_rate': 4.609491768062705e-05, 'num_tokens': 4223960.0, 'mean_token_accuracy': 0.8256543576717377, 'epoch': 2.1429848078641642}
Step 601: {'loss': 0.6057, 'grad_norm': 0.734375, 'learning_rate': 4.574537361342407e-05, 'num_tokens': 4231380.0, 'mean_token_accuracy': 0.8352015018463135, 'epoch': 2.1465594280607685}
Step 602: {'loss': 0.6493, 'grad_norm': 0.69140625, 'learning_rate': 4.539676644420966e-05, 'num_tokens': 4238842.0, 'mean_token_accuracy': 0.8284327685832977, 'epoch': 2.1501340482573728}
Step 603: {'loss': 0.6149, 'grad_norm': 0.66015625, 'learning_rate': 4.50491021929194e-05, 'num_tokens': 4245938.0, 'mean_token_accuracy': 0.834012970328331, 'epoch': 2.1537086684539766}
Step 604: {'loss': 0.6142, 'grad_norm': 0.70703125, 'learning_rate': 4.470238686320606e-05, 'num_tokens': 4253089.0, 'mean_token_accuracy': 0.8363861590623856, 'epoch': 2.157283288650581}
Step 605: {'loss': 0.6412, 'grad_norm': 0.78515625, 'learning_rate': 4.435662644233594e-05, 'num_tokens': 4259357.0, 'mean_token_accuracy': 0.8293263763189316, 'epoch': 2.160857908847185}
Step 606: {'loss': 0.5824, 'grad_norm': 0.859375, 'learning_rate': 4.401182690108534e-05, 'num_tokens': 4265970.0, 'mean_token_accuracy': 0.8376115262508392, 'epoch': 2.164432529043789}
Step 607: {'loss': 0.6006, 'grad_norm': 0.76171875, 'learning_rate': 4.3667994193637796e-05, 'num_tokens': 4273400.0, 'mean_token_accuracy': 0.837799921631813, 'epoch': 2.168007149240393}
Step 608: {'loss': 0.6783, 'grad_norm': 0.66796875, 'learning_rate': 4.33251342574809e-05, 'num_tokens': 4281081.0, 'mean_token_accuracy': 0.8186947256326675, 'epoch': 2.1715817694369974}
Step 609: {'loss': 0.6211, 'grad_norm': 0.6875, 'learning_rate': 4.298325301330383e-05, 'num_tokens': 4288206.0, 'mean_token_accuracy': 0.8327904045581818, 'epoch': 2.1751563896336013}
Step 610: {'loss': 0.5628, 'grad_norm': 0.63671875, 'learning_rate': 4.264235636489542e-05, 'num_tokens': 4295630.0, 'mean_token_accuracy': 0.8460579514503479, 'epoch': 2.1787310098302055}
Step 611: {'loss': 0.6369, 'grad_norm': 0.625, 'learning_rate': 4.23024501990417e-05, 'num_tokens': 4303450.0, 'mean_token_accuracy': 0.8275912702083588, 'epoch': 2.1823056300268098}
Step 612: {'loss': 0.6297, 'grad_norm': 0.70703125, 'learning_rate': 4.196354038542476e-05, 'num_tokens': 4310865.0, 'mean_token_accuracy': 0.8266491144895554, 'epoch': 2.1858802502234136}
Step 613: {'loss': 0.6057, 'grad_norm': 0.7578125, 'learning_rate': 4.1625632776521037e-05, 'num_tokens': 4317058.0, 'mean_token_accuracy': 0.8400953561067581, 'epoch': 2.189454870420018}
Step 614: {'loss': 0.6233, 'grad_norm': 0.70703125, 'learning_rate': 4.128873320750026e-05, 'num_tokens': 4323909.0, 'mean_token_accuracy': 0.8308946639299393, 'epoch': 2.193029490616622}
Step 615: {'loss': 0.618, 'grad_norm': 0.78125, 'learning_rate': 4.095284749612503e-05, 'num_tokens': 4330415.0, 'mean_token_accuracy': 0.830771416425705, 'epoch': 2.196604110813226}
Step 616: {'loss': 0.573, 'grad_norm': 0.61328125, 'learning_rate': 4.0617981442649855e-05, 'num_tokens': 4337842.0, 'mean_token_accuracy': 0.8434620350599289, 'epoch': 2.20017873100983}
Step 617: {'loss': 0.654, 'grad_norm': 0.78515625, 'learning_rate': 4.028414082972141e-05, 'num_tokens': 4344691.0, 'mean_token_accuracy': 0.824555367231369, 'epoch': 2.2037533512064345}
Step 618: {'loss': 0.6059, 'grad_norm': 0.83203125, 'learning_rate': 3.9951331422278426e-05, 'num_tokens': 4351592.0, 'mean_token_accuracy': 0.8334273546934128, 'epoch': 2.2073279714030383}
Step 619: {'loss': 0.6493, 'grad_norm': 0.7734375, 'learning_rate': 3.961955896745224e-05, 'num_tokens': 4358672.0, 'mean_token_accuracy': 0.8202773481607437, 'epoch': 2.2109025915996425}
Step 620: {'loss': 0.6458, 'grad_norm': 0.8515625, 'learning_rate': 3.928882919446767e-05, 'num_tokens': 4366158.0, 'mean_token_accuracy': 0.8266554474830627, 'epoch': 2.214477211796247}
Step 621: {'loss': 0.6129, 'grad_norm': 0.6484375, 'learning_rate': 3.89591478145437e-05, 'num_tokens': 4373252.0, 'mean_token_accuracy': 0.8345460742712021, 'epoch': 2.2180518319928506}
Step 622: {'loss': 0.5933, 'grad_norm': 0.6640625, 'learning_rate': 3.863052052079528e-05, 'num_tokens': 4379989.0, 'mean_token_accuracy': 0.8398709148168564, 'epoch': 2.221626452189455}
Step 623: {'loss': 0.7087, 'grad_norm': 0.81640625, 'learning_rate': 3.8302952988134756e-05, 'num_tokens': 4386749.0, 'mean_token_accuracy': 0.8091581761837006, 'epoch': 2.225201072386059}
Step 624: {'loss': 0.5945, 'grad_norm': 0.68359375, 'learning_rate': 3.7976450873174005e-05, 'num_tokens': 4393745.0, 'mean_token_accuracy': 0.8353678584098816, 'epoch': 2.228775692582663}
Step 625: {'loss': 0.6134, 'grad_norm': 0.6875, 'learning_rate': 3.7651019814126654e-05, 'num_tokens': 4400188.0, 'mean_token_accuracy': 0.8349009603261948, 'epoch': 2.232350312779267}
Step 626: {'loss': 0.6104, 'grad_norm': 0.73046875, 'learning_rate': 3.732666543071079e-05, 'num_tokens': 4407096.0, 'mean_token_accuracy': 0.8366300910711288, 'epoch': 2.2359249329758715}
Step 627: {'loss': 0.5883, 'grad_norm': 0.82421875, 'learning_rate': 3.7003393324051874e-05, 'num_tokens': 4413578.0, 'mean_token_accuracy': 0.8323846608400345, 'epoch': 2.2394995531724753}
Step 628: {'loss': 0.6558, 'grad_norm': 0.71484375, 'learning_rate': 3.668120907658603e-05, 'num_tokens': 4420451.0, 'mean_token_accuracy': 0.8278390020132065, 'epoch': 2.2430741733690795}
Step 629: {'loss': 0.6483, 'grad_norm': 0.70703125, 'learning_rate': 3.6360118251963645e-05, 'num_tokens': 4427696.0, 'mean_token_accuracy': 0.827328160405159, 'epoch': 2.246648793565684}
Step 630: {'loss': 0.6138, 'grad_norm': 0.703125, 'learning_rate': 3.6040126394953335e-05, 'num_tokens': 4434346.0, 'mean_token_accuracy': 0.8387098610401154, 'epoch': 2.2502234137622876}
Step 631: {'loss': 0.6123, 'grad_norm': 0.703125, 'learning_rate': 3.5721239031346066e-05, 'num_tokens': 4440964.0, 'mean_token_accuracy': 0.8340030163526535, 'epoch': 2.253798033958892}
Step 632: {'loss': 0.6133, 'grad_norm': 0.7109375, 'learning_rate': 3.540346166785994e-05, 'num_tokens': 4448197.0, 'mean_token_accuracy': 0.8359550982713699, 'epoch': 2.257372654155496}
Step 633: {'loss': 0.5785, 'grad_norm': 0.8125, 'learning_rate': 3.508679979204481e-05, 'num_tokens': 4455899.0, 'mean_token_accuracy': 0.8403095155954361, 'epoch': 2.2609472743521}
Step 634: {'loss': 0.6224, 'grad_norm': 0.90625, 'learning_rate': 3.477125887218792e-05, 'num_tokens': 4461987.0, 'mean_token_accuracy': 0.8327139616012573, 'epoch': 2.2645218945487042}
Step 635: {'loss': 0.6363, 'grad_norm': 0.7734375, 'learning_rate': 3.445684435721897e-05, 'num_tokens': 4469508.0, 'mean_token_accuracy': 0.8281501233577728, 'epoch': 2.2680965147453085}
Step 636: {'loss': 0.6235, 'grad_norm': 0.66796875, 'learning_rate': 3.4143561676616575e-05, 'num_tokens': 4476556.0, 'mean_token_accuracy': 0.8345731943845749, 'epoch': 2.2716711349419123}
Step 637: {'loss': 0.6216, 'grad_norm': 0.8125, 'learning_rate': 3.383141624031408e-05, 'num_tokens': 4482755.0, 'mean_token_accuracy': 0.8315183967351913, 'epoch': 2.2752457551385166}
Step 638: {'loss': 0.6111, 'grad_norm': 0.6796875, 'learning_rate': 3.352041343860621e-05, 'num_tokens': 4489815.0, 'mean_token_accuracy': 0.8321174681186676, 'epoch': 2.278820375335121}
Step 639: {'loss': 0.5715, 'grad_norm': 0.6640625, 'learning_rate': 3.3210558642056275e-05, 'num_tokens': 4496945.0, 'mean_token_accuracy': 0.8427786976099014, 'epoch': 2.2823949955317246}
Step 640: {'loss': 0.656, 'grad_norm': 0.78515625, 'learning_rate': 3.290185720140301e-05, 'num_tokens': 4504071.0, 'mean_token_accuracy': 0.8193088918924332, 'epoch': 2.285969615728329}
Step 641: {'loss': 0.622, 'grad_norm': 0.89453125, 'learning_rate': 3.259431444746846e-05, 'num_tokens': 4510846.0, 'mean_token_accuracy': 0.8292661011219025, 'epoch': 2.289544235924933}
Step 642: {'loss': 0.6023, 'grad_norm': 0.7734375, 'learning_rate': 3.228793569106594e-05, 'num_tokens': 4517508.0, 'mean_token_accuracy': 0.8340977728366852, 'epoch': 2.293118856121537}
Step 643: {'loss': 0.5837, 'grad_norm': 0.7265625, 'learning_rate': 3.198272622290804e-05, 'num_tokens': 4524720.0, 'mean_token_accuracy': 0.8386059701442719, 'epoch': 2.2966934763181412}
Step 644: {'loss': 0.6337, 'grad_norm': 0.765625, 'learning_rate': 3.167869131351569e-05, 'num_tokens': 4531225.0, 'mean_token_accuracy': 0.8248125612735748, 'epoch': 2.3002680965147455}
Step 645: {'loss': 0.5789, 'grad_norm': 0.59375, 'learning_rate': 3.137583621312665e-05, 'num_tokens': 4538964.0, 'mean_token_accuracy': 0.8437094688415527, 'epoch': 2.3038427167113493}
Step 646: {'loss': 0.6239, 'grad_norm': 0.62890625, 'learning_rate': 3.10741661516053e-05, 'num_tokens': 4546357.0, 'mean_token_accuracy': 0.8345252871513367, 'epoch': 2.3074173369079536}
Step 647: {'loss': 0.6422, 'grad_norm': 0.80078125, 'learning_rate': 3.077368633835205e-05, 'num_tokens': 4553341.0, 'mean_token_accuracy': 0.8297390788793564, 'epoch': 2.310991957104558}
Step 648: {'loss': 0.5873, 'grad_norm': 0.63671875, 'learning_rate': 3.0474401962213484e-05, 'num_tokens': 4560046.0, 'mean_token_accuracy': 0.8429664224386215, 'epoch': 2.3145665773011617}
Step 649: {'loss': 0.6285, 'grad_norm': 0.71484375, 'learning_rate': 3.0176318191392726e-05, 'num_tokens': 4566968.0, 'mean_token_accuracy': 0.8331137001514435, 'epoch': 2.318141197497766}
Step 650: {'loss': 0.6116, 'grad_norm': 0.6953125, 'learning_rate': 2.9879440173360228e-05, 'num_tokens': 4573832.0, 'mean_token_accuracy': 0.8348138183355331, 'epoch': 2.32171581769437}
Step 651: {'loss': 0.604, 'grad_norm': 0.79296875, 'learning_rate': 2.9583773034764826e-05, 'num_tokens': 4581123.0, 'mean_token_accuracy': 0.8381848484277725, 'epoch': 2.325290437890974}
Step 652: {'loss': 0.6139, 'grad_norm': 0.6875, 'learning_rate': 2.9289321881345254e-05, 'num_tokens': 4588306.0, 'mean_token_accuracy': 0.8282604813575745, 'epoch': 2.3288650580875783}
Step 653: {'loss': 0.6499, 'grad_norm': 0.72265625, 'learning_rate': 2.8996091797841973e-05, 'num_tokens': 4595384.0, 'mean_token_accuracy': 0.8294756412506104, 'epoch': 2.3324396782841825}
Step 654: {'loss': 0.64, 'grad_norm': 0.765625, 'learning_rate': 2.8704087847909335e-05, 'num_tokens': 4602746.0, 'mean_token_accuracy': 0.8272410333156586, 'epoch': 2.3360142984807863}
Step 655: {'loss': 0.6284, 'grad_norm': 0.70703125, 'learning_rate': 2.8413315074028158e-05, 'num_tokens': 4610137.0, 'mean_token_accuracy': 0.8275638222694397, 'epoch': 2.3395889186773906}
Step 656: {'loss': 0.5966, 'grad_norm': 0.75, 'learning_rate': 2.8123778497418685e-05, 'num_tokens': 4617195.0, 'mean_token_accuracy': 0.8298362493515015, 'epoch': 2.343163538873995}
Step 657: {'loss': 0.6086, 'grad_norm': 0.6796875, 'learning_rate': 2.7835483117953788e-05, 'num_tokens': 4624186.0, 'mean_token_accuracy': 0.8389684706926346, 'epoch': 2.3467381590705987}
Step 658: {'loss': 0.5821, 'grad_norm': 0.6875, 'learning_rate': 2.7548433914072734e-05, 'num_tokens': 4630685.0, 'mean_token_accuracy': 0.8440744578838348, 'epoch': 2.350312779267203}
Step 659: {'loss': 0.6684, 'grad_norm': 0.78125, 'learning_rate': 2.7262635842695127e-05, 'num_tokens': 4637664.0, 'mean_token_accuracy': 0.8236801326274872, 'epoch': 2.353887399463807}
Step 660: {'loss': 0.6553, 'grad_norm': 0.67578125, 'learning_rate': 2.6978093839135364e-05, 'num_tokens': 4644890.0, 'mean_token_accuracy': 0.827091246843338, 'epoch': 2.357462019660411}
Step 661: {'loss': 0.6354, 'grad_norm': 0.73828125, 'learning_rate': 2.669481281701739e-05, 'num_tokens': 4651272.0, 'mean_token_accuracy': 0.827429324388504, 'epoch': 2.3610366398570153}
Step 662: {'loss': 0.6619, 'grad_norm': 0.7421875, 'learning_rate': 2.641279766818977e-05, 'num_tokens': 4658165.0, 'mean_token_accuracy': 0.8187110871076584, 'epoch': 2.3646112600536195}
Step 663: {'loss': 0.6283, 'grad_norm': 0.65625, 'learning_rate': 2.6132053262641466e-05, 'num_tokens': 4666479.0, 'mean_token_accuracy': 0.8262032121419907, 'epoch': 2.3681858802502234}
Step 664: {'loss': 0.6545, 'grad_norm': 0.7890625, 'learning_rate': 2.5852584448417328e-05, 'num_tokens': 4673959.0, 'mean_token_accuracy': 0.8254733830690384, 'epoch': 2.3717605004468276}
Step 665: {'loss': 0.6489, 'grad_norm': 0.7421875, 'learning_rate': 2.5574396051534832e-05, 'num_tokens': 4681417.0, 'mean_token_accuracy': 0.8200570046901703, 'epoch': 2.3753351206434314}
Step 666: {'loss': 0.6024, 'grad_norm': 0.65234375, 'learning_rate': 2.529749287590042e-05, 'num_tokens': 4688783.0, 'mean_token_accuracy': 0.8368875682353973, 'epoch': 2.3789097408400357}
Step 667: {'loss': 0.5623, 'grad_norm': 0.6015625, 'learning_rate': 2.502187970322657e-05, 'num_tokens': 4696465.0, 'mean_token_accuracy': 0.8437209874391556, 'epoch': 2.38248436103664}
Step 668: {'loss': 0.6075, 'grad_norm': 0.7109375, 'learning_rate': 2.4747561292949496e-05, 'num_tokens': 4702967.0, 'mean_token_accuracy': 0.8365833014249802, 'epoch': 2.386058981233244}
Step 669: {'loss': 0.6275, 'grad_norm': 0.67578125, 'learning_rate': 2.4474542382146537e-05, 'num_tokens': 4710419.0, 'mean_token_accuracy': 0.8287703394889832, 'epoch': 2.389633601429848}
Step 670: {'loss': 0.6097, 'grad_norm': 0.734375, 'learning_rate': 2.420282768545469e-05, 'num_tokens': 4717521.0, 'mean_token_accuracy': 0.8320148438215256, 'epoch': 2.3932082216264523}
Step 671: {'loss': 0.6095, 'grad_norm': 0.734375, 'learning_rate': 2.3932421894989167e-05, 'num_tokens': 4724606.0, 'mean_token_accuracy': 0.8350636214017868, 'epoch': 2.396782841823056}
Step 672: {'loss': 0.6661, 'grad_norm': 0.80859375, 'learning_rate': 2.366332968026207e-05, 'num_tokens': 4731278.0, 'mean_token_accuracy': 0.824740007519722, 'epoch': 2.4003574620196604}
Step 673: {'loss': 0.6051, 'grad_norm': 0.7578125, 'learning_rate': 2.339555568810221e-05, 'num_tokens': 4738517.0, 'mean_token_accuracy': 0.8372744023799896, 'epoch': 2.4039320822162646}
Step 674: {'loss': 0.6013, 'grad_norm': 0.73046875, 'learning_rate': 2.312910454257442e-05, 'num_tokens': 4745340.0, 'mean_token_accuracy': 0.8381190001964569, 'epoch': 2.4075067024128685}
Step 675: {'loss': 0.6499, 'grad_norm': 0.734375, 'learning_rate': 2.2863980844900036e-05, 'num_tokens': 4752550.0, 'mean_token_accuracy': 0.8249889761209488, 'epoch': 2.4110813226094727}
Step 676: {'loss': 0.6, 'grad_norm': 0.76171875, 'learning_rate': 2.260018917337726e-05, 'num_tokens': 4759679.0, 'mean_token_accuracy': 0.8366488069295883, 'epoch': 2.414655942806077}
Step 677: {'loss': 0.5754, 'grad_norm': 0.6875, 'learning_rate': 2.2337734083302164e-05, 'num_tokens': 4767288.0, 'mean_token_accuracy': 0.8462183326482773, 'epoch': 2.418230563002681}
Step 678: {'loss': 0.605, 'grad_norm': 0.734375, 'learning_rate': 2.207662010689002e-05, 'num_tokens': 4774329.0, 'mean_token_accuracy': 0.8417876362800598, 'epoch': 2.421805183199285}
Step 679: {'loss': 0.608, 'grad_norm': 0.62109375, 'learning_rate': 2.181685175319702e-05, 'num_tokens': 4781935.0, 'mean_token_accuracy': 0.8347950279712677, 'epoch': 2.4253798033958893}
Step 680: {'loss': 0.6385, 'grad_norm': 0.70703125, 'learning_rate': 2.155843350804243e-05, 'num_tokens': 4788970.0, 'mean_token_accuracy': 0.8316400051116943, 'epoch': 2.428954423592493}
Step 681: {'loss': 0.6552, 'grad_norm': 0.7109375, 'learning_rate': 2.1301369833931117e-05, 'num_tokens': 4795663.0, 'mean_token_accuracy': 0.8283213078975677, 'epoch': 2.4325290437890974}
Step 682: {'loss': 0.5893, 'grad_norm': 0.6328125, 'learning_rate': 2.1045665169976468e-05, 'num_tokens': 4802744.0, 'mean_token_accuracy': 0.8391067087650299, 'epoch': 2.4361036639857017}
Step 683: {'loss': 0.608, 'grad_norm': 0.67578125, 'learning_rate': 2.079132393182378e-05, 'num_tokens': 4809766.0, 'mean_token_accuracy': 0.8348133713006973, 'epoch': 2.4396782841823055}
Step 684: {'loss': 0.6317, 'grad_norm': 0.703125, 'learning_rate': 2.0538350511573968e-05, 'num_tokens': 4816117.0, 'mean_token_accuracy': 0.8277329057455063, 'epoch': 2.4432529043789097}
Step 685: {'loss': 0.6512, 'grad_norm': 0.65625, 'learning_rate': 2.0286749277707782e-05, 'num_tokens': 4824269.0, 'mean_token_accuracy': 0.8172081410884857, 'epoch': 2.446827524575514}
Step 686: {'loss': 0.6281, 'grad_norm': 0.69140625, 'learning_rate': 2.0036524575010172e-05, 'num_tokens': 4831884.0, 'mean_token_accuracy': 0.8336929380893707, 'epoch': 2.450402144772118}
Step 687: {'loss': 0.6504, 'grad_norm': 0.6953125, 'learning_rate': 1.9787680724495617e-05, 'num_tokens': 4838249.0, 'mean_token_accuracy': 0.8224799782037735, 'epoch': 2.453976764968722}
Step 688: {'loss': 0.5936, 'grad_norm': 0.76171875, 'learning_rate': 1.9540222023333166e-05, 'num_tokens': 4844351.0, 'mean_token_accuracy': 0.834945797920227, 'epoch': 2.4575513851653263}
Step 689: {'loss': 0.6405, 'grad_norm': 0.65234375, 'learning_rate': 1.929415274477239e-05, 'num_tokens': 4851073.0, 'mean_token_accuracy': 0.8327791541814804, 'epoch': 2.46112600536193}
Step 690: {'loss': 0.6178, 'grad_norm': 0.75390625, 'learning_rate': 1.9049477138069604e-05, 'num_tokens': 4857544.0, 'mean_token_accuracy': 0.8287771195173264, 'epoch': 2.4647006255585344}
Step 691: {'loss': 0.5778, 'grad_norm': 0.59375, 'learning_rate': 1.880619942841435e-05, 'num_tokens': 4865203.0, 'mean_token_accuracy': 0.8412453234195709, 'epoch': 2.4682752457551387}
Step 692: {'loss': 0.6296, 'grad_norm': 0.7890625, 'learning_rate': 1.8564323816856688e-05, 'num_tokens': 4871735.0, 'mean_token_accuracy': 0.8314765989780426, 'epoch': 2.4718498659517425}
Step 693: {'loss': 0.6627, 'grad_norm': 0.796875, 'learning_rate': 1.832385448023435e-05, 'num_tokens': 4878422.0, 'mean_token_accuracy': 0.8227910548448563, 'epoch': 2.4754244861483468}
Step 694: {'loss': 0.6184, 'grad_norm': 0.76171875, 'learning_rate': 1.808479557110081e-05, 'num_tokens': 4885419.0, 'mean_token_accuracy': 0.8292128592729568, 'epoch': 2.478999106344951}
Step 695: {'loss': 0.6252, 'grad_norm': 0.7109375, 'learning_rate': 1.7847151217653624e-05, 'num_tokens': 4892655.0, 'mean_token_accuracy': 0.8299077302217484, 'epoch': 2.482573726541555}
Step 696: {'loss': 0.6155, 'grad_norm': 0.66015625, 'learning_rate': 1.7610925523662835e-05, 'num_tokens': 4899949.0, 'mean_token_accuracy': 0.8350165486335754, 'epoch': 2.486148346738159}
Step 697: {'loss': 0.6262, 'grad_norm': 0.8046875, 'learning_rate': 1.7376122568400532e-05, 'num_tokens': 4906685.0, 'mean_token_accuracy': 0.8327650874853134, 'epoch': 2.4897229669347634}
Step 698: {'loss': 0.633, 'grad_norm': 0.63671875, 'learning_rate': 1.714274640657001e-05, 'num_tokens': 4913692.0, 'mean_token_accuracy': 0.8255780041217804, 'epoch': 2.493297587131367}
Step 699: {'loss': 0.5998, 'grad_norm': 0.6796875, 'learning_rate': 1.6910801068236016e-05, 'num_tokens': 4920847.0, 'mean_token_accuracy': 0.8355130702257156, 'epoch': 2.4968722073279714}
Step 700: {'loss': 0.5919, 'grad_norm': 0.765625, 'learning_rate': 1.668029055875512e-05, 'num_tokens': 4927939.0, 'mean_token_accuracy': 0.8382171541452408, 'epoch': 2.5004468275245753}
Step 701: {'loss': 0.6331, 'grad_norm': 0.75390625, 'learning_rate': 1.6451218858706374e-05, 'num_tokens': 4934646.0, 'mean_token_accuracy': 0.8273299038410187, 'epoch': 2.5040214477211795}
Step 702: {'loss': 0.5912, 'grad_norm': 0.671875, 'learning_rate': 1.6223589923822767e-05, 'num_tokens': 4941800.0, 'mean_token_accuracy': 0.8466405421495438, 'epoch': 2.5075960679177838}
Step 703: {'loss': 0.6659, 'grad_norm': 0.71875, 'learning_rate': 1.5997407684922862e-05, 'num_tokens': 4948834.0, 'mean_token_accuracy': 0.8144772946834564, 'epoch': 2.5111706881143876}
Step 704: {'loss': 0.5927, 'grad_norm': 0.671875, 'learning_rate': 1.577267604784286e-05, 'num_tokens': 4956200.0, 'mean_token_accuracy': 0.8395044654607773, 'epoch': 2.514745308310992}
Step 705: {'loss': 0.6329, 'grad_norm': 0.73828125, 'learning_rate': 1.5549398893369216e-05, 'num_tokens': 4963544.0, 'mean_token_accuracy': 0.8307614028453827, 'epoch': 2.518319928507596}
Step 706: {'loss': 0.5962, 'grad_norm': 0.66796875, 'learning_rate': 1.5327580077171587e-05, 'num_tokens': 4970456.0, 'mean_token_accuracy': 0.8389472216367722, 'epoch': 2.5218945487042}
Step 707: {'loss': 0.6132, 'grad_norm': 0.6640625, 'learning_rate': 1.5107223429736272e-05, 'num_tokens': 4978148.0, 'mean_token_accuracy': 0.8313870131969452, 'epoch': 2.525469168900804}
Step 708: {'loss': 0.5907, 'grad_norm': 0.73046875, 'learning_rate': 1.4888332756300027e-05, 'num_tokens': 4985199.0, 'mean_token_accuracy': 0.8413964807987213, 'epoch': 2.5290437890974085}
Step 709: {'loss': 0.6038, 'grad_norm': 0.73828125, 'learning_rate': 1.467091183678444e-05, 'num_tokens': 4991638.0, 'mean_token_accuracy': 0.8349540829658508, 'epoch': 2.5326184092940123}
Step 710: {'loss': 0.6518, 'grad_norm': 0.75390625, 'learning_rate': 1.4454964425730533e-05, 'num_tokens': 4998376.0, 'mean_token_accuracy': 0.8298815935850143, 'epoch': 2.5361930294906165}
Step 711: {'loss': 0.6329, 'grad_norm': 0.76953125, 'learning_rate': 1.4240494252234049e-05, 'num_tokens': 5006113.0, 'mean_token_accuracy': 0.8344462066888809, 'epoch': 2.539767649687221}
Step 712: {'loss': 0.6197, 'grad_norm': 0.71484375, 'learning_rate': 1.402750501988097e-05, 'num_tokens': 5012474.0, 'mean_token_accuracy': 0.8344555050134659, 'epoch': 2.5433422698838246}
Step 713: {'loss': 0.6056, 'grad_norm': 0.65625, 'learning_rate': 1.3816000406683604e-05, 'num_tokens': 5019491.0, 'mean_token_accuracy': 0.8319502025842667, 'epoch': 2.546916890080429}
Step 714: {'loss': 0.5933, 'grad_norm': 0.61328125, 'learning_rate': 1.3605984065017074e-05, 'num_tokens': 5026501.0, 'mean_token_accuracy': 0.8419067710638046, 'epoch': 2.550491510277033}
Step 715: {'loss': 0.5899, 'grad_norm': 0.7109375, 'learning_rate': 1.339745962155613e-05, 'num_tokens': 5033675.0, 'mean_token_accuracy': 0.8352816998958588, 'epoch': 2.554066130473637}
Step 716: {'loss': 0.5834, 'grad_norm': 0.75390625, 'learning_rate': 1.3190430677212794e-05, 'num_tokens': 5041126.0, 'mean_token_accuracy': 0.8426182121038437, 'epoch': 2.557640750670241}
Step 717: {'loss': 0.601, 'grad_norm': 0.69921875, 'learning_rate': 1.2984900807073919e-05, 'num_tokens': 5048151.0, 'mean_token_accuracy': 0.8382483571767807, 'epoch': 2.5612153708668455}
Step 718: {'loss': 0.5917, 'grad_norm': 0.72265625, 'learning_rate': 1.2780873560339468e-05, 'num_tokens': 5055625.0, 'mean_token_accuracy': 0.840363621711731, 'epoch': 2.5647899910634493}
Step 719: {'loss': 0.6283, 'grad_norm': 0.6953125, 'learning_rate': 1.2578352460261456e-05, 'num_tokens': 5062775.0, 'mean_token_accuracy': 0.8289330303668976, 'epoch': 2.5683646112600536}
Step 720: {'loss': 0.601, 'grad_norm': 0.6796875, 'learning_rate': 1.2377341004082776e-05, 'num_tokens': 5069594.0, 'mean_token_accuracy': 0.8377665430307388, 'epoch': 2.571939231456658}
Step 721: {'loss': 0.5878, 'grad_norm': 0.74609375, 'learning_rate': 1.2177842662977135e-05, 'num_tokens': 5076877.0, 'mean_token_accuracy': 0.8415777087211609, 'epoch': 2.5755138516532616}
Step 722: {'loss': 0.632, 'grad_norm': 0.78515625, 'learning_rate': 1.1979860881988902e-05, 'num_tokens': 5084225.0, 'mean_token_accuracy': 0.8339696675539017, 'epoch': 2.579088471849866}
Step 723: {'loss': 0.5629, 'grad_norm': 0.7421875, 'learning_rate': 1.1783399079973578e-05, 'num_tokens': 5091046.0, 'mean_token_accuracy': 0.8457188010215759, 'epoch': 2.58266309204647}
Step 724: {'loss': 0.6686, 'grad_norm': 0.7578125, 'learning_rate': 1.1588460649539035e-05, 'num_tokens': 5097778.0, 'mean_token_accuracy': 0.8209723979234695, 'epoch': 2.586237712243074}
Step 725: {'loss': 0.6215, 'grad_norm': 0.671875, 'learning_rate': 1.1395048956986575e-05, 'num_tokens': 5104580.0, 'mean_token_accuracy': 0.8289083242416382, 'epoch': 2.5898123324396782}
Step 726: {'loss': 0.6229, 'grad_norm': 0.71875, 'learning_rate': 1.1203167342253062e-05, 'num_tokens': 5111398.0, 'mean_token_accuracy': 0.837352454662323, 'epoch': 2.5933869526362825}
Step 727: {'loss': 0.6885, 'grad_norm': 0.6953125, 'learning_rate': 1.1012819118853147e-05, 'num_tokens': 5118952.0, 'mean_token_accuracy': 0.810179129242897, 'epoch': 2.5969615728328863}
Step 728: {'loss': 0.5963, 'grad_norm': 0.69921875, 'learning_rate': 1.0824007573822025e-05, 'num_tokens': 5125935.0, 'mean_token_accuracy': 0.8431037813425064, 'epoch': 2.6005361930294906}
Step 729: {'loss': 0.6206, 'grad_norm': 0.78515625, 'learning_rate': 1.0636735967658784e-05, 'num_tokens': 5132679.0, 'mean_token_accuracy': 0.830163836479187, 'epoch': 2.604110813226095}
Step 730: {'loss': 0.6302, 'grad_norm': 0.65625, 'learning_rate': 1.0451007534269907e-05, 'num_tokens': 5140474.0, 'mean_token_accuracy': 0.8263330012559891, 'epoch': 2.6076854334226987}
Step 731: {'loss': 0.6343, 'grad_norm': 0.66015625, 'learning_rate': 1.0266825480913611e-05, 'num_tokens': 5147643.0, 'mean_token_accuracy': 0.8316231817007065, 'epoch': 2.611260053619303}
Step 732: {'loss': 0.617, 'grad_norm': 0.84375, 'learning_rate': 1.0084192988144392e-05, 'num_tokens': 5153909.0, 'mean_token_accuracy': 0.833036258816719, 'epoch': 2.614834673815907}
Step 733: {'loss': 0.6055, 'grad_norm': 0.81640625, 'learning_rate': 9.903113209758096e-06, 'num_tokens': 5160622.0, 'mean_token_accuracy': 0.8382019698619843, 'epoch': 2.618409294012511}
Step 734: {'loss': 0.5678, 'grad_norm': 0.6640625, 'learning_rate': 9.723589272737443e-06, 'num_tokens': 5167470.0, 'mean_token_accuracy': 0.8421511799097061, 'epoch': 2.6219839142091153}
Step 735: {'loss': 0.6295, 'grad_norm': 0.703125, 'learning_rate': 9.545624277198084e-06, 'num_tokens': 5174365.0, 'mean_token_accuracy': 0.836630329489708, 'epoch': 2.6255585344057195}
Step 736: {'loss': 0.626, 'grad_norm': 0.78125, 'learning_rate': 9.369221296335006e-06, 'num_tokens': 5180863.0, 'mean_token_accuracy': 0.8315001279115677, 'epoch': 2.6291331546023233}
Step 737: {'loss': 0.6138, 'grad_norm': 0.734375, 'learning_rate': 9.194383376369508e-06, 'num_tokens': 5188015.0, 'mean_token_accuracy': 0.8384783267974854, 'epoch': 2.6327077747989276}
Step 738: {'loss': 0.6243, 'grad_norm': 0.72265625, 'learning_rate': 9.02111353649655e-06, 'num_tokens': 5194844.0, 'mean_token_accuracy': 0.8371545970439911, 'epoch': 2.636282394995532}
Step 739: {'loss': 0.6209, 'grad_norm': 0.70703125, 'learning_rate': 8.849414768832687e-06, 'num_tokens': 5201965.0, 'mean_token_accuracy': 0.8341609090566635, 'epoch': 2.6398570151921357}
Step 740: {'loss': 0.6015, 'grad_norm': 0.8046875, 'learning_rate': 8.679290038364319e-06, 'num_tokens': 5208152.0, 'mean_token_accuracy': 0.8396148383617401, 'epoch': 2.64343163538874}
Step 741: {'loss': 0.64, 'grad_norm': 0.66015625, 'learning_rate': 8.510742282896544e-06, 'num_tokens': 5215293.0, 'mean_token_accuracy': 0.8296355903148651, 'epoch': 2.647006255585344}
Step 742: {'loss': 0.6256, 'grad_norm': 0.671875, 'learning_rate': 8.343774413002381e-06, 'num_tokens': 5222560.0, 'mean_token_accuracy': 0.8301732689142227, 'epoch': 2.650580875781948}
Step 743: {'loss': 0.6256, 'grad_norm': 0.80078125, 'learning_rate': 8.178389311972612e-06, 'num_tokens': 5229729.0, 'mean_token_accuracy': 0.8296895176172256, 'epoch': 2.6541554959785523}
Step 744: {'loss': 0.5637, 'grad_norm': 0.71875, 'learning_rate': 8.014589835765806e-06, 'num_tokens': 5236526.0, 'mean_token_accuracy': 0.8504170477390289, 'epoch': 2.6577301161751565}
Step 745: {'loss': 0.5833, 'grad_norm': 0.67578125, 'learning_rate': 7.852378812959227e-06, 'num_tokens': 5244265.0, 'mean_token_accuracy': 0.8354266285896301, 'epoch': 2.6613047363717603}
Step 746: {'loss': 0.5962, 'grad_norm': 0.80078125, 'learning_rate': 7.69175904469982e-06, 'num_tokens': 5250606.0, 'mean_token_accuracy': 0.8420100212097168, 'epoch': 2.6648793565683646}
Step 747: {'loss': 0.6246, 'grad_norm': 0.70703125, 'learning_rate': 7.532733304655848e-06, 'num_tokens': 5257662.0, 'mean_token_accuracy': 0.8325654119253159, 'epoch': 2.668453976764969}
Step 748: {'loss': 0.6098, 'grad_norm': 0.66015625, 'learning_rate': 7.375304338969136e-06, 'num_tokens': 5264793.0, 'mean_token_accuracy': 0.8356545716524124, 'epoch': 2.6720285969615727}
Step 749: {'loss': 0.6493, 'grad_norm': 0.77734375, 'learning_rate': 7.219474866207465e-06, 'num_tokens': 5272000.0, 'mean_token_accuracy': 0.8288295120000839, 'epoch': 2.675603217158177}
Step 750: {'loss': 0.6014, 'grad_norm': 0.66015625, 'learning_rate': 7.0652475773177464e-06, 'num_tokens': 5279118.0, 'mean_token_accuracy': 0.8387552946805954, 'epoch': 2.679177837354781}
Step 751: {'loss': 0.63, 'grad_norm': 0.7265625, 'learning_rate': 6.9126251355795864e-06, 'num_tokens': 5286065.0, 'mean_token_accuracy': 0.8243020623922348, 'epoch': 2.682752457551385}
Step 752: {'loss': 0.5851, 'grad_norm': 0.71875, 'learning_rate': 6.761610176559085e-06, 'num_tokens': 5292657.0, 'mean_token_accuracy': 0.842752680182457, 'epoch': 2.6863270777479893}
Step 753: {'loss': 0.5818, 'grad_norm': 0.63671875, 'learning_rate': 6.612205308063646e-06, 'num_tokens': 5300580.0, 'mean_token_accuracy': 0.8418648391962051, 'epoch': 2.6899016979445936}
Step 754: {'loss': 0.5612, 'grad_norm': 0.7265625, 'learning_rate': 6.464413110096601e-06, 'num_tokens': 5307543.0, 'mean_token_accuracy': 0.8508715480566025, 'epoch': 2.6934763181411974}
Step 755: {'loss': 0.6184, 'grad_norm': 0.73828125, 'learning_rate': 6.318236134812916e-06, 'num_tokens': 5313945.0, 'mean_token_accuracy': 0.8376858234405518, 'epoch': 2.6970509383378016}
Step 756: {'loss': 0.5849, 'grad_norm': 0.640625, 'learning_rate': 6.173676906475012e-06, 'num_tokens': 5321131.0, 'mean_token_accuracy': 0.8418890982866287, 'epoch': 2.700625558534406}
Step 757: {'loss': 0.5797, 'grad_norm': 0.7734375, 'learning_rate': 6.030737921409169e-06, 'num_tokens': 5327641.0, 'mean_token_accuracy': 0.8370855450630188, 'epoch': 2.7042001787310097}
Step 758: {'loss': 0.5798, 'grad_norm': 0.6640625, 'learning_rate': 5.889421647962457e-06, 'num_tokens': 5334787.0, 'mean_token_accuracy': 0.8357532173395157, 'epoch': 2.707774798927614}
Step 759: {'loss': 0.6073, 'grad_norm': 0.73828125, 'learning_rate': 5.749730526460073e-06, 'num_tokens': 5342207.0, 'mean_token_accuracy': 0.8322588801383972, 'epoch': 2.7113494191242182}
Step 760: {'loss': 0.6454, 'grad_norm': 0.6953125, 'learning_rate': 5.611666969163243e-06, 'num_tokens': 5349302.0, 'mean_token_accuracy': 0.823619470000267, 'epoch': 2.714924039320822}
Step 761: {'loss': 0.6154, 'grad_norm': 0.6015625, 'learning_rate': 5.475233360227516e-06, 'num_tokens': 5356438.0, 'mean_token_accuracy': 0.8297262042760849, 'epoch': 2.7184986595174263}
Step 762: {'loss': 0.6138, 'grad_norm': 0.62890625, 'learning_rate': 5.340432055661637e-06, 'num_tokens': 5363332.0, 'mean_token_accuracy': 0.831213653087616, 'epoch': 2.7220732797140306}
Step 763: {'loss': 0.63, 'grad_norm': 0.69140625, 'learning_rate': 5.20726538328683e-06, 'num_tokens': 5370234.0, 'mean_token_accuracy': 0.8309499472379684, 'epoch': 2.7256478999106344}
Step 764: {'loss': 0.5991, 'grad_norm': 0.65234375, 'learning_rate': 5.075735642696611e-06, 'num_tokens': 5376960.0, 'mean_token_accuracy': 0.8348791301250458, 'epoch': 2.7292225201072386}
Step 765: {'loss': 0.6034, 'grad_norm': 0.6796875, 'learning_rate': 4.945845105217117e-06, 'num_tokens': 5384543.0, 'mean_token_accuracy': 0.8315974324941635, 'epoch': 2.732797140303843}
Step 766: {'loss': 0.6103, 'grad_norm': 0.6640625, 'learning_rate': 4.817596013867764e-06, 'num_tokens': 5391717.0, 'mean_token_accuracy': 0.8329445719718933, 'epoch': 2.7363717605004467}
Step 767: {'loss': 0.604, 'grad_norm': 0.71875, 'learning_rate': 4.6909905833226966e-06, 'num_tokens': 5398144.0, 'mean_token_accuracy': 0.8325253278017044, 'epoch': 2.739946380697051}
Step 768: {'loss': 0.6239, 'grad_norm': 0.84375, 'learning_rate': 4.566030999872384e-06, 'num_tokens': 5405330.0, 'mean_token_accuracy': 0.8272086530923843, 'epoch': 2.7435210008936552}
Step 769: {'loss': 0.6074, 'grad_norm': 0.67578125, 'learning_rate': 4.442719421385922e-06, 'num_tokens': 5413266.0, 'mean_token_accuracy': 0.8323387503623962, 'epoch': 2.747095621090259}
Step 770: {'loss': 0.6009, 'grad_norm': 0.7421875, 'learning_rate': 4.321057977273823e-06, 'num_tokens': 5420473.0, 'mean_token_accuracy': 0.835925817489624, 'epoch': 2.7506702412868633}
Step 771: {'loss': 0.6303, 'grad_norm': 0.73828125, 'learning_rate': 4.20104876845111e-06, 'num_tokens': 5427517.0, 'mean_token_accuracy': 0.8319149762392044, 'epoch': 2.7542448614834676}
Step 772: {'loss': 0.5829, 'grad_norm': 0.71484375, 'learning_rate': 4.082693867301224e-06, 'num_tokens': 5434821.0, 'mean_token_accuracy': 0.8377025723457336, 'epoch': 2.7578194816800714}
Step 773: {'loss': 0.6337, 'grad_norm': 0.71484375, 'learning_rate': 3.965995317640025e-06, 'num_tokens': 5441836.0, 'mean_token_accuracy': 0.8279736191034317, 'epoch': 2.7613941018766757}
Step 774: {'loss': 0.5884, 'grad_norm': 0.64453125, 'learning_rate': 3.850955134680678e-06, 'num_tokens': 5449439.0, 'mean_token_accuracy': 0.8301761001348495, 'epoch': 2.76496872207328}
Step 775: {'loss': 0.6134, 'grad_norm': 0.8359375, 'learning_rate': 3.7375753049987973e-06, 'num_tokens': 5455972.0, 'mean_token_accuracy': 0.8381270468235016, 'epoch': 2.7685433422698837}
Step 776: {'loss': 0.6409, 'grad_norm': 0.8359375, 'learning_rate': 3.625857786498055e-06, 'num_tokens': 5462760.0, 'mean_token_accuracy': 0.8275736272335052, 'epoch': 2.772117962466488}
Step 777: {'loss': 0.6465, 'grad_norm': 0.703125, 'learning_rate': 3.515804508376508e-06, 'num_tokens': 5469741.0, 'mean_token_accuracy': 0.8294682055711746, 'epoch': 2.7756925826630923}
Step 778: {'loss': 0.5756, 'grad_norm': 0.8828125, 'learning_rate': 3.40741737109318e-06, 'num_tokens': 5476889.0, 'mean_token_accuracy': 0.8408986330032349, 'epoch': 2.779267202859696}
Step 779: {'loss': 0.6506, 'grad_norm': 0.77734375, 'learning_rate': 3.3006982463352766e-06, 'num_tokens': 5484069.0, 'mean_token_accuracy': 0.8198964148759842, 'epoch': 2.7828418230563003}
Step 780: {'loss': 0.5675, 'grad_norm': 0.625, 'learning_rate': 3.195648976985921e-06, 'num_tokens': 5491348.0, 'mean_token_accuracy': 0.8503882437944412, 'epoch': 2.7864164432529046}
Step 781: {'loss': 0.6835, 'grad_norm': 0.75, 'learning_rate': 3.092271377092215e-06, 'num_tokens': 5498215.0, 'mean_token_accuracy': 0.8167990744113922, 'epoch': 2.7899910634495084}
Step 782: {'loss': 0.6438, 'grad_norm': 0.78125, 'learning_rate': 2.9905672318339963e-06, 'num_tokens': 5504923.0, 'mean_token_accuracy': 0.8260563760995865, 'epoch': 2.7935656836461127}
Step 783: {'loss': 0.5885, 'grad_norm': 0.82421875, 'learning_rate': 2.8905382974930172e-06, 'num_tokens': 5511766.0, 'mean_token_accuracy': 0.8384526073932648, 'epoch': 2.797140303842717}
Step 784: {'loss': 0.5997, 'grad_norm': 0.75390625, 'learning_rate': 2.7921863014225503e-06, 'num_tokens': 5518191.0, 'mean_token_accuracy': 0.8389955312013626, 'epoch': 2.8007149240393208}
Step 785: {'loss': 0.6118, 'grad_norm': 0.765625, 'learning_rate': 2.6955129420176196e-06, 'num_tokens': 5525423.0, 'mean_token_accuracy': 0.8413934260606766, 'epoch': 2.804289544235925}
Step 786: {'loss': 0.5956, 'grad_norm': 0.7421875, 'learning_rate': 2.6005198886856487e-06, 'num_tokens': 5531977.0, 'mean_token_accuracy': 0.8409542888402939, 'epoch': 2.8078641644325293}
Step 787: {'loss': 0.6454, 'grad_norm': 0.70703125, 'learning_rate': 2.5072087818176382e-06, 'num_tokens': 5538802.0, 'mean_token_accuracy': 0.8269369304180145, 'epoch': 2.811438784629133}
Step 788: {'loss': 0.6542, 'grad_norm': 0.83984375, 'learning_rate': 2.4155812327598338e-06, 'num_tokens': 5545931.0, 'mean_token_accuracy': 0.8232912868261337, 'epoch': 2.8150134048257374}
Step 789: {'loss': 0.5944, 'grad_norm': 0.6796875, 'learning_rate': 2.3256388237858807e-06, 'num_tokens': 5553292.0, 'mean_token_accuracy': 0.8362542688846588, 'epoch': 2.8185880250223416}
Step 790: {'loss': 0.6119, 'grad_norm': 0.75, 'learning_rate': 2.237383108069546e-06, 'num_tokens': 5560357.0, 'mean_token_accuracy': 0.8317124247550964, 'epoch': 2.8221626452189454}
Step 791: {'loss': 0.5917, 'grad_norm': 0.69140625, 'learning_rate': 2.150815609657875e-06, 'num_tokens': 5567231.0, 'mean_token_accuracy': 0.839521735906601, 'epoch': 2.8257372654155497}
Step 792: {'loss': 0.6162, 'grad_norm': 0.671875, 'learning_rate': 2.0659378234448525e-06, 'num_tokens': 5574578.0, 'mean_token_accuracy': 0.8342411369085312, 'epoch': 2.829311885612154}
Step 793: {'loss': 0.6129, 'grad_norm': 0.65625, 'learning_rate': 1.9827512151456173e-06, 'num_tokens': 5581618.0, 'mean_token_accuracy': 0.8309946805238724, 'epoch': 2.832886505808758}
Step 794: {'loss': 0.6589, 'grad_norm': 0.83984375, 'learning_rate': 1.9012572212711465e-06, 'num_tokens': 5588589.0, 'mean_token_accuracy': 0.8197690844535828, 'epoch': 2.836461126005362}
Step 795: {'loss': 0.6314, 'grad_norm': 0.78125, 'learning_rate': 1.8214572491034198e-06, 'num_tokens': 5595372.0, 'mean_token_accuracy': 0.8282099664211273, 'epoch': 2.8400357462019663}
Step 796: {'loss': 0.6203, 'grad_norm': 0.75, 'learning_rate': 1.7433526766711728e-06, 'num_tokens': 5601989.0, 'mean_token_accuracy': 0.8358150571584702, 'epoch': 2.84361036639857}
Step 797: {'loss': 0.6105, 'grad_norm': 0.765625, 'learning_rate': 1.66694485272606e-06, 'num_tokens': 5609129.0, 'mean_token_accuracy': 0.8330391347408295, 'epoch': 2.8471849865951744}
Step 798: {'loss': 0.5987, 'grad_norm': 0.6875, 'learning_rate': 1.5922350967193523e-06, 'num_tokens': 5615965.0, 'mean_token_accuracy': 0.8342199623584747, 'epoch': 2.8507596067917786}
Step 799: {'loss': 0.6065, 'grad_norm': 0.71875, 'learning_rate': 1.5192246987791981e-06, 'num_tokens': 5622926.0, 'mean_token_accuracy': 0.8359124809503555, 'epoch': 2.8543342269883825}
Step 800: {'loss': 0.6152, 'grad_norm': 0.68359375, 'learning_rate': 1.4479149196882979e-06, 'num_tokens': 5630128.0, 'mean_token_accuracy': 0.8352220803499222, 'epoch': 2.8579088471849867}
Step 801: {'loss': 0.6119, 'grad_norm': 0.72265625, 'learning_rate': 1.378306990862177e-06, 'num_tokens': 5637326.0, 'mean_token_accuracy': 0.8352862894535065, 'epoch': 2.8614834673815905}
Step 802: {'loss': 0.6394, 'grad_norm': 0.75, 'learning_rate': 1.3104021143278911e-06, 'num_tokens': 5644055.0, 'mean_token_accuracy': 0.8276674747467041, 'epoch': 2.865058087578195}
Step 803: {'loss': 0.6232, 'grad_norm': 0.625, 'learning_rate': 1.2442014627032316e-06, 'num_tokens': 5651378.0, 'mean_token_accuracy': 0.8257108181715012, 'epoch': 2.868632707774799}
Step 804: {'loss': 0.653, 'grad_norm': 0.75390625, 'learning_rate': 1.1797061791766205e-06, 'num_tokens': 5658661.0, 'mean_token_accuracy': 0.8279244601726532, 'epoch': 2.872207327971403}
Step 805: {'loss': 0.6377, 'grad_norm': 0.69140625, 'learning_rate': 1.1169173774871478e-06, 'num_tokens': 5666033.0, 'mean_token_accuracy': 0.8297932595014572, 'epoch': 2.875781948168007}
Step 806: {'loss': 0.6239, 'grad_norm': 0.59375, 'learning_rate': 1.055836141905553e-06, 'num_tokens': 5673450.0, 'mean_token_accuracy': 0.8276805132627487, 'epoch': 2.8793565683646114}
Step 807: {'loss': 0.6327, 'grad_norm': 0.78515625, 'learning_rate': 9.964635272153633e-07, 'num_tokens': 5680431.0, 'mean_token_accuracy': 0.8250585794448853, 'epoch': 2.882931188561215}
Step 808: {'loss': 0.6342, 'grad_norm': 0.7421875, 'learning_rate': 9.388005586947191e-07, 'num_tokens': 5687557.0, 'mean_token_accuracy': 0.8280980288982391, 'epoch': 2.8865058087578195}
Step 809: {'loss': 0.6029, 'grad_norm': 0.76953125, 'learning_rate': 8.828482320987319e-07, 'num_tokens': 5694354.0, 'mean_token_accuracy': 0.8351132124662399, 'epoch': 2.8900804289544237}
Step 810: {'loss': 0.6363, 'grad_norm': 0.671875, 'learning_rate': 8.286075136421434e-07, 'num_tokens': 5701834.0, 'mean_token_accuracy': 0.8305511474609375, 'epoch': 2.8936550491510276}
Step 811: {'loss': 0.6002, 'grad_norm': 1.0859375, 'learning_rate': 7.760793399827937e-07, 'num_tokens': 5709182.0, 'mean_token_accuracy': 0.8382600992918015, 'epoch': 2.897229669347632}
Step 812: {'loss': 0.6402, 'grad_norm': 0.7109375, 'learning_rate': 7.252646182053568e-07, 'num_tokens': 5716094.0, 'mean_token_accuracy': 0.8213381767272949, 'epoch': 2.900804289544236}
Step 813: {'loss': 0.5972, 'grad_norm': 0.61328125, 'learning_rate': 6.761642258056978e-07, 'num_tokens': 5723096.0, 'mean_token_accuracy': 0.8355346024036407, 'epoch': 2.90437890974084}
Step 814: {'loss': 0.6129, 'grad_norm': 0.66015625, 'learning_rate': 6.287790106757396e-07, 'num_tokens': 5730677.0, 'mean_token_accuracy': 0.8344118595123291, 'epoch': 2.907953529937444}
Step 815: {'loss': 0.5951, 'grad_norm': 0.68359375, 'learning_rate': 5.831097910887873e-07, 'num_tokens': 5737415.0, 'mean_token_accuracy': 0.8421041369438171, 'epoch': 2.9115281501340484}
Step 816: {'loss': 0.5727, 'grad_norm': 0.7578125, 'learning_rate': 5.391573556854157e-07, 'num_tokens': 5744081.0, 'mean_token_accuracy': 0.8423478007316589, 'epoch': 2.9151027703306522}
Step 817: {'loss': 0.6608, 'grad_norm': 0.8828125, 'learning_rate': 4.969224634598591e-07, 'num_tokens': 5750178.0, 'mean_token_accuracy': 0.825874537229538, 'epoch': 2.9186773905272565}
Step 818: {'loss': 0.6203, 'grad_norm': 0.67578125, 'learning_rate': 4.5640584374688766e-07, 'num_tokens': 5757505.0, 'mean_token_accuracy': 0.8349693268537521, 'epoch': 2.9222520107238603}
Step 819: {'loss': 0.5955, 'grad_norm': 0.68359375, 'learning_rate': 4.176081962092182e-07, 'num_tokens': 5764281.0, 'mean_token_accuracy': 0.839116245508194, 'epoch': 2.9258266309204646}
Step 820: {'loss': 0.6419, 'grad_norm': 0.8515625, 'learning_rate': 3.805301908254455e-07, 'num_tokens': 5771132.0, 'mean_token_accuracy': 0.8313606828451157, 'epoch': 2.929401251117069}
Step 821: {'loss': 0.6295, 'grad_norm': 0.70703125, 'learning_rate': 3.451724678784518e-07, 'num_tokens': 5777976.0, 'mean_token_accuracy': 0.8273409754037857, 'epoch': 2.9329758713136727}
Step 822: {'loss': 0.627, 'grad_norm': 0.65234375, 'learning_rate': 3.1153563794436015e-07, 'num_tokens': 5785392.0, 'mean_token_accuracy': 0.828990250825882, 'epoch': 2.936550491510277}
Step 823: {'loss': 0.6389, 'grad_norm': 0.64453125, 'learning_rate': 2.7962028188198706e-07, 'num_tokens': 5792490.0, 'mean_token_accuracy': 0.8266888111829758, 'epoch': 2.940125111706881}
Step 824: {'loss': 0.613, 'grad_norm': 0.6953125, 'learning_rate': 2.494269508228175e-07, 'num_tokens': 5799077.0, 'mean_token_accuracy': 0.8333218395709991, 'epoch': 2.943699731903485}
Step 825: {'loss': 0.6406, 'grad_norm': 0.671875, 'learning_rate': 2.2095616616150115e-07, 'num_tokens': 5806327.0, 'mean_token_accuracy': 0.8279467672109604, 'epoch': 2.9472743521000893}
Step 826: {'loss': 0.6897, 'grad_norm': 0.7734375, 'learning_rate': 1.9420841954681525e-07, 'num_tokens': 5813476.0, 'mean_token_accuracy': 0.8155946433544159, 'epoch': 2.9508489722966935}
Step 827: {'loss': 0.6517, 'grad_norm': 0.69140625, 'learning_rate': 1.6918417287318245e-07, 'num_tokens': 5820584.0, 'mean_token_accuracy': 0.8242767453193665, 'epoch': 2.9544235924932973}
Step 828: {'loss': 0.565, 'grad_norm': 0.69140625, 'learning_rate': 1.4588385827272176e-07, 'num_tokens': 5828091.0, 'mean_token_accuracy': 0.8428808003664017, 'epoch': 2.9579982126899016}
Step 829: {'loss': 0.604, 'grad_norm': 0.6953125, 'learning_rate': 1.2430787810776555e-07, 'num_tokens': 5835809.0, 'mean_token_accuracy': 0.8280133903026581, 'epoch': 2.961572832886506}
Step 830: {'loss': 0.6266, 'grad_norm': 0.74609375, 'learning_rate': 1.0445660496390952e-07, 'num_tokens': 5843502.0, 'mean_token_accuracy': 0.8279871344566345, 'epoch': 2.9651474530831097}
Step 831: {'loss': 0.6188, 'grad_norm': 0.76953125, 'learning_rate': 8.633038164358454e-08, 'num_tokens': 5850404.0, 'mean_token_accuracy': 0.8279399126768112, 'epoch': 2.968722073279714}
Step 832: {'loss': 0.6186, 'grad_norm': 0.71875, 'learning_rate': 6.992952116013918e-08, 'num_tokens': 5857868.0, 'mean_token_accuracy': 0.8335038125514984, 'epoch': 2.972296693476318}
Step 833: {'loss': 0.6547, 'grad_norm': 0.73828125, 'learning_rate': 5.5254306732444025e-08, 'num_tokens': 5864732.0, 'mean_token_accuracy': 0.826517328619957, 'epoch': 2.975871313672922}
Step 834: {'loss': 0.6384, 'grad_norm': 0.73828125, 'learning_rate': 4.230499177994007e-08, 'num_tokens': 5871525.0, 'mean_token_accuracy': 0.827747106552124, 'epoch': 2.9794459338695263}
Step 835: {'loss': 0.6149, 'grad_norm': 0.90625, 'learning_rate': 3.1081799918375454e-08, 'num_tokens': 5877864.0, 'mean_token_accuracy': 0.8315642476081848, 'epoch': 2.9830205540661305}
Step 836: {'loss': 0.6057, 'grad_norm': 0.6171875, 'learning_rate': 2.1584924955819764e-08, 'num_tokens': 5885103.0, 'mean_token_accuracy': 0.8398723006248474, 'epoch': 2.9865951742627344}
Step 837: {'loss': 0.5952, 'grad_norm': 0.7109375, 'learning_rate': 1.3814530889433296e-08, 'num_tokens': 5891965.0, 'mean_token_accuracy': 0.8358792066574097, 'epoch': 2.9901697944593386}
Step 838: {'loss': 0.6176, 'grad_norm': 0.72265625, 'learning_rate': 7.770751902513862e-09, 'num_tokens': 5899182.0, 'mean_token_accuracy': 0.8340379446744919, 'epoch': 2.993744414655943}
Step 839: {'loss': 0.6106, 'grad_norm': 0.68359375, 'learning_rate': 3.453692362309635e-09, 'num_tokens': 5906063.0, 'mean_token_accuracy': 0.8379215896129608, 'epoch': 2.9973190348525467}
Step 840: {'loss': 0.4958, 'grad_norm': 0.76171875, 'learning_rate': 8.634268181095806e-10, 'num_tokens': 5910135.0, 'mean_token_accuracy': 0.8223099708557129, 'epoch': 3.0}
Step 840: {'train_runtime': 3045.2782, 'train_samples_per_second': 8.814, 'train_steps_per_second': 0.276, 'total_flos': 0.0, 'train_loss': 0.6812248661759354, 'epoch': 3.0}
