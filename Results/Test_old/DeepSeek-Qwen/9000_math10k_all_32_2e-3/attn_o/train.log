Step 1: {'loss': 0.0026, 'grad_norm': 0.0027923583984375, 'learning_rate': 0.0, 'epoch': 0.0071111111111111115}
Step 2: {'loss': 0.0041, 'grad_norm': 0.0023345947265625, 'learning_rate': 4.651162790697674e-05, 'epoch': 0.014222222222222223}
Step 3: {'loss': 0.0073, 'grad_norm': 0.0052490234375, 'learning_rate': 9.302325581395348e-05, 'epoch': 0.021333333333333333}
Step 4: {'loss': 0.0074, 'grad_norm': 0.006134033203125, 'learning_rate': 0.00013953488372093022, 'epoch': 0.028444444444444446}
Step 5: {'loss': 0.0068, 'grad_norm': 0.00628662109375, 'learning_rate': 0.00018604651162790697, 'epoch': 0.035555555555555556}
Step 6: {'loss': 0.0047, 'grad_norm': 0.004302978515625, 'learning_rate': 0.00023255813953488373, 'epoch': 0.042666666666666665}
Step 7: {'loss': 0.009, 'grad_norm': 0.0069580078125, 'learning_rate': 0.00027906976744186045, 'epoch': 0.049777777777777775}
Step 8: {'loss': 0.0079, 'grad_norm': 0.008056640625, 'learning_rate': 0.00032558139534883724, 'epoch': 0.05688888888888889}
Step 9: {'loss': 0.0105, 'grad_norm': 0.01092529296875, 'learning_rate': 0.00037209302325581393, 'epoch': 0.064}
Step 10: {'loss': 0.0073, 'grad_norm': 0.005462646484375, 'learning_rate': 0.0004186046511627907, 'epoch': 0.07111111111111111}
Step 11: {'loss': 0.0103, 'grad_norm': 0.00927734375, 'learning_rate': 0.00046511627906976747, 'epoch': 0.07822222222222222}
Step 12: {'loss': 0.0051, 'grad_norm': 0.004669189453125, 'learning_rate': 0.0005116279069767442, 'epoch': 0.08533333333333333}
Step 13: {'loss': 0.0057, 'grad_norm': 0.004608154296875, 'learning_rate': 0.0005581395348837209, 'epoch': 0.09244444444444444}
Step 14: {'loss': 0.0052, 'grad_norm': 0.004302978515625, 'learning_rate': 0.0006046511627906977, 'epoch': 0.09955555555555555}
Step 15: {'loss': 0.0054, 'grad_norm': 0.005828857421875, 'learning_rate': 0.0006511627906976745, 'epoch': 0.10666666666666667}
Step 16: {'loss': 0.0054, 'grad_norm': 0.005828857421875, 'learning_rate': 0.0006976744186046512, 'epoch': 0.11377777777777778}
Step 17: {'loss': 0.0049, 'grad_norm': 0.0059814453125, 'learning_rate': 0.0007441860465116279, 'epoch': 0.12088888888888889}
Step 18: {'loss': 0.0056, 'grad_norm': 0.006988525390625, 'learning_rate': 0.0007906976744186047, 'epoch': 0.128}
Step 19: {'loss': 0.0042, 'grad_norm': 0.005584716796875, 'learning_rate': 0.0008372093023255815, 'epoch': 0.1351111111111111}
Step 20: {'loss': 0.0061, 'grad_norm': 0.00732421875, 'learning_rate': 0.0008837209302325581, 'epoch': 0.14222222222222222}
Step 21: {'loss': 0.0026, 'grad_norm': 0.0032196044921875, 'learning_rate': 0.0009302325581395349, 'epoch': 0.14933333333333335}
Step 22: {'loss': 0.0026, 'grad_norm': 0.0029144287109375, 'learning_rate': 0.0009767441860465116, 'epoch': 0.15644444444444444}
Step 23: {'loss': 0.003, 'grad_norm': 0.004180908203125, 'learning_rate': 0.0010232558139534884, 'epoch': 0.16355555555555557}
Step 24: {'loss': 0.0014, 'grad_norm': 0.00060272216796875, 'learning_rate': 0.001069767441860465, 'epoch': 0.17066666666666666}
Step 25: {'loss': 0.0021, 'grad_norm': 0.0019683837890625, 'learning_rate': 0.0011162790697674418, 'epoch': 0.17777777777777778}
Step 26: {'loss': 0.0019, 'grad_norm': 0.0027923583984375, 'learning_rate': 0.0011627906976744186, 'epoch': 0.18488888888888888}
Step 27: {'loss': 0.0018, 'grad_norm': 0.00142669677734375, 'learning_rate': 0.0012093023255813954, 'epoch': 0.192}
Step 28: {'loss': 0.0017, 'grad_norm': 0.001007080078125, 'learning_rate': 0.0012558139534883722, 'epoch': 0.1991111111111111}
Step 29: {'loss': 0.0016, 'grad_norm': 0.0012359619140625, 'learning_rate': 0.001302325581395349, 'epoch': 0.20622222222222222}
Step 30: {'loss': 0.0014, 'grad_norm': 0.00048065185546875, 'learning_rate': 0.0013488372093023256, 'epoch': 0.21333333333333335}
Step 31: {'loss': 0.0014, 'grad_norm': 0.0003795623779296875, 'learning_rate': 0.0013953488372093023, 'epoch': 0.22044444444444444}
Step 32: {'loss': 0.0015, 'grad_norm': 0.00070953369140625, 'learning_rate': 0.001441860465116279, 'epoch': 0.22755555555555557}
Step 33: {'loss': 0.0016, 'grad_norm': 0.00078582763671875, 'learning_rate': 0.0014883720930232557, 'epoch': 0.23466666666666666}
Step 34: {'loss': 0.0014, 'grad_norm': 0.0003414154052734375, 'learning_rate': 0.0015348837209302327, 'epoch': 0.24177777777777779}
Step 35: {'loss': 0.0015, 'grad_norm': 0.000446319580078125, 'learning_rate': 0.0015813953488372093, 'epoch': 0.24888888888888888}
Step 36: {'loss': 0.0015, 'grad_norm': 0.0003032684326171875, 'learning_rate': 0.0016279069767441861, 'epoch': 0.256}
Step 37: {'loss': 0.0013, 'grad_norm': 0.00030517578125, 'learning_rate': 0.001674418604651163, 'epoch': 0.26311111111111113}
Step 38: {'loss': 0.0014, 'grad_norm': 0.00025177001953125, 'learning_rate': 0.0017209302325581395, 'epoch': 0.2702222222222222}
Step 39: {'loss': 0.0014, 'grad_norm': 0.000308990478515625, 'learning_rate': 0.0017674418604651163, 'epoch': 0.2773333333333333}
Step 40: {'loss': 0.0013, 'grad_norm': 0.0002651214599609375, 'learning_rate': 0.0018139534883720929, 'epoch': 0.28444444444444444}
Step 41: {'loss': 0.0013, 'grad_norm': 0.0003032684326171875, 'learning_rate': 0.0018604651162790699, 'epoch': 0.29155555555555557}
Step 42: {'loss': 0.0012, 'grad_norm': 0.0002346038818359375, 'learning_rate': 0.0019069767441860467, 'epoch': 0.2986666666666667}
Step 43: {'loss': 0.0012, 'grad_norm': 0.00021648406982421875, 'learning_rate': 0.0019534883720930232, 'epoch': 0.30577777777777776}
Step 44: {'loss': 0.0013, 'grad_norm': 0.0002117156982421875, 'learning_rate': 0.002, 'epoch': 0.3128888888888889}
Step 45: {'loss': 0.0014, 'grad_norm': 0.00023555755615234375, 'learning_rate': 0.0019999658256641744, 'epoch': 0.32}
Step 46: {'loss': 0.0012, 'grad_norm': 0.000213623046875, 'learning_rate': 0.001999863304992469, 'epoch': 0.32711111111111113}
Step 47: {'loss': 0.0013, 'grad_norm': 0.00022029876708984375, 'learning_rate': 0.001999692444992035, 'epoch': 0.3342222222222222}
Step 48: {'loss': 0.0014, 'grad_norm': 0.00022983551025390625, 'learning_rate': 0.001999453257340926, 'epoch': 0.3413333333333333}
Step 49: {'loss': 0.0013, 'grad_norm': 0.00023174285888671875, 'learning_rate': 0.001999145758387301, 'epoch': 0.34844444444444445}
Step 50: {'loss': 0.0012, 'grad_norm': 0.0002346038818359375, 'learning_rate': 0.0019987699691483046, 'epoch': 0.35555555555555557}
Step 51: {'loss': 0.0013, 'grad_norm': 0.00020599365234375, 'learning_rate': 0.0019983259153086327, 'epoch': 0.3626666666666667}
Step 52: {'loss': 0.0014, 'grad_norm': 0.00021839141845703125, 'learning_rate': 0.0019978136272187745, 'epoch': 0.36977777777777776}
Step 53: {'loss': 0.0012, 'grad_norm': 0.000217437744140625, 'learning_rate': 0.001997233139892941, 'epoch': 0.3768888888888889}
Step 54: {'loss': 0.0013, 'grad_norm': 0.0002288818359375, 'learning_rate': 0.00199658449300667, 'epoch': 0.384}
Step 55: {'loss': 0.0012, 'grad_norm': 0.0001983642578125, 'learning_rate': 0.0019958677308941136, 'epoch': 0.39111111111111113}
Step 56: {'loss': 0.0013, 'grad_norm': 0.000240325927734375, 'learning_rate': 0.0019950829025450114, 'epoch': 0.3982222222222222}
Step 57: {'loss': 0.0013, 'grad_norm': 0.0002269744873046875, 'learning_rate': 0.001994230061601338, 'epoch': 0.4053333333333333}
Step 58: {'loss': 0.0012, 'grad_norm': 0.00020503997802734375, 'learning_rate': 0.0019933092663536383, 'epoch': 0.41244444444444445}
Step 59: {'loss': 0.0012, 'grad_norm': 0.00019931793212890625, 'learning_rate': 0.001992320579737045, 'epoch': 0.41955555555555557}
Step 60: {'loss': 0.0014, 'grad_norm': 0.00022125244140625, 'learning_rate': 0.001991264069326975, 'epoch': 0.4266666666666667}
Step 61: {'loss': 0.0012, 'grad_norm': 0.00020599365234375, 'learning_rate': 0.001990139807334512, 'epoch': 0.43377777777777776}
Step 62: {'loss': 0.0011, 'grad_norm': 0.0001735687255859375, 'learning_rate': 0.0019889478706014685, 'epoch': 0.4408888888888889}
Step 63: {'loss': 0.0012, 'grad_norm': 0.00018405914306640625, 'learning_rate': 0.0019876883405951376, 'epoch': 0.448}
Step 64: {'loss': 0.0013, 'grad_norm': 0.00018596649169921875, 'learning_rate': 0.0019863613034027225, 'epoch': 0.45511111111111113}
Step 65: {'loss': 0.0012, 'grad_norm': 0.00023365020751953125, 'learning_rate': 0.001984966849725452, 'epoch': 0.4622222222222222}
Step 66: {'loss': 0.0012, 'grad_norm': 0.000217437744140625, 'learning_rate': 0.0019835050748723826, 'epoch': 0.4693333333333333}
Step 67: {'loss': 0.0012, 'grad_norm': 0.00019168853759765625, 'learning_rate': 0.001981976078753884, 'epoch': 0.47644444444444445}
Step 68: {'loss': 0.0012, 'grad_norm': 0.000194549560546875, 'learning_rate': 0.0019803799658748094, 'epoch': 0.48355555555555557}
Step 69: {'loss': 0.0013, 'grad_norm': 0.00018215179443359375, 'learning_rate': 0.0019787168453273547, 'epoch': 0.49066666666666664}
Step 70: {'loss': 0.0011, 'grad_norm': 0.00018215179443359375, 'learning_rate': 0.0019769868307835995, 'epoch': 0.49777777777777776}
Step 71: {'loss': 0.0012, 'grad_norm': 0.0002002716064453125, 'learning_rate': 0.00197519004048774, 'epoch': 0.5048888888888889}
Step 72: {'loss': 0.0011, 'grad_norm': 0.00017833709716796875, 'learning_rate': 0.001973326597248006, 'epoch': 0.512}
Step 73: {'loss': 0.001, 'grad_norm': 0.0001583099365234375, 'learning_rate': 0.0019713966284282676, 'epoch': 0.5191111111111111}
Step 74: {'loss': 0.0011, 'grad_norm': 0.00021839141845703125, 'learning_rate': 0.0019694002659393305, 'epoch': 0.5262222222222223}
Step 75: {'loss': 0.0011, 'grad_norm': 0.0001697540283203125, 'learning_rate': 0.0019673376462299185, 'epoch': 0.5333333333333333}
Step 76: {'loss': 0.0011, 'grad_norm': 0.000171661376953125, 'learning_rate': 0.0019652089102773487, 'epoch': 0.5404444444444444}
Step 77: {'loss': 0.0011, 'grad_norm': 0.00016880035400390625, 'learning_rate': 0.001963014203577896, 'epoch': 0.5475555555555556}
Step 78: {'loss': 0.001, 'grad_norm': 0.00015735626220703125, 'learning_rate': 0.0019607536761368483, 'epoch': 0.5546666666666666}
Step 79: {'loss': 0.001, 'grad_norm': 0.00015544891357421875, 'learning_rate': 0.001958427482458253, 'epoch': 0.5617777777777778}
Step 80: {'loss': 0.0011, 'grad_norm': 0.00016498565673828125, 'learning_rate': 0.0019560357815343577, 'epoch': 0.5688888888888889}
Step 81: {'loss': 0.001, 'grad_norm': 0.0001678466796875, 'learning_rate': 0.0019535787368347443, 'epoch': 0.576}
Step 82: {'loss': 0.0012, 'grad_norm': 0.00019073486328125, 'learning_rate': 0.0019510565162951536, 'epoch': 0.5831111111111111}
Step 83: {'loss': 0.0011, 'grad_norm': 0.000152587890625, 'learning_rate': 0.0019484692923060095, 'epoch': 0.5902222222222222}
Step 84: {'loss': 0.001, 'grad_norm': 0.0001583099365234375, 'learning_rate': 0.0019458172417006346, 'epoch': 0.5973333333333334}
Step 85: {'loss': 0.001, 'grad_norm': 0.00013637542724609375, 'learning_rate': 0.0019431005457431652, 'epoch': 0.6044444444444445}
Step 86: {'loss': 0.0011, 'grad_norm': 0.00016021728515625, 'learning_rate': 0.0019403193901161614, 'epoch': 0.6115555555555555}
Step 87: {'loss': 0.0011, 'grad_norm': 0.00022983551025390625, 'learning_rate': 0.0019374739649079154, 'epoch': 0.6186666666666667}
Step 88: {'loss': 0.0012, 'grad_norm': 0.000164031982421875, 'learning_rate': 0.0019345644645994608, 'epoch': 0.6257777777777778}
Step 89: {'loss': 0.001, 'grad_norm': 0.00014209747314453125, 'learning_rate': 0.001931591088051279, 'epoch': 0.6328888888888888}
Step 90: {'loss': 0.0011, 'grad_norm': 0.000152587890625, 'learning_rate': 0.0019285540384897072, 'epoch': 0.64}
Step 91: {'loss': 0.001, 'grad_norm': 0.00014591217041015625, 'learning_rate': 0.0019254535234930485, 'epoch': 0.6471111111111111}
Step 92: {'loss': 0.001, 'grad_norm': 0.000171661376953125, 'learning_rate': 0.0019222897549773848, 'epoch': 0.6542222222222223}
Step 93: {'loss': 0.001, 'grad_norm': 0.0001468658447265625, 'learning_rate': 0.001919062949182091, 'epoch': 0.6613333333333333}
Step 94: {'loss': 0.0011, 'grad_norm': 0.00014209747314453125, 'learning_rate': 0.0019157733266550573, 'epoch': 0.6684444444444444}
Step 95: {'loss': 0.001, 'grad_norm': 0.0001468658447265625, 'learning_rate': 0.0019124211122376136, 'epoch': 0.6755555555555556}
Step 96: {'loss': 0.001, 'grad_norm': 0.00013446807861328125, 'learning_rate': 0.0019090065350491625, 'epoch': 0.6826666666666666}
Step 97: {'loss': 0.001, 'grad_norm': 0.0001373291015625, 'learning_rate': 0.001905529828471519, 'epoch': 0.6897777777777778}
Step 98: {'loss': 0.0011, 'grad_norm': 0.00014019012451171875, 'learning_rate': 0.0019019912301329591, 'epoch': 0.6968888888888889}
Step 99: {'loss': 0.001, 'grad_norm': 0.000141143798828125, 'learning_rate': 0.001898390981891979, 'epoch': 0.704}
Step 100: {'loss': 0.0009, 'grad_norm': 0.0001239776611328125, 'learning_rate': 0.0018947293298207635, 'epoch': 0.7111111111111111}
Step 101: {'loss': 0.0012, 'grad_norm': 0.000156402587890625, 'learning_rate': 0.0018910065241883678, 'epoch': 0.7182222222222222}
Step 102: {'loss': 0.0009, 'grad_norm': 0.000133514404296875, 'learning_rate': 0.001887222819443612, 'epoch': 0.7253333333333334}
Step 103: {'loss': 0.0009, 'grad_norm': 0.000141143798828125, 'learning_rate': 0.0018833784741976887, 'epoch': 0.7324444444444445}
Step 104: {'loss': 0.0009, 'grad_norm': 0.00011444091796875, 'learning_rate': 0.001879473751206489, 'epoch': 0.7395555555555555}
Step 105: {'loss': 0.0009, 'grad_norm': 0.00011444091796875, 'learning_rate': 0.001875508917352643, 'epoch': 0.7466666666666667}
Step 106: {'loss': 0.001, 'grad_norm': 0.00014019012451171875, 'learning_rate': 0.0018714842436272773, 'epoch': 0.7537777777777778}
Step 107: {'loss': 0.0008, 'grad_norm': 0.00010776519775390625, 'learning_rate': 0.0018674000051114952, 'epoch': 0.7608888888888888}
Step 108: {'loss': 0.0009, 'grad_norm': 0.0001125335693359375, 'learning_rate': 0.001863256480957574, 'epoch': 0.768}
Step 109: {'loss': 0.0009, 'grad_norm': 0.00011968612670898438, 'learning_rate': 0.0018590539543698853, 'epoch': 0.7751111111111111}
Step 110: {'loss': 0.001, 'grad_norm': 0.00015163421630859375, 'learning_rate': 0.001854792712585539, 'epoch': 0.7822222222222223}
Step 111: {'loss': 0.0009, 'grad_norm': 0.00010633468627929688, 'learning_rate': 0.0018504730468547508, 'epoch': 0.7893333333333333}
Step 112: {'loss': 0.0008, 'grad_norm': 0.00016117095947265625, 'learning_rate': 0.0018460952524209354, 'epoch': 0.7964444444444444}
Step 113: {'loss': 0.0009, 'grad_norm': 0.00010967254638671875, 'learning_rate': 0.001841659628500527, 'epoch': 0.8035555555555556}
Step 114: {'loss': 0.001, 'grad_norm': 0.00012683868408203125, 'learning_rate': 0.0018371664782625286, 'epoch': 0.8106666666666666}
Step 115: {'loss': 0.001, 'grad_norm': 0.00011396408081054688, 'learning_rate': 0.0018326161088077905, 'epoch': 0.8177777777777778}
Step 116: {'loss': 0.001, 'grad_norm': 0.00012063980102539062, 'learning_rate': 0.00182800883114802, 'epoch': 0.8248888888888889}
Step 117: {'loss': 0.0009, 'grad_norm': 0.00010824203491210938, 'learning_rate': 0.0018233449601845258, 'epoch': 0.832}
Step 118: {'loss': 0.0009, 'grad_norm': 0.0001277923583984375, 'learning_rate': 0.0018186248146866927, 'epoch': 0.8391111111111111}
Step 119: {'loss': 0.001, 'grad_norm': 0.00013065338134765625, 'learning_rate': 0.001813848717270195, 'epoch': 0.8462222222222222}
Step 120: {'loss': 0.001, 'grad_norm': 0.00012302398681640625, 'learning_rate': 0.0018090169943749475, 'epoch': 0.8533333333333334}
Step 121: {'loss': 0.0009, 'grad_norm': 0.0001659393310546875, 'learning_rate': 0.0018041299762427917, 'epoch': 0.8604444444444445}
Step 122: {'loss': 0.001, 'grad_norm': 0.00011444091796875, 'learning_rate': 0.0017991879968949247, 'epoch': 0.8675555555555555}
Step 123: {'loss': 0.0009, 'grad_norm': 0.00010442733764648438, 'learning_rate': 0.001794191394109071, 'epoch': 0.8746666666666667}
Step 124: {'loss': 0.0008, 'grad_norm': 9.679794311523438e-05, 'learning_rate': 0.0017891405093963938, 'epoch': 0.8817777777777778}
Step 125: {'loss': 0.0009, 'grad_norm': 0.00012063980102539062, 'learning_rate': 0.001784035687978153, 'epoch': 0.8888888888888888}
Step 126: {'loss': 0.0009, 'grad_norm': 0.00012159347534179688, 'learning_rate': 0.0017788772787621127, 'epoch': 0.896}
Step 127: {'loss': 0.0009, 'grad_norm': 0.00010728836059570312, 'learning_rate': 0.0017736656343186894, 'epoch': 0.9031111111111111}
Step 128: {'loss': 0.0009, 'grad_norm': 9.584426879882812e-05, 'learning_rate': 0.0017684011108568592, 'epoch': 0.9102222222222223}
Step 129: {'loss': 0.0009, 'grad_norm': 0.00010585784912109375, 'learning_rate': 0.0017630840681998065, 'epoch': 0.9173333333333333}
Step 130: {'loss': 0.0009, 'grad_norm': 9.441375732421875e-05, 'learning_rate': 0.001757714869760335, 'epoch': 0.9244444444444444}
Step 131: {'loss': 0.0008, 'grad_norm': 9.632110595703125e-05, 'learning_rate': 0.001752293882516025, 'epoch': 0.9315555555555556}
Step 132: {'loss': 0.0009, 'grad_norm': 0.000102996826171875, 'learning_rate': 0.001746821476984154, 'epoch': 0.9386666666666666}
Step 133: {'loss': 0.0009, 'grad_norm': 0.0001087188720703125, 'learning_rate': 0.001741298027196371, 'epoch': 0.9457777777777778}
Step 134: {'loss': 0.0009, 'grad_norm': 0.0001087188720703125, 'learning_rate': 0.0017357239106731317, 'epoch': 0.9528888888888889}
Step 135: {'loss': 0.001, 'grad_norm': 0.00010442733764648438, 'learning_rate': 0.0017300995083978964, 'epoch': 0.96}
Step 136: {'loss': 0.0008, 'grad_norm': 9.202957153320312e-05, 'learning_rate': 0.0017244252047910892, 'epoch': 0.9671111111111111}
Step 137: {'loss': 0.0008, 'grad_norm': 8.96453857421875e-05, 'learning_rate': 0.001718701387683824, 'epoch': 0.9742222222222222}
Step 138: {'loss': 0.0008, 'grad_norm': 0.00011444091796875, 'learning_rate': 0.0017129284482913971, 'epoch': 0.9813333333333333}
Step 139: {'loss': 0.0009, 'grad_norm': 9.632110595703125e-05, 'learning_rate': 0.0017071067811865474, 'epoch': 0.9884444444444445}
Step 140: {'loss': 0.0008, 'grad_norm': 0.00010538101196289062, 'learning_rate': 0.0017012367842724886, 'epoch': 0.9955555555555555}
Step 141: {'loss': 0.0005, 'grad_norm': 5.745887756347656e-05, 'learning_rate': 0.0016953188587557123, 'epoch': 1.0}
Step 142: {'loss': 0.0009, 'grad_norm': 9.1552734375e-05, 'learning_rate': 0.0016893534091185658, 'epoch': 1.007111111111111}
Step 143: {'loss': 0.0008, 'grad_norm': 8.58306884765625e-05, 'learning_rate': 0.0016833408430916084, 'epoch': 1.0142222222222221}
Step 144: {'loss': 0.0009, 'grad_norm': 9.489059448242188e-05, 'learning_rate': 0.0016772815716257412, 'epoch': 1.0213333333333334}
Step 145: {'loss': 0.0009, 'grad_norm': 9.202957153320312e-05, 'learning_rate': 0.0016711760088641196, 'epoch': 1.0284444444444445}
Step 146: {'loss': 0.0009, 'grad_norm': 9.489059448242188e-05, 'learning_rate': 0.0016650245721138482, 'epoch': 1.0355555555555556}
Step 147: {'loss': 0.0009, 'grad_norm': 9.298324584960938e-05, 'learning_rate': 0.0016588276818174578, 'epoch': 1.0426666666666666}
Step 148: {'loss': 0.0009, 'grad_norm': 8.392333984375e-05, 'learning_rate': 0.0016525857615241686, 'epoch': 1.0497777777777777}
Step 149: {'loss': 0.0009, 'grad_norm': 9.870529174804688e-05, 'learning_rate': 0.0016462992378609407, 'epoch': 1.056888888888889}
Step 150: {'loss': 0.0008, 'grad_norm': 9.107589721679688e-05, 'learning_rate': 0.0016399685405033167, 'epoch': 1.064}
Step 151: {'loss': 0.001, 'grad_norm': 8.7738037109375e-05, 'learning_rate': 0.0016335941021460504, 'epoch': 1.0711111111111111}
Step 152: {'loss': 0.0009, 'grad_norm': 0.00017261505126953125, 'learning_rate': 0.001627176358473537, 'epoch': 1.0782222222222222}
Step 153: {'loss': 0.001, 'grad_norm': 0.00014400482177734375, 'learning_rate': 0.0016207157481300312, 'epoch': 1.0853333333333333}
Step 154: {'loss': 0.0008, 'grad_norm': 8.106231689453125e-05, 'learning_rate': 0.001614212712689668, 'epoch': 1.0924444444444443}
Step 155: {'loss': 0.0009, 'grad_norm': 9.632110595703125e-05, 'learning_rate': 0.0016076676966262813, 'epoch': 1.0995555555555556}
Step 156: {'loss': 0.0009, 'grad_norm': 9.918212890625e-05, 'learning_rate': 0.001601081147283025, 'epoch': 1.1066666666666667}
Step 157: {'loss': 0.0008, 'grad_norm': 8.487701416015625e-05, 'learning_rate': 0.0015944535148417981, 'epoch': 1.1137777777777778}
Step 158: {'loss': 0.0008, 'grad_norm': 8.7738037109375e-05, 'learning_rate': 0.0015877852522924731, 'epoch': 1.1208888888888888}
Step 159: {'loss': 0.0008, 'grad_norm': 9.393692016601562e-05, 'learning_rate': 0.0015810768154019383, 'epoch': 1.1280000000000001}
Step 160: {'loss': 0.0008, 'grad_norm': 9.34600830078125e-05, 'learning_rate': 0.0015743286626829435, 'epoch': 1.1351111111111112}
Step 161: {'loss': 0.0008, 'grad_norm': 8.249282836914062e-05, 'learning_rate': 0.0015675412553627637, 'epoch': 1.1422222222222222}
Step 162: {'loss': 0.0008, 'grad_norm': 9.584426879882812e-05, 'learning_rate': 0.001560715057351673, 'epoch': 1.1493333333333333}
Step 163: {'loss': 0.0008, 'grad_norm': 8.0108642578125e-05, 'learning_rate': 0.0015538505352112374, 'epoch': 1.1564444444444444}
Step 164: {'loss': 0.0007, 'grad_norm': 9.72747802734375e-05, 'learning_rate': 0.001546948158122427, 'epoch': 1.1635555555555555}
Step 165: {'loss': 0.0009, 'grad_norm': 9.72747802734375e-05, 'learning_rate': 0.0015400083978535472, 'epoch': 1.1706666666666667}
Step 166: {'loss': 0.0009, 'grad_norm': 8.58306884765625e-05, 'learning_rate': 0.0015330317287279938, 'epoch': 1.1777777777777778}
Step 167: {'loss': 0.0007, 'grad_norm': 8.7738037109375e-05, 'learning_rate': 0.0015260186275918343, 'epoch': 1.1848888888888889}
Step 168: {'loss': 0.0009, 'grad_norm': 9.393692016601562e-05, 'learning_rate': 0.0015189695737812153, 'epoch': 1.192}
Step 169: {'loss': 0.0009, 'grad_norm': 9.822845458984375e-05, 'learning_rate': 0.001511885049089601, 'epoch': 1.199111111111111}
Step 170: {'loss': 0.0008, 'grad_norm': 7.867813110351562e-05, 'learning_rate': 0.001504765537734844, 'epoch': 1.2062222222222223}
Step 171: {'loss': 0.0009, 'grad_norm': 8.678436279296875e-05, 'learning_rate': 0.0014976115263260874, 'epoch': 1.2133333333333334}
Step 172: {'loss': 0.0007, 'grad_norm': 7.867813110351562e-05, 'learning_rate': 0.0014904235038305082, 'epoch': 1.2204444444444444}
Step 173: {'loss': 0.0008, 'grad_norm': 0.00010013580322265625, 'learning_rate': 0.0014832019615398963, 'epoch': 1.2275555555555555}
Step 174: {'loss': 0.0009, 'grad_norm': 9.1552734375e-05, 'learning_rate': 0.0014759473930370736, 'epoch': 1.2346666666666666}
Step 175: {'loss': 0.0008, 'grad_norm': 8.821487426757812e-05, 'learning_rate': 0.0014686602941621615, 'epoch': 1.2417777777777779}
Step 176: {'loss': 0.0009, 'grad_norm': 9.250640869140625e-05, 'learning_rate': 0.001461341162978688, 'epoch': 1.248888888888889}
Step 177: {'loss': 0.0008, 'grad_norm': 8.630752563476562e-05, 'learning_rate': 0.0014539904997395467, 'epoch': 1.256}
Step 178: {'loss': 0.001, 'grad_norm': 0.00011777877807617188, 'learning_rate': 0.0014466088068528066, 'epoch': 1.263111111111111}
Step 179: {'loss': 0.0008, 'grad_norm': 8.153915405273438e-05, 'learning_rate': 0.0014391965888473704, 'epoch': 1.2702222222222221}
Step 180: {'loss': 0.0008, 'grad_norm': 8.96453857421875e-05, 'learning_rate': 0.0014317543523384927, 'epoch': 1.2773333333333334}
Step 181: {'loss': 0.0009, 'grad_norm': 8.58306884765625e-05, 'learning_rate': 0.0014242826059931537, 'epoch': 1.2844444444444445}
Step 182: {'loss': 0.0008, 'grad_norm': 7.82012939453125e-05, 'learning_rate': 0.0014167818604952905, 'epoch': 1.2915555555555556}
Step 183: {'loss': 0.0008, 'grad_norm': 8.296966552734375e-05, 'learning_rate': 0.0014092526285108939, 'epoch': 1.2986666666666666}
Step 184: {'loss': 0.0008, 'grad_norm': 8.7738037109375e-05, 'learning_rate': 0.0014016954246529696, 'epoch': 1.3057777777777777}
Step 185: {'loss': 0.0008, 'grad_norm': 7.295608520507812e-05, 'learning_rate': 0.001394110765446362, 'epoch': 1.3128888888888888}
Step 186: {'loss': 0.0009, 'grad_norm': 0.000102996826171875, 'learning_rate': 0.0013864991692924523, 'epoch': 1.32}
Step 187: {'loss': 0.0007, 'grad_norm': 7.152557373046875e-05, 'learning_rate': 0.0013788611564337276, 'epoch': 1.3271111111111111}
Step 188: {'loss': 0.0008, 'grad_norm': 8.392333984375e-05, 'learning_rate': 0.0013711972489182207, 'epoch': 1.3342222222222222}
Step 189: {'loss': 0.0009, 'grad_norm': 8.344650268554688e-05, 'learning_rate': 0.0013635079705638297, 'epoch': 1.3413333333333333}
Step 190: {'loss': 0.0008, 'grad_norm': 8.153915405273438e-05, 'learning_rate': 0.0013557938469225166, 'epoch': 1.3484444444444446}
Step 191: {'loss': 0.0008, 'grad_norm': 7.62939453125e-05, 'learning_rate': 0.0013480554052443846, 'epoch': 1.3555555555555556}
Step 192: {'loss': 0.0008, 'grad_norm': 9.489059448242188e-05, 'learning_rate': 0.0013402931744416432, 'epoch': 1.3626666666666667}
Step 193: {'loss': 0.0007, 'grad_norm': 0.000133514404296875, 'learning_rate': 0.001332507685052457, 'epoch': 1.3697777777777778}
Step 194: {'loss': 0.0009, 'grad_norm': 9.298324584960938e-05, 'learning_rate': 0.0013246994692046837, 'epoch': 1.3768888888888888}
Step 195: {'loss': 0.0008, 'grad_norm': 8.058547973632812e-05, 'learning_rate': 0.0013168690605795043, 'epoch': 1.384}
Step 196: {'loss': 0.001, 'grad_norm': 9.250640869140625e-05, 'learning_rate': 0.0013090169943749475, 'epoch': 1.3911111111111112}
Step 197: {'loss': 0.0008, 'grad_norm': 8.821487426757812e-05, 'learning_rate': 0.0013011438072693077, 'epoch': 1.3982222222222223}
Step 198: {'loss': 0.0008, 'grad_norm': 7.62939453125e-05, 'learning_rate': 0.0012932500373844648, 'epoch': 1.4053333333333333}
Step 199: {'loss': 0.0009, 'grad_norm': 7.534027099609375e-05, 'learning_rate': 0.0012853362242491054, 'epoch': 1.4124444444444444}
Step 200: {'loss': 0.0007, 'grad_norm': 9.202957153320312e-05, 'learning_rate': 0.0012774029087618446, 'epoch': 1.4195555555555557}
Step 201: {'loss': 0.0008, 'grad_norm': 7.390975952148438e-05, 'learning_rate': 0.0012694506331542578, 'epoch': 1.4266666666666667}
Step 202: {'loss': 0.0008, 'grad_norm': 8.249282836914062e-05, 'learning_rate': 0.0012614799409538199, 'epoch': 1.4337777777777778}
Step 203: {'loss': 0.0008, 'grad_norm': 7.581710815429688e-05, 'learning_rate': 0.001253491376946754, 'epoch': 1.4408888888888889}
Step 204: {'loss': 0.0007, 'grad_norm': 6.389617919921875e-05, 'learning_rate': 0.0012454854871407992, 'epoch': 1.448}
Step 205: {'loss': 0.0008, 'grad_norm': 8.487701416015625e-05, 'learning_rate': 0.0012374628187278886, 'epoch': 1.455111111111111}
Step 206: {'loss': 0.0008, 'grad_norm': 0.00010204315185546875, 'learning_rate': 0.0012294239200467516, 'epoch': 1.462222222222222}
Step 207: {'loss': 0.0009, 'grad_norm': 8.106231689453125e-05, 'learning_rate': 0.0012213693405454344, 'epoch': 1.4693333333333334}
Step 208: {'loss': 0.0009, 'grad_norm': 9.822845458984375e-05, 'learning_rate': 0.0012132996307437469, 'epoch': 1.4764444444444444}
Step 209: {'loss': 0.0007, 'grad_norm': 8.58306884765625e-05, 'learning_rate': 0.0012052153421956342, 'epoch': 1.4835555555555555}
Step 210: {'loss': 0.0008, 'grad_norm': 7.772445678710938e-05, 'learning_rate': 0.0011971170274514803, 'epoch': 1.4906666666666666}
Step 211: {'loss': 0.0008, 'grad_norm': 8.106231689453125e-05, 'learning_rate': 0.0011890052400203403, 'epoch': 1.4977777777777779}
Step 212: {'loss': 0.0008, 'grad_norm': 7.724761962890625e-05, 'learning_rate': 0.00118088053433211, 'epoch': 1.504888888888889}
Step 213: {'loss': 0.0007, 'grad_norm': 6.67572021484375e-05, 'learning_rate': 0.0011727434656996305, 'epoch': 1.512}
Step 214: {'loss': 0.0009, 'grad_norm': 9.34600830078125e-05, 'learning_rate': 0.001164594590280734, 'epoch': 1.519111111111111}
Step 215: {'loss': 0.0008, 'grad_norm': 0.00013828277587890625, 'learning_rate': 0.001156434465040231, 'epoch': 1.5262222222222221}
Step 216: {'loss': 0.001, 'grad_norm': 0.00011491775512695312, 'learning_rate': 0.001148263647711842, 'epoch': 1.5333333333333332}
Step 217: {'loss': 0.0007, 'grad_norm': 6.866455078125e-05, 'learning_rate': 0.001140082696760078, 'epoch': 1.5404444444444443}
Step 218: {'loss': 0.0008, 'grad_norm': 9.822845458984375e-05, 'learning_rate': 0.001131892171342069, 'epoch': 1.5475555555555556}
Step 219: {'loss': 0.0008, 'grad_norm': 7.43865966796875e-05, 'learning_rate': 0.001123692631269348, 'epoch': 1.5546666666666666}
Step 220: {'loss': 0.0007, 'grad_norm': 6.818771362304688e-05, 'learning_rate': 0.0011154846369695864, 'epoch': 1.561777777777778}
Step 221: {'loss': 0.0008, 'grad_norm': 8.058547973632812e-05, 'learning_rate': 0.0011072687494482918, 'epoch': 1.568888888888889}
Step 222: {'loss': 0.0009, 'grad_norm': 8.440017700195312e-05, 'learning_rate': 0.001099045530250463, 'epoch': 1.576}
Step 223: {'loss': 0.0007, 'grad_norm': 7.009506225585938e-05, 'learning_rate': 0.0010908155414222083, 'epoch': 1.5831111111111111}
Step 224: {'loss': 0.0007, 'grad_norm': 7.486343383789062e-05, 'learning_rate': 0.0010825793454723326, 'epoch': 1.5902222222222222}
Step 225: {'loss': 0.0009, 'grad_norm': 9.584426879882812e-05, 'learning_rate': 0.0010743375053338877, 'epoch': 1.5973333333333333}
Step 226: {'loss': 0.0008, 'grad_norm': 0.00010824203491210938, 'learning_rate': 0.0010660905843256994, 'epoch': 1.6044444444444443}
Step 227: {'loss': 0.0008, 'grad_norm': 6.580352783203125e-05, 'learning_rate': 0.0010578391461138642, 'epoch': 1.6115555555555554}
Step 228: {'loss': 0.0008, 'grad_norm': 7.534027099609375e-05, 'learning_rate': 0.0010495837546732223, 'epoch': 1.6186666666666667}
Step 229: {'loss': 0.0006, 'grad_norm': 6.771087646484375e-05, 'learning_rate': 0.001041324974248813, 'epoch': 1.6257777777777778}
Step 230: {'loss': 0.0009, 'grad_norm': 9.441375732421875e-05, 'learning_rate': 0.0010330633693173082, 'epoch': 1.6328888888888888}
Step 231: {'loss': 0.0008, 'grad_norm': 6.67572021484375e-05, 'learning_rate': 0.0010247995045484302, 'epoch': 1.6400000000000001}
Step 232: {'loss': 0.0008, 'grad_norm': 7.867813110351562e-05, 'learning_rate': 0.0010165339447663587, 'epoch': 1.6471111111111112}
Step 233: {'loss': 0.0009, 'grad_norm': 7.867813110351562e-05, 'learning_rate': 0.001008267254911125, 'epoch': 1.6542222222222223}
Step 234: {'loss': 0.0007, 'grad_norm': 0.00011205673217773438, 'learning_rate': 0.001, 'epoch': 1.6613333333333333}
Step 235: {'loss': 0.0007, 'grad_norm': 6.341934204101562e-05, 'learning_rate': 0.000991732745088875, 'epoch': 1.6684444444444444}
Step 236: {'loss': 0.0009, 'grad_norm': 8.058547973632812e-05, 'learning_rate': 0.0009834660552336416, 'epoch': 1.6755555555555555}
Step 237: {'loss': 0.0008, 'grad_norm': 8.296966552734375e-05, 'learning_rate': 0.0009752004954515699, 'epoch': 1.6826666666666665}
Step 238: {'loss': 0.0007, 'grad_norm': 7.200241088867188e-05, 'learning_rate': 0.0009669366306826918, 'epoch': 1.6897777777777778}
Step 239: {'loss': 0.0007, 'grad_norm': 6.532669067382812e-05, 'learning_rate': 0.0009586750257511868, 'epoch': 1.696888888888889}
Step 240: {'loss': 0.0008, 'grad_norm': 7.200241088867188e-05, 'learning_rate': 0.0009504162453267776, 'epoch': 1.704}
Step 241: {'loss': 0.0009, 'grad_norm': 8.058547973632812e-05, 'learning_rate': 0.0009421608538861361, 'epoch': 1.7111111111111112}
Step 242: {'loss': 0.0008, 'grad_norm': 0.0001010894775390625, 'learning_rate': 0.0009339094156743007, 'epoch': 1.7182222222222223}
Step 243: {'loss': 0.0008, 'grad_norm': 8.0108642578125e-05, 'learning_rate': 0.0009256624946661125, 'epoch': 1.7253333333333334}
Step 244: {'loss': 0.0007, 'grad_norm': 7.200241088867188e-05, 'learning_rate': 0.0009174206545276678, 'epoch': 1.7324444444444445}
Step 245: {'loss': 0.0008, 'grad_norm': 7.867813110351562e-05, 'learning_rate': 0.0009091844585777918, 'epoch': 1.7395555555555555}
Step 246: {'loss': 0.0008, 'grad_norm': 0.000217437744140625, 'learning_rate': 0.0009009544697495374, 'epoch': 1.7466666666666666}
Step 247: {'loss': 0.0008, 'grad_norm': 7.104873657226562e-05, 'learning_rate': 0.0008927312505517084, 'epoch': 1.7537777777777777}
Step 248: {'loss': 0.0008, 'grad_norm': 7.104873657226562e-05, 'learning_rate': 0.0008845153630304139, 'epoch': 1.7608888888888887}
Step 249: {'loss': 0.0008, 'grad_norm': 7.963180541992188e-05, 'learning_rate': 0.0008763073687306523, 'epoch': 1.768}
Step 250: {'loss': 0.0007, 'grad_norm': 8.344650268554688e-05, 'learning_rate': 0.000868107828657931, 'epoch': 1.775111111111111}
Step 251: {'loss': 0.0008, 'grad_norm': 9.584426879882812e-05, 'learning_rate': 0.0008599173032399221, 'epoch': 1.7822222222222224}
Step 252: {'loss': 0.0007, 'grad_norm': 6.246566772460938e-05, 'learning_rate': 0.0008517363522881579, 'epoch': 1.7893333333333334}
Step 253: {'loss': 0.0007, 'grad_norm': 7.534027099609375e-05, 'learning_rate': 0.000843565534959769, 'epoch': 1.7964444444444445}
Step 254: {'loss': 0.0009, 'grad_norm': 8.440017700195312e-05, 'learning_rate': 0.0008354054097192659, 'epoch': 1.8035555555555556}
Step 255: {'loss': 0.0007, 'grad_norm': 7.724761962890625e-05, 'learning_rate': 0.00082725653430037, 'epoch': 1.8106666666666666}
Step 256: {'loss': 0.0008, 'grad_norm': 7.581710815429688e-05, 'learning_rate': 0.0008191194656678904, 'epoch': 1.8177777777777777}
Step 257: {'loss': 0.0007, 'grad_norm': 6.914138793945312e-05, 'learning_rate': 0.0008109947599796599, 'epoch': 1.8248888888888888}
Step 258: {'loss': 0.0008, 'grad_norm': 6.771087646484375e-05, 'learning_rate': 0.0008028829725485198, 'epoch': 1.8319999999999999}
Step 259: {'loss': 0.0008, 'grad_norm': 7.581710815429688e-05, 'learning_rate': 0.0007947846578043659, 'epoch': 1.8391111111111111}
Step 260: {'loss': 0.0008, 'grad_norm': 7.43865966796875e-05, 'learning_rate': 0.0007867003692562533, 'epoch': 1.8462222222222222}
Step 261: {'loss': 0.0009, 'grad_norm': 9.775161743164062e-05, 'learning_rate': 0.0007786306594545657, 'epoch': 1.8533333333333335}
Step 262: {'loss': 0.0008, 'grad_norm': 8.916854858398438e-05, 'learning_rate': 0.0007705760799532485, 'epoch': 1.8604444444444446}
Step 263: {'loss': 0.0008, 'grad_norm': 0.0001163482666015625, 'learning_rate': 0.0007625371812721115, 'epoch': 1.8675555555555556}
Step 264: {'loss': 0.0007, 'grad_norm': 8.678436279296875e-05, 'learning_rate': 0.0007545145128592009, 'epoch': 1.8746666666666667}
Step 265: {'loss': 0.0008, 'grad_norm': 9.72747802734375e-05, 'learning_rate': 0.0007465086230532459, 'epoch': 1.8817777777777778}
Step 266: {'loss': 0.0007, 'grad_norm': 6.580352783203125e-05, 'learning_rate': 0.0007385200590461803, 'epoch': 1.8888888888888888}
Step 267: {'loss': 0.0009, 'grad_norm': 9.1552734375e-05, 'learning_rate': 0.000730549366845742, 'epoch': 1.896}
Step 268: {'loss': 0.0008, 'grad_norm': 8.106231689453125e-05, 'learning_rate': 0.0007225970912381557, 'epoch': 1.903111111111111}
Step 269: {'loss': 0.0007, 'grad_norm': 9.918212890625e-05, 'learning_rate': 0.0007146637757508949, 'epoch': 1.9102222222222223}
Step 270: {'loss': 0.0008, 'grad_norm': 7.390975952148438e-05, 'learning_rate': 0.0007067499626155354, 'epoch': 1.9173333333333333}
Step 271: {'loss': 0.0008, 'grad_norm': 8.20159912109375e-05, 'learning_rate': 0.0006988561927306927, 'epoch': 1.9244444444444444}
Step 272: {'loss': 0.0008, 'grad_norm': 7.724761962890625e-05, 'learning_rate': 0.0006909830056250527, 'epoch': 1.9315555555555557}
Step 273: {'loss': 0.0008, 'grad_norm': 8.487701416015625e-05, 'learning_rate': 0.0006831309394204956, 'epoch': 1.9386666666666668}
Step 274: {'loss': 0.0007, 'grad_norm': 6.723403930664062e-05, 'learning_rate': 0.0006753005307953167, 'epoch': 1.9457777777777778}
Step 275: {'loss': 0.0009, 'grad_norm': 0.0002288818359375, 'learning_rate': 0.0006674923149475433, 'epoch': 1.952888888888889}
Step 276: {'loss': 0.0007, 'grad_norm': 6.580352783203125e-05, 'learning_rate': 0.000659706825558357, 'epoch': 1.96}
Step 277: {'loss': 0.0007, 'grad_norm': 6.771087646484375e-05, 'learning_rate': 0.0006519445947556155, 'epoch': 1.967111111111111}
Step 278: {'loss': 0.0008, 'grad_norm': 8.7738037109375e-05, 'learning_rate': 0.0006442061530774834, 'epoch': 1.974222222222222}
Step 279: {'loss': 0.0007, 'grad_norm': 7.963180541992188e-05, 'learning_rate': 0.00063649202943617, 'epoch': 1.9813333333333332}
Step 280: {'loss': 0.0008, 'grad_norm': 8.487701416015625e-05, 'learning_rate': 0.000628802751081779, 'epoch': 1.9884444444444445}
Step 281: {'loss': 0.0008, 'grad_norm': 7.486343383789062e-05, 'learning_rate': 0.0006211388435662722, 'epoch': 1.9955555555555555}
Step 282: {'loss': 0.0005, 'grad_norm': 7.534027099609375e-05, 'learning_rate': 0.000613500830707548, 'epoch': 2.0}
Step 283: {'loss': 0.0007, 'grad_norm': 6.67572021484375e-05, 'learning_rate': 0.0006058892345536386, 'epoch': 2.007111111111111}
Step 284: {'loss': 0.0008, 'grad_norm': 7.677078247070312e-05, 'learning_rate': 0.0005983045753470308, 'epoch': 2.014222222222222}
Step 285: {'loss': 0.0008, 'grad_norm': 8.440017700195312e-05, 'learning_rate': 0.000590747371489106, 'epoch': 2.021333333333333}
Step 286: {'loss': 0.0007, 'grad_norm': 6.723403930664062e-05, 'learning_rate': 0.0005832181395047098, 'epoch': 2.0284444444444443}
Step 287: {'loss': 0.0008, 'grad_norm': 8.392333984375e-05, 'learning_rate': 0.0005757173940068465, 'epoch': 2.0355555555555553}
Step 288: {'loss': 0.0008, 'grad_norm': 8.487701416015625e-05, 'learning_rate': 0.0005682456476615072, 'epoch': 2.042666666666667}
Step 289: {'loss': 0.0009, 'grad_norm': 7.772445678710938e-05, 'learning_rate': 0.0005608034111526298, 'epoch': 2.049777777777778}
Step 290: {'loss': 0.0008, 'grad_norm': 7.867813110351562e-05, 'learning_rate': 0.0005533911931471935, 'epoch': 2.056888888888889}
Step 291: {'loss': 0.0007, 'grad_norm': 6.67572021484375e-05, 'learning_rate': 0.0005460095002604533, 'epoch': 2.064}
Step 292: {'loss': 0.0007, 'grad_norm': 7.677078247070312e-05, 'learning_rate': 0.0005386588370213123, 'epoch': 2.071111111111111}
Step 293: {'loss': 0.0007, 'grad_norm': 6.771087646484375e-05, 'learning_rate': 0.0005313397058378386, 'epoch': 2.078222222222222}
Step 294: {'loss': 0.0008, 'grad_norm': 6.723403930664062e-05, 'learning_rate': 0.0005240526069629264, 'epoch': 2.0853333333333333}
Step 295: {'loss': 0.0008, 'grad_norm': 7.963180541992188e-05, 'learning_rate': 0.0005167980384601041, 'epoch': 2.0924444444444443}
Step 296: {'loss': 0.0009, 'grad_norm': 9.965896606445312e-05, 'learning_rate': 0.0005095764961694922, 'epoch': 2.0995555555555554}
Step 297: {'loss': 0.0008, 'grad_norm': 7.82012939453125e-05, 'learning_rate': 0.0005023884736739131, 'epoch': 2.1066666666666665}
Step 298: {'loss': 0.0008, 'grad_norm': 7.2479248046875e-05, 'learning_rate': 0.0004952344622651566, 'epoch': 2.113777777777778}
Step 299: {'loss': 0.0008, 'grad_norm': 6.723403930664062e-05, 'learning_rate': 0.00048811495091039925, 'epoch': 2.120888888888889}
Step 300: {'loss': 0.0007, 'grad_norm': 8.296966552734375e-05, 'learning_rate': 0.0004810304262187851, 'epoch': 2.128}
Step 301: {'loss': 0.0008, 'grad_norm': 7.343292236328125e-05, 'learning_rate': 0.0004739813724081661, 'epoch': 2.135111111111111}
Step 302: {'loss': 0.0008, 'grad_norm': 8.296966552734375e-05, 'learning_rate': 0.0004669682712720065, 'epoch': 2.1422222222222222}
Step 303: {'loss': 0.0007, 'grad_norm': 6.29425048828125e-05, 'learning_rate': 0.00045999160214645307, 'epoch': 2.1493333333333333}
Step 304: {'loss': 0.0007, 'grad_norm': 7.534027099609375e-05, 'learning_rate': 0.0004530518418775733, 'epoch': 2.1564444444444444}
Step 305: {'loss': 0.0008, 'grad_norm': 7.534027099609375e-05, 'learning_rate': 0.00044614946478876305, 'epoch': 2.1635555555555555}
Step 306: {'loss': 0.0007, 'grad_norm': 9.298324584960938e-05, 'learning_rate': 0.0004392849426483274, 'epoch': 2.1706666666666665}
Step 307: {'loss': 0.0008, 'grad_norm': 6.866455078125e-05, 'learning_rate': 0.00043245874463723646, 'epoch': 2.1777777777777776}
Step 308: {'loss': 0.0007, 'grad_norm': 7.82012939453125e-05, 'learning_rate': 0.0004256713373170564, 'epoch': 2.1848888888888887}
Step 309: {'loss': 0.0007, 'grad_norm': 6.818771362304688e-05, 'learning_rate': 0.0004189231845980618, 'epoch': 2.192}
Step 310: {'loss': 0.0007, 'grad_norm': 7.343292236328125e-05, 'learning_rate': 0.00041221474770752696, 'epoch': 2.1991111111111112}
Step 311: {'loss': 0.0007, 'grad_norm': 6.914138793945312e-05, 'learning_rate': 0.00040554648515820214, 'epoch': 2.2062222222222223}
Step 312: {'loss': 0.0008, 'grad_norm': 6.866455078125e-05, 'learning_rate': 0.00039891885271697494, 'epoch': 2.2133333333333334}
Step 313: {'loss': 0.0008, 'grad_norm': 9.298324584960938e-05, 'learning_rate': 0.00039233230337371884, 'epoch': 2.2204444444444444}
Step 314: {'loss': 0.0008, 'grad_norm': 7.915496826171875e-05, 'learning_rate': 0.00038578728731033217, 'epoch': 2.2275555555555555}
Step 315: {'loss': 0.0009, 'grad_norm': 7.82012939453125e-05, 'learning_rate': 0.00037928425186996886, 'epoch': 2.2346666666666666}
Step 316: {'loss': 0.0009, 'grad_norm': 0.00011157989501953125, 'learning_rate': 0.00037282364152646297, 'epoch': 2.2417777777777776}
Step 317: {'loss': 0.0008, 'grad_norm': 8.487701416015625e-05, 'learning_rate': 0.0003664058978539495, 'epoch': 2.2488888888888887}
Step 318: {'loss': 0.0008, 'grad_norm': 7.43865966796875e-05, 'learning_rate': 0.00036003145949668335, 'epoch': 2.2560000000000002}
Step 319: {'loss': 0.0008, 'grad_norm': 9.298324584960938e-05, 'learning_rate': 0.000353700762139059, 'epoch': 2.2631111111111113}
Step 320: {'loss': 0.0009, 'grad_norm': 8.96453857421875e-05, 'learning_rate': 0.0003474142384758313, 'epoch': 2.2702222222222224}
Step 321: {'loss': 0.0007, 'grad_norm': 8.487701416015625e-05, 'learning_rate': 0.000341172318182542, 'epoch': 2.2773333333333334}
Step 322: {'loss': 0.0008, 'grad_norm': 8.153915405273438e-05, 'learning_rate': 0.00033497542788615164, 'epoch': 2.2844444444444445}
Step 323: {'loss': 0.0008, 'grad_norm': 7.867813110351562e-05, 'learning_rate': 0.0003288239911358807, 'epoch': 2.2915555555555556}
Step 324: {'loss': 0.0007, 'grad_norm': 6.628036499023438e-05, 'learning_rate': 0.00032271842837425913, 'epoch': 2.2986666666666666}
Step 325: {'loss': 0.0008, 'grad_norm': 8.344650268554688e-05, 'learning_rate': 0.00031665915690839167, 'epoch': 2.3057777777777777}
Step 326: {'loss': 0.0007, 'grad_norm': 6.532669067382812e-05, 'learning_rate': 0.0003106465908814342, 'epoch': 2.3128888888888888}
Step 327: {'loss': 0.0008, 'grad_norm': 8.535385131835938e-05, 'learning_rate': 0.000304681141244288, 'epoch': 2.32}
Step 328: {'loss': 0.0007, 'grad_norm': 6.771087646484375e-05, 'learning_rate': 0.00029876321572751144, 'epoch': 2.327111111111111}
Step 329: {'loss': 0.0008, 'grad_norm': 8.535385131835938e-05, 'learning_rate': 0.00029289321881345256, 'epoch': 2.3342222222222224}
Step 330: {'loss': 0.0009, 'grad_norm': 0.00017261505126953125, 'learning_rate': 0.000287071551708603, 'epoch': 2.3413333333333335}
Step 331: {'loss': 0.0007, 'grad_norm': 7.05718994140625e-05, 'learning_rate': 0.00028129861231617615, 'epoch': 2.3484444444444446}
Step 332: {'loss': 0.0008, 'grad_norm': 6.771087646484375e-05, 'learning_rate': 0.00027557479520891104, 'epoch': 2.3555555555555556}
Step 333: {'loss': 0.0009, 'grad_norm': 9.107589721679688e-05, 'learning_rate': 0.00026990049160210386, 'epoch': 2.3626666666666667}
Step 334: {'loss': 0.0007, 'grad_norm': 8.20159912109375e-05, 'learning_rate': 0.0002642760893268684, 'epoch': 2.3697777777777778}
Step 335: {'loss': 0.0007, 'grad_norm': 7.200241088867188e-05, 'learning_rate': 0.00025870197280362917, 'epoch': 2.376888888888889}
Step 336: {'loss': 0.0008, 'grad_norm': 0.00010919570922851562, 'learning_rate': 0.0002531785230158464, 'epoch': 2.384}
Step 337: {'loss': 0.0008, 'grad_norm': 8.249282836914062e-05, 'learning_rate': 0.00024770611748397555, 'epoch': 2.391111111111111}
Step 338: {'loss': 0.0007, 'grad_norm': 0.0001201629638671875, 'learning_rate': 0.00024228513023966548, 'epoch': 2.398222222222222}
Step 339: {'loss': 0.0007, 'grad_norm': 6.389617919921875e-05, 'learning_rate': 0.00023691593180019365, 'epoch': 2.405333333333333}
Step 340: {'loss': 0.0008, 'grad_norm': 7.200241088867188e-05, 'learning_rate': 0.0002315988891431412, 'epoch': 2.4124444444444446}
Step 341: {'loss': 0.0008, 'grad_norm': 7.43865966796875e-05, 'learning_rate': 0.00022633436568131072, 'epoch': 2.4195555555555557}
Step 342: {'loss': 0.0007, 'grad_norm': 0.0002002716064453125, 'learning_rate': 0.0002211227212378877, 'epoch': 2.4266666666666667}
Step 343: {'loss': 0.0008, 'grad_norm': 6.628036499023438e-05, 'learning_rate': 0.00021596431202184707, 'epoch': 2.433777777777778}
Step 344: {'loss': 0.0006, 'grad_norm': 6.031990051269531e-05, 'learning_rate': 0.00021085949060360655, 'epoch': 2.440888888888889}
Step 345: {'loss': 0.0009, 'grad_norm': 8.058547973632812e-05, 'learning_rate': 0.000205808605890929, 'epoch': 2.448}
Step 346: {'loss': 0.0007, 'grad_norm': 7.2479248046875e-05, 'learning_rate': 0.0002008120031050753, 'epoch': 2.455111111111111}
Step 347: {'loss': 0.0008, 'grad_norm': 7.05718994140625e-05, 'learning_rate': 0.00019587002375720864, 'epoch': 2.462222222222222}
Step 348: {'loss': 0.0007, 'grad_norm': 7.152557373046875e-05, 'learning_rate': 0.00019098300562505265, 'epoch': 2.469333333333333}
Step 349: {'loss': 0.0008, 'grad_norm': 6.818771362304688e-05, 'learning_rate': 0.00018615128272980508, 'epoch': 2.4764444444444447}
Step 350: {'loss': 0.0007, 'grad_norm': 7.581710815429688e-05, 'learning_rate': 0.00018137518531330764, 'epoch': 2.4835555555555557}
Step 351: {'loss': 0.0008, 'grad_norm': 7.152557373046875e-05, 'learning_rate': 0.00017665503981547425, 'epoch': 2.490666666666667}
Step 352: {'loss': 0.0008, 'grad_norm': 7.486343383789062e-05, 'learning_rate': 0.00017199116885197995, 'epoch': 2.497777777777778}
Step 353: {'loss': 0.0007, 'grad_norm': 7.152557373046875e-05, 'learning_rate': 0.00016738389119220964, 'epoch': 2.504888888888889}
Step 354: {'loss': 0.0009, 'grad_norm': 8.678436279296875e-05, 'learning_rate': 0.00016283352173747145, 'epoch': 2.512}
Step 355: {'loss': 0.0009, 'grad_norm': 7.43865966796875e-05, 'learning_rate': 0.0001583403714994729, 'epoch': 2.519111111111111}
Step 356: {'loss': 0.0008, 'grad_norm': 7.43865966796875e-05, 'learning_rate': 0.00015390474757906447, 'epoch': 2.526222222222222}
Step 357: {'loss': 0.0007, 'grad_norm': 6.723403930664062e-05, 'learning_rate': 0.00014952695314524912, 'epoch': 2.533333333333333}
Step 358: {'loss': 0.0007, 'grad_norm': 6.914138793945312e-05, 'learning_rate': 0.00014520728741446086, 'epoch': 2.5404444444444443}
Step 359: {'loss': 0.0007, 'grad_norm': 8.96453857421875e-05, 'learning_rate': 0.0001409460456301147, 'epoch': 2.5475555555555554}
Step 360: {'loss': 0.0007, 'grad_norm': 6.198883056640625e-05, 'learning_rate': 0.0001367435190424261, 'epoch': 2.554666666666667}
Step 361: {'loss': 0.0008, 'grad_norm': 9.72747802734375e-05, 'learning_rate': 0.0001325999948885047, 'epoch': 2.561777777777778}
Step 362: {'loss': 0.0008, 'grad_norm': 7.152557373046875e-05, 'learning_rate': 0.00012851575637272262, 'epoch': 2.568888888888889}
Step 363: {'loss': 0.001, 'grad_norm': 9.34600830078125e-05, 'learning_rate': 0.0001244910826473572, 'epoch': 2.576}
Step 364: {'loss': 0.0008, 'grad_norm': 7.343292236328125e-05, 'learning_rate': 0.00012052624879351104, 'epoch': 2.583111111111111}
Step 365: {'loss': 0.0007, 'grad_norm': 7.343292236328125e-05, 'learning_rate': 0.00011662152580231145, 'epoch': 2.590222222222222}
Step 366: {'loss': 0.0007, 'grad_norm': 6.29425048828125e-05, 'learning_rate': 0.0001127771805563882, 'epoch': 2.5973333333333333}
Step 367: {'loss': 0.0008, 'grad_norm': 7.772445678710938e-05, 'learning_rate': 0.00010899347581163222, 'epoch': 2.6044444444444443}
Step 368: {'loss': 0.0008, 'grad_norm': 7.343292236328125e-05, 'learning_rate': 0.00010527067017923653, 'epoch': 2.6115555555555554}
Step 369: {'loss': 0.0008, 'grad_norm': 7.152557373046875e-05, 'learning_rate': 0.00010160901810802114, 'epoch': 2.618666666666667}
Step 370: {'loss': 0.0007, 'grad_norm': 6.961822509765625e-05, 'learning_rate': 9.80087698670411e-05, 'epoch': 2.6257777777777775}
Step 371: {'loss': 0.0007, 'grad_norm': 8.106231689453125e-05, 'learning_rate': 9.447017152848125e-05, 'epoch': 2.632888888888889}
Step 372: {'loss': 0.0007, 'grad_norm': 7.62939453125e-05, 'learning_rate': 9.099346495083749e-05, 'epoch': 2.64}
Step 373: {'loss': 0.0007, 'grad_norm': 6.961822509765625e-05, 'learning_rate': 8.757888776238621e-05, 'epoch': 2.647111111111111}
Step 374: {'loss': 0.0008, 'grad_norm': 8.153915405273438e-05, 'learning_rate': 8.42266733449425e-05, 'epoch': 2.6542222222222223}
Step 375: {'loss': 0.0008, 'grad_norm': 7.963180541992188e-05, 'learning_rate': 8.093705081790892e-05, 'epoch': 2.6613333333333333}
Step 376: {'loss': 0.0008, 'grad_norm': 9.489059448242188e-05, 'learning_rate': 7.771024502261526e-05, 'epoch': 2.6684444444444444}
Step 377: {'loss': 0.0008, 'grad_norm': 8.726119995117188e-05, 'learning_rate': 7.454647650695156e-05, 'epoch': 2.6755555555555555}
Step 378: {'loss': 0.0009, 'grad_norm': 7.534027099609375e-05, 'learning_rate': 7.144596151029303e-05, 'epoch': 2.6826666666666665}
Step 379: {'loss': 0.0008, 'grad_norm': 9.1552734375e-05, 'learning_rate': 6.840891194872111e-05, 'epoch': 2.6897777777777776}
Step 380: {'loss': 0.0007, 'grad_norm': 9.441375732421875e-05, 'learning_rate': 6.543553540053926e-05, 'epoch': 2.696888888888889}
Step 381: {'loss': 0.0008, 'grad_norm': 7.534027099609375e-05, 'learning_rate': 6.252603509208465e-05, 'epoch': 2.7039999999999997}
Step 382: {'loss': 0.0008, 'grad_norm': 8.392333984375e-05, 'learning_rate': 5.968060988383883e-05, 'epoch': 2.7111111111111112}
Step 383: {'loss': 0.0007, 'grad_norm': 9.393692016601562e-05, 'learning_rate': 5.6899454256834735e-05, 'epoch': 2.7182222222222223}
Step 384: {'loss': 0.0009, 'grad_norm': 9.632110595703125e-05, 'learning_rate': 5.418275829936536e-05, 'epoch': 2.7253333333333334}
Step 385: {'loss': 0.0008, 'grad_norm': 6.866455078125e-05, 'learning_rate': 5.15307076939906e-05, 'epoch': 2.7324444444444445}
Step 386: {'loss': 0.0007, 'grad_norm': 6.67572021484375e-05, 'learning_rate': 4.894348370484647e-05, 'epoch': 2.7395555555555555}
Step 387: {'loss': 0.0008, 'grad_norm': 0.000148773193359375, 'learning_rate': 4.642126316525586e-05, 'epoch': 2.7466666666666666}
Step 388: {'loss': 0.0008, 'grad_norm': 8.535385131835938e-05, 'learning_rate': 4.396421846564236e-05, 'epoch': 2.7537777777777777}
Step 389: {'loss': 0.0008, 'grad_norm': 7.200241088867188e-05, 'learning_rate': 4.157251754174729e-05, 'epoch': 2.7608888888888887}
Step 390: {'loss': 0.0008, 'grad_norm': 6.914138793945312e-05, 'learning_rate': 3.9246323863151856e-05, 'epoch': 2.768}
Step 391: {'loss': 0.0009, 'grad_norm': 9.298324584960938e-05, 'learning_rate': 3.698579642210398e-05, 'epoch': 2.7751111111111113}
Step 392: {'loss': 0.0008, 'grad_norm': 8.58306884765625e-05, 'learning_rate': 3.4791089722651434e-05, 'epoch': 2.7822222222222224}
Step 393: {'loss': 0.0008, 'grad_norm': 7.05718994140625e-05, 'learning_rate': 3.266235377008175e-05, 'epoch': 2.7893333333333334}
Step 394: {'loss': 0.0007, 'grad_norm': 7.534027099609375e-05, 'learning_rate': 3.059973406066963e-05, 'epoch': 2.7964444444444445}
Step 395: {'loss': 0.0007, 'grad_norm': 7.2479248046875e-05, 'learning_rate': 2.860337157173243e-05, 'epoch': 2.8035555555555556}
Step 396: {'loss': 0.0008, 'grad_norm': 7.343292236328125e-05, 'learning_rate': 2.6673402751994258e-05, 'epoch': 2.8106666666666666}
Step 397: {'loss': 0.0007, 'grad_norm': 6.723403930664062e-05, 'learning_rate': 2.4809959512260284e-05, 'epoch': 2.8177777777777777}
Step 398: {'loss': 0.0009, 'grad_norm': 9.584426879882812e-05, 'learning_rate': 2.301316921640073e-05, 'epoch': 2.824888888888889}
Step 399: {'loss': 0.0009, 'grad_norm': 0.00011157989501953125, 'learning_rate': 2.128315467264552e-05, 'epoch': 2.832}
Step 400: {'loss': 0.0008, 'grad_norm': 7.05718994140625e-05, 'learning_rate': 1.9620034125190644e-05, 'epoch': 2.8391111111111114}
Step 401: {'loss': 0.0007, 'grad_norm': 7.963180541992188e-05, 'learning_rate': 1.8023921246116403e-05, 'epoch': 2.846222222222222}
Step 402: {'loss': 0.0007, 'grad_norm': 0.0001964569091796875, 'learning_rate': 1.649492512761763e-05, 'epoch': 2.8533333333333335}
Step 403: {'loss': 0.0007, 'grad_norm': 8.678436279296875e-05, 'learning_rate': 1.5033150274548324e-05, 'epoch': 2.8604444444444446}
Step 404: {'loss': 0.0008, 'grad_norm': 7.534027099609375e-05, 'learning_rate': 1.3638696597277678e-05, 'epoch': 2.8675555555555556}
Step 405: {'loss': 0.0009, 'grad_norm': 9.632110595703125e-05, 'learning_rate': 1.231165940486234e-05, 'epoch': 2.8746666666666667}
Step 406: {'loss': 0.0008, 'grad_norm': 7.867813110351562e-05, 'learning_rate': 1.1052129398531507e-05, 'epoch': 2.8817777777777778}
Step 407: {'loss': 0.0008, 'grad_norm': 9.393692016601562e-05, 'learning_rate': 9.86019266548821e-06, 'epoch': 2.888888888888889}
Step 408: {'loss': 0.0008, 'grad_norm': 8.678436279296875e-05, 'learning_rate': 8.735930673024805e-06, 'epoch': 2.896}
Step 409: {'loss': 0.0008, 'grad_norm': 7.43865966796875e-05, 'learning_rate': 7.679420262954984e-06, 'epoch': 2.903111111111111}
Step 410: {'loss': 0.0009, 'grad_norm': 7.724761962890625e-05, 'learning_rate': 6.690733646361857e-06, 'epoch': 2.910222222222222}
Step 411: {'loss': 0.0008, 'grad_norm': 6.771087646484375e-05, 'learning_rate': 5.769938398662355e-06, 'epoch': 2.9173333333333336}
Step 412: {'loss': 0.0008, 'grad_norm': 8.106231689453125e-05, 'learning_rate': 4.917097454988584e-06, 'epoch': 2.924444444444444}
Step 413: {'loss': 0.0007, 'grad_norm': 9.679794311523438e-05, 'learning_rate': 4.132269105886155e-06, 'epoch': 2.9315555555555557}
Step 414: {'loss': 0.0008, 'grad_norm': 7.915496826171875e-05, 'learning_rate': 3.415506993330153e-06, 'epoch': 2.9386666666666668}
Step 415: {'loss': 0.0008, 'grad_norm': 7.724761962890625e-05, 'learning_rate': 2.7668601070588438e-06, 'epoch': 2.945777777777778}
Step 416: {'loss': 0.0008, 'grad_norm': 8.440017700195312e-05, 'learning_rate': 2.186372781225465e-06, 'epoch': 2.952888888888889}
Step 417: {'loss': 0.0007, 'grad_norm': 7.05718994140625e-05, 'learning_rate': 1.674084691367428e-06, 'epoch': 2.96}
Step 418: {'loss': 0.0007, 'grad_norm': 6.437301635742188e-05, 'learning_rate': 1.230030851695263e-06, 'epoch': 2.967111111111111}
Step 419: {'loss': 0.0008, 'grad_norm': 7.152557373046875e-05, 'learning_rate': 8.542416126989805e-07, 'epoch': 2.974222222222222}
Step 420: {'loss': 0.0008, 'grad_norm': 7.534027099609375e-05, 'learning_rate': 5.46742659073951e-07, 'epoch': 2.981333333333333}
Step 421: {'loss': 0.0007, 'grad_norm': 7.486343383789062e-05, 'learning_rate': 3.0755500796531e-07, 'epoch': 2.9884444444444442}
Step 422: {'loss': 0.0008, 'grad_norm': 0.00010251998901367188, 'learning_rate': 1.3669500753099584e-07, 'epoch': 2.9955555555555557}
Step 423: {'loss': 0.0006, 'grad_norm': 5.4836273193359375e-05, 'learning_rate': 3.417433582542096e-08, 'epoch': 3.0}
Step 423: {'train_runtime': 2553.967, 'train_samples_per_second': 10.572, 'train_steps_per_second': 0.166, 'total_flos': 0.0, 'train_loss': 0.0011570642343953464, 'epoch': 3.0}
